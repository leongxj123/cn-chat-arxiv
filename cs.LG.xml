<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2404.01847</link><description>&lt;p&gt;
&#20351;&#29992;2:4&#31232;&#30095;&#24615;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Pre-Training with 2:4 Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#65292;&#32467;&#21512;&#20004;&#31181;&#26032;&#25216;&#26415;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#30740;&#31350;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#35745;&#31639;2:4&#25513;&#30721;&#21644;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#23454;&#29616;&#35757;&#32451;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;Transformer&#24456;&#24930;&#65292;&#20294;&#26368;&#36817;GPU&#26550;&#26500;&#30340;&#21019;&#26032;&#20351;&#25105;&#20204;&#21344;&#25454;&#20248;&#21183;&#12290;NVIDIA&#30340;Ampere GPU&#21487;&#20197;&#27604;&#20854;&#23494;&#38598;&#31561;&#20215;&#29289;&#24555;&#20004;&#20493;&#30340;&#36895;&#24230;&#25191;&#34892;&#32454;&#31890;&#24230;&#30340;2:4&#31232;&#30095;&#30697;&#38453;&#20056;&#27861;&#12290;&#22312;&#27492;&#24615;&#36136;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#21152;&#36895;Transformer&#39044;&#35757;&#32451;&#20013;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#8220;&#32763;&#36716;&#29575;&#8221;&#26469;&#30417;&#35270;2:4&#35757;&#32451;&#36807;&#31243;&#30340;&#31283;&#23450;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#65306;&#36890;&#36807;&#22312;&#26799;&#24230;&#19978;&#24212;&#29992;&#25513;&#30721;&#34928;&#20943;&#39033;&#20462;&#25913;&#31232;&#30095;&#31934;&#21270;&#30340;&#30452;&#36890;&#20272;&#35745;&#22120;&#65292;&#24182;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32467;&#26463;&#26102;&#38468;&#36817;&#24212;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23494;&#38598;&#24494;&#35843;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23454;&#38469;&#21152;&#36895;&#35757;&#32451;&#65306;&#36890;&#36807;&#21367;&#31215;&#35745;&#31639;&#21487;&#36716;&#32622;&#30340;2:4&#25513;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#20943;&#23569;GPU L2&#32531;&#23384;&#26469;&#21152;&#36895;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01847v1 Announce Type: new  Abstract: Training large Transformers is slow, but recent innovations on GPU architecture gives us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of Transformers in pre-training. First, we define a "flip rate" to monitor the stability of a 2:4 training process. Utilizing this metric, we suggest two techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the mask decay term on gradients, and to enhance the model's quality by a simple yet effective dense fine-tuning procedure near the end of pre-training. Besides, we devise two effective techniques to practically accelerate training: to calculate transposable 2:4 mask by convolution, and to accelerate gated activation functions by reducing GPU L2 cach
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.20009</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
On Large Language Models' Hallucination with Regard to Known Facts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20009
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20107;&#23454;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#21644;&#36755;&#20986; token &#27010;&#29575;&#21160;&#24577;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#20107;&#23454;&#31867;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20063;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#29702;&#21160;&#24577;&#30340;&#35282;&#24230;&#30740;&#31350;LLMs&#20855;&#26377;&#27491;&#30830;&#31572;&#26696;&#30693;&#35782;&#21364;&#20173;&#28982;&#20135;&#29983;&#24187;&#35273;&#30340;&#29616;&#35937;&#65292;&#36825;&#26159;&#20197;&#24448;&#20851;&#20110;&#24187;&#35273;&#30740;&#31350;&#23578;&#26410;&#28085;&#30422;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#24605;&#36335;&#36827;&#34892;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26597;&#35810;&#30456;&#21516;&#19977;&#20803;&#30693;&#35782;&#20294;&#23548;&#33268;&#19981;&#21516;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#38382;&#39064;&#12290;&#27169;&#22411;&#22312;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#36755;&#20986;&#19978;&#30340;&#34892;&#20026;&#24046;&#24322;&#22240;&#27492;&#26263;&#31034;&#20102;&#24187;&#35273;&#21457;&#29983;&#30340;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#34913;&#37327;&#36825;&#31181;&#27169;&#24335;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#21097;&#20313;&#27969;&#21040;&#35789;&#27719;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36755;&#20986;&#20196;&#29260;&#27010;&#29575;&#22312;&#27491;&#30830;&#21644;&#24187;&#35273;&#24773;&#20917;&#19979;&#22312;&#23618;&#28145;&#24230;&#19978;&#30340;&#19981;&#21516;&#21160;&#24577;&#12290;&#22312;&#24187;&#35273;&#26696;&#20363;&#20013;&#65292;&#36755;&#20986;&#20196;&#29260;&#30340;&#20449;&#24687;&#24456;&#23569;&#34920;&#29616;&#20986;&#31361;&#22686;&#21644;&#25345;&#32493;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20009v1 Announce Type: new  Abstract: Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen. Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space. We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output token's information rarely demonstrates abrupt increases and consistent
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16576</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Antigen-Specific Antibody Design via Direct Energy-based Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20013;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29702;&#24615;&#32467;&#26500;&#21644;&#33391;&#22909;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#35774;&#35745;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23545;&#21508;&#31181;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#22914;&#27835;&#30103;&#21644;&#29983;&#29289;&#23398;&#65292;&#30001;&#20110;&#20854;&#38169;&#32508;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25239;&#21407;&#29305;&#24322;&#24615;&#25239;&#20307;&#35774;&#35745;&#20316;&#20026;&#19968;&#20010;&#34507;&#30333;&#36136;&#24207;&#21015;-&#32467;&#26500;&#20849;&#35774;&#35745;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#29702;&#24615;&#21644;&#21151;&#33021;&#24615;&#12290;&#21033;&#29992;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#25239;&#20307;&#20013;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30452;&#25509;&#22522;&#20110;&#33021;&#37327;&#20559;&#22909;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24341;&#23548;&#29983;&#25104;&#26082;&#20855;&#26377;&#21512;&#29702;&#32467;&#26500;&#21448;&#20855;&#26377;&#26126;&#26174;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#25239;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27531;&#22522;&#32423;&#20998;&#35299;&#33021;&#37327;&#20559;&#22909;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#26799;&#24230;&#25163;&#26415;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#33021;&#37327;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#20363;&#22914;&#21560;&#24341;&#21644;&#26021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16576v1 Announce Type: cross  Abstract: Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody design as a protein sequence-structure co-design problem, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks, we propose direct energy-based preference optimization to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repu
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14593</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#20174;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14593
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AIRL&#65289;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;AIRL&#30340;&#20004;&#20010;&#19981;&#21516;&#35282;&#24230;&#65306;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#12290;&#25105;&#20204;&#20174;&#29992;Soft Actor-Critic&#65288;SAC&#65289;&#26367;&#25442;AIRL&#20013;&#30340;&#20869;&#32622;&#31639;&#27861;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;SAC&#30340;&#31163;&#31574;&#30053;&#24418;&#24335;&#21644;&#30456;&#23545;&#20110;AIRL&#32780;&#35328;&#21487;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#36825;&#30830;&#23454;&#22312;&#31574;&#30053;&#27169;&#20223;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#19981;&#24910;&#32473;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#24102;&#26469;&#20102;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;SAC&#31639;&#27861;&#26412;&#36523;&#22312;AIRL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;PPO-AIRL + SAC&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#36716;&#31227;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29615;&#22659;&#25552;&#21462;&#35299;&#24320;&#30340;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14593v1 Announce Type: new  Abstract: Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewa
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.09048</link><description>&lt;p&gt;
&#39535;&#26381;&#24322;&#26500;&#25968;&#25454;&#22495;&#20013;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#20013;&#30340;&#36328;&#39046;&#22495;&#34920;&#31034;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09048
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;FedPLVM&#36890;&#36807;&#24314;&#31435;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#21644;&#20351;&#29992;&#26032;&#22411;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;FL&#26041;&#27861;&#20551;&#35774;&#23458;&#25143;&#31471;&#20043;&#38388;&#20855;&#26377;&#30456;&#21516;&#30340;&#25968;&#25454;&#39046;&#22495;&#65292;&#20294;&#29616;&#23454;&#22330;&#26223;&#20013;&#36890;&#24120;&#28041;&#21450;&#24322;&#26500;&#25968;&#25454;&#39046;&#22495;&#12290;&#32852;&#37030;&#21407;&#22411;&#23398;&#20064;&#65288;FedPL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#24179;&#22343;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#21407;&#22411;&#26469;&#22686;&#24378;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FedPL&#26041;&#27861;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21019;&#24314;&#30456;&#21516;&#25968;&#37327;&#30340;&#21407;&#22411;&#65292;&#23548;&#33268;&#36328;&#39046;&#22495;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20351;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#20943;&#36731;&#36328;&#39046;&#22495;&#29305;&#24449;&#34920;&#31034;&#24046;&#24322;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedPLVM&#65292;&#23427;&#24314;&#31435;&#20102;&#26041;&#24046;&#24863;&#30693;&#30340;&#21452;&#23618;&#21407;&#22411;&#32858;&#31867;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\alpha$-&#31232;&#30095;&#21407;&#22411;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.04805</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#31080;&#25454;&#37117;&#26159;&#24179;&#31561;&#30340;&#65292;&#32780;&#25105;&#20204;&#30693;&#36947;&#65306;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#24341;&#23548;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Not all tickets are equal and we know it: Guiding pruning with domain-specific knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04805
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#30340;&#26041;&#27861; DASH &#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#23398;&#20064;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#20391;&#37325;&#20110;&#35745;&#31639;&#36164;&#28304;&#25928;&#29575;&#30340;&#20462;&#21098;&#31639;&#27861;&#22312;&#36873;&#25321;&#31526;&#21512;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#26377;&#24847;&#20041;&#27169;&#22411;&#26041;&#38754;&#38754;&#20020;&#31639;&#27861;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DASH&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#32467;&#26500;&#20449;&#24687;&#26469;&#24341;&#23548;&#20462;&#21098;&#12290;&#22312;&#23398;&#20064;&#21160;&#24577;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DASH&#19982;&#29616;&#26377;&#19968;&#33324;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19982;&#29983;&#29289;&#23398;&#19968;&#33268;&#30340;&#25968;&#25454;&#29305;&#23450;&#35265;&#35299;&#12290;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20855;&#26377;&#22320;&#38754;&#30495;&#23454;&#20449;&#24687;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;DASH&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20248;&#20110;&#31454;&#20105;&#26041;&#27861;&#24456;&#22823;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#23398;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#32467;&#26500;&#20449;&#24687;&#20855;&#26377;&#25552;&#39640;&#27169;&#22411;&#34893;&#29983;&#31185;&#23398;&#27934;&#35265;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04805v1 Announce Type: new  Abstract: Neural structure learning is of paramount importance for scientific discovery and interpretability. Yet, contemporary pruning algorithms that focus on computational resource efficiency face algorithmic barriers to select a meaningful model that aligns with domain expertise. To mitigate this challenge, we propose DASH, which guides pruning by available domain-specific structural information. In the context of learning dynamic gene regulatory network models, we show that DASH combined with existing general knowledge on interaction partners provides data-specific insights aligned with biology. For this task, we show on synthetic data with ground truth information and two real world applications the effectiveness of DASH, which outperforms competing methods by a large margin and provides more meaningful biological insights. Our work shows that domain specific structural information bears the potential to improve model-derived scientific insi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04306</link><description>&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Effectiveness Assessment of Recent Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#20986;&#29616;&#20195;&#34920;&#30528;&#36808;&#21521;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#31243;&#24230;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#27969;&#34892;&#30340;LVLMs&#22312;&#19987;&#19994;&#21644;&#36890;&#29992;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#22312;&#19987;&#19994;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#37327;&#36523;&#23450;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#33258;&#28982;&#12289;&#21307;&#30103;&#21644;&#24037;&#19994;&#19977;&#31181;&#19981;&#21516;&#22330;&#26223;&#30340;&#20840;&#38754;&#27979;&#35797;&#24179;&#21488;&#65292;&#28085;&#30422;&#20845;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#26174;&#33879;&#12289;&#20266;&#35013;&#21644;&#36879;&#26126;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#24687;&#32905;&#21644;&#30382;&#32932;&#30149;&#21464;&#26816;&#27979;&#65292;&#20197;&#21450;&#24037;&#19994;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#26368;&#36817;&#19977;&#31181;&#24320;&#28304;LVLMs--MiniGPT-v2&#12289;LLaVA-1.5&#21644;Shikra--&#22312;&#35270;&#35273;&#35782;&#21035;&#21644;&#23450;&#20301;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04306v1 Announce Type: cross  Abstract: The advent of large vision-language models (LVLMs) represents a noteworthy advancement towards the pursuit of artificial general intelligence. However, the extent of their efficacy across both specialized and general tasks warrants further investigation. This article endeavors to evaluate the competency of popular LVLMs in specialized and general tasks, respectively, aiming to offer a comprehensive comprehension of these innovative methodologies. To gauge their efficacy in specialized tasks, we tailor a comprehensive testbed comprising three distinct scenarios: natural, healthcare, and industrial, encompassing six challenging tasks. These tasks include salient, camouflaged, and transparent object detection, as well as polyp and skin lesion detection, alongside industrial anomaly detection. We examine the performance of three recent open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of visual recognition and localiza
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01183</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#22330;&#26223;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01183
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#20799;&#31461;&#24615;&#34384;&#24453;&#22270;&#20687;&#25968;&#25454;&#30340;&#22330;&#26223;&#35782;&#21035;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
21&#19990;&#32426;&#30340;&#29359;&#32618;&#20998;&#20026;&#34394;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24050;&#32463;&#25104;&#20026;&#23545;&#21518;&#32773;&#20154;&#20204;&#31119;&#31049;&#21644;&#23433;&#20840;&#26500;&#25104;&#20840;&#29699;&#23041;&#32961;&#12290;&#23427;&#25552;&#20986;&#30340;&#25361;&#25112;&#24517;&#39035;&#36890;&#36807;&#32479;&#19968;&#30340;&#20840;&#29699;&#21512;&#20316;&#26469;&#38754;&#23545;&#65292;&#25105;&#20204;&#24517;&#39035;&#27604;&#20197;&#24448;&#26356;&#21152;&#20381;&#36182;&#33258;&#21160;&#21270;&#20294;&#20540;&#24471;&#20449;&#36182;&#30340;&#24037;&#20855;&#26469;&#24212;&#23545;&#32593;&#32476;&#29359;&#32618;&#26085;&#30410;&#22686;&#38271;&#30340;&#26412;&#36136;&#12290;&#27599;&#24180;&#26377;&#36229;&#36807;1000&#19975;&#36215;&#20799;&#31461;&#24615;&#34384;&#24453;&#25253;&#21578;&#25552;&#20132;&#32473;&#32654;&#22269;&#22269;&#23478;&#22833;&#36394;&#21644;&#34987;&#21093;&#21066;&#20799;&#31461;&#20013;&#24515;&#65292;&#36229;&#36807;80%&#26469;&#33258;&#32593;&#32476;&#26469;&#28304;&#12290;&#22240;&#27492;&#65292;&#35843;&#26597;&#20013;&#24515;&#21644;&#28165;&#38500;&#20013;&#24515;&#26080;&#27861;&#25163;&#21160;&#22788;&#29702;&#21644;&#27491;&#30830;&#35843;&#26597;&#25152;&#26377;&#22270;&#20687;&#12290;&#22522;&#20110;&#27492;&#65292;&#33021;&#22815;&#23433;&#20840;&#39640;&#25928;&#22788;&#29702;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#38752;&#33258;&#21160;&#21270;&#24037;&#20855;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22330;&#26223;&#35782;&#21035;&#20219;&#21153;&#23547;&#25214;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#33021;&#22815;&#32452;&#32455;&#21644;&#20998;&#31867;&#20799;&#31461;&#24615;&#34384;&#24453;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22312;&#25935;&#24863;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing &amp; Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
&lt;/p&gt;</description></item><item><title>Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18512</link><description>&lt;p&gt;
Log&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65306;&#26446;&#25324;&#21495;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18512
&lt;/p&gt;
&lt;p&gt;
Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#30340;&#30690;&#37327;&#22330;&#25551;&#36848;&#20102;&#25511;&#21046;&#36335;&#24452;&#19982;&#35299;&#36335;&#24452;&#28436;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31070;&#32463;CDE&#65288;NCDE&#65289;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35270;&#20026;&#23545;&#25511;&#21046;&#36335;&#24452;&#30340;&#35266;&#27979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;CDE&#30340;&#30690;&#37327;&#22330;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#23558;&#35299;&#36335;&#24452;&#20316;&#20026;&#25345;&#32493;&#28436;&#21270;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#30001;&#20110;&#20854;&#26500;&#36896;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#19981;&#35268;&#21017;&#37319;&#26679;&#29575;&#65292;NCDE&#26159;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#22312;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Log-NCDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;NCDE&#30340;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;Log-NCDE&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;Log-ODE&#26041;&#27861;&#65292;&#36825;&#26159;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#30340;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;CDE&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#19968;&#31995;&#21015;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#65292;&#23637;&#31034;&#20102;Log-NCDE&#27604;NCDE&#65292;NRDE&#21644;&#20004;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;S5&#21644;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18512v1 Announce Type: new  Abstract: The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurren
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16731</link><description>&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Accelerating Graph Neural Networks on Real Processing-In-Memory Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16731
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22788;&#29702;&#20869;&#23384;&#31995;&#32479;&#19978;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#30340;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#21644;&#28151;&#21512;&#24335;&#25191;&#34892;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25191;&#34892;&#28041;&#21450;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#65292;&#21518;&#32773;&#22312;&#24635;&#26102;&#38388;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#21463;&#25968;&#25454;&#22312;&#20869;&#23384;&#21644;&#22788;&#29702;&#22120;&#20043;&#38388;&#31227;&#21160;&#30340;&#20005;&#37325;&#29942;&#39048;&#25152;&#38480;&#21046;&#12290;&#22788;&#29702;&#20869;&#23384;&#65288;PIM&#65289;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#22312;&#20869;&#23384;&#38453;&#21015;&#38468;&#36817;&#25110;&#20869;&#37096;&#25918;&#32622;&#31616;&#21333;&#22788;&#29702;&#22120;&#26469;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#31227;&#21160;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PyGim&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;PIM&#31995;&#32479;&#19978;&#21152;&#36895;GNNs&#12290;&#25105;&#20204;&#20026;&#38024;&#23545;&#23454;&#38469;PIM&#31995;&#32479;&#23450;&#21046;&#30340;GNN&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#25552;&#20986;&#26234;&#33021;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#24182;&#20026;&#23427;&#20204;&#24320;&#21457;&#20102;&#26041;&#20415;&#30340;Python API&#12290;&#25105;&#20204;&#25552;&#20379;&#28151;&#21512;&#24335;GNN&#25191;&#34892;&#65292;&#20854;&#20013;&#35745;&#31639;&#23494;&#38598;&#22411;&#21644;&#20869;&#23384;&#23494;&#38598;&#22411;&#26680;&#24515;&#20998;&#21035;&#22312;&#20197;&#22788;&#29702;&#22120;&#20026;&#20013;&#24515;&#21644;&#20197;&#20869;&#23384;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#31995;&#32479;&#20013;&#25191;&#34892;&#65292;&#20197;&#21305;&#37197;&#23427;&#20204;&#30340;&#31639;&#27861;&#29305;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16300</link><description>&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
Conformalized Selective Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#32622;&#20449;&#24230;&#37327;&#65292;&#20197;&#35299;&#20915;&#36873;&#25321;&#24615;&#22238;&#24402;&#20013;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#26159;&#21542;&#24635;&#26159;&#35201;&#25552;&#20379;&#39044;&#27979;&#65311;&#22312;&#36861;&#27714;&#26368;&#22823;&#39044;&#27979;&#24615;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#21487;&#38752;&#24615;&#21644;&#20844;&#24179;&#24615;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#36873;&#25321;&#24615;&#22238;&#24402;&#65292;&#20063;&#31216;&#20026;&#8220;&#25298;&#32477;&#36873;&#39033;&#8221;&#65292;&#20801;&#35768;&#27169;&#22411;&#22312;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#24773;&#20917;&#19979;&#25918;&#24323;&#39044;&#27979;&#12290;&#23613;&#31649;7&#21313;&#24180;&#21069;&#23601;&#26368;&#21021;&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#29992;&#20110;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#20195;&#29702;&#65292;&#23588;&#20854;&#26159;&#26465;&#20214;&#26041;&#24046;&#12290;&#20294;&#36825;&#31181;&#20851;&#27880;&#24573;&#35270;&#20102;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#20026;&#22522;&#20110;&#27169;&#22411;&#29305;&#23450;&#20559;&#24046;&#30340;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#26377;&#26681;&#25454;&#30340;&#32622;&#20449;&#24230;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20415;&#36827;&#34892;&#24688;&#24403;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16300v1 Announce Type: new  Abstract: Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the "reject option," allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper compar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15734</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Operator Learning via Unsupervised Pretraining and In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#22806;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#35265;&#35777;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19982;&#29289;&#29702;&#39046;&#22495;&#29305;&#23450;&#27934;&#23519;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#31185;&#23398;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;PDE&#25968;&#25454;&#12290; &#36825;&#37325;&#26032;&#24341;&#20837;&#20102;&#23545;&#26114;&#36149;&#30340;&#25968;&#20540;PDE&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#37096;&#20998;&#21066;&#24369;&#20102;&#36991;&#20813;&#36825;&#20123;&#26114;&#36149;&#27169;&#25311;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#23547;&#27714;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29992;&#20110;PDE&#36816;&#31639;&#31526;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290; &#20026;&#20102;&#20943;&#23569;&#23545;&#24102;&#26377;&#27169;&#25311;&#35299;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#37325;&#26500;&#30340;&#20195;&#29702;&#20219;&#21153;&#22312;&#26410;&#26631;&#35760;&#30340;PDE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#36816;&#31639;&#31526;&#12290; &#20026;&#20102;&#25552;&#39640;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24110;&#21161;&#31070;&#32463;&#36816;&#31639;&#31526;&#28789;&#27963;&#22320;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#25110;&#35774;&#35745;&#12290; &#22312;&#21508;&#31181;PD&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15734v1 Announce Type: new  Abstract: Recent years have witnessed the promise of coupling machine learning methods and physical domain-specific insight for solving scientific problems based on partial differential equations (PDEs). However, being data-intensive, these methods still require a large amount of PDE data. This reintroduces the need for expensive numerical PDE solutions, partially undermining the original goal of avoiding these expensive simulations. In this work, seeking data efficiency, we design unsupervised pretraining and in-context learning methods for PDE operator learning. To reduce the need for training data with simulated solutions, we pretrain neural operators on unlabeled PDE data using reconstruction-based proxy tasks. To improve out-of-distribution performance, we further assist neural operators in flexibly leveraging in-context learning methods, without incurring extra training costs or designs. Extensive empirical evaluations on a diverse set of PD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.14904</link><description>&lt;p&gt;
&#25968;&#23383;&#27700;&#21360;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#25918;&#23556;&#24615;
&lt;/p&gt;
&lt;p&gt;
Watermarking Makes Language Models Radioactive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#35757;&#32451;&#25968;&#25454;&#33021;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#65292;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#27604;&#20363;&#30340;&#27700;&#21360;&#35757;&#32451;&#25991;&#26412;&#65292;&#20173;&#21487;&#20197;&#39640;&#32622;&#20449;&#24230;&#22320;&#26816;&#27979;&#20986;&#20351;&#29992;&#25968;&#23383;&#27700;&#21360;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#25918;&#23556;&#24615;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#36755;&#20837;&#34987;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#25104;&#21592;&#25512;&#26029;&#21487;&#20197;&#20197;&#19968;&#23450;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#36825;&#31181;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#35757;&#32451;&#25968;&#25454;&#30041;&#19979;&#30340;&#30165;&#36857;&#27604;&#25104;&#21592;&#25512;&#26029;&#26356;&#23481;&#26131;&#26816;&#27979;&#19988;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23558;&#27745;&#26579;&#27700;&#24179;&#19982;&#27700;&#21360;&#30340;&#40065;&#26834;&#24615;&#12289;&#22312;&#35757;&#32451;&#38598;&#20013;&#30340;&#27604;&#20363;&#21644;&#24494;&#35843;&#36807;&#31243;&#32852;&#31995;&#36215;&#26469;&#12290;&#29305;&#21035;&#26159;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#21482;&#26377;5&#65285;&#30340;&#35757;&#32451;&#25991;&#26412;&#34987;&#25968;&#23383;&#27700;&#21360;&#26631;&#35760;&#65292;&#35757;&#32451;&#22312;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;&#21512;&#25104;&#25351;&#20196;&#19978;&#20173;&#28982;&#21487;&#20197;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#65288;p&#20540;&lt;1e-5&#65289;&#34987;&#26816;&#27979;&#21040;&#12290;&#22240;&#27492;&#65292;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;LLM&#27700;&#21360;&#25216;&#26415;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#30830;&#23450;&#24102;&#26377;&#25968;&#23383;&#27700;&#21360;&#30340;LLM&#30340;&#36755;&#20986;&#26159;&#21542;&#34987;&#29992;&#26469;&#23545;&#21478;&#19968;&#20010;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14904v1 Announce Type: cross  Abstract: This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value &lt; 1e-5) even when as little as 5% of training text is watermarked. Thus, LLM watermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09900</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Recurrent Reinforcement Learning with Memory Monoids
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20351;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#12289;&#22686;&#21152;&#20102;&#22238;&#25253;&#24182;&#31616;&#21270;&#20102;&#23454;&#29616;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20687;RNN&#21644;transformers&#36825;&#26679;&#30340;&#35760;&#24518;&#27169;&#22411;&#36890;&#36807;&#23558;&#36712;&#36857;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38271;&#24207;&#21015;&#30340;&#35268;&#27169;&#21270;&#22788;&#29702;&#33021;&#21147;&#24182;&#19981;&#29305;&#21035;&#22909;&#65292;&#23588;&#20854;&#26159;&#19982;&#19968;&#31867;&#26032;&#20852;&#30340;&#35760;&#24518;&#27169;&#22411;&#65288;&#26377;&#26102;&#31216;&#20026;&#32447;&#24615;&#24490;&#29615;&#27169;&#22411;&#65289;&#30456;&#27604;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#24490;&#29615;&#26356;&#26032;&#26159;&#19968;&#20010;&#21333;&#23376;&#65292;&#22240;&#27492;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20869;&#23384;&#21333;&#23376;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20256;&#32479;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#21033;&#29992;&#20869;&#23384;&#21333;&#23376;&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25209;&#22788;&#29702;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#22686;&#21152;&#20102;&#22238;&#25253;&#65292;&#24182;&#31616;&#21270;&#20102;&#24490;&#29615;&#20002;&#22833;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09900v1 Announce Type: cross  Abstract: In RL, memory models such as RNNs and transformers address Partially Observable Markov Decision Processes (POMDPs) by mapping trajectories to latent Markov states. Neither model scales particularly well to long sequences, especially compared to an emerging class of memory models sometimes called linear recurrent models. We discover that the recurrent update of these models is a monoid, leading us to formally define a novel memory monoid framework. We revisit the traditional approach to batching in recurrent RL, highlighting both theoretical and empirical deficiencies. Leveraging the properties of memory monoids, we propose a new batching method that improves sample efficiency, increases the return, and simplifies the implementation of recurrent loss functions in RL.
&lt;/p&gt;</description></item><item><title>PANORAMIA&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.09477</link><description>&lt;p&gt;
PANORAMIA: &#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
PANORAMIA: Privacy Auditing of Machine Learning Models without Retraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09477
&lt;/p&gt;
&lt;p&gt;
PANORAMIA&#26159;&#19968;&#31181;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38544;&#31169;&#23457;&#35745;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20381;&#36182;&#20110;&#20351;&#29992;&#29983;&#25104;&#30340;&#8220;&#38750;&#25104;&#21592;&#8221;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23545;ML&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#23457;&#35745;&#12290;&#36825;&#20010;&#26041;&#26696;&#34987;&#31216;&#20026;PANORAMIA&#65292;&#23427;&#21487;&#20197;&#37327;&#21270;&#22823;&#35268;&#27169;ML&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#32780;&#26080;&#38656;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;ML&#39046;&#22495;&#36827;&#34892;&#20102;&#23457;&#35745;&#65292;&#21253;&#25324;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20197;&#21450;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09477v1 Announce Type: cross  Abstract: We introduce a privacy auditing scheme for ML models that relies on membership inference attacks using generated data as "non-members". This scheme, which we call PANORAMIA, quantifies the privacy leakage for large-scale ML models without control of the training process or model re-training and only requires access to a subset of the training data. To demonstrate its applicability, we evaluate our auditing scheme across multiple ML domains, ranging from image and tabular data classification to large-scale language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.06465</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On Differentially Private Subspace Estimation Without Distributional Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#20998;&#24067;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#24046;&#20998;&#38544;&#31169;&#23376;&#31354;&#38388;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30340;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#20302;&#32500;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#39640;&#32500;&#24230;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#38754;&#20020;&#30528;&#19968;&#20010;&#34987;&#31216;&#20026;&#32500;&#25968;&#35781;&#21650;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#20102;&#25104;&#26412;&#30340;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#20855;&#26377;&#22266;&#26377;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#26799;&#24230;&#32463;&#24120;&#20301;&#20110;&#19968;&#20010;&#20302;&#32500;&#23376;&#31354;&#38388;&#38468;&#36817;&#12290;&#22914;&#26524;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#28857;&#31169;&#23494;&#22320;&#35782;&#21035;&#20986;&#36825;&#31181;&#20302;&#32500;&#32467;&#26500;&#65292;&#23601;&#21487;&#20197;&#36991;&#20813;&#22240;&#39640;&#32500;&#24230;&#32780;&#25903;&#20184;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying (in terms of privacy and accuracy) for the high ambient dimension.   On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that depends on the dimension. But Singhal and Steinke (NeurIPS 2021) bypassed this limitation by considering points that are i.i.d. samples from a Gaussian distribution whose covariance matrix has a certain eigenvalue gap. Yet, it was still left unclear whether we could provide similar upper bounds without distributional assumptions and whether we 
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04216</link><description>&lt;p&gt;
&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23558;&#24453;&#35831;&#27714;&#20869;&#23481;&#23384;&#20648;&#22312;&#19981;&#21516;&#32423;&#21035;&#19978;&#65292;&#21487;&#20197;&#20943;&#36731;&#30001;&#23569;&#25968;&#28909;&#38376;&#25991;&#20214;&#30340;&#35270;&#39057;&#27969;&#37327;&#36896;&#25104;&#30340;&#22238;&#31243;&#25317;&#22622;&#12290;&#36890;&#24120;&#65292;&#20869;&#23481;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;CSP&#65289;&#25317;&#26377;&#20869;&#23481;&#65292;&#29992;&#25143;&#20351;&#29992;&#20854;&#65288;&#26080;&#32447;&#65289;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;ISP&#65289;&#20174;CSP&#35831;&#27714;&#20854;&#39318;&#36873;&#20869;&#23481;&#12290;&#30001;&#20110;&#36825;&#20123;&#21442;&#19982;&#26041;&#19981;&#20250;&#36879;&#38706;&#20854;&#31169;&#23494;&#20449;&#24687;&#21644;&#21830;&#19994;&#26426;&#23494;&#65292;&#20256;&#32479;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#38656;&#27714;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;RawHFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#25968;&#25454;&#33719;&#21462;&#25216;&#26415;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#35831;&#27714;&#30340;&#20869;&#23481;&#26356;&#26032;&#20854;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32593;&#32476;&#21644;&#20854;&#20182;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#32771;&#34385;&#21040;&#21482;&#26377;&#19968;&#37096;&#20998;&#29992;&#25143;&#21442;&#19982;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03771</link><description>&lt;p&gt;
&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#20363;&#32423;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03771
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#23558;&#20026;&#20195;&#29702;&#29983;&#25104;&#65292;&#20197;&#20415;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#20197;&#33719;&#21462;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#26080;&#27861;&#33719;&#21462;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#12290;&#30456;&#21453;&#65292;&#23398;&#20064;&#22120;&#21482;&#22312;&#36335;&#24452;&#30340;&#32467;&#26463;&#22788;&#33719;&#21462;&#22870;&#21169;&#65292;&#20854;&#20013;&#36335;&#24452;&#30340;&#37096;&#20998;&#24207;&#21015;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#22120;&#24517;&#39035;&#38754;&#23545;&#25506;&#32034;&#21253;&#20013;&#26410;&#30693;&#21363;&#26102;&#22870;&#21169;&#30340;&#26174;&#33879;&#22256;&#38590;&#65292;&#36825;&#19981;&#33021;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#35299;&#20915;&#65292;&#21253;&#25324;&#20165;&#32771;&#34385;&#23436;&#25972;&#36335;&#24452;&#24182;&#24573;&#30053;&#20869;&#37096;&#22870;&#21169;&#20998;&#24067;&#30340;&#36712;&#36857;&#26041;&#27861;&#12290;&#20026;&#20102;&#27491;&#24335;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#31216;&#20026;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;Reinforcement Learning from Bagged Rewards&#65292;RLBR&#65289;&#65292;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;RLBR&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01887</link><description>&lt;p&gt;
&#22522;&#20110;f-&#25955;&#24230;&#21407;&#29702;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65306;&#19968;&#20010;&#25913;&#36827;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
On f-Divergence Principled Domain Adaptation: An Improved Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25913;&#36827;&#20102;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;f-&#39046;&#22495;&#24046;&#24322;&#24230;&#37327;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#21644;&#24341;&#20837;&#32553;&#25918;&#21442;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20174;&#32780;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#30340;KL&#32467;&#26524;&#65292;&#23558;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#32553;&#23567;&#65292;&#24182;&#36890;&#36807;&#23450;&#20301;&#25216;&#26415;&#24320;&#21457;&#20102;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#25552;&#20986;&#30340;UDA&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#23545;&#20854;&#22522;&#20110;f-&#25955;&#24230;&#30340;&#24046;&#24322;&#24230;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#21363;f-&#39046;&#22495;&#24046;&#24322;&#65288;f-DD&#65289;&#12290;&#36890;&#36807;&#21435;&#38500;&#32477;&#23545;&#20540;&#20989;&#25968;&#24182;&#24341;&#20837;&#19968;&#20010;&#32553;&#25918;&#21442;&#25968;&#65292;f-DD&#20135;&#29983;&#20102;&#26032;&#30340;&#30446;&#26631;&#35823;&#24046;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24674;&#22797;&#20197;&#21069;&#22522;&#20110;KL&#30340;&#32467;&#26524;&#65292;&#24182;&#24357;&#21512;&#20102;Acuna&#31561;&#20154;&#65288;2021&#24180;&#65289;&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#21644;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#21033;&#29992;&#23450;&#20301;&#25216;&#26415;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27969;&#34892;&#30340;UDA&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;f-DD&#30340;&#39046;&#22495;&#23398;&#20064;&#31639;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) plays a crucial role in addressing distribution shifts in machine learning. In this work, we improve the theoretical foundations of UDA proposed by Acuna et al. (2021) by refining their f-divergence-based discrepancy and additionally introducing a new measure, f-domain discrepancy (f-DD). By removing the absolute value function and incorporating a scaling parameter, f-DD yields novel target error and sample complexity bounds, allowing us to recover previous KL-based results and bridging the gap between algorithms and theory presented in Acuna et al. (2021). Leveraging a localization technique, we also develop a fast-rate generalization bound. Empirical results demonstrate the superior performance of f-DD-based domain learning algorithms over previous works in popular UDA benchmarks.
&lt;/p&gt;</description></item><item><title>KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;</title><link>https://arxiv.org/abs/2401.18079</link><description>&lt;p&gt;
KVQuant: &#20197;KV&#32531;&#23384;&#37327;&#21270;&#23454;&#29616;1000&#19975;&#19978;&#19979;&#25991;&#38271;&#24230;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18079
&lt;/p&gt;
&lt;p&gt;
KVQuant&#26159;&#19968;&#31181;&#35299;&#20915;LLM&#25512;&#29702;&#20013;&#22823;&#37327;&#20869;&#23384;&#28040;&#32791;&#30340;KV&#32531;&#23384;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#12289;RoPE&#21069;&#37327;&#21270;&#38190;&#21644;&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#30340;KV&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#25991;&#26723;&#20998;&#26512;&#21644;&#25688;&#35201;&#31561;&#38656;&#35201;&#22823;&#31383;&#21475;&#19978;&#19979;&#25991;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;KV&#32531;&#23384;&#28608;&#27963;&#25104;&#20026;&#35760;&#24518;&#28040;&#32791;&#30340;&#20027;&#35201;&#36129;&#29486;&#32773;&#12290;&#37327;&#21270;&#26159;&#19968;&#31181;&#21387;&#32553;KV&#32531;&#23384;&#28608;&#27963;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#36229;&#20302;&#31934;&#24230;&#65288;&#22914;&#20302;&#20110;4&#20301;&#65289;&#30340;&#28608;&#27963;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;KVQuant&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#26041;&#27861;&#37327;&#21270;&#32531;&#23384;&#30340;KV&#28608;&#27963;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#65306;(i)&#20998;&#36890;&#36947;&#38190;&#37327;&#21270;&#65292;&#22312;&#37327;&#21270;&#38190;&#28608;&#27963;&#26102;&#35843;&#25972;&#32500;&#24230;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20998;&#24067;&#65307;(ii)RoPE&#21069;&#37327;&#21270;&#38190;&#65292;&#22312;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#20043;&#21069;&#37327;&#21270;&#38190;&#28608;&#27963;&#20197;&#20943;&#36731;&#20854;&#23545;&#37327;&#21270;&#30340;&#24433;&#21709;&#65307;(iii)&#38750;&#22343;&#21248;KV&#32531;&#23384;&#37327;&#21270;&#65292;&#22312;&#27599;&#23618;&#25512;&#23548;&#20986;&#26435;&#37325;&#24863;&#30693;&#30340;&#38750;&#22343;&#21248;&#25968;&#25454;&#31867;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#19981;&#21516;&#23618;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are seeing growing use for applications such as document analysis and summarization which require large context windows, and with these large context windows KV cache activations surface as the dominant contributor to memory consumption during inference. Quantization is a promising approach for compressing KV cache activations; however, existing solutions fail to represent activations accurately in ultra-low precisions, such as sub-4-bit. In this work, we present KVQuant, which addresses this problem by incorporating novel methods for quantizing cached KV activations, including: (i) Per-Channel Key Quantization, where we adjust the dimension along which we quantize the Key activations to better match the distribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations before the rotary positional embedding to mitigate its impact on quantization; (iii) Non-Uniform KV Cache Quantization, where we derive per-layer sensitivity-weighted non-uniform datatypes that better 
&lt;/p&gt;</description></item><item><title>GraphMETRO&#26159;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#22270;&#20998;&#24067;&#21464;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#23545;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2312.04693</link><description>&lt;p&gt;
GraphMETRO&#65306;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#30340;&#22270;&#20998;&#24067;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of Aligned Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04693
&lt;/p&gt;
&lt;p&gt;
GraphMETRO&#26159;&#19968;&#31181;&#36890;&#36807;&#28151;&#21512;&#23545;&#40784;&#19987;&#23478;&#26469;&#20943;&#36731;&#22797;&#26434;&#22270;&#20998;&#24067;&#21464;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#23545;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#24179;&#28369;&#20248;&#21270;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#22266;&#26377;&#22797;&#26434;&#19988;&#24322;&#26500;&#65292;&#23548;&#33268;&#33258;&#28982;&#22810;&#26679;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#24456;&#39640;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#36866;&#29992;&#20110;&#22797;&#26434;&#38750;&#21512;&#25104;&#20998;&#24067;&#21464;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;GraphMETRO&#65292;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#21487;&#38752;&#22320;&#24314;&#27169;&#33258;&#28982;&#22810;&#26679;&#24615;&#24182;&#25429;&#25417;&#22797;&#26434;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;GraphMETRO&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38376;&#25511;&#27169;&#22411;&#21644;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#65292;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#38024;&#23545;&#29305;&#23450;&#30340;&#20998;&#24067;&#21464;&#21270;&#20135;&#29983;&#19981;&#21464;&#34920;&#31034;&#65292;&#38376;&#25511;&#27169;&#22411;&#21017;&#35782;&#21035;&#21464;&#21270;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#23558;&#19981;&#21516;&#19987;&#23478;&#27169;&#22411;&#30340;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#20197;&#30830;&#20445;&#24179;&#28369;&#20248;&#21270;&#12290;GraphMETRO&#22312;&#30001;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#21464;&#21270;&#32452;&#25104;&#30340;GOOD&#22522;&#20934;&#27979;&#35797;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#25552;&#39640;&#20102;67&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data are inherently complex and heterogeneous, leading to a high natural diversity of distributional shifts. However, it remains unclear how to build machine learning architectures that generalize to complex non-synthetic distributional shifts naturally occurring in the real world. Here we develop GraphMETRO, a Graph Neural Network architecture, that reliably models natural diversity and captures complex distributional shifts. GraphMETRO employs a Mixture-of-Experts (MoE) architecture with a gating model and multiple expert models, where each expert model targets a specific distributional shift to produce a shift-invariant representation, and the gating model identifies shift components. Additionally, we design a novel objective that aligns the representations from different expert models to ensure smooth optimization. GraphMETRO achieves state-of-the-art results on four datasets from GOOD benchmark comprised of complex and natural real-world distribution shifts, improving by 67%
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;</title><link>https://arxiv.org/abs/2311.13261</link><description>&lt;p&gt;
&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#30340;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#24341;&#23548;&#20998;&#21106;&#20986;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#12289;&#21407;&#20301;&#30149;&#21464;&#21644;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#21160;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#20999;&#29255;&#12290;&#33258;&#21160;&#35780;&#20272;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#25928;&#29575;&#65292;&#24182;&#24110;&#21161;&#25214;&#21040;&#24418;&#24577;&#29305;&#24449;&#19982;&#20020;&#24202;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#36776;&#35748;&#28024;&#28070;&#24615;&#19978;&#30382;&#32454;&#32990;&#65292;&#24182;&#23558;&#20854;&#19982;&#33391;&#24615;&#19978;&#30382;&#32454;&#32990;&#21644;&#21407;&#20301;&#30149;&#21464;&#20998;&#24320;&#23558;&#26159;&#31532;&#19968;&#27493;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#30284;&#20999;&#29255;&#20013;&#19978;&#30382;&#32454;&#32990;&#20998;&#21106;&#30340;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#26579;&#33394;&#34880;&#32418;&#34507;&#30333;&#21644;&#21980;&#37240;&#24615;&#26579;&#33394;&#32454;&#32990;&#35282;&#34507;&#30333;(CK) AE1/AE3 HE&#20999;&#29255;&#65292;&#20197;&#21450;&#30149;&#29702;&#23398;&#23478;&#30340;&#27880;&#37322;&#29983;&#25104;&#20102;&#19978;&#30382;&#22522;&#26412;&#30495;&#20540;&#25513;&#27169;&#12290;HE/CK&#22270;&#20687;&#23545;&#34987;&#29992;&#20110;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#29992;&#26469;&#20351;&#27169;&#22411;&#26356;&#31283;&#20581;&#12290;839&#21517;&#24739;&#32773;&#30340;&#32452;&#32455;&#24494;&#38453;&#21015;&#65288;TMAs&#65289;&#21644;&#20004;&#21517;&#24739;&#32773;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#29992;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13261v2 Announce Type: replace-cross  Abstract: Digital pathology enables automatic analysis of histopathological sections using artificial intelligence (AI). Automatic evaluation could improve diagnostic efficiency and help find associations between morphological features and clinical outcome. For development of such prediction models, identifying invasive epithelial cells, and separating these from benign epithelial cells and in situ lesions would be the first step. In this study, we aimed to develop an AI model for segmentation of epithelial cells in sections from breast cancer. We generated epithelial ground truth masks by restaining hematoxylin and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists' annotations. HE/CK image pairs were used to train a convolutional neural network, and data augmentation was used to make the model more robust. Tissue microarrays (TMAs) from 839 patients, and whole slide images from two patients were used for training an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.03520</link><description>&lt;p&gt;
&#22823;&#33041;&#32593;&#32476;&#19982;&#26234;&#21147;&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BrainRGIN&#24314;&#27169;&#26550;&#26500;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26234;&#21147;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#24182;&#32467;&#21512;&#20102;&#32858;&#31867;&#23884;&#20837;&#12289;&#22270;&#21516;&#26500;&#32593;&#32476;&#12289;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24687;&#24577;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;rsfMRI&#65289;&#26159;&#19968;&#31181;&#30740;&#31350;&#22823;&#33041;&#21151;&#33021;&#21644;&#35748;&#30693;&#36807;&#31243;&#20851;&#31995;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25429;&#33719;&#22823;&#33041;&#30340;&#21151;&#33021;&#32452;&#32455;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#25110;&#21050;&#28608;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;BrainRGIN&#30340;&#26032;&#39062;&#24314;&#27169;&#26550;&#26500;&#65292;&#21033;&#29992;rsfMRI&#25512;&#23548;&#30340;&#38745;&#24577;&#21151;&#33021;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#26234;&#21147;&#65288;&#27969;&#20307;&#12289;&#26230;&#20307;&#21644;&#24635;&#20307;&#26234;&#21147;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#23558;&#32858;&#31867;&#23884;&#20837;&#21644;&#22270;&#21516;&#26500;&#32593;&#32476;&#32435;&#20837;&#21040;&#22270;&#21367;&#31215;&#23618;&#20013;&#65292;&#20197;&#21453;&#26144;&#22823;&#33041;&#23376;&#32593;&#32476;&#32452;&#32455;&#30340;&#24615;&#36136;&#21644;&#39640;&#25928;&#32593;&#32476;&#34920;&#36798;&#65292;&#20877;&#36741;&#20197;TopK&#27744;&#21270;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35835;&#20986;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.13544</link><description>&lt;p&gt;
&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#22914;&#20309;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#65311;
&lt;/p&gt;
&lt;p&gt;
Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?. (arXiv:2401.13544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36229;&#36234;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#40657;&#30418;&#27169;&#22411;&#21487;&#24178;&#39044;&#12290;&#36890;&#36807;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#26469;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#37325;&#26032;&#25506;&#32034;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#65292;&#21253;&#25324;&#20174;&#21407;&#22987;&#29305;&#24449;&#20013;&#36880;&#27493;&#39044;&#27979;&#39640;&#32423;&#27010;&#24565;&#21644;&#20174;&#39044;&#27979;&#30340;&#27010;&#24565;&#20013;&#39044;&#27979;&#30446;&#26631;&#21464;&#37327;&#12290;&#36825;&#20010;&#27169;&#22411;&#31867;&#21035;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#20248;&#21183;&#26159;&#29992;&#25143;&#33021;&#22815;&#23545;&#39044;&#27979;&#30340;&#27010;&#24565;&#20540;&#36827;&#34892;&#24178;&#39044;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#30340;&#19979;&#28216;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#24050;&#32463;&#35757;&#32451;&#22909;&#20294;&#26412;&#36136;&#19978;&#19981;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#24178;&#39044;&#65292;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#39564;&#35777;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#21487;&#24178;&#39044;&#24615;&#23450;&#20041;&#20026;&#22522;&#20110;&#27010;&#24565;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#24230;&#37327;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#23450;&#20041;&#26469;&#23545;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#21644;&#33258;&#28982;&#22270;&#20687;&#22522;&#20934;&#19978;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#24178;&#39044;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24494;&#35843;&#25552;&#39640;&#20102;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#24182;&#32463;&#24120;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, interpretable machine learning has re-explored concept bottleneck models (CBM), comprising step-by-step prediction of the high-level concepts from the raw features and the target variable from the predicted concepts. A compelling advantage of this model class is the user's ability to intervene on the predicted concept values, affecting the model's downstream output. In this work, we introduce a method to perform such concept-based interventions on already-trained neural networks, which are not interpretable by design, given an annotated validation set. Furthermore, we formalise the model's intervenability as a measure of the effectiveness of concept-based interventions and leverage this definition to fine-tune black-box models. Empirically, we explore the intervenability of black-box classifiers on synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning improves intervention effectiveness and often yields better-calibrated predictions. To showcase the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10300</link><description>&lt;p&gt;
&#29992;&#20110;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#26816;&#27979;&#30340;&#20855;&#26377;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#30340;&#20998;&#23618;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Framework with Spatio-Temporal Consistency Learning for Emergence Detection in Complex Adaptive Systems. (arXiv:2401.10300v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#21644;&#20195;&#29702;&#30340;&#34920;&#31034;&#65292;&#20351;&#29992;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#26469;&#25429;&#25417;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#20013;&#30340;&#29616;&#35937;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#19981;&#33021;&#25429;&#25417;&#31354;&#38388;&#27169;&#24335;&#21644;&#24314;&#27169;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#20132;&#20114;&#20195;&#29702;&#32452;&#25104;&#30340;&#22797;&#26434;&#36866;&#24212;&#24615;&#31995;&#32479;&#65288;CAS&#65289;&#20013;&#65292;&#29616;&#35937;&#26159;&#19968;&#31181;&#20840;&#23616;&#23646;&#24615;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#32593;&#32476;&#23618;&#27425;&#30340;&#20132;&#36890;&#25317;&#22581;&#12290;&#26816;&#27979;&#23427;&#30340;&#24418;&#25104;&#21644;&#28040;&#25955;&#26377;&#21161;&#20110;&#30417;&#27979;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#24182;&#21457;&#20986;&#26377;&#23475;&#29616;&#35937;&#30340;&#35686;&#25253;&#20449;&#21495;&#12290;&#30001;&#20110;CAS&#27809;&#26377;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#65292;&#22522;&#20110;&#27599;&#20010;&#20195;&#29702;&#30340;&#23616;&#37096;&#35266;&#23519;&#26469;&#26816;&#27979;&#29616;&#35937;&#26159;&#21487;&#21462;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#19981;&#33021;&#25429;&#25417;&#19982;&#29616;&#35937;&#30456;&#20851;&#30340;&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#19988;&#26080;&#27861;&#24314;&#27169;&#20195;&#29702;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31995;&#32479;&#34920;&#31034;&#21644;&#20195;&#29702;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26102;&#31354;&#19968;&#33268;&#24615;&#23398;&#20064;&#22120;&#38024;&#23545;&#20195;&#29702;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#31995;&#32479;&#30340;&#22797;&#26434;&#28436;&#21270;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#36890;&#36807;&#20445;&#30041;&#26368;&#26032;100&#20010;&#20195;&#29702;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#29366;&#24577;&#26469;&#23398;&#20064;&#20195;&#29702;&#21644;&#31995;&#32479;&#30340;&#34920;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;
Emergence, a global property of complex adaptive systems (CASs) constituted by interactive agents, is prevalent in real-world dynamic systems, e.g., network-level traffic congestions. Detecting its formation and evaporation helps to monitor the state of a system, allowing to issue a warning signal for harmful emergent phenomena. Since there is no centralized controller of CAS, detecting emergence based on each agent's local observation is desirable but challenging. Existing works are unable to capture emergence-related spatial patterns, and fail to model the nonlinear relationships among agents. This paper proposes a hierarchical framework with spatio-temporal consistency learning to solve these two problems by learning the system representation and agent representations, respectively. Especially, spatio-temporal encoders are tailored to capture agents' nonlinear relationships and the system's complex evolution. Representations of the agents and the system are learned by preserving the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2401.02739</link><description>&lt;p&gt;
&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65306;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;
&lt;/p&gt;
&lt;p&gt;
Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#65292;&#24182;&#36890;&#36807;&#21453;&#36716;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#35813;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#20860;&#23481;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65292;&#24182;&#22312;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#25512;&#26029;&#65288;DDVI&#65289;&#65292;&#19968;&#31181;&#29992;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#34920;&#36798;&#24615;&#21464;&#20998;&#21518;&#39564;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36817;&#20284;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#28508;&#21464;&#37327;&#22686;&#21152;&#20102;&#21464;&#20998;&#21518;&#39564;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#34920;&#36798;&#24615;&#30340;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#21453;&#36716;&#29992;&#25143;&#25351;&#23450;&#30340;&#21152;&#22122;&#36807;&#31243;&#22312;&#28508;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#12290;&#25105;&#20204;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#21463;&#21040;&#35273;&#37266;-&#30561;&#30496;&#31639;&#27861;&#21551;&#21457;&#30340;&#36793;&#38469;&#20284;&#28982;&#26032;&#19979;&#30028;&#26469;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#23454;&#29616;&#65288;&#23427;&#36866;&#37197;&#20102;&#27491;&#21017;&#21270;&#30340;ELBO&#25193;&#23637;&#65289;&#65292;&#19982;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20860;&#23481;&#65292;&#24182;&#19988;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#25110;&#23545;&#25239;&#32593;&#32476;&#30340;&#26367;&#20195;&#36817;&#20284;&#21518;&#39564;&#31867;&#21035;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#21435;&#22122;&#25193;&#25955;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;DD-VAE&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#29983;&#29289;&#23398;&#20013;&#30340;&#19968;&#20010;&#28608;&#21169;&#20219;&#21153; -- &#20174;&#20154;&#31867;&#22522;&#22240;&#32452;&#20013;&#25512;&#26029;&#28508;&#22312;&#34880;&#32479; -- &#36229;&#36807;&#20102;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2401.01426</link><description>&lt;p&gt;
&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#22240;&#26524;&#25512;&#26029;&#30340;&#27169;&#22359;&#21270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#22238;&#31572;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#22312;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#26126;&#30830;&#30340;&#20998;&#31163;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35745;&#31639;&#21487;&#36776;&#35782;&#22240;&#26524;&#26597;&#35810;&#30340;&#22768;&#38899;&#21644;&#23436;&#25972;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#23618;&#27425;&#30340;&#22240;&#26524;&#32467;&#26500;&#21644;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#36739;&#20302;&#23618;&#27425;&#30340;&#23618;&#27425;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#31639;&#27861;&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#22914;&#22270;&#20687;&#36825;&#26679;&#30340;&#39640;&#32500;&#21464;&#37327;&#26159;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21487;&#20197;&#34987;&#35757;&#32451;&#26469;&#23398;&#20064;&#22914;&#20309;&#20934;&#30830;&#22320;&#20174;&#36825;&#26679;&#30340;&#39640;&#32500;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#29305;&#21035;&#26159;&#38543;&#30528;&#22270;&#20687;&#22522;&#27169;&#22411;&#30340;&#26368;&#36817;&#20852;&#36215;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#22238;&#31572;&#24102;&#26377;&#36825;&#26679;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39034;&#24207;&#35757;&#32451;&#31639;&#27861;&#65292;&#32473;&#23450;&#22240;&#26524;&#32467;&#26500;&#21644;&#39044;&#35757;&#32451;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#26469;&#20272;&#35745;&#30001;&#39640;&#32500;&#25968;&#25454;&#24341;&#36215;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.00625</link><description>&lt;p&gt;
&#36229;&#36234;&#25928;&#29575;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. (arXiv:2401.00625v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#36805;&#29467;&#21457;&#23637;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#27493;&#65292;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24102;&#26469;&#20102;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#21644;&#36130;&#21153;&#36164;&#28304;&#39640;&#28040;&#32791;&#31561;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#22686;&#24378;LLMs&#36164;&#28304;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#26681;&#25454;&#20248;&#21270;&#37325;&#28857;&#23545;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65306;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;LLM&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65288;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#12289;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#31995;&#32479;&#35774;&#35745;&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#35843;&#30740;&#36890;&#36807;&#29305;&#23450;&#36164;&#28304;&#31867;&#22411;&#24341;&#20837;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36164;&#28304;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2311.00519</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Based Reconstruction For Time-series Contrastive Learning. (arXiv:2311.00519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;REBAR&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#20449;&#24687;&#21644;&#37325;&#24314;&#23376;&#24207;&#21015;&#26469;&#26500;&#24314;&#27491;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;REBAR&#35823;&#24046;&#21487;&#20197;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#65292;&#24182;&#19988;&#22312;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#38598;&#25104;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#26377;&#29992;&#20449;&#24687;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#37492;&#21035;&#20986;&#30340;&#27491;&#26679;&#26412;&#23545;&#65292;&#24403;&#23427;&#20204;&#34987;&#25512;&#21040;&#23884;&#20837;&#31354;&#38388;&#26102;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#30340;&#19979;&#28216;&#20219;&#21153;&#32534;&#30721;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#21019;&#24314;&#27491;&#26679;&#26412;&#23545;&#21487;&#33021;&#20250;&#30772;&#22351;&#21407;&#22987;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#25105;&#20204;&#33021;&#20174;&#19968;&#20010;&#23376;&#24207;&#21015;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#25104;&#21151;&#37325;&#24314;&#21478;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#37027;&#20040;&#23427;&#20204;&#24212;&#35813;&#26159;&#19968;&#20010;&#27491;&#26679;&#26412;&#23545;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26816;&#32034;&#37325;&#24314;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;REBAR&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21367;&#31215;&#20132;&#21449;&#27880;&#24847;&#21147;&#26550;&#26500;&#35745;&#31639;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;REBAR&#35823;&#24046;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#39564;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;REBAR&#35823;&#24046;&#26159;&#20114;&#30456;&#31867;&#21035;&#25104;&#21592;&#30340;&#39044;&#27979;&#22120;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#23427;&#20316;&#20026;&#27491;/&#36127;&#26631;&#35760;&#22120;&#30340;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#19968;&#26086;&#38598;&#25104;&#21040;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#30340;REBAR&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
The success of self-supervised contrastive learning hinges on identifying positive data pairs that, when pushed together in embedding space, encode useful information for subsequent downstream tasks. However, in time-series, this is challenging because creating positive pairs via augmentations may break the original semantic meaning. We hypothesize that if we can retrieve information from one subsequence to successfully reconstruct another subsequence, then they should form a positive pair. Harnessing this intuition, we introduce our novel approach: REtrieval-BAsed Reconstruction (REBAR) contrastive learning. First, we utilize a convolutional cross-attention architecture to calculate the REBAR error between two different time-series. Then, through validation experiments, we show that the REBAR error is a predictor of mutual class membership, justifying its usage as a positive/negative labeler. Finally, once integrated into a contrastive learning framework, our REBAR method can learn an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.16638</link><description>&lt;p&gt;
&#36866;&#24212;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Covariate Shift Adaptation Robust to Density-Ratio Estimation. (arXiv:2310.16638v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#32597;&#35265;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#26469;&#20943;&#36731;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20559;&#24046;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#20855;&#26377;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#27979;&#35797;&#25968;&#25454;&#21482;&#21253;&#21547;&#21327;&#21464;&#37327;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#21327;&#21464;&#37327;&#20559;&#31227;&#19979;&#35757;&#32451;&#21442;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#20998;&#24067;&#19981;&#21516;&#12290;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26469;&#36827;&#34892;&#21327;&#21464;&#37327;&#20559;&#31227;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#25439;&#22833;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#27599;&#20010;&#26435;&#37325;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#21327;&#21464;&#37327;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#65292;&#20197;&#36817;&#20284;&#27979;&#35797;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#23427;&#20801;&#35768;&#25105;&#20204;&#33719;&#24471;&#19968;&#20010;&#26368;&#23567;&#21270;&#27979;&#35797;&#25968;&#25454;&#39118;&#38505;&#30340;&#27169;&#22411;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23494;&#24230;&#27604;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#65292;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#35823;&#24046;&#20063;&#20250;&#23548;&#33268;&#22238;&#24402;&#27169;&#22411;&#30340;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a scenario where we have access to train data with both covariates and outcomes while test data only contains covariates. In this scenario, our primary aim is to predict the missing outcomes of the test data. With this objective in mind, we train parametric regression models under a covariate shift, where covariate distributions are different between the train and test data. For this problem, existing studies have proposed covariate shift adaptation via importance weighting using the density ratio. This approach averages the train data losses, each weighted by an estimated ratio of the covariate densities between the train and test data, to approximate the test-data risk. Although it allows us to obtain a test-data risk minimizer, its performance heavily relies on the accuracy of the density ratio estimation. Moreover, even if the density ratio can be consistently estimated, the estimation errors of the density ratio also yield bias in the estimators of the regression model's 
&lt;/p&gt;</description></item><item><title>Nebula&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21160;&#24577;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12290;&#23427;&#33021;&#22815;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#24182;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Nebula&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.10664</link><description>&lt;p&gt;
Nebula:&#29992;&#20110;&#21160;&#24577;&#24694;&#24847;&#36719;&#20214;&#20998;&#26512;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Nebula: Self-Attention for Dynamic Malware Analysis. (arXiv:2310.10664v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10664
&lt;/p&gt;
&lt;p&gt;
Nebula&#26159;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21160;&#24577;&#20998;&#26512;&#24694;&#24847;&#36719;&#20214;&#12290;&#23427;&#33021;&#22815;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#24182;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;Nebula&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20998;&#26512;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#25191;&#34892;&#31243;&#24207;&#24182;&#23558;&#20854;&#34892;&#20026;&#23384;&#20648;&#22312;&#26085;&#24535;&#25253;&#21578;&#20013;&#65292;&#21487;&#20197;&#26816;&#27979;Windows&#24694;&#24847;&#36719;&#20214;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#24320;&#22987;&#22312;&#36825;&#20123;&#25253;&#21578;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#36827;&#34892;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#25110;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#21367;&#31215;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65292;&#21482;&#20851;&#27880;&#36816;&#34892;&#26102;&#35843;&#29992;&#30340;API&#65292;&#24182;&#26410;&#32771;&#34385;&#20854;&#20182;&#30456;&#20851;&#30340;&#24322;&#26500;&#20449;&#24687;&#26469;&#28304;&#65292;&#22914;&#32593;&#32476;&#21644;&#25991;&#20214;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#24456;&#38590;&#33719;&#21462;&#65292;&#36825;&#38480;&#21046;&#20102;&#35813;&#30740;&#31350;&#39046;&#22495;&#20013;&#32467;&#26524;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Nebula&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#12289;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#36716;&#25442;&#22120;&#31070;&#32463;&#26550;&#26500;&#65292;&#21487;&#20197;&#27010;&#25324;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#31034;&#21644;&#26684;&#24335;&#65292;&#32467;&#21512;&#21160;&#24577;&#26085;&#24535;&#25253;&#21578;&#20013;&#30340;&#24322;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Nebula&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#22312;&#19977;&#20010;&#37325;&#35201;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic analysis enables detecting Windows malware by executing programs in a controlled environment, and storing their actions in log reports. Previous work has started training machine learning models on such reports to perform either malware detection or malware classification. However, most of the approaches (i) have only considered convolutional and long-short term memory networks, (ii) they have been built focusing only on APIs called at runtime, without considering other relevant though heterogeneous sources of information like network and file operations, and (iii) the code and pretrained models are hardly available, hindering reproducibility of results in this research area. In this work, we overcome these limitations by presenting Nebula, a versatile, self-attention transformer-based neural architecture that can generalize across different behavior representations and formats, combining heterogeneous information from dynamic log reports. We show the efficacy of Nebula on thre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.02505</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#23558;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#30446;&#26631;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25918;&#22312;&#25193;&#25955;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#26031;&#22122;&#22768;&#21019;&#24314;&#38543;&#26426;&#36712;&#36857;&#65292;&#20351;&#20854;&#36828;&#31163;&#25968;&#25454;&#27969;&#24418;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#36828;&#31163;&#28508;&#22312;&#30446;&#26631;&#29366;&#24577;&#30340;&#36712;&#36857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#20284;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20010;&#31216;&#20026;Merlin&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#36873;&#25321;&#65292;&#29992;&#20110;&#21462;&#20195;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411; - &#32531;&#20914;&#21306;&#20013;&#30340;&#21453;&#21521;&#25773;&#25918;&#65292;&#21453;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02469</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#33391;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Good Privacy Protection Learners. (arXiv:2310.02469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#19981;&#21516;&#25216;&#26415;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#30340;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#36523;&#20221;&#20449;&#24687;&#65288;PII&#65289;&#12290;&#22312;&#27809;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#24494;&#35843; LLMs &#20250;&#23384;&#22312;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#20445;&#25252;&#35821;&#35328;&#27169;&#22411;&#65288;PPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#26377;&#25928;&#27880;&#20837;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27169;&#22411;&#35774;&#35745;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#27604;&#22914;&#35821;&#26009;&#24211;&#31574;&#23637;&#12289;&#22522;&#20110;&#24809;&#32602;&#30340;&#38750;&#27010;&#28982;&#24615;&#35757;&#32451;&#25439;&#22833;&#20197;&#21450;&#22522;&#20110;&#25351;&#20196;&#30340;&#24494;&#35843;&#31561;&#31561;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#20351;&#29992;&#27491;&#21521;&#21644;&#36127;&#21521;&#31034;&#20363;&#36827;&#34892;&#25351;&#20196;&#24494;&#35843;&#65292;&#26174;&#31034;&#20986;&#24456;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28857;&#23545;&#28857;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#31574;&#30053;&#30340;&#39044;&#26465;&#20214;PI&#20849;&#35782;&#31639;&#27861;&#65292;&#20445;&#35777;&#20102;&#20854;&#22312;&#21463;&#38480;&#24378;&#20984;&#20989;&#25968;&#19979;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#26080;&#38656;&#20010;&#20307;&#23616;&#37096;&#20195;&#20215;&#20989;&#25968;&#30340;&#20984;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#39044;&#26465;&#20214;&#36827;&#19968;&#27493;&#21152;&#36895;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00419</link><description>&lt;p&gt;
&#21463;&#38480;&#24378;&#20984;&#24615;&#19979;&#30340;&#39044;&#26465;&#20214;PI&#20849;&#35782;&#31639;&#27861;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Pre-Conditioned PI Consensus Algorithm under Restricted Strong Convexity. (arXiv:2310.00419v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28857;&#23545;&#28857;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#31574;&#30053;&#30340;&#39044;&#26465;&#20214;PI&#20849;&#35782;&#31639;&#27861;&#65292;&#20445;&#35777;&#20102;&#20854;&#22312;&#21463;&#38480;&#24378;&#20984;&#20989;&#25968;&#19979;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#26080;&#38656;&#20010;&#20307;&#23616;&#37096;&#20195;&#20215;&#20989;&#25968;&#30340;&#20984;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#39044;&#26465;&#20214;&#36827;&#19968;&#27493;&#21152;&#36895;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#28857;&#23545;&#28857;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#35299;&#20915;&#20998;&#24067;&#24335;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#32593;&#32476;&#34987;&#20551;&#23450;&#20026;&#21516;&#27493;&#21644;&#36830;&#36890;&#30340;&#12290;&#37319;&#29992;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#31574;&#30053;&#65292;&#24320;&#21457;&#20102;&#22810;&#31181;&#20855;&#26377;&#22266;&#23450;&#27493;&#38271;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#26368;&#26089;&#30340;&#26159;PI&#20849;&#35782;&#31639;&#27861;&#12290;&#21033;&#29992;&#26446;&#38597;&#26222;&#35834;&#22827;&#29702;&#35770;&#65292;&#25105;&#20204;&#39318;&#27425;&#20445;&#35777;&#20102;&#20855;&#26377;&#36895;&#29575;&#21305;&#37197;&#31163;&#25955;&#21270;&#30340;&#21463;&#38480;&#24378;&#20984;&#20989;&#25968;&#30340;PI&#20849;&#35782;&#31639;&#27861;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#20010;&#20307;&#23616;&#37096;&#20195;&#20215;&#20989;&#25968;&#30340;&#20984;&#24615;&#12290;&#20026;&#20102;&#21152;&#36895;PI&#20849;&#35782;&#31639;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#23616;&#37096;&#39044;&#26465;&#20214;&#30340;&#24418;&#24335;&#65292;&#21363;&#24120;&#25968;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#39564;&#35777;&#20854;&#30456;&#27604;&#20110;&#31361;&#20986;&#30340;&#20998;&#24067;&#24335;&#20984;&#20248;&#21270;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers solving distributed convex optimization problems in peer-to-peer multi-agent networks. The network is assumed to be synchronous and connected. By using the proportional-integral (PI) control strategy, various algorithms with fixed stepsize have been developed. The earliest among them is the PI consensus algorithm. Using Lyapunov theory, we guarantee exponential convergence of the PI consensus algorithm for restricted strongly convex functions with rate-matching discretization, without requiring convexity of individual local cost functions, for the first time. In order to accelerate the PI consensus algorithm, we incorporate local pre-conditioning in the form of constant positive definite matrices and numerically validate its efficiency compared to the prominent distributed convex optimization algorithms. Unlike classical pre-conditioning, where only the gradients are multiplied by a pre-conditioner, the proposed pre-conditioning modifies both the gradients and the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;&#30340;&#25913;&#36827;&#21644;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-means&#32858;&#31867;&#38382;&#39064;&#20013;&#33021;&#22815;&#33719;&#24471;$9 + \varepsilon$&#36817;&#20284;&#27604;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.16384</link><description>&lt;p&gt;
&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Swap $k$-Means++. (arXiv:2309.16384v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;&#30340;&#25913;&#36827;&#21644;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-means&#32858;&#31867;&#38382;&#39064;&#20013;&#33021;&#22815;&#33719;&#24471;$9 + \varepsilon$&#36817;&#20284;&#27604;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arthur&#21644;Vassilvitskii&#25552;&#20986;&#30340;$k$-means++&#31639;&#27861;&#36890;&#24120;&#34987;&#23454;&#36341;&#32773;&#36873;&#25321;&#29992;&#20110;&#20248;&#21270;&#27969;&#34892;&#30340;$k$-means&#32858;&#31867;&#30446;&#26631;&#65292;&#24182;&#22312;&#26399;&#26395;&#20013;&#33719;&#24471;$O(\log k)$&#30340;&#36817;&#20284;&#24230;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;Lattanzi&#21644;Sohler&#25552;&#20986;&#20102;&#36890;&#36807;$k$-means++&#37319;&#26679;&#20998;&#24067;&#33719;&#24471;&#30340;$O(k \log \log k)$&#20010;&#23616;&#37096;&#25628;&#32034;&#27493;&#39588;&#30340;&#22686;&#24378;$k$-means++&#31639;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;$k$-means&#32858;&#31867;&#38382;&#39064;&#30340;$c$&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;$c$&#26159;&#19968;&#20010;&#36739;&#22823;&#30340;&#24120;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#23616;&#37096;&#25628;&#32034;&#37051;&#22495;&#26469;&#25512;&#24191;&#21644;&#25193;&#23637;&#20182;&#20204;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#21516;&#26102;&#20132;&#25442;&#22810;&#20010;&#20013;&#24515;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;$9 + \varepsilon$&#30340;&#36817;&#20284;&#27604;&#65292;&#36825;&#26159;&#23616;&#37096;&#25628;&#32034;&#21487;&#33021;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#19982;Lattanzi&#21644;Sohler&#30340;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \log \log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.06090</link><description>&lt;p&gt;
&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#30340;&#21160;&#24577;&#19982;&#25511;&#21046;&#27169;&#22411;&#30340;&#36890;&#29992;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Verification Framework for Dynamical and Control Models via Certificate Synthesis. (arXiv:2309.06090v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35770;&#30340;&#19968;&#20010;&#26032;&#20852;&#20998;&#25903;&#19987;&#38376;&#30740;&#31350;&#35777;&#20070;&#23398;&#20064;&#65292;&#28041;&#21450;&#23545;&#33258;&#20027;&#25110;&#25511;&#21046;&#27169;&#22411;&#30340;&#25152;&#38656;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#31995;&#32479;&#34892;&#20026;&#30340;&#35268;&#33539;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20989;&#25968;&#30340;&#35777;&#26126;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#22797;&#26434;&#35201;&#27714;&#30340;&#25511;&#21046;&#22120;&#30340;&#21512;&#25104;&#36890;&#24120;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#19987;&#23478;&#25511;&#21046;&#24037;&#31243;&#24072;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33258;&#21160;&#25216;&#26415;&#33021;&#22815;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#21508;&#31181;&#22797;&#26434;&#35268;&#33539;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#32534;&#30721;&#31995;&#32479;&#35268;&#33539;&#24182;&#23450;&#20041;&#30456;&#24212;&#30340;&#35777;&#20070;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#27491;&#24335;&#21512;&#25104;&#25511;&#21046;&#22120;&#21644;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#25552;&#20379;&#20505;&#36873;&#30340;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of co
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2308.09730</link><description>&lt;p&gt;
&#22522;&#20110;COVID-19&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#25104;&#20687;&#30340;AI&#35786;&#26029;&#65306;&#20197;&#30149;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#26032;&#22411;&#20896;&#29366;&#30149;&#27602;&#65288;COVID-19&#65289;&#30340;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#35768;&#22810;&#25253;&#36947;&#31216;&#20854;&#24615;&#33021;&#20960;&#20046;&#23436;&#32654;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#30340;&#21464;&#24322;&#24615;&#21644;&#28508;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#24341;&#21457;&#20102;&#23545;&#20020;&#24202;&#36866;&#29992;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#28041;&#21450;&#20351;&#29992;&#20020;&#24202;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;COVID-19&#35786;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#65292;&#20197;&#35780;&#20272;AI&#24615;&#33021;&#21463;&#30142;&#30149;&#33539;&#22260;&#12289;&#36752;&#23556;&#21058;&#37327;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#33016;&#37096;&#25918;&#23556;&#25668;&#24433;&#65288;CXR&#65289;&#25104;&#20687;&#27169;&#24577;&#31561;&#20960;&#20010;&#24739;&#32773;&#21644;&#29289;&#29702;&#24615;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#21253;&#25324;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#24739;&#30149;&#29575;&#65289;&#24378;&#28872;&#24433;&#21709;&#20102;AI&#30340;&#24615;&#33021;&#65292;&#23548;&#33268;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#19979;&#38477;&#20102;&#39640;&#36798;20&#65285;&#65292;&#19988;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2307.04075</link><description>&lt;p&gt;
&#22522;&#20110;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#30340;&#30284;&#30151;&#26032;&#20122;&#22411;&#21644;&#27835;&#30103;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Attention Mechanism Learning for Cancer New Subtypes and Treatment Based on Cancer Multi-Omics Data. (arXiv:2307.04075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#24182;&#35782;&#21035;&#30284;&#30151;&#26032;&#20122;&#22411;&#12290;&#36890;&#36807;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#24182;&#36827;&#34892;&#20122;&#22411;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30284;&#30151;&#30340;&#39640;&#24322;&#36136;&#24615;&#21644;&#20020;&#24202;&#29305;&#24449;&#65292;&#19981;&#21516;&#30284;&#30151;&#20122;&#22411;&#20043;&#38388;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#21644;&#20020;&#24202;&#29305;&#24449;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#30284;&#30151;&#20122;&#22411;&#30340;&#35782;&#21035;&#21644;&#21457;&#29616;&#23545;&#20110;&#30284;&#30151;&#30340;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#39044;&#21518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;AMUCL&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;&#30284;&#30151;&#22810;&#32452;&#23398;&#25968;&#25454;&#65292;&#20174;&#32780;&#35782;&#21035;&#21644;&#34920;&#24449;&#30284;&#30151;&#20122;&#22411;&#12290;AMUCL&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#28145;&#24230;&#25552;&#21462;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#32806;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;DMACL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#32452;&#23398;&#25968;&#25454;&#29305;&#24449;&#21644;&#32858;&#31867;&#65292;&#24182;&#35782;&#21035;&#26032;&#30340;&#30284;&#30151;&#20122;&#22411;&#12290;&#36825;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#31354;&#38388;&#20013;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#32858;&#31867;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omics data and clinical features among subtypes of different cancers. Therefore, the identification and discovery of cancer subtypes are crucial for the diagnosis, treatment, and prognosis of cancer. In this study, we proposed a generalization framework based on attention mechanisms for unsupervised contrastive learning (AMUCL) to analyze cancer multi-omics data for the identification and characterization of cancer subtypes. AMUCL framework includes a unsupervised multi-head attention mechanism, which deeply extracts multi-omics data features. Importantly, a decoupled contrastive learning model (DMACL) based on a multi-head attention mechanism is proposed to learn multi-omics data features and clusters and identify new cancer subtypes. This unsupervised contrastive learning method clusters subtypes by calculating the similarity between samples in the feature space and sample
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.14522</link><description>&lt;p&gt;
&#38750;&#20984;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230;&#27861;&#21450;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#38750;&#20984;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;Lipschitz&#24179;&#28369;&#24615;, &#20294;&#36825;&#19968;&#35201;&#27714;&#23545;&#20110;&#21253;&#25324;&#20108;&#27425;&#36870;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38382;&#39064;&#31867;&#21035;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#26063;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230; (SBPG) &#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;&#24179;&#28369;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.08157</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#22240;&#26524;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#37329;&#34701;&#21644;&#25237;&#36164;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#29420;&#29305;&#30340;&#21306;&#22359;&#38142;&#30456;&#20851;&#29305;&#24615;&#65292;&#22914;&#38544;&#31169;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#19981;&#21487;&#36861;&#36394;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#21463;&#27426;&#36814;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#27874;&#21160;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21152;&#23494;&#36135;&#24065;&#20173;&#28982;&#26159;&#19968;&#31181;&#39640;&#39118;&#38505;&#25237;&#36164;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;DBN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20803;&#35774;&#32622;&#19979;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#65292;&#20197;&#39044;&#27979;&#20116;&#31181;&#27969;&#34892;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#26684;&#36816;&#21160;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.16905</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65306;&#36125;&#21494;&#26031;&#25512;&#29702;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplace-Approximated Neural Additive Models: Improving Interpretability with Bayesian Inference. (arXiv:2305.16905v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#22312;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#20013;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65292;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#38544;&#24335;&#29305;&#24449;&#36873;&#25321;&#24182;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#24615;&#36136;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;NAM&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#32593;&#32476;&#20998;&#20026;&#21152;&#24615;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#20351;&#36755;&#20837;&#29305;&#24449;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#20114;&#21464;&#24471;&#26126;&#26174;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#36125;&#21494;&#26031;&#35282;&#24230;&#32771;&#34385;&#21152;&#24615;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65306;a&#65289;&#23427;&#36890;&#36807;&#20272;&#35745;&#23376;&#32593;&#32476;&#30340;&#20989;&#25968;&#31354;&#38388;&#19981;&#30830;&#23450;&#24615;&#20026;&#24674;&#22797;&#30340;&#29305;&#24449;&#20132;&#20114;&#25552;&#20379;&#21487;&#20449;&#21306;&#38388;&#65307;b&#65289;&#23427;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#32463;&#39564;&#36125;&#21494;&#26031;&#36807;&#31243;&#25191;&#34892;&#29305;&#24449;&#30340;&#38544;&#24335;&#36873;&#25321;&#65307;c&#65289;&#23427;&#21487;&#29992;&#20110;&#23545;&#29305;&#24449;&#23545;&#36827;&#34892;&#25490;&#21517;&#65292;&#20316;&#20026;&#31934;&#32454;&#35843;&#25972;&#30340;&#20132;&#20114;&#27169;&#22411;&#20505;&#36873;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#36924;&#36817;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#65288;LA-NAM&#65289;&#25552;&#39640;&#20102;NAM&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#23398;&#20064;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#20132;&#20114;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have found successful applications in many fields, but their black-box nature hinders interpretability. This is addressed by the neural additive model (NAM), in which the network is divided into additive sub-networks, thus making apparent the interaction between input features and predictions. In this paper, we approach the additive structure from a Bayesian perspective and develop a practical Laplace approximation. This enhances interpretability in three primary ways: a) It provides credible intervals for the recovered feature interactions by estimating function-space uncertainty of the sub-networks; b) it yields a tractable estimate of the marginal likelihood, which can be used to perform an implicit selection of features through an empirical Bayes procedure; and c) it can be used to rank feature pairs as candidates for second-order interactions in fine-tuned interaction models. We show empirically that our proposed Laplace-approximated NAM (LA-NAM) improv
&lt;/p&gt;</description></item><item><title>C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.16209</link><description>&lt;p&gt;
C-MCTS: &#23433;&#20840;&#35268;&#21010;&#19982;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
C-MCTS: Safe Planning with Monte Carlo Tree Search. (arXiv:2305.16209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16209
&lt;/p&gt;
&lt;p&gt;
C-MCTS &#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26377;&#32422;&#26463;&#30340;&#20915;&#31574;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23433;&#20840;&#35780;&#21028;&#22120;&#36827;&#34892;&#25104;&#26412;&#20272;&#35745;&#65292;&#24182;&#22312;&#37096;&#32626;&#26399;&#38388;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22870;&#21169;&#21644;&#26356;&#39640;&#25928;&#30340;&#35268;&#21010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#21487;&#20197;&#35299;&#20915;&#21463;&#32422;&#26463;&#30340;&#23433;&#20840;&#20915;&#31574;&#38382;&#39064;&#12290;&#23613;&#31649;CMDP&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;MCTS&#31561;&#22522;&#20110;&#37319;&#26679;&#30340;&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;CMDP&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#22312;&#25104;&#26412;&#26041;&#38754;&#20445;&#23432;&#34892;&#20107;&#65292;&#36890;&#36807;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25104;&#26412;&#20272;&#35745;&#26469;&#36991;&#20813;&#36829;&#21453;&#32422;&#26463;&#65292;&#20294;&#36825;&#31181;&#20272;&#35745;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;MCTS&#65288;C-MCTS&#65289;&#65292;&#23427;&#20351;&#29992;&#20808;&#21069;&#22312;&#20195;&#29702;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#35757;&#32451;&#30340;&#23433;&#20840;&#35780;&#21028;&#22120;&#26469;&#20272;&#35745;&#25104;&#26412;&#12290;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#35780;&#21028;&#22120;&#36890;&#36807;&#21098;&#26525;&#19981;&#23433;&#20840;&#36712;&#36857;&#26469;&#38480;&#21046;&#25506;&#32034;&#12290;C-MCTS&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65292;&#20294;&#25805;&#20316;&#25509;&#36817;&#32422;&#26463;&#36793;&#30028;&#65292;&#27604;&#20197;&#24448;&#30340;&#24037;&#20316;&#33719;&#24471;&#26356;&#39640;&#30340;&#22870;&#21169;&#12290;&#20316;&#20026;&#19968;&#20010;&#24456;&#22909;&#30340;&#21103;&#20135;&#21697;&#65292;&#36825;&#20010;&#35268;&#21010;&#22120;&#22312;&#35268;&#21010;&#27493;&#39588;&#26041;&#38754;&#26356;&#21152;&#39640;&#25928;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27169;&#22411;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
The Constrained Markov Decision Process (CMDP) formulation allows to solve safety-critical decision making tasks that are subject to constraints. While CMDPs have been extensively studied in the Reinforcement Learning literature, little attention has been given to sampling-based planning algorithms such as MCTS for solving them. Previous approaches perform conservatively with respect to costs as they avoid constraint violations by using Monte Carlo cost estimates that suffer from high variance. We propose Constrained MCTS (C-MCTS), which estimates cost using a safety critic that is trained with Temporal Difference learning in an offline phase prior to agent deployment. The critic limits exploration by pruning unsafe trajectories within MCTS during deployment. C-MCTS satisfies cost constraints but operates closer to the constraint boundary, achieving higher rewards than previous work. As a nice byproduct, the planner is more efficient w.r.t. planning steps. Most importantly, under model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13948</link><description>&lt;p&gt;
&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Decoupled Kullback-Leibler Divergence Loss. (arXiv:2305.13948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#35299;&#20915;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#21644;&#24341;&#20837;&#20840;&#23616;&#20449;&#24687;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#25506;&#31350;&#20102;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#21457;&#29616;&#23427;&#19982;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#31561;&#20215;&#65292;&#21518;&#32773;&#30001;&#21152;&#26435;&#22343;&#26041;&#24046;&#25439;&#22833;&#21644;&#21253;&#21547;&#36719;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#32452;&#25104;&#12290;&#36890;&#36807;&#23545;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#30830;&#23450;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30693;&#35782;&#33976;&#39311;&#31561;&#22330;&#26223;&#19979;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#38480;&#21046;&#38382;&#39064;&#12290;&#36825;&#20010;&#25913;&#36827;&#20445;&#35777;&#20102;&#22312;&#35757;&#32451;&#26399;&#38388;wMSE&#32452;&#20214;&#22987;&#32456;&#26377;&#25928;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#26500;&#36896;&#24615;&#26263;&#31034;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#20840;&#23616;&#20449;&#24687;&#24341;&#20837;&#35299;&#32806;&#24335;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#29992;&#20110;&#31867;&#20869;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#25913;&#36827;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;KL&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;CIFAR-10/100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#37325;&#28857;&#26159;&#23545;&#25239;&#35757;&#32451;&#21644;&#30693;&#35782;&#33976;&#39311;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve deeper into the Kullback-Leibler (KL) Divergence loss and observe that it is equivalent to the Doupled Kullback-Leibler (DKL) Divergence loss that consists of 1) a weighted Mean Square Error (wMSE) loss and 2) a Cross-Entropy loss incorporating soft labels. From our analysis of the DKL loss, we have identified two areas for improvement. Firstly, we address the limitation of DKL in scenarios like knowledge distillation by breaking its asymmetry property in training optimization. This modification ensures that the wMSE component is always effective during training, providing extra constructive cues. Secondly, we introduce global information into DKL for intra-class consistency regularization. With these two enhancements, we derive the Improved Kullback-Leibler (IKL) Divergence loss and evaluate its effectiveness by conducting experiments on CIFAR-10/100 and ImageNet datasets, focusing on adversarial training and knowledge distillation tasks. The proposed approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2304.09875</link><description>&lt;p&gt;
GREAT&#20998;&#25968;&#65306;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GREAT Score: Global Robustness Evaluation of Adversarial Perturbation using Generative Models. (arXiv:2304.09875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;&#35813;&#20998;&#25968;&#25429;&#25417;&#20102;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#20110;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#19968;&#32452;&#25968;&#25454;&#26679;&#26412;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#32467;&#26524;&#19978;&#65292;&#20197;&#35780;&#20272;&#21644;&#25490;&#21517;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#32479;&#35745;&#37327;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20195;&#34920;&#22522;&#30784;&#26410;&#30693;&#25968;&#25454;&#20998;&#24067;&#30340;&#30495;&#27491;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;GREAT&#20998;&#25968;&#65292;&#29992;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25200;&#21160;&#36827;&#34892;&#20840;&#23616;&#40065;&#26834;&#24615;&#35780;&#20272;&#12290;GREAT&#20998;&#25968;&#27491;&#24335;&#20855;&#26377;&#19968;&#20010;&#20840;&#23616;&#32479;&#35745;&#37327;&#30340;&#29289;&#29702;&#24847;&#20041;&#65292;&#25429;&#25417;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#26679;&#26412;&#20013;&#30340;&#24179;&#22343;&#35748;&#35777;&#38450;&#25915;&#20987;&#25200;&#21160;&#27700;&#24179;&#12290;&#23545;&#20110;&#26377;&#38480;&#26679;&#26412;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26679;&#26412;&#22343;&#20540;&#19982;&#30495;&#23454;&#22343;&#20540;&#20043;&#38388;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;GREAT&#20998;&#25968;&#26377;&#20960;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20351;&#29992;GREAT&#20998;&#25968;&#36827;&#34892;&#40065;&#26834;&#24615;&#35780;&#20272;&#39640;&#25928;&#32780;&#19988;&#35268;&#27169;&#21487;&#25193;&#23637;&#65292;&#26080;&#38656;&#36816;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current studies on adversarial robustness mainly focus on aggregating local robustness results from a set of data samples to evaluate and rank different models. However, the local statistics may not well represent the true global robustness of the underlying unknown data distribution. To address this challenge, this paper makes the first attempt to present a new framework, called GREAT Score , for global robustness evaluation of adversarial perturbation using generative models. Formally, GREAT Score carries the physical meaning of a global statistic capturing a mean certified attack-proof perturbation level over all samples drawn from a generative model. For finite-sample evaluation, we also derive a probabilistic guarantee on the sample complexity and the difference between the sample mean and the true mean. GREAT Score has several advantages: (1) Robustness evaluations using GREAT Score are efficient and scalable to large models, by sparing the need of running adversarial attacks. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#65292;&#25512;&#23548;&#24182;&#24471;&#20986;&#20102;&#22312;&#21508;&#31181;&#36941;&#21382;&#24615;&#20551;&#35774;&#19979;&#28176;&#36817;&#26041;&#24046;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01111</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#21464;&#37327;&#22312;MCMC&#20013;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees for neural control variates in MCMC. (arXiv:2304.01111v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#25511;&#21046;&#21464;&#37327;&#30340;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#65292;&#25512;&#23548;&#24182;&#24471;&#20986;&#20102;&#22312;&#21508;&#31181;&#36941;&#21382;&#24615;&#20551;&#35774;&#19979;&#28176;&#36817;&#26041;&#24046;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#25511;&#21046;&#21464;&#37327;&#21644;&#26368;&#23567;&#21270;&#28176;&#36817;&#26041;&#24046;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#26041;&#24046;&#32553;&#20943;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25511;&#21046;&#21464;&#37327;&#34920;&#31034;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;&#22312;&#22522;&#30784;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#21508;&#31181;&#36941;&#21382;&#24615;&#20551;&#35774;&#19979;&#65292;&#25512;&#23548;&#20102;&#28176;&#36817;&#26041;&#24046;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#26041;&#24046;&#32553;&#20943;&#31639;&#27861;&#21644;&#20989;&#25968;&#36924;&#36817;&#29702;&#35770;&#30340;&#38543;&#26426;&#35823;&#24046;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a variance reduction approach for Markov chains based on additive control variates and the minimization of an appropriate estimate for the asymptotic variance. We focus on the particular case when control variates are represented as deep neural networks. We derive the optimal convergence rate of the asymptotic variance under various ergodicity assumptions on the underlying Markov chain. The proposed approach relies upon recent results on the stochastic errors of variance reduction algorithms and function approximation theory.
&lt;/p&gt;</description></item><item><title>LANET&#26159;&#19968;&#31181;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#24207;&#21015;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.00280</link><description>&lt;p&gt;
&#24207;&#21015;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65306;&#20320;&#20197;&#21069;&#30475;&#38169;&#20102;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Label Attention Network for sequential multi-label classification: you were looking at a wrong self-attention. (arXiv:2303.00280v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00280
&lt;/p&gt;
&lt;p&gt;
LANET&#26159;&#19968;&#31181;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#24207;&#21015;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21487;&#29992;&#30340;&#29992;&#25143;&#20449;&#24687;&#21487;&#20197;&#34920;&#31034;&#20026;&#26102;&#38388;&#25139;&#20107;&#20214;&#30340;&#24207;&#21015;&#12290;&#27599;&#20010;&#20107;&#20214;&#34987;&#20998;&#37197;&#20102;&#19968;&#32452;&#20998;&#31867;&#26631;&#31614;&#65292;&#20854;&#26410;&#26469;&#32467;&#26500;&#38750;&#24120;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#19979;&#19968;&#20010;&#23458;&#25143;&#36141;&#20080;&#30340;&#29289;&#21697;&#25110;&#26126;&#22825;&#30340;&#23458;&#25143;&#20132;&#26131;&#20013;&#30340;&#19968;&#32452;&#29289;&#21697;&#12290;&#36825;&#26159;&#19968;&#20010;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#20195;&#26041;&#27861;&#38024;&#23545;&#24207;&#21015;&#25968;&#25454;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#24341;&#20837;&#20102;&#20803;&#32032;&#30340;&#33258;&#20851;&#27880;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20107;&#20214;&#30340;&#26102;&#38388;&#20132;&#20114;&#20316;&#29992;&#65292;&#20294;&#26159;&#22833;&#21435;&#20102;&#26631;&#31614;&#20043;&#38388;&#30340;&#20449;&#24687;&#20381;&#36182;&#20851;&#31995;&#12290;&#21463;&#21040;&#36825;&#20010;&#32570;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#20808;&#20110;&#39044;&#27979;&#27493;&#39588;&#30340;&#26631;&#31614;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26631;&#31614;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#22240;&#27492;&#25105;&#20204;&#31216;&#20043;&#20026;LANET&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;LANET&#20248;&#20110;&#24050;&#24314;&#31435;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#26497;&#22823;&#22320;&#25429;&#25417;&#20102;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24494;&#35266;AUC&#26159;0.9&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the available user information can be represented as a sequence of timestamped events. Each event is assigned a set of categorical labels whose future structure is of great interest. For instance, our goal is to predict a group of items in the next customer's purchase or tomorrow's client transactions. This is a multi-label classification problem for sequential data. Modern approaches focus on transformer architecture for sequential data introducing self-attention for the elements in a sequence. In that case, we take into account events' time interactions but lose information on label inter-dependencies. Motivated by this shortcoming, we propose leveraging a self-attention mechanism over labels preceding the predicted step. As our approach is a Label-Attention NETwork, we call it LANET. Experimental evidence suggests that LANET outperforms the established models' performance and greatly captures interconnections between labels. For example, the micro-AUC of our approach is $0.9
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#12290;</title><link>http://arxiv.org/abs/2301.02060</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A first-order augmented Lagrangian method for constrained minimax optimization. (arXiv:2301.02060v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#38454;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20854;&#23376;&#38382;&#39064;&#34987;&#21457;&#29616;&#26159;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#32467;&#26500;&#21270;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20316;&#32773;&#22312; [26] &#20013;&#26368;&#36817;&#24320;&#21457;&#30340;&#19968;&#38454;&#26041;&#27861;&#26469;&#36866;&#24403;&#22320;&#35299;&#20915;&#12290;&#22312;&#19968;&#20123;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#20026;&#20102;&#25214;&#21040;&#32422;&#26463;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#19968;&#20010; $\varepsilon$-KKT &#35299;&#65292;&#35813;&#26041;&#27861;&#30340;&#25805;&#20316;&#22797;&#26434;&#24230;&#20026; ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$&#65292;&#35813;&#22797;&#26434;&#24230;&#26159;&#30001;&#22522;&#26412;&#25805;&#20316;&#27979;&#37327;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method recently developed in [26] by the authors. Under some suitable assumptions, an \emph{operation complexity} of ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$, measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $\varepsilon$-KKT solution of the constrained minimax problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.03923</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#20960;&#20309;&#35282;&#24230;&#29702;&#35299;VAEs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#25915;&#20987;&#26102;&#65292;&#23545;&#25163;&#20250;&#25214;&#21040;&#19968;&#20010;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#23567;&#25200;&#21160;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21464;&#20854;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#19968;&#20010;&#22266;&#23450;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#24050;&#30693;&#30340;&#21407;&#22240;&#26159;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#23558;&#20854;&#32534;&#30721;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20302;/&#38646;&#23494;&#24230;&#21306;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#26080;&#38480;&#21046;&#30340;&#29983;&#25104;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;&#32534;&#30721;&#22120;&#30340;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#27979;&#37327;&#23427;&#20174;&#36755;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#24494;&#23567;&#28508;&#22312;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20998;&#26512;&#36755;&#20837;&#25200;&#21160;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#25928;&#26524;&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
&lt;/p&gt;</description></item></channel></rss>