<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Empirical Transfer Function Estimate&#65288;ETFE&#65289;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#20934;&#30830;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#65292;&#24182;&#35777;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#20934;&#30830;&#21487;&#38752;&#12290;</title><link>https://arxiv.org/abs/2404.01100</link><description>&lt;p&gt;
&#26377;&#38480;&#26679;&#26412;&#39057;&#22495;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Finite Sample Frequency Domain Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;Empirical Transfer Function Estimate&#65288;ETFE&#65289;&#22312;&#29305;&#23450;&#39057;&#29575;&#22788;&#20934;&#30830;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#65292;&#24182;&#35777;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#20934;&#30830;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#26377;&#38480;&#26679;&#26412;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#39057;&#22495;&#31995;&#32479;&#35782;&#21035;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#24320;&#29615;&#24773;&#20917;&#19979;&#65292;&#28608;&#21169;&#36755;&#20837;&#26159;&#21608;&#26399;&#24615;&#30340;&#65292;&#24182;&#32771;&#34385;&#32463;&#39564;&#20256;&#36882;&#20989;&#25968;&#20272;&#35745;&#65288;ETFE&#65289;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#36755;&#20837;-&#36755;&#20986;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#22312;&#26576;&#20123;&#25152;&#38656;&#65288;&#22343;&#21248;&#38388;&#38548;&#30340;&#65289;&#39057;&#29575;&#22788;&#20272;&#35745;&#39057;&#29575;&#21709;&#24212;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#27425;&#39640;&#26031;&#24425;&#33394;&#22122;&#22768;&#65288;&#22312;&#26102;&#22495;&#65289;&#21644;&#31283;&#23450;&#24615;&#20551;&#35774;&#19979;&#65292;ETFE&#20272;&#35745;&#20540;&#38598;&#20013;&#22312;&#30495;&#23454;&#20540;&#21608;&#22260;&#12290;&#35823;&#24046;&#29575;&#20026;$\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$&#65292;&#20854;&#20013;$N_{\mathrm{tot}}$&#26159;&#26679;&#26412;&#30340;&#24635;&#25968;&#65292;$M$&#26159;&#25152;&#38656;&#39057;&#29575;&#30340;&#25968;&#37327;&#65292;$d_{\mathrm{u}},\,d_{\mathrm{y}}$&#20998;&#21035;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#20449;&#21495;&#30340;&#32500;&#25968;&#12290;&#36825;&#20010;&#36895;&#29575;&#23545;&#20110;&#19968;&#33324;&#30340;&#38750;&#29702;&#24615;&#20256;&#36882;&#20989;&#25968;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26377;&#38480;&#38454;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01100v1 Announce Type: cross  Abstract: We study non-parametric frequency-domain system identification from a finite-sample perspective. We assume an open loop scenario where the excitation input is periodic and consider the Empirical Transfer Function Estimate (ETFE), where the goal is to estimate the frequency response at certain desired (evenly-spaced) frequencies, given input-output samples. We show that under sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE estimates are concentrated around the true values. The error rate is of the order of $\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$, where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of desired frequencies, and $d_{\mathrm{u}},\,d_{\mathrm{y}}$ are the dimensions of the input and output signals respectively. This rate remains valid for general irrational transfer functions and does not require a finite order state-sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.06366</link><description>&lt;p&gt;
&#36719;Q-learning&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65306;&#20999;&#25442;&#31995;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Error Analysis of Soft Q-Learning: Switching System Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#38024;&#23545;&#36719;Q-learning&#31639;&#27861;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#20998;&#26512;&#65292;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft Q-learning&#26159;Q-learning&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#29109;&#27491;&#21017;&#21270;&#20540;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23545;&#36719;Q-learning&#30340;&#29702;&#35770;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#36719;Q-learning&#31639;&#27861;&#30340;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26377;&#38480;&#26102;&#38388;&#12289;&#25511;&#21046;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31181;&#31867;&#22411;&#30340;&#36719;Q-learning&#31639;&#27861;&#65306;&#19968;&#31181;&#21033;&#29992;&#23545;&#25968;&#21644;&#25351;&#25968;&#36816;&#31639;&#23376;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#29627;&#23572;&#20857;&#26364;&#36816;&#31639;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#36719;Q-learning&#31639;&#27861;&#25512;&#23548;&#20986;&#20102;&#26032;&#39062;&#30340;&#26377;&#38480;&#26102;&#38388;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#20998;&#26512;&#33021;&#22815;&#36890;&#36807;&#19982;&#20999;&#25442;&#31995;&#32479;&#27169;&#22411;&#24314;&#31435;&#32852;&#31995;&#26469;&#21152;&#28145;&#23545;&#36719;Q-learning&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#29978;&#33267;&#20026;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06366v1 Announce Type: new  Abstract: Soft Q-learning is a variation of Q-learning designed to solve entropy regularized Markov decision problems where an agent aims to maximize the entropy regularized value function. Despite its empirical success, there have been limited theoretical studies of soft Q-learning to date. This paper aims to offer a novel and unified finite-time, control-theoretic analysis of soft Q-learning algorithms. We focus on two types of soft Q-learning algorithms: one utilizing the log-sum-exp operator and the other employing the Boltzmann operator. By using dynamical switching system models, we derive novel finite-time error bounds for both soft Q-learning algorithms. We hope that our analysis will deepen the current understanding of soft Q-learning by establishing connections with switching system models and may even pave the way for new frameworks in the finite-time analysis of other reinforcement learning algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17988</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#22312;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Constrained Decoding for Code Language Models via Efficient Left and Right Quotienting of Context-Sensitive Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#25935;&#24863;&#35821;&#27861;&#36827;&#34892;&#39640;&#25928;&#24038;&#21491;&#21830;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#27861;&#27491;&#30830;&#24615;&#30340;&#26089;&#26399;&#25298;&#32477;&#21644;&#23545;&#23436;&#25972;&#31243;&#24207;&#30340;&#26377;&#25928;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#31243;&#24207;&#21512;&#25104;&#21644;&#39640;&#32423;&#33258;&#21160;&#23436;&#25104;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#20854;&#36755;&#20986;&#20195;&#30721;&#22312;&#35821;&#27861;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#37327;&#35299;&#26512;&#22120;&#65292;&#20801;&#35768;&#26089;&#26399;&#25298;&#32477;&#35821;&#27861;&#19978;&#19981;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#26816;&#27979;&#29992;&#20110;&#22635;&#20805;&#20219;&#21153;&#30340;&#23436;&#25972;&#31243;&#24207;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#33021;&#22815;&#22312;&#20219;&#24847;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#30340;&#24038;&#21491;&#21830;&#19978;&#25805;&#20316;&#30340;Earley&#24335;&#35299;&#26512;&#22120;&#65292;&#24182;&#23558;&#22686;&#37327;&#35299;&#26512;&#21644;&#21830;&#25805;&#20316;&#25193;&#23637;&#21040;&#35768;&#22810;&#24120;&#35265;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#19978;&#19979;&#25991;&#25935;&#24863;&#29305;&#24615;&#12290;&#36825;&#20123;&#36129;&#29486;&#30340;&#32467;&#26524;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#36890;&#29992;&#21644;&#25166;&#23454;&#30340;&#24038;&#21491;&#21830;&#35299;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17988v1 Announce Type: cross  Abstract: Large Language Models are powerful tools for program synthesis and advanced auto-completion, but come with no guarantee that their output code is syntactically correct. This paper contributes an incremental parser that allows early rejection of syntactically incorrect code, as well as efficient detection of complete programs for fill-in-the-middle (FItM) tasks. We develop Earley-style parsers that operate over left and right quotients of arbitrary context-free grammars, and we extend our incremental parsing and quotient operations to several context-sensitive features present in the grammars of many common programming languages. The result of these contributions is an efficient, general, and well-grounded method for left and right quotient parsing.   To validate our theoretical contributions -- and the practical effectiveness of certain design decisions -- we evaluate our method on the particularly difficult case of FItM completion for
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.01695</link><description>&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65306;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language-Guided World Models: A Model-Based Approach to AI Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01695
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27010;&#29575;&#19990;&#30028;&#27169;&#22411;&#23433;&#35013;&#21040;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20013;&#65292;&#20026;&#20154;&#31867;&#19982;&#36825;&#20123;&#20195;&#29702;&#27807;&#36890;&#21644;&#25511;&#21046;&#25171;&#24320;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#28192;&#36947;&#12290;&#38500;&#20102;&#26356;&#26032;&#20195;&#29702;&#31574;&#30053;&#65292;&#20154;&#31867;&#36824;&#21487;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#20869;&#37096;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#24433;&#21709;&#20195;&#29702;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#29616;&#26377;&#30340;&#19990;&#30028;&#27169;&#22411;&#38590;&#20197;&#36866;&#24212;&#20154;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#33258;&#28982;&#30340;&#36890;&#20449;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35821;&#35328;&#24341;&#23548;&#30340;&#19990;&#30028;&#27169;&#22411;&#65288;LWMs&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#38405;&#35835;&#35821;&#35328;&#25551;&#36848;&#26469;&#25429;&#25417;&#29615;&#22659;&#21160;&#24577;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#39640;&#20102;&#20195;&#29702;&#30340;&#27807;&#36890;&#25928;&#29575;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#31616;&#27905;&#30340;&#35821;&#35328;&#21453;&#39304;&#21516;&#26102;&#25913;&#21464;&#20182;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#23427;&#20204;&#36824;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#26368;&#21021;&#29992;&#20110;&#25351;&#23548;&#20154;&#31867;&#30340;&#25991;&#26412;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#20026;&#20102;&#20419;&#36827;LWMs&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;MESSENGER&#28216;&#25103;&#65288;Hanjie&#31561;&#20154;&#65292;2021&#65289;&#30340;&#25361;&#25112;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#26032;&#22330;&#26223;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Installing probabilistic world models into artificial agents opens an efficient channel for humans to communicate with and control these agents. In addition to updating agent policies, humans can modify their internal world models in order to influence their decisions. The challenge, however, is that currently existing world models are difficult for humans to adapt because they lack a natural communication interface. Aimed at addressing this shortcoming, we develop Language-Guided World Models (LWMs), which can capture environment dynamics by reading language descriptions. These models enhance agent communication efficiency, allowing humans to simultaneously alter their behavior on multiple tasks with concise language feedback. They also enable agents to self-learn from texts originally written to instruct humans. To facilitate the development of LWMs, we design a challenging benchmark based on the game of MESSENGER (Hanjie et al., 2021), requiring compositional generalization to new l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00515</link><description>&lt;p&gt;
&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20316;&#21453;&#24212;&#24615;&#26234;&#33021;&#20307;&#20197;&#22312;&#39640;&#24230;&#21160;&#33633;&#30340;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#24555;&#36895;&#23398;&#20064;&#24182;&#21709;&#24212;&#26032;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#34892;&#19994;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#22797;&#26434;&#30340;&#20851;&#32852;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#36235;&#21183;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#26368;&#22823;&#21270;&#26032;&#21046;&#23450;&#30340;&#25237;&#36164;&#32452;&#21512;&#30340;&#24635;&#22238;&#25253;&#65292;&#32780;&#24573;&#35270;&#20854;&#22312;&#20840;&#29699;&#25110;&#21306;&#22495;&#37096;&#38376;&#30340;&#21508;&#31181;&#24066;&#22330;&#26465;&#20214;&#21160;&#33633;&#19979;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASA&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21644;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#37319;&#29992;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20180;&#32454;&#21160;&#24577;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
&lt;/p&gt;</description></item><item><title>FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;</title><link>https://arxiv.org/abs/2311.15327</link><description>&lt;p&gt;
FRAC-Q-Learning: &#19968;&#31181;&#20855;&#26377;&#36991;&#20813;&#21388;&#28902;&#36807;&#31243;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning: A Reinforcement Learning with Boredom Avoidance Processes for Social Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15327
&lt;/p&gt;
&lt;p&gt;
FRAC-Q-Learning&#26159;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#65292;&#33021;&#36991;&#20813;&#29992;&#25143;&#21388;&#28902;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27604;&#20256;&#32479;&#31639;&#27861;&#22312;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#34987;&#24212;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#26410;&#38024;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#26032;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;FRAC-Q-Learning&#65292;&#21487;&#20197;&#36991;&#20813;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#38543;&#26426;&#21270;&#21644;&#20998;&#31867;&#36807;&#31243;&#22806;&#65292;&#36824;&#21253;&#25324;&#19968;&#20010;&#36951;&#24536;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#19982;&#20256;&#32479;Q-Learning&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;FRAC-Q-Learning&#30340;&#20852;&#36259;&#21644;&#21388;&#28902;&#31243;&#24230;&#20998;&#25968;&#12290;FRAC-Q-Learning&#26174;&#31034;&#20986;&#26126;&#26174;&#26356;&#39640;&#30340;&#20852;&#36259;&#20998;&#25968;&#36235;&#21183;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;Q-Learning&#26356;&#38590;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#12290;&#22240;&#27492;&#65292;FRAC-Q-Learning&#26377;&#21161;&#20110;&#24320;&#21457;&#19981;&#20250;&#35753;&#29992;&#25143;&#24863;&#21040;&#26080;&#32842;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#12290;&#35813;&#31639;&#27861;&#36824;&#21487;&#20197;&#22312;&#22522;&#20110;Web&#30340;&#36890;&#20449;&#21644;&#25945;&#32946;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15327v3 Announce Type: replace-cross  Abstract: The reinforcement learning algorithms have often been applied to social robots. However, most reinforcement learning algorithms were not optimized for the use of social robots, and consequently they may bore users. We proposed a new reinforcement learning method specialized for the social robot, the FRAC-Q-learning, that can avoid user boredom. The proposed algorithm consists of a forgetting process in addition to randomizing and categorizing processes. This study evaluated interest and boredom hardness scores of the FRAC-Q-learning by a comparison with the traditional Q-learning. The FRAC-Q-learning showed significantly higher trend of interest score, and indicated significantly harder to bore users compared to the traditional Q-learning. Therefore, the FRAC-Q-learning can contribute to develop a social robot that will not bore users. The proposed algorithm can also find applications in Web-based communication and educational 
&lt;/p&gt;</description></item><item><title>MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.16917</link><description>&lt;p&gt;
MimicTouch: &#20351;&#29992;&#22810;&#27169;&#24577;&#35302;&#35273;&#21453;&#39304;&#23398;&#20064;&#20154;&#31867;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MimicTouch: Learning Human's Control Strategy with Multi-Modal Tactile Feedback. (arXiv:2310.16917v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16917
&lt;/p&gt;
&lt;p&gt;
MimicTouch&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#26469;&#23398;&#20064;&#24182;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#35302;&#35273;&#22788;&#29702;&#30340;&#25972;&#21512;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#25191;&#34892;&#20687;&#23545;&#20934;&#21644;&#25554;&#20837;&#36825;&#26679;&#22797;&#26434;&#20219;&#21153;&#26102;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#26426;&#22120;&#20154;&#36965;&#25805;&#20316;&#25968;&#25454;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#21463;&#35302;&#35273;&#21453;&#39304;&#24341;&#23548;&#19979;&#30340;&#25511;&#21046;&#31574;&#30053;&#25152;&#25552;&#20379;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#20026;&#20102;&#21033;&#29992;&#20154;&#31867;&#24863;&#35273;&#65292;&#29616;&#26377;&#30340;&#20174;&#20154;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#35270;&#35273;&#21453;&#39304;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20154;&#31867;&#26412;&#33021;&#22320;&#21033;&#29992;&#35302;&#35273;&#21453;&#39304;&#23436;&#25104;&#22797;&#26434;&#25805;&#20316;&#30340;&#23453;&#36149;&#32463;&#39564;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;"MimicTouch"&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#20154;&#31867;&#31034;&#33539;&#32773;&#37027;&#37324;&#25910;&#38598;&#22810;&#27169;&#24577;&#35302;&#35273;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20154;&#31867;&#35302;&#35273;&#24341;&#23548;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#28041;&#21450;&#25351;&#20196;&#30340;&#20256;&#36882;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#30340;&#35302;&#35273;&#24341;&#23548;&#31574;&#30053;&#26469;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics and artificial intelligence, the integration of tactile processing is becoming increasingly pivotal, especially in learning to execute intricate tasks like alignment and insertion. However, existing works focusing on tactile methods for insertion tasks predominantly rely on robot teleoperation data and reinforcement learning, which do not utilize the rich insights provided by human's control strategy guided by tactile feedback. For utilizing human sensations, methodologies related to learning from humans predominantly leverage visual feedback, often overlooking the invaluable tactile feedback that humans inherently employ to finish complex manipulations. Addressing this gap, we introduce "MimicTouch", a novel framework that mimics human's tactile-guided control strategy. In this framework, we initially collect multi-modal tactile datasets from human demonstrators, incorporating human tactile-guided control strategies for task completion. The subsequent step involves instruc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.04644</link><description>&lt;p&gt;
&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#65306;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. (arXiv:2309.04644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#22349;&#22604;&#29616;&#35937;&#36827;&#34892;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#22312;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#33021;&#22815;&#20419;&#20351;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#26159;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26368;&#21518;&#19968;&#23618;&#20013;&#20986;&#29616;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#25351;&#30340;&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#32456;&#31471;&#38454;&#27573;&#65292;1&#65289;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#30340;&#31867;&#20869;&#21464;&#24322;&#36235;&#21521;&#20110;&#38646;&#65292;2&#65289;&#31867;&#29305;&#24449;&#22343;&#20540;&#26500;&#25104;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;3&#65289;&#26368;&#21518;&#19968;&#23618;&#31867;&#29305;&#24449;&#21644;&#26435;&#37325;&#22312;&#32553;&#25918;&#19978;&#30456;&#31561;&#65292;4&#65289;&#20998;&#31867;&#34892;&#20026;&#23849;&#28291;&#21040;&#26368;&#36817;&#30340;&#31867;&#20013;&#24515;&#65288;NCC&#65289;&#20915;&#31574;&#35268;&#21017;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#23545;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#30452;&#35266;&#30340;&#31867;&#20869;&#21644;&#31867;&#38388;&#20313;&#24358;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#25429;&#25417;&#20102;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#22810;&#20010;&#26680;&#24515;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#65292;&#25105;&#20204;&#22312;&#27491;&#21017;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#25509;&#36817;&#26368;&#20248;&#26102;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25209;&#24402;&#19968;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#19979;&#31070;&#32463;&#32593;&#32476;&#22349;&#22604;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse is a recently observed geometric structure that emerges in the final layer of neural network classifiers. Specifically, Neural Collapse states that at the terminal phase of neural networks training, 1) the intra-class variability of last-layer features tends to zero, 2) the class feature means form an Equiangular Tight Frame (ETF), 3) last-layer class features and weights becomes equal up the scaling, and 4) classification behavior collapses to the nearest class center (NCC) decision rule. This paper investigates the effect of batch normalization and weight decay on the emergence of Neural Collapse. We propose the geometrically intuitive intra-class and inter-class cosine similarity measure which captures multiple core aspects of Neural Collapse. With this measure, we provide theoretical guarantees of Neural Collapse emergence with last-layer batch normalization and weight decay when the regularized cross-entropy loss is near optimal. We also perform further experiments
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.08561</link><description>&lt;p&gt;
&#26410;&#26469;&#33647;&#29289;&#21457;&#29616;&#30340;&#23454;&#26045;&#65306;&#22522;&#20110;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;(QMLS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementation of The Future of Drug Discovery: QuantumBased Machine Learning Simulation (QMLS). (arXiv:2308.08561v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QMLS&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32553;&#30701;&#33647;&#29289;&#30740;&#21457;&#30340;&#26102;&#38388;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#29983;&#25104;&#21629;&#20013;&#29289;&#21644;&#20248;&#21270;&#20998;&#23376;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#33647;&#29289;&#21457;&#29616;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30740;&#21457;&#30340;&#30740;&#31350;&#19982;&#24320;&#21457;(R&amp;D)&#38454;&#27573;&#26159;&#19968;&#20010;&#28459;&#38271;&#32780;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#25913;&#38761;&#36825;&#20010;&#36807;&#31243;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#27010;&#24565;QMLS&#65292;&#23558;&#25972;&#20010;R&amp;D&#38454;&#27573;&#32553;&#30701;&#21040;&#19977;&#21040;&#20845;&#20010;&#26376;&#65292;&#25104;&#26412;&#20165;&#20026;&#20116;&#21040;&#20843;&#19975;&#32654;&#20803;&#12290;&#23545;&#20110;&#21629;&#20013;&#20135;&#29983;&#65292;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#29983;&#25104;(MLMG)&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#30340;&#20998;&#23376;&#32467;&#26500;&#29983;&#25104;&#21487;&#33021;&#30340;&#21629;&#20013;&#29289;&#65292;&#32780;&#37327;&#23376;&#27169;&#25311;(QS)&#26681;&#25454;&#19982;&#30446;&#26631;&#34507;&#30333;&#30340;&#21453;&#24212;&#21644;&#32467;&#21512;&#25928;&#26524;&#36807;&#28388;&#21407;&#22987;&#23454;&#39564;&#20013;&#30340;&#20998;&#23376;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#38085;&#20248;&#21270;&#65292;&#20174;MLMG&#21644;QS&#29983;&#25104;&#21644;&#36807;&#28388;&#30340;&#32467;&#26524;&#20998;&#23376;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21464;&#24322;(MLMV)&#23558;&#37027;&#20123;&#20986;&#29616;&#22312;&#20004;&#20010;&#36807;&#31243;&#20013;&#30340;&#20998;&#23376;&#21046;&#25104;&#25968;&#21313;&#31181;&#20998;&#23376;&#21464;&#20307;&#65292;&#32780;&#20854;&#20182;&#20998;&#23376;&#21482;&#21046;&#25104;&#20960;&#31181;&#21464;&#20307;&#12290;&#26368;&#21518;&#65292;&#25152;&#26377;&#20248;&#21270;&#30340;&#20998;&#23376;&#23558;&#32463;&#36807;&#22810;&#36718;&#39640;&#26631;&#20934;&#30340;QS&#36807;&#28388;&#65292;&#20197;&#30830;&#20445;&#21453;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Research &amp; Development (R&amp;D) phase of drug development is a lengthy and costly process. To revolutionize this process, we introduce our new concept QMLS to shorten the whole R&amp;D phase to three to six months and decrease the cost to merely fifty to eighty thousand USD. For Hit Generation, Machine Learning Molecule Generation (MLMG) generates possible hits according to the molecular structure of the target protein while the Quantum Simulation (QS) filters molecules from the primary essay based on the reaction and binding effectiveness with the target protein. Then, For Lead Optimization, the resultant molecules generated and filtered from MLMG and QS are compared, and molecules that appear as a result of both processes will be made into dozens of molecular variations through Machine Learning Molecule Variation (MLMV), while others will only be made into a few variations. Lastly, all optimized molecules would undergo multiple rounds of QS filtering with a high standard for reaction ef
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26500;&#36896;&#28385;&#36275;&#27491;&#23450;&#24615;&#21644;&#21487;&#23454;&#38469;&#35745;&#31639;&#23646;&#24615;&#30340;&#39532;&#27663;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#33021;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.12438</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#65306;&#36890;&#36807;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Multifidelity Covariance Estimation via Regression on the Manifold of Symmetric Positive Definite Matrices. (arXiv:2307.12438v2 [stat.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#36827;&#34892;&#22238;&#24402;&#27714;&#35299;&#30340;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#26500;&#36896;&#28385;&#36275;&#27491;&#23450;&#24615;&#21644;&#21487;&#23454;&#38469;&#35745;&#31639;&#23646;&#24615;&#30340;&#39532;&#27663;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#35813;&#20272;&#35745;&#22120;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#33021;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#65292;&#20854;&#26500;&#24314;&#20026;&#22312;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#12290;&#35813;&#20272;&#35745;&#22120;&#36890;&#36807;&#26500;&#36896;&#26159;&#27491;&#23450;&#30340;&#65292;&#24182;&#19988;&#20854;&#26368;&#23567;&#21270;&#30340;&#39532;&#27663;&#36317;&#31163;&#20855;&#26377;&#21487;&#23454;&#38469;&#35745;&#31639;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#24418;&#22238;&#24402;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#26159;&#22312;&#29305;&#23450;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#40654;&#26364;&#22238;&#24402;&#26694;&#26550;&#21253;&#21547;&#20102;&#20174;&#25511;&#21046;&#21464;&#37327;&#26500;&#24314;&#30340;&#29616;&#26377;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#31034;&#20363;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#21333;&#20445;&#30495;&#24230;&#21644;&#20854;&#20182;&#22810;&#20445;&#30495;&#24230;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#30340;&#24179;&#26041;&#65292;&#20943;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#27492;&#22806;&#65292;&#27491;&#23450;&#24615;&#30340;&#20445;&#25345;&#30830;&#20445;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#19982;&#19979;&#28216;&#20219;&#21153;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multifidelity estimator of covariance matrices formulated as the solution to a regression problem on the manifold of symmetric positive definite matrices. The estimator is positive definite by construction, and the Mahalanobis distance minimized to obtain it possesses properties which enable practical computation. We show that our manifold regression multifidelity (MRMF) covariance estimator is a maximum likelihood estimator under a certain error model on manifold tangent space. More broadly, we show that our Riemannian regression framework encompasses existing multifidelity covariance estimators constructed from control variates. We demonstrate via numerical examples that our estimator can provide significant decreases, up to one order of magnitude, in squared estimation error relative to both single-fidelity and other multifidelity covariance estimators. Furthermore, preservation of positive definiteness ensures that our estimator is compatible with downstream tasks, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18160</link><description>&lt;p&gt;
&#23545;&#31561;&#20844;&#24179;&#24615;&#8212;&#8212;&#35299;&#20915;&#20844;&#24179;&#35780;&#20272;&#20013;&#32676;&#20307;&#20043;&#38388;&#31995;&#32479;&#24046;&#24322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation. (arXiv:2305.18160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#20915;&#31574;&#26102;&#65292;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21363;&#19981;&#27495;&#35270;&#29305;&#23450;&#20010;&#20307;/&#32676;&#20307;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#24369;&#21183;&#32676;&#20307;&#30340;&#20154;&#12290;&#29616;&#26377;&#30340;&#32676;&#20307;&#20844;&#24179;&#26041;&#27861;&#35201;&#27714;&#36827;&#34892;&#24179;&#31561;&#30340;&#32676;&#20307;&#27979;&#37327;&#65292;&#20294;&#26410;&#32771;&#34385;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#34429;&#28982;&#19982;&#25935;&#24863;&#21464;&#37327;&#26080;&#20851;&#65292;&#20294;&#34920;&#29616;&#20986;&#31995;&#32479;&#24046;&#24322;&#65292;&#20250;&#23545;&#20844;&#24179;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#20844;&#24179;&#27979;&#37327;&#24212;&#35813;&#22522;&#20110;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#20110;&#24863;&#20852;&#36259;&#20219;&#21153;&#30340;&#23545;&#31561;&#20154;&#65288;&#21363;&#24444;&#27492;&#30456;&#20284;&#30340;&#20010;&#20307;&#65289;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20854;&#32676;&#20307;&#36523;&#20221;&#19981;&#21487;&#36890;&#36807;&#25506;&#32034;&#28151;&#28102;&#22240;&#32032;&#31639;&#27861;&#22320;&#21306;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#20197;&#36991;&#20813;&#20844;&#24179;&#35780;&#20272;&#27604;&#36739;&#8220;&#27225;&#23376;&#8221;&#21644;&#8220;&#33529;&#26524;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using machine learning (ML) to aid decision-making, it is critical to ensure that an algorithmic decision is fair, i.e., it does not discriminate against specific individuals/groups, particularly those from underprivileged populations. Existing group fairness methods require equal group-wise measures, which however fails to consider systematic between-group differences. The confounding factors, which are non-sensitive variables but manifest systematic differences, can significantly affect fairness evaluation. To tackle this problem, we believe that a fairness measurement should be based on the comparison between counterparts (i.e., individuals who are similar to each other with respect to the task of interest) from different groups, whose group identities cannot be distinguished algorithmically by exploring confounding factors. We have developed a propensity-score-based method for identifying counterparts, which prevents fairness evaluation from comparing "oranges" with "apples". 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11962</link><description>&lt;p&gt;
&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#32479;&#19968;&#25910;&#25947;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods. (arXiv:2302.11962v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21487;&#33021;&#38750;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#36741;&#21161;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#24102;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#36741;&#21161;&#26694;&#26550;&#20026;&#31639;&#27861;&#35774;&#35745;&#32773;&#25552;&#20379;&#20102;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#24182;&#19988;&#20351;&#29992;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#23558;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#26032;&#30340;&#24816;&#24615;&#38543;&#26426;&#20108;&#38454;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#25913;&#36827;&#20102;&#22823;&#32500;&#38382;&#39064;&#30340;&#31639;&#26415;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic Cubic Newton methods for solving general possibly non-convex minimization problems. We propose a new framework, which we call the helper framework, that provides a unified view of the stochastic and variance-reduced second-order algorithms equipped with global complexity guarantees. It can also be applied to learning with auxiliary information. Our helper framework offers the algorithm designer high flexibility for constructing and analyzing the stochastic Cubic Newton methods, allowing arbitrary size batches, and the use of noisy and possibly biased estimates of the gradients and Hessians, incorporating both the variance reduction and the lazy Hessian updates. We recover the best-known complexities for the stochastic and variance-reduced Cubic Newton, under weak assumptions on the noise. A direct consequence of our theory is the new lazy stochastic second-order method, which significantly improves the arithmetic complexity for large dimension problems. We also esta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2302.04823</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#29983;&#25104;&#23545;&#25239;&#27169;&#25311;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29616;&#23454;&#20013;&#30340;&#22478;&#24066;&#23548;&#33322;&#22330;&#26223;&#65292;&#35774;&#35745;&#20581;&#22766;&#30340;&#25511;&#21046;&#31574;&#30053;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#20013;&#65292;&#36825;&#20123;&#31574;&#30053;&#24517;&#39035;&#23558;&#36710;&#36742;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#39640;&#32500;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#65292;&#22914;&#36716;&#21521;&#21644;&#27833;&#38376;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;hGAIL&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#36742;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#20449;&#24687;&#30452;&#25509;&#26144;&#23556;&#21040;&#20302;&#32423;&#21160;&#20316;&#30340;&#21516;&#26102;&#65292;&#23398;&#20064;&#36710;&#36742;&#29615;&#22659;&#30340;&#20013;&#32423;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.06388</link><description>&lt;p&gt;
TSFool: &#36890;&#36807;&#22810;&#30446;&#26631;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers. (arXiv:2209.06388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26799;&#24230;&#25915;&#20987;&#26041;&#27861;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#26159;&#22240;&#20026;RNN&#30340;&#24490;&#29615;&#32467;&#26500;&#38459;&#27490;&#20102;&#30452;&#25509;&#30340;&#27169;&#22411;&#24046;&#20998;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#25200;&#21160;&#30340;&#35270;&#35273;&#25935;&#24863;&#24615;&#25361;&#25112;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20256;&#32479;&#23616;&#37096;&#20248;&#21270;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#65292;&#31216;&#20026;Camouflage Coefficient&#65292;&#20174;&#31867;&#20998;&#24067;&#30340;&#35282;&#24230;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#30340;&#38590;&#20197;&#23519;&#35273;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#25670;&#33073;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#36716;&#31227;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#22238;&#36991;&#35268;&#21017;&#12290;&#22312;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TSFool&#21487;&#20197;&#29983;&#25104;&#39640;&#38590;&#24230;&#25915;&#20987;&#21516;&#26102;&#20445;&#25345;&#23545;&#25239;&#26679;&#26412;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#24615;&#65292;&#24182;&#26377;&#24456;&#39640;&#30340;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) classifiers are vulnerable to adversarial attacks. Although the existing gradient-based attacks achieve state-of-the-art performance in feed-forward NNs and image recognition tasks, they do not perform as well on time series classification with recurrent neural network (RNN) models. This is because the cyclical structure of RNN prevents direct model differentiation and the visual sensitivity of time series data to perturbations challenges the traditional local optimization objective of the adversarial attack. In this paper, a black-box method called TSFool is proposed to efficiently craft highly-imperceptible adversarial time series for RNN classifiers. We propose a novel global optimization objective named Camouflage Coefficient to consider the imperceptibility of adversarial samples from the perspective of class distribution, and accordingly refine the adversarial attack as a multi-objective optimization problem to enhance the perturbation quality. To get rid of t
&lt;/p&gt;</description></item></channel></rss>