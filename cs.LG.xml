<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.18878</link><description>&lt;p&gt;
AIC-UNet: &#29992;&#20110;&#20581;&#22766;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#35299;&#21078;&#20449;&#24687;&#39537;&#21160;&#32423;&#32852;UNet
&lt;/p&gt;
&lt;p&gt;
AIC-UNet: Anatomy-informed Cascaded UNet for Robust Multi-Organ Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18878
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#29616;&#26377;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#26469;&#26045;&#21152;&#35299;&#21078;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21152;&#20851;&#38190;&#35299;&#21078;&#29305;&#24449;&#65292;&#20363;&#22914;&#22120;&#23448;&#25968;&#37327;&#12289;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#30456;&#23545;&#20301;&#32622;&#65292;&#23545;&#20110;&#26500;&#24314;&#20581;&#22766;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#32534;&#30721;-&#35299;&#30721;&#20998;&#21106;&#27169;&#22411;&#19978;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#35299;&#21078;&#20808;&#39564;&#65292;&#26469;&#23454;&#26045;&#35299;&#21078;&#32422;&#26463;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#33145;&#37096;&#25195;&#25551;&#26102;&#65292;&#32534;&#30721;&#22120;&#30340;&#19968;&#37096;&#20998;&#36890;&#36807;&#34180;&#26495;&#26679;&#26465;&#65288;TPS&#65289;&#32593;&#26684;&#25554;&#20540;&#23558;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#31354;&#38388;&#23545;&#20934;&#32473;&#23450;&#30340;&#36755;&#20837;&#25195;&#25551;&#12290;&#28982;&#21518;&#22312;&#35299;&#30721;&#38454;&#27573;&#25972;&#21512;&#21464;&#24418;&#30340;&#20808;&#39564;&#20197;&#25351;&#23548;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18878v1 Announce Type: cross  Abstract: Imposing key anatomical features, such as the number of organs, their shapes, sizes, and relative positions, is crucial for building a robust multi-organ segmentation model. Current attempts to incorporate anatomical features include broadening effective receptive fields (ERF) size with resource- and data-intensive modules such as self-attention or introducing organ-specific topology regularizers, which may not scale to multi-organ segmentation problems where inter-organ relation also plays a huge role. We introduce a new approach to impose anatomical constraints on any existing encoder-decoder segmentation model by conditioning model prediction with learnable anatomy prior. More specifically, given an abdominal scan, a part of the encoder spatially warps a learnable prior to align with the given input scan using thin plate spline (TPS) grid interpolation. The warped prior is then integrated during the decoding phase to guide the model
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14772</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#39640;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness to Model Inversion Attacks via Sparse Coding Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14772
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23618;&#35774;&#35745;&#26032;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23545;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#31639;&#27861;&#20801;&#35768;&#23545;&#25163;&#36890;&#36807;&#21453;&#22797;&#26597;&#35810;&#31070;&#32463;&#32593;&#32476;&#24182;&#26816;&#26597;&#20854;&#36755;&#20986;&#26469;&#37325;&#24314;&#32593;&#32476;&#30340;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#31232;&#30095;&#32534;&#30721;&#23618;&#26469;&#33719;&#24471;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#21331;&#36234;&#40065;&#26834;&#24615;&#12290; &#19977;&#21313;&#24180;&#26469;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#24050;&#32463;&#30740;&#31350;&#20102;&#31232;&#30095;&#32534;&#30721;&#22312;&#22270;&#20687;&#21435;&#22122;&#65292;&#30446;&#26631;&#35782;&#21035;&#21644;&#23545;&#25239;&#24615;&#35823;&#20998;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;&#28431;&#27934;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#31232;&#30095;&#32534;&#30721;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#30340;&#25163;&#27573;&#26469;&#25269;&#24481;&#27169;&#22411;&#36870;&#25512;&#25915;&#20987;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#30721;&#22312;&#32593;&#32476;&#30340;&#20013;&#38388;&#34920;&#31034;&#20013;&#30340;&#26080;&#20851;&#31169;&#20154;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#32780;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#20247;&#25152;&#21608;&#30693;&#21482;&#26377;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14772v1 Announce Type: cross  Abstract: Recent model inversion attack algorithms permit adversaries to reconstruct a neural network's private training data just by repeatedly querying the network and inspecting its outputs. In this work, we develop a novel network architecture that leverages sparse-coding layers to obtain superior robustness to this class of attacks. Three decades of computer science research has studied sparse coding in the context of image denoising, object recognition, and adversarial misclassification settings, but to the best of our knowledge, its connection to state-of-the-art privacy vulnerabilities remains unstudied. However, sparse coding architectures suggest an advantageous means to defend against model inversion attacks because they allow us to control the amount of irrelevant private information encoded in a network's intermediate representations in a manner that can be computed efficiently during training and that is known to have little effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.12856</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Ensembles and Regularization for Reinforcement Learning in Map-based Path Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#19987;&#38376;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#26500;&#24314;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#20013;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#28145;&#24230;RL&#31574;&#30053;&#21644;&#20540;&#32593;&#32476;&#20998;&#21035;&#26159;&#31561;&#21464;&#21644;&#19981;&#21464;&#30340;&#20197;&#21033;&#29992;&#36825;&#20123;&#23545;&#31216;&#24615;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#30456;&#20851;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#26500;&#36896;&#20855;&#26377;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#30340;&#32593;&#32476;&#26469;&#35774;&#35745;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#21482;&#33021;&#20351;&#29992;&#38750;&#24120;&#21463;&#38480;&#30340;&#32452;&#20214;&#24211;&#65292;&#36827;&#32780;&#38459;&#30861;&#20102;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#31561;&#21464;&#31574;&#30053;&#21644;&#19981;&#21464;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#19987;&#38376;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#31561;&#21464;&#38598;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#28155;&#21152;&#20102;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#22522;&#20110;&#22320;&#22270;&#30340;&#36335;&#24452;&#35268;&#21010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31561;&#21464;&#38598;&#21512;&#21644;&#27491;&#21017;&#21270;&#22914;&#20309;&#26377;&#30410;&#20110;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12856v1 Announce Type: new  Abstract: In reinforcement learning (RL), exploiting environmental symmetries can significantly enhance efficiency, robustness, and performance. However, ensuring that the deep RL policy and value networks are respectively equivariant and invariant to exploit these symmetries is a substantial challenge. Related works try to design networks that are equivariant and invariant by construction, limiting them to a very restricted library of components, which in turn hampers the expressiveness of the networks. This paper proposes a method to construct equivariant policies and invariant value functions without specialized neural network components, which we term equivariant ensembles. We further add a regularization term for adding inductive bias during training. In a map-based path planning case study, we show how equivariant ensembles and regularization benefit sample efficiency and performance.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.08525</link><description>&lt;p&gt;
&#20174;&#24369;&#21040;&#24378;&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#22768;&#38899;&#20107;&#20214;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#65288;A-CPD&#65289;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#24341;&#23548;&#30340;&#38899;&#39057;&#24405;&#21046;&#27573;&#30340;&#24369;&#26631;&#31614;&#27880;&#37322;&#12290;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20851;&#20110;&#30446;&#26631;&#22768;&#38899;&#26102;&#38388;&#28608;&#27963;&#30340;&#20449;&#24687;&#33719;&#21462;&#37327;&#12290;&#23545;&#20110;&#27599;&#20010;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#24405;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#25512;&#23548;&#27010;&#29575;&#26354;&#32447;&#65292;&#29992;&#20110;&#25351;&#23548;&#27880;&#37322;&#12290;&#39044;&#27979;&#27169;&#22411;&#26368;&#21021;&#22312;&#21487;&#29992;&#30340;&#24102;&#26631;&#27880;&#22768;&#38899;&#20107;&#20214;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#31867;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#19981;&#30456;&#20132;&#12290;&#28982;&#21518;&#65292;&#39044;&#27979;&#27169;&#22411;&#36880;&#28176;&#36866;&#24212;&#27880;&#37322;&#32773;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#25552;&#20379;&#30340;&#27880;&#37322;&#12290;&#29992;&#20110;&#24341;&#23548;&#24369;&#26631;&#31614;&#27880;&#37322;&#32773;&#36208;&#21521;&#24378;&#26631;&#31614;&#30340;&#26597;&#35810;&#26159;&#20351;&#29992;&#36825;&#20123;&#27010;&#29575;&#19978;&#30340;&#21464;&#28857;&#26816;&#27979;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08525v1 Announce Type: cross  Abstract: In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favor
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.03636</link><description>&lt;p&gt;
SheetAgent&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03636
&lt;/p&gt;
&lt;p&gt;
SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#24191;&#27867;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26085;&#24120;&#24037;&#20316;&#20013;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;&#26368;&#36817;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#33258;&#21160;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#65292;&#20294;&#23578;&#26410;&#22312;&#23384;&#22312;&#25512;&#29702;&#25361;&#25112;&#30340;&#22797;&#26434;&#21644;&#29616;&#23454;&#20219;&#21153;&#20013;&#36827;&#34892;&#25506;&#31350;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#21644;&#27169;&#31946;&#35201;&#27714;&#30340;&#38271;&#35270;&#37326;&#25805;&#20316;&#65289;&#12290;&#20026;&#20102;&#24357;&#21512;&#19982;&#30495;&#23454;&#19990;&#30028;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\textbf{SheetRM}$&#65292;&#19968;&#20010;&#29305;&#28857;&#26159;&#38271;&#35270;&#37326;&#21644;&#22810;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20855;&#26377;&#25512;&#29702;&#30456;&#20851;&#25805;&#32437;&#65292;&#30001;&#30495;&#23454;&#25361;&#25112;&#24341;&#36215;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;$\textbf{SheetAgent}$&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#33021;&#21147;&#30340;&#26032;&#22411;&#33258;&#20027;&#20195;&#29702;&#12290;SheetAgent&#30001;&#19977;&#20010;&#21327;&#20316;&#27169;&#22359;&#32452;&#25104;&#65306;$\textit{Planner}$&#12289;$\textit{Informer}$&#21644;$\textit{Retriever}$&#65292;&#23454;&#29616;&#20102;&#23545;&#30005;&#23376;&#34920;&#26684;&#30340;&#39640;&#32423;&#25512;&#29702;&#21644;&#20934;&#30830;&#25805;&#20316;&#65292;&#32780;&#19981;&#38656;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.03375</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03375
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#32463;&#24120;&#23558;&#34394;&#20551;&#29305;&#24449;&#23450;&#20041;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#8220;&#26356;&#23481;&#26131;&#8221;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#30456;&#23545;&#31616;&#21333;&#24615;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#26368;&#32456;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#65288;&#19982;&#26680;&#24515;&#29305;&#24449;&#30456;&#27604;&#65289;&#21644;&#30456;&#20851;&#24615;&#24378;&#24230;&#65288;&#30456;&#23545;&#20110;&#26631;&#31614;&#65289;&#36827;&#34892;&#32454;&#33268;&#25511;&#21046;&#65292;&#20197;&#30740;&#31350;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#65288;1&#65289;&#26356;&#24378;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#26356;&#31616;&#21333;&#30340;&#34394;&#20551;&#29305;&#24449;&#20250;&#20943;&#24930;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#65288;2&#65289;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#24182;&#38750;&#24635;&#26159;&#21487;&#20197;&#34987;&#20998;&#24320;&#65292;&#65288;3&#65289;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#20043;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03375v1 Announce Type: new  Abstract: Existing research often posits spurious features as "easier" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even af
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.01758</link><description>&lt;p&gt;
AFBT GAN: &#36890;&#36807;&#21453;&#20107;&#23454;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22686;&#24378;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01758
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26500;&#24314;&#30340; AFBT GAN &#22686;&#24378;&#20102;&#23545;&#35748;&#30693;&#34928;&#36864;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#35786;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21151;&#33021;&#36830;&#25509;&#65288;FC&#65289;&#30340;&#35299;&#37322;&#32467;&#26524;&#36890;&#24120;&#26159;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#32467;&#26524;&#26631;&#31614;&#21644;&#35832;&#22914;Pearson&#30456;&#20851;&#24615;&#25110;&#26799;&#24230;&#21453;&#25512;&#31561;&#30456;&#20851;&#20998;&#26512;&#26041;&#27861;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#27169;&#22411;&#20173;&#28982;&#26159;&#22312;&#40657;&#30418;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#32570;&#20047;&#23545;&#37325;&#35201;&#21306;&#22495;FC&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#35786;&#26029;&#24615;&#33021;&#65292;&#22312;&#35786;&#26029;&#27169;&#22411;&#20013;&#25552;&#20379;&#20851;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#24403;&#20581;&#24247;&#21463;&#35797;&#32773;&#65288;HC&#65289;&#21457;&#23637;&#20026;&#20027;&#35266;&#35748;&#30693;&#34928;&#36864;&#65288;SCD&#65289;&#21644;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#26102;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#30830;&#23450;&#31070;&#32463;&#36864;&#34892;&#24615;&#30456;&#20851;&#21306;&#22495;&#65292;&#25105;&#20204;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#28304;&#26631;&#31614;FC&#27966;&#29983;&#30340;&#30446;&#26631;&#26631;&#31614;FC&#30697;&#38453;&#65292;&#28982;&#21518;&#23558;&#28304;&#26631;&#31614;FC&#20943;&#21435;&#30446;&#26631;&#26631;&#31614;FC&#12290;&#33258;&#36866;&#24212;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#25442;&#26500;&#25104;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01758v1 Announce Type: cross  Abstract: Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transfo
&lt;/p&gt;</description></item><item><title>UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.13630</link><description>&lt;p&gt;
UniGraph: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064;&#36328;&#39046;&#22495;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13630
&lt;/p&gt;
&lt;p&gt;
UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: ChatGPT &#21644; GPT-4 &#31561;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#27867;&#21270;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#23427;&#20204;&#26368;&#21021;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20010;&#27010;&#24565;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26102;&#65292;&#20986;&#29616;&#20102;&#40092;&#26126;&#30340;&#23545;&#27604;&#12290;&#22270;&#23398;&#20064;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#25110;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#21333;&#20010;&#22270;&#27169;&#22411;&#19978;&#65292;&#32570;&#20047;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#29305;&#23450;&#20110;&#22270;&#25968;&#25454;&#30340;&#19981;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;UniGraph&#26694;&#26550;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11237</link><description>&lt;p&gt;
&#23545;&#25239;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#25552;&#20986;&#19968;&#20010;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26816;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24555;&#25463;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#24555;&#25463;&#23398;&#20064;&#30340;&#24433;&#21709;&#65306;&#23427;&#20204;&#20542;&#21521;&#20110;&#24314;&#31435;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#26080;&#20851;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#39044;&#26399;&#30340;&#20219;&#21153;&#12290;&#24555;&#25463;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36825;&#19968;&#29616;&#35937;&#30340;&#30165;&#36857;&#21487;&#35265;&#20110;&#20854;&#27867;&#21270;&#38382;&#39064;&#12289;&#39046;&#22495;&#36716;&#31227;&#12289;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#65292;&#29978;&#33267;&#23545;&#22810;&#25968;&#32676;&#20307;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21508;&#31181;DNN&#38382;&#39064;&#30340;&#20849;&#21516;&#21407;&#22240;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#37325;&#35201;&#26426;&#20250;&#65292;&#24212;&#35813;&#21033;&#29992;&#36825;&#19968;&#28857;&#25214;&#21040;&#23545;&#25239;&#24555;&#25463;&#23398;&#20064;&#30340;&#32479;&#19968;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#29305;&#21035;&#26159;&#25345;&#32493;&#21516;&#35843;(PH)&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20026;&#25506;&#27979;&#28145;&#24230;&#23398;&#20064;&#20013;&#24555;&#25463;&#26041;&#24335;&#21246;&#30011;&#20102;&#32479;&#19968;&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;DNNs&#20013;&#35745;&#31639;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#20351;&#29992;&#26080;&#27861;&#23398;&#20064;&#30340;&#31034;&#20363;&#21644;&#20559;&#35265;&#20026;&#20004;&#31181;&#24773;&#20917;&#65292;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#35770;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.09558</link><description>&lt;p&gt;
&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21452;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Pre-training for Improving Time Series Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09558
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BiTimelyGPT&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#34920;&#31034;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21487;&#29992;&#20110;&#31070;&#32463;&#21151;&#33021;&#39044;&#27979;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20197;&#29992;&#20110;&#21028;&#21035;&#20219;&#21153;&#19968;&#30452;&#26159;&#19968;&#39033;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#35201;&#20040;&#26159;&#21333;&#21521;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#35201;&#20040;&#26159;&#38543;&#26426;&#23631;&#34109;&#26631;&#35760;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21452;&#21521;&#21450;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;BiTimelyGPT&#65289;&#65292;&#23427;&#36890;&#36807;&#20132;&#26367;&#30340;Transformer&#23618;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19979;&#19968;&#20010;&#26631;&#35760;&#21644;&#19978;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20445;&#30041;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#21407;&#22987;&#20998;&#24067;&#21644;&#25968;&#25454;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#20840;&#31209;&#21069;&#21521;&#21644;&#21518;&#21521;&#27880;&#24847;&#21147;&#30697;&#38453;&#20855;&#26377;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290; &#20351;&#29992;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#65292;BiTimelyGPT&#22312;&#39044;&#27979;&#31070;&#32463;&#21151;&#33021;&#12289;&#30142;&#30149;&#35786;&#26029;&#21644;&#29983;&#29702;&#30149;&#24449;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#27880;&#24847;&#21147;&#28909;&#22270;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#30340;BiTimelyGPT&#33021;&#22815;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#21028;&#21035;&#24615;&#30340;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09558v1 Announce Type: new  Abstract: Learning time-series representations for discriminative tasks has been a long-standing challenge. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on time-series data by both next-token and previous-token predictions in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignal data, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from time-s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.05605</link><description>&lt;p&gt;
&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#20013;&#30340;&#25480;&#26435;
&lt;/p&gt;
&lt;p&gt;
Optimizing Delegation in Collaborative Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#21327;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#28151;&#21512;&#22242;&#38431;&#25480;&#26435;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#65292;&#23398;&#20064;&#22242;&#38431;&#20195;&#29702;&#20154;&#30340;&#34892;&#20026;&#27169;&#22411;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#21644;&#33258;&#20027;&#31995;&#32479;&#20316;&#20026;&#28151;&#21512;&#22242;&#38431;&#20849;&#21516;&#36816;&#20316;&#26102;&#65292;&#25105;&#20204;&#24076;&#26395;&#30830;&#20445;&#22242;&#38431;&#30340;&#25104;&#21151;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#22242;&#38431;&#25104;&#21592;&#31216;&#20026;&#20195;&#29702;&#20154;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28151;&#21512;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#20219;&#20309;&#26102;&#20505;&#65292;&#21482;&#26377;&#19968;&#20010;&#22242;&#38431;&#25104;&#21592;&#65288;&#25511;&#21046;&#20195;&#29702;&#20154;&#65289;&#34987;&#25480;&#26435;&#20026;&#22242;&#38431;&#30340;&#25511;&#21046;&#32773;&#12290;&#20026;&#20102;&#30830;&#23450;&#26368;&#20339;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#20837;AI&#32463;&#29702;&#65288;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#24819;&#27861;&#65292;&#35813;&#32463;&#29702;&#20316;&#20026;&#22242;&#38431;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#23398;&#20064;&#12290;&#32463;&#29702;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#34920;&#29616;&#21644;&#22242;&#38431;&#25152;&#22788;&#30340;&#29615;&#22659;/&#19990;&#30028;&#26469;&#23398;&#20064;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#36873;&#25321;&#20986;&#26368;&#29702;&#24819;&#30340;&#25511;&#21046;&#20195;&#29702;&#20154;&#12290;&#20026;&#20102;&#38480;&#23450;&#32463;&#29702;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#32463;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#25351;&#31034;&#22242;&#38431;&#30340;&#21487;&#25509;&#21463;&#36816;&#20316;&#26041;&#24335;&#65292;&#22240;&#27492;&#22914;&#26524;&#22242;&#38431;&#36827;&#20837;&#19981;&#21487;&#25509;&#21463;&#24182;&#38656;&#35201;&#32463;&#29702;&#20171;&#20837;&#30340;&#29366;&#24577;&#65292;&#23601;&#20250;&#36829;&#21453;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To
&lt;/p&gt;</description></item><item><title>&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.03903</link><description>&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Compound Returns Reduce Variance in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03903
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#22238;&#25253;&#65292;&#20363;&#22914;$n$&#27493;&#22238;&#25253;&#21644;$\lambda$&#22238;&#25253;&#65292;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22810;&#27493;&#22238;&#25253;&#30340;&#26041;&#24046;&#25104;&#20026;&#20854;&#38271;&#24230;&#30340;&#38480;&#21046;&#22240;&#32032;&#65292;&#36807;&#24230;&#36828;&#26395;&#26410;&#26469;&#20250;&#22686;&#21152;&#26041;&#24046;&#24182;&#36870;&#36716;&#22810;&#27493;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21512;&#22238;&#25253;&#65288;$n$&#27493;&#22238;&#25253;&#30340;&#21152;&#26435;&#24179;&#22343;&#65289;&#38477;&#20302;&#26041;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#20219;&#20309;&#19982;&#32473;&#23450;$n$&#27493;&#22238;&#25253;&#20855;&#26377;&#30456;&#21516;&#25910;&#32553;&#27169;&#25968;&#30340;&#22797;&#21512;&#22238;&#25253;&#30340;&#26041;&#24046;&#20005;&#26684;&#36739;&#20302;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#38477;&#20302;&#26041;&#24046;&#30340;&#29305;&#24615;&#25913;&#21892;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#19968;&#33324;&#22797;&#21512;&#22238;&#25253;&#30340;&#23454;&#26045;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21161;&#22238;&#25253;&#65292;&#23427;&#20204;&#22312;&#20445;&#25345;&#39640;&#25928;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#26041;&#24046;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#23567;&#25209;&#37327;&#32463;&#39564;&#22238;&#25918;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;BISECT&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#12290;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#21487;&#26377;&#25928;&#22686;&#24378;&#22810;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#65292;&#22312;&#19982;&#22522;&#32447;&#27604;&#36739;&#26102;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.03846</link><description>&lt;p&gt;
&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#39640;&#25928;&#38544;&#34255;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generation of Hidden Outliers for Improved Outlier Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;BISECT&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#12290;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#21487;&#26377;&#25928;&#22686;&#24378;&#22810;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#65292;&#22312;&#19982;&#22522;&#32447;&#27604;&#36739;&#26102;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#28857;&#29983;&#25104;&#26159;&#35299;&#20915;&#37325;&#35201;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#27969;&#34892;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#39640;&#32500;&#31354;&#38388;&#20013;&#24322;&#24120;&#28857;&#30340;&#8220;&#22810;&#35270;&#22270;&#8221;&#23646;&#24615;&#12290;&#21807;&#19968;&#32771;&#34385;&#21040;&#27492;&#23646;&#24615;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISECT&#30340;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#19988;&#27169;&#20223;&#35813;&#23646;&#24615;&#30340;&#24322;&#24120;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;BISECT&#37319;&#29992;&#20102;&#26412;&#25991;&#20171;&#32461;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21629;&#39064;&#65292;&#35828;&#26126;&#22914;&#20309;&#39640;&#25928;&#22320;&#29983;&#25104;&#36825;&#20123;&#30495;&#23454;&#24322;&#24120;&#28857;&#12290;&#19982;&#24403;&#21069;&#37325;&#26032;&#21019;&#24314;&#8220;&#22810;&#35270;&#22270;&#8221;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20445;&#35777;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#29992;&#20363;&#12290;&#20363;&#22914;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier generation is a popular technique used for solving important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the 'multiple views' property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose BISECT, a new outlier generation method that creates realistic outliers mimicking said property. To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current methodology for recreating 'multiple views'. We use the synthetic outliers generated by BISECT to effectively enhance outlier detection in diverse datasets, for multiple use cases. For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.09481</link><description>&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Continual Adversarial Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27599;&#26376;&#38024;&#23545;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24555;&#36895;&#28436;&#21464;&#30340;&#29305;&#24615;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#36890;&#29992;&#21270;&#20197;&#25269;&#24481;&#23613;&#21487;&#33021;&#22810;&#30340;&#24050;&#30693;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#23545;&#25239;&#25152;&#26377;&#31867;&#22411;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#24182;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#38450;&#24481;&#31995;&#32479;&#36816;&#34892;&#30340;&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#21253;&#21547;&#38543;&#30528;&#26102;&#38388;&#20986;&#29616;&#30340;&#21508;&#31181;&#29420;&#29305;&#25915;&#20987;&#12290;&#38450;&#24481;&#31995;&#32479;&#24517;&#39035;&#25910;&#38598;&#22312;&#32447;&#23569;&#26679;&#26412;&#23545;&#25239;&#21453;&#39304;&#20197;&#36805;&#36895;&#22686;&#24378;&#33258;&#36523;&#65292;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#31181;&#25915;&#20987;&#36880;&#20010;&#38454;&#27573;&#20986;&#29616;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;CAD&#22522;&#20110;&#22235;&#39033;&#21407;&#21017;&#36827;&#34892;&#24314;&#27169;&#65306;(1) &#25345;&#32493;&#36866;&#24212;&#26032;&#25915;&#20987;&#32780;&#26080;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;(2) &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;(3) &#20869;&#23384;&#39640;&#25928;&#36866;&#24212;&#65292;&#20197;&#21450;(4) &#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#27979;&#27169;&#22411;&#30340;&#37096;&#32626;&#23545;&#20915;&#31574;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#25104;&#20026;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#36896;&#25104;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#32780;&#20351;&#20854;&#39044;&#27979;&#33021;&#21147;&#21464;&#26080;&#25928;&#12290;</title><link>https://arxiv.org/abs/2312.01210</link><description>&lt;p&gt;
&#24403;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#23548;&#33268;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;
&lt;/p&gt;
&lt;p&gt;
When accurate prediction models yield harmful self-fulfilling prophecies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#39044;&#27979;&#27169;&#22411;&#30340;&#37096;&#32626;&#23545;&#20915;&#31574;&#20135;&#29983;&#26377;&#23475;&#24433;&#21709;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#25104;&#20026;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#20026;&#23545;&#26576;&#20123;&#24739;&#32773;&#36896;&#25104;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#32780;&#20351;&#20854;&#39044;&#27979;&#33021;&#21147;&#21464;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#39044;&#27979;&#27169;&#22411;&#22312;&#21307;&#23398;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#36890;&#36807;&#20026;&#29305;&#23450;&#24739;&#32773;&#39044;&#27979;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#22256;&#38590;&#30340;&#27835;&#30103;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#24120;&#34987;&#35465;&#20026;&#20010;&#24615;&#21270;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#21307;&#30103;&#20445;&#20581;&#30340;&#26480;&#20986;&#20195;&#34920;&#12290;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#22312;&#39564;&#35777;&#30740;&#31350;&#20013;&#22522;&#20110;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#32780;&#37096;&#32626;&#29992;&#20110;&#20915;&#31574;&#25903;&#25345;&#12290;&#25105;&#20204;&#35843;&#26597;&#36825;&#26159;&#21542;&#26159;&#19968;&#31181;&#23433;&#20840;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#21487;&#20197;&#23548;&#33268;&#26377;&#23475;&#30340;&#20915;&#31574;&#65292;&#21363;&#20351;&#22312;&#37096;&#32626;&#21518;&#36825;&#20123;&#39044;&#27979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21306;&#20998;&#24230;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#26377;&#23475;&#30340;&#33258;&#25105;&#23454;&#29616;&#39044;&#35328;&#65306;&#23427;&#20204;&#30340;&#37096;&#32626;&#25439;&#23475;&#20102;&#19968;&#32676;&#24739;&#32773;&#65292;&#20294;&#36825;&#20123;&#24739;&#32773;&#30340;&#26356;&#31967;&#31957;&#30340;&#32467;&#26524;&#24182;&#19981;&#20351;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#26080;&#25928;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#36825;&#20123;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#30340;&#24418;&#24335;&#21270;&#25551;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37096;&#32626;&#21069;&#21518;&#37117;&#36827;&#34892;&#20102;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Objective: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. Many prediction models are deployed for decision support based on their prediction accuracy in validation studies. We investigate whether this is a safe and valid approach.   Materials and Methods: We show that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model.   Results: Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment 
&lt;/p&gt;</description></item><item><title>TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.02971</link><description>&lt;p&gt;
TabRepo&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#34920;&#26684;&#27169;&#22411;&#35780;&#20272;&#24211;&#21450;&#20854;AutoML&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02971
&lt;/p&gt;
&lt;p&gt;
TabRepo&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#36827;&#34892;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;AutoML&#31995;&#32479;&#27604;&#36739;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TabRepo&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;1310&#20010;&#27169;&#22411;&#22312;200&#20010;&#20998;&#31867;&#21644;&#22238;&#24402;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#21644;&#25351;&#26631;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#22810;&#31181;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#27604;&#36739;&#36229;&#21442;&#25968;&#20248;&#21270;&#19982;&#24403;&#21069;AutoML&#31995;&#32479;&#20197;&#21450;&#22312;&#20351;&#29992;&#39044;&#35745;&#31639;&#27169;&#22411;&#39044;&#27979;&#30340;&#21516;&#26102;&#32771;&#34385;&#38598;&#25104;&#30340;&#20998;&#26512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#25191;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#24212;&#29992;&#26631;&#20934;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#24310;&#36831;&#26041;&#38754;&#32988;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#34920;&#26684;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02971v2 Announce Type: replace-cross  Abstract: We introduce TabRepo, a new dataset of tabular model evaluations and predictions. TabRepo contains the predictions and metrics of 1310 models evaluated on 200 classification and regression datasets. We illustrate the benefit of our dataset in multiple ways. First, we show that it allows to perform analysis such as comparing Hyperparameter Optimization against current AutoML systems while also considering ensembling at marginal cost by using precomputed model predictions. Second, we show that our dataset can be readily leveraged to perform transfer-learning. In particular, we show that applying standard transfer-learning techniques allows to outperform current state-of-the-art tabular systems in accuracy, runtime and latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#30340;&#30701;&#23567;&#36845;&#20195;&#36807;&#31243;&#65292;&#20943;&#36731;&#20102;&#31163;&#32676;&#20540;&#23545;&#28388;&#27874;&#24615;&#33021;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#19988;&#23545;&#31163;&#32676;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.09505</link><description>&lt;p&gt;
&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#65306;&#29702;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Outlier-Insensitive Kalman Filtering: Theory and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#19981;&#25935;&#24863;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#30340;&#30701;&#23567;&#36845;&#20195;&#36807;&#31243;&#65292;&#20943;&#36731;&#20102;&#31163;&#32676;&#20540;&#23545;&#28388;&#27874;&#24615;&#33021;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20272;&#35745;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#19988;&#23545;&#31163;&#32676;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20174;&#22122;&#22768;&#35266;&#27979;&#20013;&#36827;&#34892;&#29366;&#24577;&#20272;&#35745;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#36890;&#24120;&#20351;&#29992;&#32447;&#24615;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;KF&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#20984;&#20108;&#27425;&#30446;&#26631;&#20989;&#25968;&#30340;&#25935;&#24863;&#24615;&#65292;&#24403;&#35266;&#27979;&#20013;&#23384;&#22312;&#31163;&#32676;&#20540;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#21487;&#20197;&#24212;&#29992;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;KF&#30340;&#26631;&#20934;&#26356;&#26032;&#27493;&#39588;&#36827;&#34892;&#30701;&#23567;&#30340;&#36845;&#20195;&#36807;&#31243;&#26102;&#65292;&#20943;&#36731;&#31163;&#32676;&#20540;&#30340;&#26377;&#23475;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#28508;&#22312;&#30340;&#31163;&#32676;&#20540;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#26041;&#24046;&#30340;&#27491;&#24577;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#25110;&#20132;&#26367;&#26368;&#22823;&#21270;&#31639;&#27861;&#36827;&#34892;&#22312;&#32447;&#20272;&#35745;&#12290;&#20223;&#30495;&#21644;&#23454;&#22320;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28388;&#27874;&#22330;&#26223;&#20013;&#23545;&#31163;&#32676;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09505v2 Announce Type: replace-cross  Abstract: State estimation of dynamical systems from noisy observations is a fundamental task in many applications. It is commonly addressed using the linear Kalman filter (KF), whose performance can significantly degrade in the presence of outliers in the observations, due to the sensitivity of its convex quadratic objective function. To mitigate such behavior, outlier detection algorithms can be applied. In this work, we propose a parameter-free algorithm which mitigates the harmful effect of outliers while requiring only a short iterative process of the standard update step of the KF. To that end, we model each potential outlier as a normal process with unknown variance and apply online estimation through either expectation maximization or alternating maximization algorithms. Simulations and field experiment evaluations demonstrate competitive performance of our method, showcasing its robustness to outliers in filtering scenarios comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;18&#20010;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#24182;&#22312;&#21253;&#21547;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#21644;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26032;&#23454;&#39564;&#65292;&#21457;&#29616;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#23601;&#33021;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.12778</link><description>&lt;p&gt;
&#25506;&#31350;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#37325;&#35201;&#24615;&#65306;&#19968;&#39033;&#23454;&#26045;&#35843;&#26597;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Investigating Feature and Model Importance in Android Malware Detection: An Implemented Survey and Experimental Comparison of ML-Based Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;18&#20010;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#65292;&#24182;&#22312;&#21253;&#21547;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#21644;&#26368;&#26032;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#26032;&#23454;&#39564;&#65292;&#21457;&#29616;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#23601;&#33021;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#30340;&#26222;&#21450;&#24847;&#21619;&#30528;&#23427;&#25104;&#20026;&#24694;&#24847;&#36719;&#20214;&#30340;&#24120;&#35265;&#30446;&#26631;&#12290;&#22810;&#24180;&#26469;&#65292;&#21508;&#31181;&#30740;&#31350;&#21457;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#24694;&#24847;&#36719;&#20214;&#21644;&#33391;&#24615;&#24212;&#29992;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25805;&#20316;&#31995;&#32479;&#30340;&#28436;&#36827;&#65292;&#24694;&#24847;&#36719;&#20214;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#65292;&#23545;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20854;&#20013;&#35768;&#22810;&#25253;&#21578;&#31216;&#20351;&#29992;&#23567;&#22411;&#12289;&#36807;&#26102;&#19988;&#32463;&#24120;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#33021;&#22815;&#33719;&#24471;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;18&#39033;&#20855;&#20195;&#34920;&#24615;&#30340;&#36807;&#21435;&#30740;&#31350;&#24182;&#20351;&#29992;&#21253;&#25324;124,000&#20010;&#24212;&#29992;&#31243;&#24207;&#30340;&#24179;&#34913;&#12289;&#30456;&#20851;&#19988;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#23545;&#23427;&#20204;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#26032;&#30340;&#23454;&#39564;&#65292;&#20197;&#22635;&#34917;&#29616;&#26377;&#30693;&#35782;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#21033;&#29992;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#22312;&#24403;&#20195;&#29615;&#22659;&#20013;&#29992;&#20110;&#23433;&#21331;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26368;&#26377;&#25928;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#38745;&#24577;&#20998;&#26512;&#25552;&#21462;&#30340;&#29305;&#24449;&#21363;&#21487;&#23454;&#29616;&#39640;&#36798;96.8%&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.12778v2 Announce Type: replace  Abstract: The popularity of Android means it is a common target for malware. Over the years, various studies have found that machine learning models can effectively discriminate malware from benign applications. However, as the operating system evolves, so does malware, bringing into question the findings of these previous studies, many of which report very high accuracies using small, outdated, and often imbalanced datasets. In this paper, we reimplement 18 representative past works and reevaluate them using a balanced, relevant, and up-to-date dataset comprising 124,000 applications. We also carry out new experiments designed to fill holes in existing knowledge, and use our findings to identify the most effective features and models to use for Android malware detection within a contemporary environment. We show that high detection accuracies (up to 96.8%) can be achieved using features extracted through static analysis alone, yielding a mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;</title><link>https://arxiv.org/abs/2007.15776</link><description>&lt;p&gt;
&#29992;&#20110;&#27969;&#24418;&#19978;&#20989;&#25968;&#36924;&#36817;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Vector Functional Link Networks for Function Approximation on Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2007.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20110;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#20005;&#26684;&#35777;&#26126;&#65292;&#22635;&#34917;&#20102;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#30340;&#29702;&#35770;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
feed-forward&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36895;&#24230;&#22240;&#24930;&#32780;&#33879;&#21517;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#24050;&#32463;&#25104;&#20026;&#29942;&#39048;&#25968;&#21313;&#24180;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#23581;&#35797;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20943;&#23569;&#23398;&#20064;&#38656;&#27714;&#12290;&#22522;&#20110;Igelnik&#21644;Pao&#30340;&#21407;&#22987;&#26500;&#36896;&#65292;&#20855;&#26377;&#38543;&#26426;&#36755;&#20837;&#21040;&#38544;&#34255;&#23618;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#21333;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#32570;&#20047;&#24517;&#35201;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#29702;&#35770;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#65288;&#26356;&#27491;&#30340;&#65289;&#20005;&#26684;&#35777;&#26126;&#65292;&#35777;&#26126;Igelnik&#21644;Pao&#30340;&#26500;&#36896;&#26159;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#22312;&#32039;&#33268;&#22495;&#19978;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#65292;&#36924;&#36817;&#35823;&#24046;&#20687;&#28176;&#36817;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2007.15776v3 Announce Type: replace-cross  Abstract: The learning speed of feed-forward neural networks is notoriously slow and has presented a bottleneck in deep learning applications for several decades. For instance, gradient-based learning algorithms, which are used extensively to train neural networks, tend to work slowly when all of the network parameters must be iteratively tuned. To counter this, both researchers and practitioners have tried introducing randomness to reduce the learning requirement. Based on the original construction of Igelnik and Pao, single layer neural-networks with random input-to-hidden layer weights and biases have seen success in practice, but the necessary theoretical justification is lacking. In this paper, we begin to fill this theoretical gap. We provide a (corrected) rigorous proof that the Igelnik and Pao construction is a universal approximator for continuous functions on compact domains, with approximation error decaying asymptotically lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#21253;&#25324;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#35302;&#21457;&#25915;&#20987;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.15295</link><description>&lt;p&gt;
&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#65306;&#26356;&#22810;&#35302;&#21457;&#22120;&#65292;&#26356;&#22810;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Multi-Trigger Backdoor Attacks: More Triggers, More Threats. (arXiv:2401.15295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22810;&#35302;&#21457;&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;&#12290;&#36890;&#36807;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#21253;&#25324;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#65292;&#25991;&#31456;&#25581;&#31034;&#20102;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#21333;&#35302;&#21457;&#25915;&#20987;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#65288;&#39044;&#65289;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#23613;&#31649;&#21518;&#38376;&#25915;&#20987;&#22312;&#19968;&#20123;&#30740;&#31350;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#21333;&#20010;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#26469;&#27745;&#26579;&#25968;&#25454;&#38598;&#30340;&#21333;&#35302;&#21457;&#25915;&#20987;&#19978;&#12290;&#21487;&#20197;&#35828;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#21518;&#38376;&#25915;&#20987;&#21487;&#33021;&#26356;&#21152;&#22797;&#26434;&#65292;&#20363;&#22914;&#65292;&#21516;&#19968;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#23545;&#25163;&#65292;&#22914;&#26524;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36739;&#39640;&#30340;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#35302;&#21457;&#25915;&#20987;&#35774;&#32622;&#19979;&#21518;&#38376;&#25915;&#20987;&#30340;&#23454;&#38469;&#23041;&#32961;&#65292;&#22810;&#20010;&#23545;&#25163;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#35302;&#21457;&#22120;&#26469;&#27745;&#26579;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25552;&#20986;&#21644;&#30740;&#31350;&#24182;&#34892;&#12289;&#39034;&#24207;&#21644;&#28151;&#21512;&#25915;&#20987;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#35302;&#21457;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#35302;&#21457;&#22120;&#23545;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#20849;&#23384;&#12289;&#35206;&#20889;&#21644;&#20132;&#21449;&#28608;&#27963;&#25928;&#26524;&#30340;&#37325;&#35201;&#35748;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21333;&#35302;&#21457;&#25915;&#20987;&#24448;&#24448;&#23481;&#26131;&#24341;&#36215;&#35206;&#20889;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as a primary threat to (pre-)training and deployment of deep neural networks (DNNs). While backdoor attacks have been extensively studied in a body of works, most of them were focused on single-trigger attacks that poison a dataset using a single type of trigger. Arguably, real-world backdoor attacks can be much more complex, e.g., the existence of multiple adversaries for the same dataset if it is of high value. In this work, we investigate the practical threat of backdoor attacks under the setting of \textbf{multi-trigger attacks} where multiple adversaries leverage different types of triggers to poison the same dataset. By proposing and investigating three types of multi-trigger attacks, including parallel, sequential, and hybrid attacks, we provide a set of important understandings of the coexisting, overwriting, and cross-activating effects between different triggers on the same dataset. Moreover, we show that single-trigger attacks tend to cause over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.14512</link><description>&lt;p&gt;
&#25105;&#20204;&#38169;&#36807;&#20102;&#35841;&#65311;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#25581;&#31034;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population. (arXiv:2401.14512v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;Rashomon Set of Optimal Trees (ROOT)&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#30340;&#23569;&#25968;&#20154;&#32676;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#31934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#22312;&#29702;&#35299;&#22240;&#26524;&#25928;&#24212;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#23558;&#25512;&#35770;&#25193;&#23637;&#21040;&#30446;&#26631;&#20154;&#32676;&#26102;&#38754;&#20020;&#25928;&#24212;&#24322;&#36136;&#24615;&#21644;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#20013;&#35782;&#21035;&#21644;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30446;&#26631;&#20154;&#32676;&#20197;&#25552;&#21319;&#26222;&#36866;&#24615;&#30340;&#21019;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#8212;&#8212;Rashomon Set of Optimal Trees (ROOT)&#65292;&#26469;&#25551;&#36848;&#23569;&#25968;&#20154;&#32676;&#12290;ROOT&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#26041;&#24046;&#26469;&#20248;&#21270;&#30446;&#26631;&#23376;&#32676;&#20307;&#20998;&#24067;&#65292;&#20174;&#32780;&#30830;&#20445;&#26356;&#31934;&#30830;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ROOT&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#23569;&#25968;&#20154;&#32676;&#29305;&#24449;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#26377;&#25928;&#27807;&#36890;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#23637;&#29616;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We ap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2401.10190</link><description>&lt;p&gt;
&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Kaczmarz-inspired approach to accelerate the optimization of neural network wavefunctions. (arXiv:2401.10190v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#20110;Kaczmarz&#30340;&#26041;&#27861;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#30340;&#20248;&#21270;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#24050;&#34987;&#35777;&#26126;&#22312;&#21407;&#23376;&#21644;&#23567;&#20998;&#23376;&#30340;&#30005;&#23376;&#32467;&#26500;&#26041;&#38754;&#20135;&#29983;&#39640;&#31934;&#24230;&#32467;&#26524;&#65292;&#20294;&#26159;&#20248;&#21270;&#36825;&#31181;&#27874;&#20989;&#25968;&#30340;&#39640;&#25104;&#26412;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22823;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Subsampled Projected-Increment Natural Gradient Descent (SPRING)&#20248;&#21270;&#22120;&#26469;&#20943;&#23569;&#36825;&#20010;&#29942;&#39048;&#12290;SPRING&#32467;&#21512;&#20102;&#20043;&#21069;&#24341;&#20837;&#30340;&#26368;&#23567;&#27493;&#38271;&#38543;&#26426;&#37325;&#26500;&#20248;&#21270;&#22120;(MinSR)&#21644;&#32463;&#20856;&#30340;&#38543;&#26426;Kaczmarz&#26041;&#27861;&#35299;&#20915;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#24605;&#24819;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#22810;&#20010;&#23567;&#21407;&#23376;&#21644;&#20998;&#23376;&#19978;&#65292;SPRING&#20248;&#20110;MinSR&#21644;&#27969;&#34892;&#30340;Kronecker-Factored Approximate Curvature&#26041;&#27861;(KFAC)&#65292;&#21069;&#25552;&#26159;&#21508;&#31181;&#26041;&#27861;&#30340;&#23398;&#20064;&#29575;&#37117;&#32463;&#36807;&#20102;&#26368;&#20339;&#35843;&#25972;&#12290;&#20363;&#22914;&#65292;&#22312;&#27687;&#21407;&#23376;&#19978;&#65292;SPRING&#22312;&#22235;&#19975;&#27425;&#35757;&#32451;&#36845;&#20195;&#20043;&#21518;&#36798;&#21040;&#20102;&#21270;&#23398;&#31934;&#24230;&#65292;&#32780;MinSR&#21644;KFAC&#29978;&#33267;&#22312;&#19968;&#20010;&#23567;&#26102;&#21518;&#20063;&#26080;&#27861;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network wavefunctions optimized using the variational Monte Carlo method have been shown to produce highly accurate results for the electronic structure of atoms and small molecules, but the high cost of optimizing such wavefunctions prevents their application to larger systems. We propose the Subsampled Projected-Increment Natural Gradient Descent (SPRING) optimizer to reduce this bottleneck. SPRING combines ideas from the recently introduced minimum-step stochastic reconfiguration optimizer (MinSR) and the classical randomized Kaczmarz method for solving linear least-squares problems. We demonstrate that SPRING outperforms both MinSR and the popular Kronecker-Factored Approximate Curvature method (KFAC) across a number of small atoms and molecules, given that the learning rates of all methods are optimally tuned. For example, on the oxygen atom, SPRING attains chemical accuracy after forty thousand training iterations, whereas both MinSR and KFAC fail to do so even after one h
&lt;/p&gt;</description></item><item><title>&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.05352</link><description>&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Categories Discovery for Long-tailed Recognition. (arXiv:2401.05352v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05352
&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#35782;&#21035;&#20013;&#30340;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;(GCD)&#30340;&#37325;&#22823;&#38480;&#21046;&#26159;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#65292;&#32780;&#20107;&#23454;&#19978;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#21576;&#29616;&#38271;&#23614;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#23454;&#29616;&#20102;&#23545;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23427;&#21033;&#29992;&#20102;&#36890;&#36807;&#24050;&#26631;&#35760;&#31867;&#21035;&#38598;&#21512;&#33719;&#21462;&#30340;&#27934;&#23519;&#21147;&#12290;&#29616;&#26377;&#30340;GCD&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#22343;&#34913;&#30340;&#12290;&#19982;&#36825;&#19968;&#20551;&#35774;&#30456;&#21453;&#65292;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#31867;&#21035;&#36890;&#24120;&#34920;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#24050;&#30693;&#25110;&#26222;&#36941;&#30340;&#31867;&#21035;&#27604;&#32597;&#35265;&#30340;&#31867;&#21035;&#26356;&#39057;&#32321;&#22320;&#20986;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#30528;&#37325;&#20110;&#38271;&#23614;&#36890;&#29992;&#31867;&#21035;&#21457;&#29616;&#65288;Long-tailed GCD&#65289;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#19981;&#24179;&#34913;&#24615;&#12290;&#38024;&#23545;&#38271;&#23614;GCD&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#20010;&#31574;&#30053;&#24615;&#27491;&#21017;&#21270;&#30340;&#24378;&#22823;&#26041;&#27861;:&#65288;i&#65289;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#36739;&#23569;&#20986;&#29616;&#30340;&#23614;&#37096;&#31867;&#21035;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Class Discovery (GCD) plays a pivotal role in discerning both known and unknown categories from unlabeled datasets by harnessing the insights derived from a labeled set comprising recognized classes. A significant limitation in prevailing GCD methods is their presumption of an equitably distributed category occurrence in unlabeled data. Contrary to this assumption, visual classes in natural environments typically exhibit a long-tailed distribution, with known or prevalent categories surfacing more frequently than their rarer counterparts. Our research endeavors to bridge this disconnect by focusing on the long-tailed Generalized Category Discovery (Long-tailed GCD) paradigm, which echoes the innate imbalances of real-world unlabeled datasets. In response to the unique challenges posed by Long-tailed GCD, we present a robust methodology anchored in two strategic regularizations: (i) a reweighting mechanism that bolsters the prominence of less-represented, tail-end categories
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.10305</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#40065;&#26834;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Disentangled Representation Learning for Robust Target Speech Extraction. (arXiv:2312.10305v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20998;&#31163;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#26080;&#20851;&#22240;&#32032;&#65292;&#35299;&#20915;&#20102;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20449;&#21495;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#21253;&#21547;&#20840;&#23616;&#22768;&#23398;&#29305;&#24449;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#30446;&#26631;&#35821;&#38899;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#21442;&#32771;&#35821;&#38899;&#20013;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#26080;&#20851;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#23548;&#33268;&#22312;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#20013;&#20986;&#29616;&#35828;&#35805;&#20154;&#28151;&#21472;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#20998;&#35299;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#20004;&#38454;&#27573;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21442;&#32771;&#35821;&#38899;&#32534;&#30721;&#32593;&#32476;&#21644;&#20840;&#23616;&#20449;&#24687;&#20998;&#35299;&#32593;&#32476;&#36880;&#28176;&#20998;&#35299;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#21644;&#20854;&#20182;&#19981;&#30456;&#20851;&#22240;&#32032;&#12290;&#25105;&#20204;&#19987;&#38376;&#20351;&#29992;&#20998;&#35299;&#30340;&#35828;&#35805;&#20154;&#36523;&#20221;&#20449;&#24687;&#26469;&#25351;&#23548;&#35821;&#38899;&#25552;&#21462;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#21046;Transformer&#26469;&#30830;&#20445;&#28151;&#21512;&#20449;&#21495;&#30340;&#22768;&#23398;&#34920;&#31034;&#19981;&#21463;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech signals are inherently complex as they encompass both global acoustic characteristics and local semantic information. However, in the task of target speech extraction, certain elements of global and local semantic information in the reference speech, which are irrelevant to speaker identity, can lead to speaker confusion within the speech extraction network. To overcome this challenge, we propose a self-supervised disentangled representation learning method. Our approach tackles this issue through a two-phase process, utilizing a reference speech encoding network and a global information disentanglement network to gradually disentangle the speaker identity information from other irrelevant factors. We exclusively employ the disentangled speaker identity information to guide the speech extraction network. Moreover, we introduce the adaptive modulation Transformer to ensure that the acoustic representation of the mixed signal remains undisturbed by the speaker embeddings. This com
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.10060</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#31574;&#30053;&#65292;&#20027;&#35201;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20351;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;TSC&#20013;&#30340;DA&#30740;&#31350;&#23384;&#22312;&#30528;&#25991;&#29486;&#35780;&#23457;&#30340;&#29255;&#27573;&#21270;&#65292;&#26041;&#27861;&#23398;&#20998;&#31867;&#19981;&#28165;&#26224;&#65292;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#21450;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#31561;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#23545;TSC&#39046;&#22495;&#20013;&#30340;DA&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25345;&#32493;&#21313;&#24180;&#30340;&#24191;&#27867;&#25991;&#29486;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#20195;&#32508;&#36848;&#25991;&#31456;&#24456;&#23569;&#33021;&#22815;&#28085;&#30422;DA&#22312;TSC&#19978;&#30340;&#20840;&#37096;&#36827;&#23637;&#65292;&#22240;&#27492;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;100&#22810;&#31687;&#23398;&#26415;&#25991;&#31456;&#65292;&#24635;&#32467;&#20986;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;DA&#25216;&#26415;&#12290;&#36825;&#39033;&#20005;&#26684;&#30340;&#20998;&#26512;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#19987;&#38376;&#38024;&#23545;TSC&#20013;&#30340;DA&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing tech
&lt;/p&gt;</description></item><item><title>&#22270;&#24418;-SCP&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#36739;&#23567;&#23376;&#38382;&#39064;&#26469;&#25552;&#39640;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#24418;-SCP&#33021;&#22815;&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;30-70%&#65292;&#21644;&#21830;&#19994;&#27714;&#35299;&#22120;&#30456;&#27604;&#21152;&#36895;&#39640;&#36798;25&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#19979;&#25913;&#36827;&#25110;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07979</link><description>&lt;p&gt;
&#22270;&#24418;-SCP: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks. (arXiv:2310.07979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07979
&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;-SCP&#26159;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#36739;&#23567;&#23376;&#38382;&#39064;&#26469;&#25552;&#39640;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#24418;-SCP&#33021;&#22815;&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;30-70%&#65292;&#21644;&#21830;&#19994;&#27714;&#35299;&#22120;&#30456;&#27604;&#21152;&#36895;&#39640;&#36798;25&#20493;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#19979;&#25913;&#36827;&#25110;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21152;&#36895;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#38598;&#21512;&#35206;&#30422;&#38382;&#39064;&#65288;SCP&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24418;-SCP&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#21253;&#21547;&#35299;&#31354;&#38388;&#30340;&#22823;&#22823;&#36739;&#23567;&#30340;&#23376;&#38382;&#39064;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#38382;&#39064;&#29305;&#24449;&#21644;&#22797;&#26434;&#24230;&#30340;&#21512;&#25104;&#21152;&#26435;&#21644;&#38750;&#21152;&#26435;SCP&#23454;&#20363;&#19978;&#35780;&#20272;&#20102;&#22270;&#24418;-SCP&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;OR Library&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#26159;SCP&#30340;&#19968;&#20010;&#32463;&#20856;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#24418;-SCP&#23558;&#38382;&#39064;&#22823;&#23567;&#20943;&#23569;&#20102;30-70%&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21830;&#19994;&#27714;&#35299;&#22120;&#65288;Gurobi&#65289;&#23454;&#29616;&#20102;&#39640;&#36798;25&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#12290;&#22312;&#32473;&#23450;&#25152;&#38656;&#30340;&#26368;&#20248;&#24615;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#24418;-SCP&#23558;&#25913;&#36827;&#25110;&#29978;&#33267;&#23454;&#29616;100%&#30340;&#26368;&#20248;&#24615;&#12290;&#36825;&#19982;&#24555;&#36895;&#36138;&#23146;&#35299;&#20915;&#26041;&#26696;&#24418;&#25104;&#20102;&#23545;&#27604;&#65292;&#21518;&#32773;&#22312;&#20445;&#35777;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#30340;&#21516;&#26102;&#26126;&#26174;&#25439;&#23475;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#12290;&#22270;&#24418;-SCP&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#38382;&#39064;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) approaches are increasingly being used to accelerate combinatorial optimization (CO) problems. We look specifically at the Set Cover Problem (SCP) and propose Graph-SCP, a graph neural network method that can augment existing optimization solvers by learning to identify a much smaller sub-problem that contains the solution space. We evaluate the performance of Graph-SCP on synthetic weighted and unweighted SCP instances with diverse problem characteristics and complexities, and on instances from the OR Library, a canonical benchmark for SCP. We show that Graph-SCP reduces the problem size by 30-70% and achieves run time speedups up to~25x when compared to commercial solvers (Gurobi). Given a desired optimality threshold, Graph-SCP will improve upon it or even achieve 100% optimality. This is in contrast to fast greedy solutions that significantly compromise solution quality to achieve guaranteed polynomial run time. Graph-SCP can generalize to larger problem sizes
&lt;/p&gt;</description></item><item><title>UAMM&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#23450;&#20215;&#65292;&#24182;&#19988;&#26377;&#25928;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.06375</link><description>&lt;p&gt;
UAMM: UBET&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;
&lt;/p&gt;
&lt;p&gt;
UAMM: UBET Automated Market Maker. (arXiv:2308.06375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06375
&lt;/p&gt;
&lt;p&gt;
UAMM&#26159;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#23450;&#20215;&#65292;&#24182;&#19988;&#26377;&#25928;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24066;&#22330;&#20570;&#24066;&#21830;&#65288;AMM&#65289;&#26159;&#21435;&#20013;&#24515;&#21270;&#20132;&#26131;&#25152;&#65288;DEX&#65289;&#20351;&#29992;&#30340;&#23450;&#20215;&#26426;&#21046;&#12290;&#20256;&#32479;&#30340;AMM&#26041;&#27861;&#20165;&#22522;&#20110;&#20854;&#33258;&#36523;&#30340;&#27969;&#21160;&#24615;&#27744;&#36827;&#34892;&#23450;&#20215;&#65292;&#32780;&#19981;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#25110;&#27969;&#21160;&#24615;&#25552;&#20379;&#32773;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UBET AMM&#65288;UAMM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#21644;&#27969;&#21160;&#24615;&#27744;&#30340;&#26242;&#26102;&#25439;&#22833;&#26469;&#35745;&#31639;&#20215;&#26684;&#12290;&#23613;&#31649;&#20381;&#36182;&#20110;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#28369;&#28857;&#26102;&#20173;&#28982;&#20445;&#25345;&#20102;&#24658;&#23450;&#20135;&#21697;&#26354;&#32447;&#30340;&#26399;&#26395;&#23646;&#24615;&#12290;UAMM&#30340;&#20851;&#38190;&#35201;&#32032;&#26159;&#26681;&#25454;&#26399;&#26395;&#30340;&#30446;&#26631;&#20313;&#39069;&#30830;&#23450;&#21512;&#36866;&#30340;&#28369;&#28857;&#37329;&#39069;&#65292;&#20197;&#40723;&#21169;&#27969;&#21160;&#24615;&#27744;&#26368;&#23567;&#21270;&#26242;&#26102;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22806;&#37096;&#24066;&#22330;&#20215;&#26684;&#26377;&#25928;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#22871;&#21033;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated market makers (AMMs) are pricing mechanisms utilized by decentralized exchanges (DEX). Traditional AMM approaches are constrained by pricing solely based on their own liquidity pool, without consideration of external markets or risk management for liquidity providers. In this paper, we propose a new approach known as UBET AMM (UAMM), which calculates prices by considering external market prices and the impermanent loss of the liquidity pool. Despite relying on external market prices, our method maintains the desired properties of a constant product curve when computing slippages. The key element of UAMM is determining the appropriate slippage amount based on the desired target balance, which encourages the liquidity pool to minimize impermanent loss. We demonstrate that our approach eliminates arbitrage opportunities when external market prices are efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.10529</link><description>&lt;p&gt;
&#24555;&#36895;&#26080;&#30417;&#30563;&#28145;&#24230;&#24322;&#24120;&#20540;&#27169;&#22411;&#36873;&#25321;&#19982;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;(OD)&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#65292;&#24182;&#26377;&#35768;&#22810;&#25216;&#26415;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;OD(DOD)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#26368;&#36817;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;DOD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;(HP)&#35843;&#25972;/&#27169;&#22411;&#36873;&#25321;&#12290;&#34429;&#28982;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#25253;&#21578;&#20102;OD&#27169;&#22411;&#23545;HP&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#23545;&#20110;&#23637;&#31034;&#20102;&#38271;&#21015;&#34920;HP&#30340;&#29616;&#20195;DOD&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HYPER&#26469;&#35843;&#25972;DOD&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;(1)&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#39564;&#35777;(&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#24322;&#24120;&#20540;)&#65292;&#20197;&#21450;(2) HP/&#27169;&#22411;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034; (&#30001;&#20110;HP&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;)&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#26032;&#39062;&#30340;&#36229;&#32593;&#32476;(HN)&#65292;&#20854;&#23558;HP&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;&#21453;&#36807;&#26469;&#65292;HYPER&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;HN&#65292;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;DOD&#27169;&#22411;&#30340;&#26435;&#37325; (&#23545;&#24212;&#20110;...)&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20113;&#20013;&#26500;&#24314;&#30340;&#38543;&#26426;&#20960;&#20309;&#22270;&#19982;&#27969;&#24418;&#20043;&#38388;&#30340;&#26354;&#29575;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#20998;&#26512;&#35777;&#26126;&#20102;&#28857;&#24577;&#19968;&#33268;&#24615;&#20197;&#21450;&#20840;&#23616;&#32467;&#26500;&#29305;&#24615;&#20256;&#25215;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#22270;&#19978;&#28909;&#26680;&#30340;&#25910;&#25947;&#24615;&#21644;&#20174;&#25968;&#25454;&#20113;&#20013;&#23398;&#20064;&#27969;&#24418;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.02378</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#20113;&#20013;Ollivier&#30340;Ricci&#26354;&#29575;&#30340;&#36830;&#32493;&#26497;&#38480;&#65306;&#28857;&#24577;&#19968;&#33268;&#24615;&#21644;&#20840;&#23616;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise consistency and global lower bounds. (arXiv:2307.02378v1 [math.DG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02378
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#25968;&#25454;&#20113;&#20013;&#26500;&#24314;&#30340;&#38543;&#26426;&#20960;&#20309;&#22270;&#19982;&#27969;&#24418;&#20043;&#38388;&#30340;&#26354;&#29575;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#20998;&#26512;&#35777;&#26126;&#20102;&#28857;&#24577;&#19968;&#33268;&#24615;&#20197;&#21450;&#20840;&#23616;&#32467;&#26500;&#29305;&#24615;&#20256;&#25215;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#22270;&#19978;&#28909;&#26680;&#30340;&#25910;&#25947;&#24615;&#21644;&#20174;&#25968;&#25454;&#20113;&#20013;&#23398;&#20064;&#27969;&#24418;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;$\mathcal{M} \subseteq \mathbb{R}^d$&#34920;&#31034;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;$\mathcal{X}= \{ x_1, \dots, x_n \}$&#34920;&#31034;&#20174;$\mathcal{M}$&#22343;&#21248;&#37319;&#26679;&#24471;&#21040;&#30340;&#19968;&#32452;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;$\mathcal{X}$&#26500;&#24314;&#30340;&#38543;&#26426;&#20960;&#20309;&#22270;&#19982;&#27969;&#24418;$\mathcal{M}$&#30340;&#26354;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;Ollivier&#30340;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28857;&#24577;&#12289;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#24182;&#19988;&#36824;&#34920;&#26126;&#65292;&#22914;&#26524;$\mathcal{M}$&#30340;Ricci&#26354;&#29575;&#20174;&#19979;&#38754;&#20005;&#26684;&#22320;&#34987;&#19968;&#20010;&#27491;&#24120;&#25968;&#30028;&#20303;&#65292;&#37027;&#20040;&#38543;&#26426;&#20960;&#20309;&#22270;&#23558;&#20197;&#39640;&#27010;&#29575;&#32487;&#25215;&#27492;&#20840;&#23616;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20840;&#23616;&#31163;&#25955;&#26354;&#29575;&#30028;&#38480;&#22312;&#22270;&#19978;&#28909;&#26680;&#30340;&#25910;&#25947;&#24615;&#36136;&#20197;&#21450;&#23545;&#25968;&#25454;&#20113;&#19978;&#27969;&#24418;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#20801;&#35768;&#36890;&#36807;&#22806;&#31104;&#26354;&#29575;&#34920;&#24449;&#27969;&#24418;&#30340;&#20869;&#22312;&#26354;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let $\mathcal{M} \subseteq \mathbb{R}^d$ denote a low-dimensional manifold and let $\mathcal{X}= \{ x_1, \dots, x_n \}$ be a collection of points uniformly sampled from $\mathcal{M}$. We study the relationship between the curvature of a random geometric graph built from $\mathcal{X}$ and the curvature of the manifold $\mathcal{M}$ via continuum limits of Ollivier's discrete Ricci curvature. We prove pointwise, non-asymptotic consistency results and also show that if $\mathcal{M}$ has Ricci curvature bounded from below by a positive constant, then the random geometric graph will inherit this global structural property with high probability. We discuss applications of the global discrete curvature bounds to contraction properties of heat kernels on graphs, as well as implications for manifold learning from data clouds. In particular, we show that the consistency results allow for characterizing the intrinsic curvature of a manifold from extrinsic curvature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15951</link><description>&lt;p&gt;
&#36890;&#36807;&#36339;&#36807;&#38646;&#20803;&#32032;&#38477;&#20302;&#21367;&#31215;&#23618;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#21098;&#28388;&#27874;&#22120;&#21644;&#36716;&#25442;&#31232;&#30095;&#24352;&#37327;&#20026;&#31264;&#23494;&#24352;&#37327;&#30340;&#26041;&#24335;&#65292;&#36339;&#36807;&#21367;&#31215;&#23618;&#20013;&#30340;0&#20803;&#32032;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20381;&#36182;&#24182;&#34892;&#22788;&#29702;&#22120;&#36827;&#34892;&#21152;&#36895;&#12290;&#20026;&#20102;&#20026;&#20854;&#35774;&#35745;&#36816;&#31639;&#31526;&#65292;&#38656;&#35201;&#19981;&#20165;&#26377;&#20248;&#21270;&#31639;&#27861;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#36824;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#21367;&#31215;&#23618;&#20027;&#35201;&#21253;&#21547;&#19977;&#31181;&#36816;&#31639;&#31526;&#65306;&#21069;&#21521;&#20256;&#25773;&#30340;&#21367;&#31215;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#12290;&#24403;&#25191;&#34892;&#36825;&#20123;&#36816;&#31639;&#26102;&#65292;&#22987;&#32456;&#20250;&#21521;&#24352;&#37327;&#20013;&#28155;&#21152;0&#20803;&#32032;&#65292;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;C-K-S&#31639;&#27861;&#65288;ConvV2, KS-deconv, Sk-dilated&#65289;&#65292;&#20197;&#20004;&#31181;&#26041;&#24335;&#36339;&#36807;&#36825;&#20123;0&#20803;&#32032;&#65306;&#20462;&#21098;&#28388;&#27874;&#22120;&#20197;&#25490;&#38500;&#22635;&#20805;&#30340;0&#20803;&#32032;&#65307;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#20026;&#31264;&#23494;&#24352;&#37327;&#65292;&#36991;&#20813;&#22312;&#21453;&#21367;&#31215;&#21644;&#33192;&#32960;&#21367;&#31215;&#20013;&#25554;&#20837;0&#20803;&#32032;&#12290;&#19982;&#26222;&#36890;&#21367;&#31215;&#30456;&#27604;&#65292;&#21453;&#21367;&#31215;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#32780;&#38590;&#20197;&#21152;&#36895;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;C-K-S&#30340;&#39640;&#24615;&#33021;GPU&#23454;&#29616;&#65292;&#24182;&#36890;&#36807;&#19982;PyTorch&#30340;&#27604;&#36739;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;C-K-S&#30456;&#23545;&#20110;PyTorch&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks rely on parallel processors for acceleration. To design operators for them, it requires not only good algorithm to reduce complexity, but also sufficient utilization of hardwares. Convolutional layers mainly contain 3 kinds of operators: convolution in forward propagation, deconvolution and dilated-convolution in backward propagation. When executing these operators, 0s are always added to tensors, causing redundant calculations. This paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these 0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors to dense tensors, to avoid inserted 0s in deconvolution and dilated-convolution. In contrast to regular convolution, deconvolution is hard to accelerate due to its complicacy. This paper provides high-performance GPU implementations of C-K-S, and verifies their effectiveness with comparison to PyTorch. According to the experiments, C-K-S has advantages over PyTorch in certain cas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14094</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Distributed Online Learning with Guaranteed Optimality. (arXiv:2306.14094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#30001;&#20110;&#20854;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27969;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20010;&#20154;&#31169;&#23494;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#24120;&#24120;&#38754;&#20020;&#20026;&#20102;&#38544;&#31169;&#20445;&#25252;&#32780;&#29306;&#29298;&#23398;&#20064;&#20934;&#30830;&#24615;&#30340;&#22256;&#22659;&#12290;&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#24182;&#30830;&#20445;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#22312;&#30830;&#20445;&#39044;&#26399;&#30636;&#26102;&#36951;&#25022;&#31243;&#24230;&#36880;&#28176;&#20943;&#23567;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20445;&#35777;&#26377;&#38480;&#30340;&#32047;&#31215;&#38544;&#31169;&#39044;&#31639;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#12290;&#20026;&#20102;&#24212;&#23545;&#23436;&#20840;&#20998;&#24067;&#24335;&#29615;&#22659;&#65292;&#25105;&#20204;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#23545;&#20840;&#23616;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed online learning is gaining increased traction due to its unique ability to process large-scale datasets and streaming data. To address the growing public awareness and concern on privacy protection, plenty of private distributed online learning algorithms have been proposed, mostly based on differential privacy which has emerged as the ``gold standard" for privacy protection. However, these algorithms often face the dilemma of trading learning accuracy for privacy. By exploiting the unique characteristics of online learning, this paper proposes an approach that tackles the dilemma and ensures both differential privacy and learning accuracy in distributed online learning. More specifically, while ensuring a diminishing expected instantaneous regret, the approach can simultaneously ensure a finite cumulative privacy budget, even on the infinite time horizon. To cater for the fully distributed setting, we adopt the local differential-privacy framework which avoids the reliance
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.13840</link><description>&lt;p&gt;
&#36229;&#36234;&#35268;&#27169;&#65306;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#35777;&#26126;&#20102;LLMs&#26159;&#22312;&#24418;&#24335;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;
&lt;/p&gt;
&lt;p&gt;
Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data. (arXiv:2306.13840v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#26679;&#24615;&#31995;&#25968;&#20316;&#20026;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#30340;&#25351;&#26631;&#65292;&#30740;&#31350;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#39044;&#20808;&#35757;&#32451;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36235;&#21183;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#20110;&#35757;&#32451;&#24378;&#22823;&#30340;LLMs&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#20294;&#23427;&#26159;&#19968;&#20010;&#27169;&#31946;&#30340;&#27010;&#24565;&#65292;&#23578;&#26410;&#23436;&#20840;&#34920;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;Task2Vec&#22810;&#26679;&#24615;&#31995;&#25968;&#26469;&#22522;&#20110;&#25968;&#25454;&#36136;&#37327;&#30340;&#24418;&#24335;&#26041;&#38754;&#65292;&#36229;&#36234;&#35268;&#27169;&#26412;&#36523;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#20808;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#65292;&#20197;&#35777;&#26126;&#23427;&#20204;&#30340;&#24418;&#24335;&#22810;&#26679;&#24615;&#39640;&#20110;&#29702;&#35770;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#31435;&#23545;&#22810;&#26679;&#24615;&#31995;&#25968;&#30340;&#20449;&#24515;&#65292;&#25105;&#20204;&#36827;&#34892;&#21487;&#35299;&#37322;&#24615;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#35813;&#31995;&#25968;&#19982;&#22810;&#26679;&#24615;&#30340;&#30452;&#35266;&#23646;&#24615;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#65292;&#38543;&#30528;&#28508;&#22312;&#27010;&#24565;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23427;&#22686;&#21152;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22810;&#26679;&#24615;&#31995;&#25968;&#26159;&#21487;&#38752;&#30340;&#65292;&#34920;&#26126;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#31995;&#25968;&#24456;&#39640;&#65292;&#24182;&#25512;&#27979;&#23427;&#21487;&#20197;&#20316;&#20026;&#39044;&#35757;&#32451;LLMs&#27169;&#22411;&#30340;&#25968;&#25454;&#36136;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#65292;&#29992;&#20110;&#36861;&#36394;&#22826;&#38451;&#30340;&#27963;&#21160;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#26041;&#38754;&#20934;&#30830;&#24615;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.08270</link><description>&lt;p&gt;
&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Imagery Tracking of Sun Activity Using 2D Circular Kernel Time Series Transformation, Entropy Measures and Machine Learning Approaches. (arXiv:2306.08270v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#29109;&#24230;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#22826;&#38451;&#27963;&#21160;&#25104;&#20687;&#36319;&#36394;&#25216;&#26415;&#65292;&#21487;&#20197;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#65292;&#29992;&#20110;&#36861;&#36394;&#22826;&#38451;&#30340;&#27963;&#21160;&#24773;&#20917;&#65292;&#23588;&#20854;&#22312;&#35782;&#21035;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#26041;&#38754;&#20934;&#30830;&#24615;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#30340;&#24615;&#36136;&#38750;&#24120;&#22797;&#26434;&#65292;&#20854;&#35266;&#27979;&#22270;&#20687;&#29305;&#24449;&#26159;&#20102;&#35299;&#22826;&#38451;&#27963;&#21160;&#12289;&#31354;&#38388;&#21644;&#22320;&#29699;&#22825;&#27668;&#26465;&#20214;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26469;&#28304;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#20351;&#29992;2D&#22278;&#24418;&#26680;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12289;&#32479;&#35745;&#21644;&#29109;&#24230;&#37327;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36861;&#36394;&#22826;&#38451;&#27963;&#21160;&#12290;&#35813;&#25216;&#26415;&#23558;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#36716;&#25442;&#20026;1&#32500;&#26102;&#38388;&#24207;&#21015;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#35745;&#21644;&#29109;&#24230;&#37327;&#25110;&#30452;&#25509;&#20998;&#31867;&#31561;&#26041;&#27861;&#20174;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20026;&#8220;&#22826;&#38451;&#39118;&#26292;&#8221;&#21644;&#8220;&#38750;&#39118;&#26292;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#36861;&#36394;&#22826;&#38451;&#27963;&#21160;&#30340;&#28508;&#22312;&#20934;&#30830;&#24615;&#20026;&#32422;.
&lt;/p&gt;
&lt;p&gt;
The sun is highly complex in nature and its observatory imagery features is one of the most important sources of information about the sun activity, space and Earth's weather conditions. The NASA, solar Dynamics Observatory captures approximately 70,000 images of the sun activity in a day and the continuous visual inspection of this solar observatory images is challenging. In this study, we developed a technique of tracking the sun's activity using 2D circular kernel time series transformation, statistical and entropy measures, with machine learning approaches. The technique involves transforming the solar observatory image section into 1-Dimensional time series (1-DTS) while the statistical and entropy measures (Approach 1) and direct classification (Approach 2) is used to capture the extraction features from the 1-DTS for machine learning classification into 'solar storm' and 'no storm'. We found that the potential accuracy of the model in tracking the activity of the sun is approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;</title><link>http://arxiv.org/abs/2306.01213</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35299;&#32544;&#32469;&#30340;&#22240;&#26524;&#34920;&#31034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22240;&#20854;&#23545;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35299;&#32544;&#32469;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#22240;&#26524;&#20851;&#31995;&#35266;&#23519;&#26631;&#31614;&#26469;&#30417;&#30563;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#27969;&#30340;&#24494;&#20998;&#21516;&#32986;&#20989;&#25968;&#23558;&#22122;&#22768;&#21464;&#37327;&#26144;&#23556;&#21040;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20013;&#26469;&#24314;&#27169;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#22240;&#26524;&#35201;&#32032;&#30340;&#35299;&#32544;&#32469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#32469;&#20808;&#39564;&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#26469;&#40723;&#21169;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#22240;&#26524;&#20998;&#35299;&#20998;&#24067;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#22240;&#26524;&#35201;&#32032;&#21644;&#26426;&#21046;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#36880;&#20803;&#37325;&#21442;&#25968;&#21270;&#30340;&#38480;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Learning disentangled causal representations is a challenging problem that has gained significant attention recently due to its implications for extracting meaningful information for downstream tasks. In this work, we define a new notion of causal disentanglement from the perspective of independent causal mechanisms. We propose ICM-VAE, a framework for learning causally disentangled representations supervised by causally related observed labels. We model causal mechanisms using learnable flow-based diffeomorphic functions to map noise variables to latent causal variables. Further, to promote the disentanglement of causal factors, we propose a causal disentanglement prior that utilizes the known causal structure to encourage learning a causally factorized distribution in the latent space. Under relatively mild conditions, we provide theoretical results showing the identifiability of causal factors and mechanisms up to permutation and elementwise reparameterization. We empirically demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10668</link><description>&lt;p&gt;
MetaGAD&#65306;&#23398;&#20064;&#20803;&#36716;&#31227;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#20010;&#39046;&#22495;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#37329;&#34701;&#27450;&#35784;&#12289;&#31038;&#20250;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#20837;&#20405;&#31561;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#25191;&#34892;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24448;&#24448;&#22826;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#24322;&#24120;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21487;&#33021;&#20250;&#23558;&#34987;&#35782;&#21035;&#30340;&#24322;&#24120;&#35270;&#20026;&#25968;&#25454;&#22122;&#22768;&#25110;&#19981;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#21487;&#33719;&#21462;&#26377;&#38480;&#30340;&#26631;&#35760;&#24322;&#24120;&#65292;&#36825;&#20123;&#26631;&#35760;&#24322;&#24120;&#20855;&#26377;&#25512;&#36827;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#21644;&#22823;&#37327;&#26080;&#26631;&#35760;&#33410;&#28857;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24037;&#20316;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;MetaGAD&#65292;&#23398;&#20064;&#20803;&#36716;&#31227;&#30693;&#35782;&#26469;&#36827;&#34892;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.07715</link><description>&lt;p&gt;
&#36890;&#36807;&#27531;&#24046;&#32553;&#25918;&#23454;&#29616;ResNets&#30340;&#20449;&#21495;&#26368;&#20248;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Optimal signal propagation in ResNets through residual scaling. (arXiv:2305.07715v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Residual&#32593;&#32476;&#65288;ResNets&#65289;&#22312;&#22823;&#28145;&#24230;&#19978;&#27604;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#24341;&#20837;&#36339;&#36807;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#20449;&#21495;&#21521;&#26356;&#28145;&#23618;&#30340;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20026;&#27531;&#24046;&#20998;&#25903;&#28155;&#21152;&#32553;&#25918;&#21442;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#23613;&#31649;&#20182;&#20204;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#36825;&#31181;&#32553;&#25918;&#21442;&#25968;&#29305;&#21035;&#26377;&#21033;&#30340;&#21462;&#20540;&#33539;&#22260;&#65292;&#20294;&#20854;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#21450;&#20854;&#22312;&#32593;&#32476;&#36229;&#21442;&#25968;&#19978;&#30340;&#26222;&#36866;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#23545;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNets&#65289;&#65292;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#22312;&#20449;&#21495;&#20256;&#25773;&#21644;&#36229;&#21442;&#25968;&#35843;&#33410;&#26041;&#38754;&#33719;&#24471;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20026;ResNets&#23548;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#20197;&#30740;&#31350;&#20449;&#21495;&#20256;&#25773;&#21450;&#20854;&#23545;&#27531;&#24046;&#20998;&#25903;&#32553;&#25918;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#23548;&#20986;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#34913;&#37327;&#32593;&#32476;&#23545;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#19968;&#31181;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#22312;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks (ResNets) have significantly better trainability and thus performance than feed-forward networks at large depth. Introducing skip connections facilitates signal propagation to deeper layers. In addition, previous works found that adding a scaling parameter for the residual branch further improves generalization performance. While they empirically identified a particularly beneficial range of values for this scaling parameter, the associated performance improvement and its universality across network hyperparameters yet need to be understood. For feed-forward networks (FFNets), finite-size theories have led to important insights with regard to signal propagation and hyperparameter tuning. We here derive a systematic finite-size theory for ResNets to study signal propagation and its dependence on the scaling for the residual branch. We derive analytical expressions for the response function, a measure for the network's sensitivity to inputs, and show that for deep netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2305.02217</link><description>&lt;p&gt;
&#27969;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#27969;&#39640;&#25928;&#23398;&#20064;&#8221;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26088;&#22312;&#35299;&#20915;&#20174;&#25968;&#25454;&#27969;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#38382;&#39064;&#65292;&#20854;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#21450;&#26102;&#26377;&#25928;&#22320;&#34987;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#24448;&#24448;&#38543;&#30528;&#26102;&#38388;&#30340;&#31215;&#32047;&#20197;&#27969;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#20174;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19981;&#21516;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#19981;&#33021;&#24573;&#35270;&#27969;&#20837;&#30340;&#25968;&#25454;&#27969;&#21487;&#33021;&#26159;&#26080;&#20241;&#27490;&#30340;&#12289;&#35268;&#27169;&#24040;&#22823;&#12289;&#21464;&#21270;&#26410;&#30693;&#65292;&#24182;&#19988;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;&#35745;&#31639;/&#23384;&#20648;&#36164;&#28304;&#21487;&#20197;&#21450;&#26102;&#22788;&#29702;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#25968;&#25454;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20165;&#21462;&#20915;&#20110;&#25509;&#25910;&#21040;&#20102;&#22810;&#23569;&#25968;&#25454;&#65292;&#32780;&#19988;&#21462;&#20915;&#20110;&#26377;&#22810;&#23569;&#25968;&#25454;&#33021;&#22815;&#34987;&#21450;&#26102;&#22320;&#26377;&#25928;&#21033;&#29992;&#65292;&#21152;&#19978;&#36164;&#28304;&#21644;&#36895;&#24230;&#30340;&#32771;&#34385;&#65292;&#20877;&#21152;&#19978;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#21644;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21534;&#21520;&#37327;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#27969;&#39640;&#25928;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.06879</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19979;&#30340;&#25191;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Neural Networks. (arXiv:2304.06879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#39044;&#27979;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#24182;&#24433;&#21709;&#20854;&#39044;&#27979;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#65292;&#21363;&#36866;&#29992;&#20110;&#20854;&#20135;&#29983;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#12290;&#22312;&#20351;&#29992;&#37325;&#22797;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#25910;&#25947;&#32467;&#26524;&#20013;&#65292;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#21487;Lipschitz&#36830;&#32493;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25439;&#22833;&#24517;&#39035;&#23545;&#36825;&#20123;&#21442;&#25968;&#24378;&#20984;&#21644;&#24179;&#28369;&#65307;&#21542;&#21017;&#65292;&#35813;&#26041;&#27861;&#23558;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#21457;&#25955;&#12290;&#28982;&#32780;&#26412;&#25991;&#21017;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#30340;&#65292;&#36825;&#26159;&#25191;&#34892;&#31995;&#32479;&#30340;&#26356;&#21152;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24314;&#27169;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#20854;&#26469;&#23454;&#35777;&#25191;&#34892;&#31283;&#23450;&#24615;&#30456;&#23545;&#20110;&#20854;&#20182;&#30446;&#26631;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction is a framework for learning models that influence the data they intend to predict. We focus on finding classifiers that are performatively stable, i.e. optimal for the data distribution they induce. Standard convergence results for finding a performatively stable classifier with the method of repeated risk minimization assume that the data distribution is Lipschitz continuous to the model's parameters. Under this assumption, the loss must be strongly convex and smooth in these parameters; otherwise, the method will diverge for some problems. In this work, we instead assume that the data distribution is Lipschitz continuous with respect to the model's predictions, a more natural assumption for performative systems. As a result, we are able to significantly relax the assumptions on the loss function. In particular, we do not need to assume convexity with respect to the model's parameters. As an illustration, we introduce a resampling procedure that models realisti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#35823;&#24046;&#28176;&#36817;&#20998;&#24067;&#12289;&#25552;&#20986;&#25237;&#24433;SHMM&#31639;&#27861;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12289;&#24182;&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;PSHMM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.07437</link><description>&lt;p&gt;
&#32553;&#23567;&#21487;&#29992;&#24615;&#24046;&#36317;&#65306;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#23398;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models. (arXiv:2302.07437v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#35823;&#24046;&#28176;&#36817;&#20998;&#24067;&#12289;&#25552;&#20986;&#25237;&#24433;SHMM&#31639;&#27861;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12289;&#24182;&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;PSHMM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baum-Welch&#65288;B-W&#65289;&#31639;&#27861;&#26159;&#25512;&#26029;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#26368;&#24191;&#27867;&#25509;&#21463;&#30340;&#26041;&#27861;&#12290; &#28982;&#32780;&#65292;&#23427;&#24456;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#65292;&#32780;&#19988;&#23545;&#20110;&#35768;&#22810;&#23454;&#26102;&#24212;&#29992;&#26469;&#35828;&#36895;&#24230;&#22826;&#24930;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#27861;&#65288;MOM&#65289;&#30340;HMM&#30340;&#35889;&#23398;&#20064;&#65288;SHMM&#65289;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#12290;&#23613;&#31649;&#26377;&#36825;&#26679;&#30340;&#25215;&#35834;&#65292;&#20294;SHMM&#30340;&#28176;&#36817;&#29702;&#35770;&#19968;&#30452;&#24456;&#38590;&#24471;&#21040;&#65292;&#32780;SHMM&#30340;&#38271;&#26399;&#24615;&#33021;&#21487;&#33021;&#20250;&#30001;&#20110;&#35823;&#24046;&#30340;&#26080;&#38480;&#20256;&#25773;&#32780;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;(1)&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#36817;&#20284;&#35823;&#24046;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;(2)&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#31216;&#20026;&#25237;&#24433;SHMM&#65288;PSHMM&#65289;&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;(3)&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#65292;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;SHMM&#12289;PSHMM&#21644;B-W&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Baum-Welch (B-W) algorithm is the most widely accepted method for inferring hidden Markov models (HMM). However, it is prone to getting stuck in local optima, and can be too slow for many real-time applications. Spectral learning of HMMs (SHMM), based on the method of moments (MOM) has been proposed in the literature to overcome these obstacles. Despite its promises, asymptotic theory for SHMM has been elusive, and the long-run performance of SHMM can degrade due to unchecked propagation of error. In this paper, we (1) provide an asymptotic distribution for the approximate error of the likelihood estimated by SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that mitigates the problem of error propagation, and (3) develop online learning variants of both SHMM and PSHMM that accommodate potential nonstationarity. We compare the performance of SHMM with PSHMM and estimation through the B-W algorithm on both simulated data and data from real world applications, and fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.02560</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22240;&#26524;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;: &#22312;&#32654;&#22269;&#35780;&#20272;&#26356;&#20005;&#26684;&#30340;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#30340;&#20581;&#24247;&#25928;&#30410;
&lt;/p&gt;
&lt;p&gt;
Causal Estimation of Exposure Shifts with Neural Networks: Evaluating the Health Benefits of Stricter Air Quality Standards in the US. (arXiv:2302.02560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20854;&#29702;&#35770;&#22522;&#30784;&#21644;&#23454;&#26045;&#30340;&#21487;&#34892;&#24615;&#65292;&#20174;&#32780;&#20272;&#35745;&#36830;&#32493;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#25919;&#31574;&#30456;&#20851;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#36890;&#36807;&#35780;&#20272;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#23545;PM2.5&#30340;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#36827;&#34892;&#20462;&#35746;&#21518;&#30340;&#20581;&#24247;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#31574;&#30740;&#31350;&#20013;&#65292;&#20272;&#35745;&#36830;&#32493;&#24615;&#26292;&#38706;/&#27835;&#30103;&#30340;&#20998;&#24067;&#23545;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#26159;&#26368;&#20851;&#38190;&#30340;&#20998;&#26512;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20559;&#31227;-&#21709;&#24212;&#20989;&#25968;&#65288;SRF&#65289;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#28041;&#21450;&#24378;&#20581;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#23454;&#29616;&#65292;&#29992;&#20110;SRF&#20272;&#35745;&#12290;&#21463;&#20844;&#20849;&#21355;&#29983;&#20013;&#30340;&#20851;&#38190;&#25919;&#31574;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#20854;&#29702;&#35770;&#22522;&#30784;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#24378;&#20581;&#24615;&#21644;&#25928;&#29575;&#20445;&#35777;&#30340;SRF&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#21547;6800&#19975;&#20010;&#20010;&#20307;&#21644;2700&#19975;&#20010;&#32654;&#22269;&#22659;&#20869;&#27515;&#20129;&#20107;&#20214;&#30340;&#25968;&#25454;&#20013;&#65292;&#20197;&#20272;&#35745;&#23558;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#20445;&#25252;&#23616;&#65288;EPA&#65289;&#26368;&#36817;&#25552;&#35758;&#20174;12 &#956;g/m&#179;&#25913;&#20026;9 &#956;g/m&#179;&#30340;PM2.5&#30340;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#31354;&#27668;&#36136;&#37327;&#26631;&#20934;&#65288;NAAQS&#65289;&#30340;&#20462;&#35746;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#39318;&#27425;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
In policy research, one of the most critical analytic tasks is to estimate the causal effect of a policy-relevant shift to the distribution of a continuous exposure/treatment on an outcome of interest. We call this problem shift-response function (SRF) estimation. Existing neural network methods involving robust causal-effect estimators lack theoretical guarantees and practical implementations for SRF estimation. Motivated by a key policy-relevant question in public health, we develop a neural network method and its theoretical underpinnings to estimate SRFs with robustness and efficiency guarantees. We then apply our method to data consisting of 68 million individuals and 27 million deaths across the U.S. to estimate the causal effect from revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 $\mu g/m^3$ to 9 $\mu g/m^3$. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate, for the first time, the 
&lt;/p&gt;</description></item></channel></rss>