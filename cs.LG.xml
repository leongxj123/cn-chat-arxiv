<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#27169;&#25311;&#22270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation of Graph Algorithms with Looped Transformers
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24490;&#29615;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#35813;&#32467;&#26500;&#21487;&#20197;&#27169;&#25311;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22270;&#31639;&#27861;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#30001;&#20110;&#26377;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#36827;&#23637;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#33021;&#22815;&#20351;&#29992;&#20851;&#31995;&#25968;&#25454;&#22797;&#21046;&#25512;&#29702;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#32593;&#32476;&#27169;&#25311;&#22270;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#24102;&#39069;&#22806;&#27880;&#24847;&#21147;&#22836;&#21644;&#19982;&#22270;&#24418;&#20132;&#20114;&#30340;&#24490;&#29615;&#21464;&#21387;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#27169;&#25311;&#35832;&#22914;Dijkstra&#30340;&#26368;&#30701;&#36335;&#24452;&#31639;&#27861;&#12289;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#12289;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;Kosaraju&#30340;&#24378;&#36830;&#36890;&#20998;&#37327;&#31639;&#27861;&#31561;&#31639;&#27861;&#12290;&#32593;&#32476;&#30340;&#23485;&#24230;&#19981;&#38543;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#65292;&#36825;&#24847;&#21619;&#30528;&#32593;&#32476;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#22270;&#19978;&#30340;&#19978;&#36848;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#26377;&#19968;&#20010;&#30001;&#20110;&#26377;&#38480;&#31934;&#24230;&#32780;&#21463;&#21040;&#38480;&#21046;&#30340;&#27169;&#25311;&#26497;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#22270;&#28789;&#23436;&#25972;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution of graph algorithms using neural networks has recently attracted significant interest due to promising empirical progress. This motivates further understanding of how neural networks can replicate reasoning steps with relational data. In this work, we study the ability of transformer networks to simulate algorithms on graphs from a theoretical perspective. The architecture that we utilize is a looped transformer with extra attention heads that interact with the graph. We prove by construction that this architecture can simulate algorithms such as Dijkstra's shortest path algorithm, Breadth- and Depth-First Search, and Kosaraju's strongly connected components algorithm. The width of the network does not increase with the size of the input graph, which implies that the network can simulate the above algorithms for any graph. Despite this property, we show that there is a limit to simulation in our solution due to finite precision. Finally, we show a Turing Completeness resu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.</title><link>https://arxiv.org/abs/2404.02040</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#20316;&#20026;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers as Transducers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02040
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;RASP&#30340;&#21464;&#20307;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#8220;&#20687;&#21464;&#21387;&#22120;&#19968;&#26679;&#24605;&#32771;&#8221;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#24067;&#23572;&#21464;&#20307;B-RASP&#21040;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30830;&#20999;&#35745;&#31639;&#20102;&#19968;&#38454;&#26377;&#29702;&#20989;&#25968;&#65288;&#22914;&#23383;&#31526;&#20018;&#26059;&#36716;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#25193;&#23637;&#12290;B-RASP[pos]&#20801;&#35768;&#22312;&#20301;&#32622;&#19978;&#36827;&#34892;&#35745;&#31639;&#65288;&#22914;&#22797;&#21046;&#23383;&#31526;&#20018;&#30340;&#21069;&#21322;&#37096;&#20998;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#27491;&#21017;&#20989;&#25968;&#12290;S-RASP&#28155;&#21152;&#21069;&#32512;&#21644;&#65292;&#21487;&#20197;&#36827;&#34892;&#39069;&#22806;&#30340;&#31639;&#26415;&#25805;&#20316;&#65288;&#22914;&#23545;&#23383;&#31526;&#20018;&#27714;&#24179;&#26041;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#22810;&#27491;&#21017;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#19968;&#20010;&#25512;&#35770;&#26159;n...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02040v1 Announce Type: cross  Abstract: We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people "think like transformers," as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP. A corollary of our results is a n
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2404.01569</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#38598;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#31181;&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models Using Contrast Sets: An Experimental Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01569
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#26367;&#25442;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#20026;&#21516;&#20041;&#35789;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#25991;&#26412;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#24230;&#37327;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#38169;&#35823;&#24230;&#37327;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#35813;&#24230;&#37327;&#22312;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#29702;&#35299;&#35821;&#21477;&#34164;&#28085;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#26031;&#22374;&#31119;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;SNLI&#65289;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#27604;&#38598;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#28041;&#21450;&#33258;&#21160;&#23558;&#21160;&#35789;&#12289;&#21103;&#35789;&#21644;&#24418;&#23481;&#35789;&#26367;&#25442;&#20026;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#65292;&#20197;&#20445;&#30041;&#21477;&#23376;&#30340;&#21407;&#22987;&#21547;&#20041;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#22522;&#20110;&#30495;&#23454;&#30340;&#35821;&#35328;&#29702;&#35299;&#36824;&#26159;&#20165;&#20165;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;&#25105;&#20204;&#20351;&#29992;ELECTRA-small&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#22312;&#20256;&#32479;&#30340;SNLI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;89.9%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#23545;&#27604;&#38598;&#19978;&#26174;&#31034;&#20986;&#20102;72.5%&#30340;&#20934;&#30830;&#24230;&#65292;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01569v1 Announce Type: cross  Abstract: In the domain of Natural Language Inference (NLI), especially in tasks involving the classification of multiple input texts, the Cross-Entropy Loss metric is widely employed as a standard for error measurement. However, this metric falls short in effectively evaluating a model's capacity to understand language entailments. In this study, we introduce an innovative technique for generating a contrast set for the Stanford Natural Language Inference (SNLI) dataset. Our strategy involves the automated substitution of verbs, adverbs, and adjectives with their synonyms to preserve the original meaning of sentences. This method aims to assess whether a model's performance is based on genuine language comprehension or simply on pattern recognition. We conducted our analysis using the ELECTRA-small model. The model achieved an accuracy of 89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5% on our contrast set, indicati
&lt;/p&gt;</description></item><item><title>ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.15004</link><description>&lt;p&gt;
ParFormer&#65306;&#20855;&#26377;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#30340;&#35270;&#35273;Transformer&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
ParFormer: Vision Transformer Baseline with Parallel Local Global Token Mixer and Convolution Attention Patch Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15004
&lt;/p&gt;
&lt;p&gt;
ParFormer&#25552;&#20986;&#20102;&#24182;&#34892;&#23616;&#37096;&#20840;&#23616;&#26631;&#35760;&#28151;&#21512;&#22120;&#21644;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;&#65292;&#20248;&#21270;&#20102;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ParFormer&#20316;&#20026;&#19968;&#31181;&#22686;&#24378;&#22411;Transformer&#26550;&#26500;&#65292;&#20801;&#35768;&#23558;&#19981;&#21516;&#30340;&#26631;&#35760;&#28151;&#21512;&#22120;&#25972;&#21512;&#21040;&#21333;&#20010;&#38454;&#27573;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#12290;&#21516;&#26102;&#25972;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#25968;&#25454;&#65292;&#23454;&#29616;&#23545;&#30701;&#31243;&#21644;&#38271;&#31243;&#31354;&#38388;&#20851;&#31995;&#30340;&#31934;&#30830;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#20687;&#24179;&#31227;&#31383;&#21475;&#36825;&#26679;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#38500;&#20102;&#24182;&#34892;&#26631;&#35760;&#28151;&#21512;&#22120;&#32534;&#30721;&#22120;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21367;&#31215;&#27880;&#24847;&#21147;&#34917;&#19969;&#23884;&#20837;(CAPE)&#65292;&#20316;&#20026;&#26631;&#20934;&#34917;&#19969;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#36890;&#36807;&#21367;&#31215;&#27880;&#24847;&#21147;&#27169;&#22359;&#25913;&#36827;&#26631;&#35760;&#28151;&#21512;&#22120;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ParFormer&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#22810;&#20010;&#22797;&#26434;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20110;CNN&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;CAPE&#24050;&#34987;&#35777;&#26126;&#26377;&#30410;&#20110;&#25972;&#20307;MetaFormer&#26550;&#26500;&#65292;&#21363;&#20351;&#20351;&#29992;Id&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15004v1 Announce Type: cross  Abstract: This work presents ParFormer as an enhanced transformer architecture that allows the incorporation of different token mixers into a single stage, hence improving feature extraction capabilities. Integrating both local and global data allows for precise representation of short- and long-range spatial relationships without the need for computationally intensive methods such as shifting windows. Along with the parallel token mixer encoder, We offer the Convolutional Attention Patch Embedding (CAPE) as an enhancement of standard patch embedding to improve token mixer extraction with a convolutional attention module. Our comprehensive evaluation demonstrates that our ParFormer outperforms CNN-based and state-of-the-art transformer-based architectures in image classification and several complex tasks such as object recognition. The proposed CAPE has been demonstrated to benefit the overall MetaFormer architecture, even while utilizing the Id
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.05754</link><description>&lt;p&gt;
&#27169;&#24335;&#35782;&#21035;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;ResNet&#21644;DenseNet&#21450;&#20854;&#23436;&#25972;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24403;&#20170;&#25968;&#23383;&#25216;&#26415;&#30340;&#25509;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#32321;&#33635;&#30340;&#22522;&#30784;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#21457;&#23637;&#30340;&#31038;&#20250;&#38656;&#27714;&#27491;&#22312;&#24378;&#35843;&#26367;&#20195;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#21518;&#25705;&#23572;&#26102;&#20195;&#30340;&#26469;&#20020;&#25512;&#21160;&#20102;&#20855;&#26377;&#21331;&#36234;&#28508;&#21147;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#26032;&#26087;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#27604;&#36739;&#20013;&#23384;&#22312;&#21547;&#31946;&#25351;&#26631;&#65292;&#22240;&#27492;&#19968;&#22871;&#26126;&#30830;&#30340;&#35780;&#20272;&#31995;&#32479;&#19982;&#35814;&#32454;&#30340;&#25351;&#26631;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17943</link><description>&lt;p&gt;
&#20351;&#29992;SoS&#23494;&#24230;&#20272;&#35745;&#21644;&#945;-&#31163;&#25955;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Sequential transport maps using SoS density estimation and $\alpha$-divergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20004;&#32773;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#32780;&#23454;&#29616;&#20174;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20256;&#36755;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#36817;&#20284;&#23494;&#24230;&#29983;&#25104;&#26679;&#26412;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35843;&#26597;&#20102;&#25552;&#20986;&#30340;&#39034;&#24207;&#20256;&#36755;&#26144;&#23556;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#31995;&#21015;&#32452;&#25104;&#30340;Knothe-Rosenblatt&#65288;KR&#65289;&#26144;&#23556;&#20043;&#19978;&#12290;&#20854;&#20013;&#27599;&#20010;&#26144;&#23556;&#37117;&#26159;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#20013;&#31561;&#22797;&#26434;&#24230;&#30340;&#20013;&#38388;&#23494;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#20174;&#21442;&#32771;&#23494;&#24230;&#21040;&#39044;&#35745;&#31639;&#36817;&#20284;&#23494;&#24230;&#30340;&#31934;&#30830;KR&#26144;&#23556;&#32780;&#26500;&#24314;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;SoS&#23494;&#24230;&#21644;&#945;-&#31163;&#25955;&#24230;&#26469;&#36817;&#20284;&#20013;&#38388;&#23494;&#24230;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#23558;SoS&#23494;&#24230;&#19982;&#945;-&#31163;&#25955;&#24230;&#30456;&#32467;&#21512;&#20135;&#29983;&#20102;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#21322;&#23450;&#32534;&#31243;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#945;-&#31163;&#25955;&#24230;&#30340;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#20351;&#24471;&#33021;&#22815;&#22788;&#29702;&#26410;&#26631;&#20934;&#21270;&#30340;&#23494;&#24230;&#65292;&#20174;&#32780;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17943v1 Announce Type: cross  Abstract: Transport-based density estimation methods are receiving growing interest because of their ability to efficiently generate samples from the approximated density. We further invertigate the sequential transport maps framework proposed from arXiv:2106.04170 arXiv:2303.02554, which builds on a sequence of composed Knothe-Rosenblatt (KR) maps. Each of those maps are built by first estimating an intermediate density of moderate complexity, and then by computing the exact KR map from a reference density to the precomputed approximate density. In our work, we explore the use of Sum-of-Squares (SoS) densities and $\alpha$-divergences for approximating the intermediate densities. Combining SoS densities with $\alpha$-divergence interestingly yields convex optimization problems which can be efficiently solved using semidefinite programming. The main advantage of $\alpha$-divergences is to enable working with unnormalized densities, which provide
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26102;&#31354;&#20808;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.15939</link><description>&lt;p&gt;
&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#29992;&#20110;&#24555;&#36895;&#21160;&#24577;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15939
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26102;&#31354;&#20808;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22270;&#20687;&#37325;&#24314;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#24515;&#33039;&#35786;&#26029;&#20013;&#36215;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25104;&#20687;&#65292;k-&#31354;&#38388;&#25968;&#25454;&#21487;&#20197;&#36827;&#34892;&#27424;&#37319;&#26679;&#65292;&#20294;&#22270;&#20687;&#37325;&#24314;&#38754;&#20020;&#30528;&#39640;&#32500;&#22788;&#29702;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38477;&#32500;&#21487;&#20998;&#31163;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#25968;&#25454;&#26497;&#20026;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#21331;&#36234;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20854;&#19982;&#26102;&#31354;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#21487;&#20998;&#31163;&#26102;&#31354;&#23398;&#20064;&#32593;&#32476;&#65288;DeepSSL&#65289;&#65292;&#35813;&#32593;&#32476;&#23637;&#24320;&#20102;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#20302;&#31209;&#24615;&#21644;&#31354;&#38388;&#31232;&#30095;&#24615;&#30340;&#37325;&#24314;&#27169;&#22411;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#20013;&#38388;&#36755;&#20986;&#34987;&#21487;&#35270;&#21270;&#65292;&#20197;&#25552;&#20379;&#26377;&#20851;&#32593;&#32476;&#34892;&#20026;&#30340;&#35265;&#35299;&#24182;&#22686;&#24378;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#24515;&#33039;&#30701;&#29255;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DeepSSL&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15939v1 Announce Type: cross  Abstract: Dynamic magnetic resonance imaging (MRI) plays an indispensable role in cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled but the image reconstruction poses a great challenge of high-dimensional processing. This challenge leads to necessitate extensive training data in many deep learning reconstruction methods. This work proposes a novel and efficient approach, leveraging a dimension-reduced separable learning scheme that excels even with highly limited training data. We further integrate it with spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an iteration process of a reconstruction model with both temporal low-rankness and spatial sparsity. Intermediate outputs are visualized to provide insights into the network's behavior and enhance its interpretability. Extensive results on cardiac cine datasets show that the proposed DeepSSL is superior to th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.15062</link><description>&lt;p&gt;
&#21035;&#32781;&#33457;&#25307;&#65281;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#35843;&#25972;&#20197;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Gotcha! Don't trick me with unanswerable questions! Self-aligning Large Language Models for Responding to Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15062
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#22238;&#31572;&#26410;&#30693;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25298;&#32477;&#22238;&#31572;&#24182;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#22238;&#31572;&#38382;&#39064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38382;&#39064;&#27809;&#26377;&#26126;&#30830;&#31572;&#26696;&#26102;&#24448;&#24448;&#34920;&#29616;&#20986;&#30456;&#24403;&#31243;&#24230;&#30340;&#33258;&#20449;&#36807;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#36825;&#20123;&#26410;&#30693;&#38382;&#39064;&#25552;&#20379;&#34394;&#26500;&#31572;&#26696;&#65292;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#25506;&#35752;&#25298;&#32477;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#33258;&#25105;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;LLM&#26412;&#36523;&#26469;&#22686;&#24378;&#20854;&#23545;&#19981;&#21516;&#31867;&#22411;&#26410;&#30693;&#38382;&#39064;&#30340;&#22238;&#24212;&#33021;&#21147;&#65292;&#19981;&#20165;&#33021;&#22815;&#25298;&#32477;&#22238;&#31572;&#65292;&#36824;&#33021;&#22815;&#35299;&#37322;&#26410;&#30693;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#30340;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Self-Align&#26041;&#27861;&#39318;&#20808;&#37319;&#29992;&#20004;&#38454;&#27573;&#31867;&#24863;&#30693;&#33258;&#25105;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#22823;&#37327;&#26410;&#30693;&#38382;&#39064;-&#22238;&#24212;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24046;&#24322;&#39537;&#21160;&#30340;&#33258;&#25105;&#25972;&#29702;&#65292;&#36873;&#25321;&#21512;&#26684;&#25968;&#25454;&#23545;LLM&#26412;&#36523;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35843;&#25972;&#23545;&#26410;&#30693;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15062v1 Announce Type: new  Abstract: Despite the remarkable abilities of Large Language Models (LLMs) to answer questions, they often display a considerable level of overconfidence even when the question does not have a definitive answer. To avoid providing hallucinated answers to these unknown questions, existing studies typically investigate approaches to refusing to answer these questions. In this work, we propose a novel and scalable self-alignment method to utilize the LLM itself to enhance its response-ability to different types of unknown questions, being capable of not only refusing to answer but also providing explanation to the unanswerability of unknown questions. Specifically, the Self-Align method first employ a two-stage class-aware self-augmentation approach to generate a large amount of unknown question-response data. Then we conduct disparity-driven self-curation to select qualified data for fine-tuning the LLM itself for aligning the responses to unknown q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09177</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#36718;&#20132;&#20114;&#21033;&#29992;&#19978;&#19979;&#25991;&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36234;&#29425;&#25915;&#20987;&#36890;&#36807;&#24494;&#22937;&#22320;&#20462;&#25913;&#25915;&#20987;&#26597;&#35810;&#26469;&#25552;&#21462;&#26377;&#23475;&#20449;&#24687;&#12290;&#38543;&#30528;&#38450;&#24481;&#26426;&#21046;&#30340;&#36827;&#21270;&#65292;&#36234;&#29425;&#25915;&#20987;&#30452;&#25509;&#33719;&#21462;&#26377;&#23475;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#38388;&#25509;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#30340;&#23454;&#36341;&#21551;&#21457;&#65292;&#38024;&#23545;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#12290;&#25105;&#20204;&#35748;&#20026;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#8212;&#8212;&#25915;&#20987;&#26597;&#35810;&#20043;&#21069;&#30340;&#20449;&#24687;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21021;&#27493;&#38382;&#31572;&#23545;&#19982;LLMs&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#30340;&#22238;&#31572;&#26397;&#30528;&#25581;&#31034;&#8220;&#26399;&#26395;&#30340;&#8221;&#26377;&#23475;&#20449;&#24687;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>HAMLET&#26159;&#19968;&#20010;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HAMLET&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.03541</link><description>&lt;p&gt;
HAMLET&#65306;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HAMLET: Graph Transformer Neural Operator for Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03541
&lt;/p&gt;
&lt;p&gt;
HAMLET&#26159;&#19968;&#20010;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HAMLET&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#25442;&#26694;&#26550;HAMLET&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26102;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20855;&#26377;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#21442;&#25968;&#23545;&#24212;&#25511;&#21046;&#65292;&#20351;&#24471;HAMLET&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;HAMLET&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#21040;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#65292;&#23637;&#31034;&#20986;&#20854;&#40065;&#26834;&#24615;&#12290;HAMLET&#19981;&#20165;&#36866;&#29992;&#20110;&#21333;&#19968;&#31867;&#22411;&#30340;&#29289;&#29702;&#27169;&#25311;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24377;&#24615;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#23494;&#24230;&#27604;&#20272;&#35745;(DRE)&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22240;DRE&#30340;&#25439;&#22833;&#20989;&#25968;&#32780;&#20986;&#29616;&#20102;&#20248;&#21270;&#38382;&#39064;&#65306;KL&#25955;&#24230;&#38656;&#35201;&#22823;&#26679;&#26412;&#65292;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#65292;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#26377;&#20559;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25216;&#26415;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;DRE&#30340;&#26679;&#26412;&#35201;&#27714;&#65292;&#20197;$L_1$&#35823;&#24046;&#30340;&#19978;&#30028;&#32852;&#31995;&#36215;&#26469;&#65292;&#35813;&#19978;&#30028;&#23558;&#39640;&#32500;&#24230;DRE&#20219;&#21153;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#20316;&#20026;&#19968;&#20010;&#20849;&#21516;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01879</link><description>&lt;p&gt;
$\sigma$-zero: &#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$-&#33539;&#25968;&#23545;&#25239;&#26679;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#22522;&#20110;&#26799;&#24230;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25915;&#20987;&#32771;&#34385;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#32422;&#26463;&#26469;&#21046;&#36896;&#36755;&#20837;&#25200;&#21160;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#20102;&#31232;&#30095;&#30340;$\ell_1$&#21644;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#22312;&#38750;&#20984;&#19988;&#38750;&#21487;&#24494;&#32422;&#26463;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26159;&#30740;&#31350;&#26368;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#25581;&#31034;&#22312;&#26356;&#20256;&#32479;&#30340;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#25915;&#20987;&#20013;&#26410;&#33021;&#27979;&#35797;&#20986;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#65292;&#31216;&#20026;$\sigma$-zero&#65292;&#23427;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#19968;&#20010;&#29305;&#27530;&#21487;&#24494;&#36817;&#20284;&#26469;&#20419;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#21160;&#24577;&#35843;&#25972;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#25200;&#21160;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;</title><link>https://arxiv.org/abs/2312.08255</link><description>&lt;p&gt;
OCTDL&#65306;&#22522;&#20110;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08255
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;OCTDL&#30340;&#24320;&#25918;&#33719;&#21462;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26631;&#35760;&#26377;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#30340;OCT&#22270;&#20687;&#65292;&#26377;&#21161;&#20110;&#35786;&#26029;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#22312;&#30524;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;OCT&#21487;&#20197;&#21487;&#35270;&#21270;&#35270;&#32593;&#33180;&#23618;&#65292;&#23545;&#26089;&#26399;&#26816;&#27979;&#21644;&#30417;&#27979;&#35270;&#32593;&#33180;&#30142;&#30149;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#25918;&#33719;&#21462;&#30340;OCT&#25968;&#25454;&#38598;&#65288;OCTDL&#65289;&#65292;&#21253;&#25324;&#36229;&#36807;2000&#24352;&#26681;&#25454;&#30142;&#30149;&#32452;&#21644;&#35270;&#32593;&#33180;&#30149;&#29702;&#26631;&#35760;&#30340;OCT&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#24739;&#26377;&#32769;&#24180;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#12289;&#29627;&#29827;&#20307;&#35270;&#32593;&#33180;&#33180;&#65288;ERM&#65289;&#12289;&#35270;&#32593;&#33180;&#21160;&#33033;&#38381;&#22622;&#65288;RAO&#65289;&#12289;&#35270;&#32593;&#33180;&#38745;&#33033;&#38381;&#22622;&#65288;RVO&#65289;&#21644;&#29627;&#29827;&#20307;&#40644;&#26001;&#30028;&#38754;&#30142;&#30149;&#65288;VID&#65289;&#30340;&#24739;&#32773;&#30340;OCT&#35760;&#24405;&#12290;&#36825;&#20123;&#22270;&#20687;&#26159;&#20351;&#29992;Optovue Avanti RTVue XR&#37319;&#38598;&#30340;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#25195;&#25551;&#38271;&#24230;&#30340;&#20809;&#26629;&#25195;&#25551;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08255v2 Announce Type: replace-cross  Abstract: Optical coherence tomography (OCT) is a non-invasive imaging technique with extensive clinical applications in ophthalmology. OCT enables the visualization of the retinal layers, playing a vital role in the early detection and monitoring of retinal diseases. OCT uses the principle of light wave interference to create detailed images of the retinal microstructures, making it a valuable tool for diagnosing ocular conditions. This work presents an open-access OCT dataset (OCTDL) comprising over 2000 OCT images labeled according to disease group and retinal pathology. The dataset consists of OCT records of patients with Age-related Macular Degeneration (AMD), Diabetic Macular Edema (DME), Epiretinal Membrane (ERM), Retinal Artery Occlusion (RAO), Retinal Vein Occlusion (RVO), and Vitreomacular Interface Disease (VID). The images were acquired with an Optovue Avanti RTVue XR using raster scanning protocols with dynamic scan length a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.06835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Graph Anomaly Detection via Normal Structure Regularisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#24320;&#25918;&#22270;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#20219;&#21153;&#65292;&#21363;&#24320;&#25918;&#24335;GAD&#65292;&#26088;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#35757;&#32451;&#27491;&#24120;&#33410;&#28857;&#21644;&#24322;&#24120;&#33410;&#28857;&#65288;&#31216;&#20026;&#24050;&#30693;&#24322;&#24120;&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#33410;&#28857;&#65292;&#36825;&#20123;&#33410;&#28857;&#26080;&#27861;&#23637;&#31034;&#25152;&#26377;&#21487;&#33021;&#30340;&#25512;&#29702;&#26102;&#24322;&#24120;&#12290;&#24050;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;GAD&#27169;&#22411;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20808;&#39564;&#30693;&#35782;&#65292;&#21487;&#22823;&#22823;&#38477;&#20302;&#26816;&#27979;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#24448;&#24448;&#36807;&#20998;&#24378;&#35843;&#25311;&#21512;&#24050;&#30693;&#24322;&#24120;&#65292;&#23548;&#33268;&#23545;&#26410;&#30693;&#24322;&#24120;&#65288;&#21363;&#26410;&#34987;&#26631;&#35760;&#30340;&#24322;&#24120;&#33410;&#28857;&#65289;&#30340;&#24369;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#34987;&#24341;&#20837;&#20197;&#22788;&#29702;&#27431;&#20960;&#37324;&#24503;&#25968;&#25454;&#65292;&#26410;&#33021;&#26377;&#25928;&#25429;&#25417;GAD&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24503;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24320;&#25918;&#24335;GAD&#26041;&#27861;&#65292;&#21363;&#27491;&#24120;&#32467;&#26500;&#35268;&#33539;&#21270;&#65288;NSReg&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#24322;&#24120;&#30340;&#24191;&#20041;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.12478</link><description>&lt;p&gt;
&#23567;&#25209;&#37327;&#23376;&#27169;&#26368;&#22823;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mini-batch Submodular Maximization. (arXiv:2401.12478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12478
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#65292;&#35813;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#22312;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#19979;&#26368;&#22823;&#21270;&#19968;&#20010;&#38750;&#36127;&#21333;&#35843;&#21487;&#20998;&#35299;&#30340;&#23376;&#27169;&#20989;&#25968;F&#30340;&#23567;&#25209;&#37327;&#31639;&#27861;&#65292;&#20854;&#20013;F&#31561;&#20110;$f^i$&#30340;&#21644;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#36229;&#36234;&#20102;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#30340;&#20570;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35299;&#27604;&#22522;&#20110;&#31232;&#30095;&#21270;&#26041;&#27861;&#29983;&#25104;&#30340;&#35299;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first mini-batch algorithm for maximizing a non-negative monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of constraints. We improve over the sparsifier based approach both in theory and in practice. We experimentally observe that our algorithm generates solutions that are far superior to those generated by the sparsifier based approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.07656</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24615;&#33021;&#26356;&#22909;&#30340;POMDP&#31574;&#30053;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Explainable and Better Performing Representations of POMDP Strategies. (arXiv:2401.07656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#31574;&#30053;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#26356;&#23567;&#26356;&#26131;&#29702;&#35299;&#65292;&#19988;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21487;&#25913;&#21892;&#31574;&#30053;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#35760;&#24518;&#12290;&#19968;&#31181;&#34920;&#31034;&#36825;&#31181;&#35760;&#24518;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25913;&#36827;&#30340;L*&#31639;&#27861;&#23398;&#20064;&#31574;&#30053;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#31574;&#30053;&#30340;&#34920;&#26684;&#34920;&#31034;&#30456;&#27604;&#65292;&#24471;&#21040;&#30340;&#33258;&#21160;&#26426;&#20307;&#31215;&#26174;&#33879;&#26356;&#23567;&#65292;&#22240;&#27492;&#26356;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#25913;&#21892;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#19982;&#30452;&#25509;&#20174;POMDP&#21512;&#25104;&#33258;&#21160;&#26426;&#20197;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19981;&#21487;&#27604;&#25311;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategies for partially observable Markov decision processes (POMDP) typically require memory. One way to represent this memory is via automata. We present a method to learn an automaton representation of a strategy using a modification of the L*-algorithm. Compared to the tabular representation of a strategy, the resulting automaton is dramatically smaller and thus also more explainable. Moreover, in the learning process, our heuristics may even improve the strategy's performance. In contrast to approaches that synthesize an automaton directly from the POMDP thereby solving it, our approach is incomparably more scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#23558;$n$&#20010;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21521;&#37327;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#34892;&#24615;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01181</link><description>&lt;p&gt;
&#23558;&#22823;&#37327;&#38543;&#26426;&#28857;&#25311;&#21512;&#25104;&#26925;&#29699;&#20307;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fitting an ellipsoid to a quadratic number of random points. (arXiv:2307.01181v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01181
&lt;/p&gt;
&lt;p&gt;
&#23558;$n$&#20010;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38543;&#26426;&#21521;&#37327;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#34892;&#24615;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24403;$n, d \to \infty $&#26102;&#65292;&#23558;$n$&#20010;&#26631;&#20934;&#39640;&#26031;&#38543;&#26426;&#21521;&#37327;&#25311;&#21512;&#21040;&#20197;&#21407;&#28857;&#20026;&#20013;&#24515;&#30340;&#26925;&#29699;&#20307;&#30340;&#36793;&#30028;&#30340;&#38382;&#39064;$(\mathrm{P})$&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#29468;&#27979;&#20855;&#26377;&#23574;&#38160;&#30340;&#21487;&#34892;&#24615;&#36716;&#21464;&#65306;&#23545;&#20110;&#20219;&#24847;$\varepsilon &gt; 0$&#65292;&#22914;&#26524;$n \leq (1 - \varepsilon) d^2 / 4$&#65292;&#37027;&#20040;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26377;&#35299;&#65307;&#32780;&#22914;&#26524;$n \geq (1 + \varepsilon) d^2 /4$&#65292;&#37027;&#20040;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26080;&#35299;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#36127;&#38754;&#24773;&#20917;&#65292;&#21482;&#30693;&#36947;$n \geq d^2 / 2$&#26159;&#24179;&#20961;&#30340;&#19968;&#20010;&#19978;&#30028;&#65292;&#32780;&#23545;&#20110;&#27491;&#38754;&#24773;&#20917;&#65292;&#24050;&#30693;&#30340;&#26368;&#22909;&#32467;&#26524;&#26159;&#20551;&#35774;$n \leq d^2 / \mathrm{polylog}(d)$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;Bartl&#21644;Mendelson&#20851;&#20110;&#38543;&#26426;&#21521;&#37327;&#30340;Gram&#30697;&#38453;&#38598;&#20013;&#24615;&#30340;&#19968;&#20010;&#20851;&#38190;&#32467;&#26524;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#24403;$n \leq d^2 / C$&#26102;&#65292;&#38382;&#39064;$(\mathrm{P})$&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#26159;&#21487;&#34892;&#30340;&#65292;&#20854;&#20013;$C&gt; 0$&#26159;&#19968;&#20010;&#65288;&#21487;&#33021;&#24456;&#22823;&#30340;&#65289;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as $n, d \to \infty$. This problem is conjectured to have a sharp feasibility transition: for any $\varepsilon &gt; 0$, if $n \leq (1 - \varepsilon) d^2 / 4$ then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$ has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while the best results on the positive side assume $n \leq d^2 / \mathrm{polylog}(d)$. In this work, we improve over previous approaches using a key result of Bartl &amp; Mendelson on the concentration of Gram matrices of random vectors under mild assumptions on their tail behavior. This allows us to give a simple proof that $(\mathrm{P})$ is feasible with high probability when $n \leq d^2 / C$, for a (possibly large) constant $C &gt; 0$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10079</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#24102;&#26377;&#36793;&#26102;&#24207;&#29366;&#24577;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformer for Dynamic Graph Representation Learning with Edge Temporal States. (arXiv:2304.10079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26410;&#33021;&#26126;&#30830;&#24314;&#27169;&#36793;&#26102;&#24207;&#29366;&#24577;&#21644;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#22270;&#25968;&#25454;&#20998;&#26512;&#30340;&#24191;&#27867;&#38656;&#27714;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#27491;&#25104;&#20026;&#19968;&#39033;&#36235;&#21183;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#20219;&#21153;&#12290;&#23613;&#31649;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23637;&#29616;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#26126;&#30830;&#22320;&#23545;&#33410;&#28857;&#29305;&#24449;&#38543;&#26102;&#38388;&#29255;&#27573;&#30340;&#36793;&#26102;&#24207;&#29366;&#24577;&#20135;&#29983;&#24433;&#21709;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;GNNs&#30340;&#20869;&#22312;over-smoothing&#32570;&#38519;&#65292;&#23427;&#20204;&#24456;&#38590;&#25552;&#21462;&#20840;&#23616;&#32467;&#26500;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#24046;&#20998;&#22270;&#21464;&#25442;&#22120;&#65288;RDGT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#20026;&#27599;&#20010;&#24555;&#29031;&#20013;&#30340;&#36793;&#20998;&#37197;&#20102;&#21508;&#31181;&#31867;&#22411;&#21644;&#26435;&#37325;&#65292;&#20197;&#26126;&#30830;&#22320;&#35828;&#26126;&#23427;&#20204;&#30340;&#29305;&#23450;&#26102;&#38388;&#29366;&#24577;&#65292;&#28982;&#21518;&#37319;&#29992;&#22686;&#24378;&#32467;&#26500;&#30340;&#22270;&#21464;&#25442;&#22120;&#26469;&#36890;&#36807;&#24490;&#29615;&#23398;&#20064;&#33539;&#24335;&#25429;&#33719;&#26102;&#38388;&#33410;&#28857;&#34920;&#31034;&#12290;&#22312;&#22235;&#20010;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph representation learning is growing as a trending yet challenging research task owing to the widespread demand for graph data analysis in real world applications. Despite the encouraging performance of many recent works that build upon recurrent neural networks (RNNs) and graph neural networks (GNNs), they fail to explicitly model the impact of edge temporal states on node features over time slices. Additionally, they are challenging to extract global structural features because of the inherent over-smoothing disadvantage of GNNs, which further restricts the performance. In this paper, we propose a recurrent difference graph transformer (RDGT) framework, which firstly assigns the edges in each snapshot with various types and weights to illustrate their specific temporal states explicitly, then a structure-reinforced graph transformer is employed to capture the temporal node representations by a recurrent learning paradigm. Experimental results on four real-world datasets d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.13734</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#21319;&#33945;&#29305;&#21345;&#32599;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#29616;&#22312;&#20445;&#25345;&#30456;&#21516;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21069;&#25552;&#19979;&#65292;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#30340;&#30446;&#30340;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#21487;&#20197;&#27604;&#26222;&#36890;&#30340; MC &#20272;&#35745;&#22120;&#20135;&#29983;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#35813;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#32599; (MC) &#26041;&#27861;&#26159;&#20272;&#35745;&#31574;&#30053;&#34920;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#65292;MC &#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36816;&#34892;&#35813;&#31574;&#30053;&#20197;&#25910;&#38598;&#26679;&#26412;&#24182;&#21462;&#20986;&#32467;&#26524;&#24179;&#22343;&#20540;&#26469;&#32473;&#20986;&#20272;&#35745;&#20540;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#25910;&#38598;&#30340;&#26679;&#26412;&#31216;&#20026;&#22312;&#32447;&#26679;&#26412;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#20272;&#35745;&#20540;&#65292;MC &#26041;&#27861;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#22312;&#32447;&#26679;&#26412;&#12290;&#24403;&#22312;&#32447;&#26679;&#26412;&#26114;&#36149;&#26102;&#65292;&#20363;&#22914;&#22312;&#32447;&#25512;&#33616;&#21644;&#24211;&#23384;&#31649;&#29702;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#20943;&#23569;&#22312;&#32447;&#26679;&#26412;&#25968;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#32447; MC &#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#34892;&#19981;&#21516;&#30340;&#31574;&#30053;&#65288;&#31216;&#20026;&#34892;&#20026;&#31574;&#30053;&#65289;&#35780;&#20272;&#24863;&#20852;&#36259;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20351;&#31163;&#32447; MC &#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#26126;&#26174;&#23567;&#20110;&#26222;&#36890; MC &#20272;&#35745;&#22120;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#23450;&#21046;&#34892;&#20026;&#31574;&#30053;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#31163;&#32447;&#25968;&#25454;&#65292;&#21363;&#20808;&#21069;&#35760;&#24405;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#27604;&#22312;&#32447;&#26679;&#26412;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#38656;&#20351;&#29992;&#23567;&#37096;&#20998;&#22312;&#32447;&#26679;&#26412;&#23601;&#33021;&#23454;&#29616;&#30456;&#21516;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.06140</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#65306;&#26032;&#30340;&#38544;&#31169;&#20998;&#26512;&#19982;&#25512;&#26029;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bootstrap: New Privacy Analysis and Inference Strategies. (arXiv:2210.06140v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#38544;&#31169;&#25104;&#26412;&#30340;&#26032;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#35823;&#29992;&#12290;&#38543;&#30528;&#37319;&#26679;&#27425;&#25968;&#36235;&#36817;&#26080;&#38480;&#22823;&#65292;&#27492;&#26041;&#27861;&#36880;&#28176;&#28385;&#36275;&#26356;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#24615;&#26469;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#65292;&#20294;&#22312;&#24212;&#29992;&#20013;&#65292;&#32479;&#35745;&#25512;&#26029;&#20173;&#28982;&#32570;&#20047;&#36890;&#29992;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#24067;&#22810;&#20010;&#31169;&#26377;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#26469;&#25512;&#26029;&#26679;&#26412;&#20998;&#24067;&#24182;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#38544;&#31169;&#20998;&#26512;&#25552;&#20379;&#20102;&#21333;&#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#38544;&#31169;&#25104;&#26412;&#26032;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#24341;&#23548;&#37319;&#26679;&#30340;&#19968;&#20123;&#35823;&#29992;&#12290;&#20351;&#29992;Gaussian-DP&#65288;GDP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20174;&#28385;&#36275; $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP &#30340;&#26426;&#21046;&#20013;&#37322;&#25918; $B$ &#20010;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#65292;&#22312; $B$ &#36235;&#36817;&#26080;&#38480;&#22823;&#26102;&#28176;&#36817;&#22320;&#28385;&#36275; $\mu$-GDP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#24341;&#23548;&#37319;&#26679;&#20272;&#35745;&#30340;&#21453;&#21367;&#31215;&#23545;&#26679;&#26412;&#20998;&#24067;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially private (DP) mechanisms protect individual-level information by introducing randomness into the statistical analysis procedure. Despite the availability of numerous DP tools, there remains a lack of general techniques for conducting statistical inference under DP. We examine a DP bootstrap procedure that releases multiple private bootstrap estimates to infer the sampling distribution and construct confidence intervals (CIs). Our privacy analysis presents new results on the privacy cost of a single DP bootstrap estimate, applicable to any DP mechanisms, and identifies some misapplications of the bootstrap in the existing literature. Using the Gaussian-DP (GDP) framework (Dong et al.,2022), we show that the release of $B$ DP bootstrap estimates from mechanisms satisfying $(\mu/\sqrt{(2-2/\mathrm{e})B})$-GDP asymptotically satisfies $\mu$-GDP as $B$ goes to infinity. Moreover, we use deconvolution with the DP bootstrap estimates to accurately infer the sampling distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;</title><link>http://arxiv.org/abs/2204.11970</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#23545;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#36827;&#34892;&#35270;&#21147;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#31995;&#32479;&#65292;&#21487;&#39640;&#31934;&#24230;&#39044;&#27979;&#19977;&#31181;&#30524;&#30142;&#24739;&#32773;&#30340;&#35270;&#21147;&#21464;&#21270;&#65292;&#24182;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#30524;&#31185;&#23398;&#20013;&#30340;&#29627;&#29827;&#20307;&#25163;&#26415;&#33647;&#29289;&#27835;&#30103;&#26159;&#27835;&#30103;&#24180;&#40836;&#30456;&#20851;&#24615;&#40644;&#26001;&#21464;&#24615;&#65288;AMD&#65289;&#12289;&#31958;&#23615;&#30149;&#24615;&#40644;&#26001;&#27700;&#32959;&#65288;DME&#65289;&#21644;&#35270;&#32593;&#33180;&#38745;&#33033;&#38459;&#22622;&#65288;RVO&#65289;&#30456;&#20851;&#30142;&#30149;&#30340;&#19968;&#31181;&#26222;&#36941;&#27835;&#30103;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#23436;&#25972;&#24615;&#65292;&#24739;&#32773;&#24448;&#24448;&#20250;&#22312;&#22810;&#24180;&#26102;&#38388;&#20869;&#22833;&#21435;&#35270;&#21147;&#65292;&#23613;&#31649;&#25509;&#21463;&#27835;&#30103;&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;IT&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#34701;&#21512;&#20102;&#24503;&#22269;&#19968;&#23478;&#26368;&#20339;&#21307;&#30103;&#20445;&#20581;&#21307;&#38498;&#30340;&#30524;&#31185;&#37096;&#38376;&#30340;&#19981;&#21516;IT&#31995;&#32479;&#12290;&#32463;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#24739;&#32773;&#35270;&#21147;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#19977;&#31181;&#30142;&#30149;&#30340;&#39044;&#27979;&#25552;&#20379;&#39640;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#24037;&#20855;&#65292;&#36741;&#21161;&#30524;&#31185;&#21307;&#29983;&#36827;&#34892;&#20020;&#24202;&#20915;&#31574;&#21644;&#24739;&#32773;&#21672;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In ophthalmology, intravitreal operative medication therapy (IVOM) is a widespread treatment for diseases related to the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. We found out for the disease AMD a significant deterioration of the visual acuity over time. Within our proposed m
&lt;/p&gt;</description></item></channel></rss>