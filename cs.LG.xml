<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02175</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#21453;&#24212;&#30340;&#31038;&#20250;&#21160;&#24577;&#65306;&#34701;&#21512;&#32479;&#35745;&#29289;&#29702;&#23398;&#19982;&#33829;&#38144;&#21160;&#24577;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02175
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28040;&#36153;&#32773;&#23545;&#24191;&#21578;&#36755;&#20837;&#30340;&#21453;&#24212;&#23545;&#20110;&#26088;&#22312;&#20248;&#21270;&#24191;&#21578;&#31574;&#30053;&#24182;&#25552;&#39640;&#24191;&#21578;&#27963;&#21160;&#26377;&#25928;&#24615;&#30340;&#33829;&#38144;&#20154;&#21592;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#28304;&#33258;&#29289;&#29702;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#20102;&#35832;&#22914;&#23545;&#31216;&#24615;&#12289;&#26631;&#24230;&#24459;&#21644;&#30456;&#21464;&#31561;&#27010;&#24565;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#31243;&#39564;&#35777;&#19982;Michaelis-Menten&#21644;Hill&#26041;&#31243;&#31561;&#33879;&#21517;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#20934;&#30830;&#34920;&#31034;&#28040;&#36153;&#32773;&#21453;&#24212;&#21160;&#24577;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20998;&#26512;&#24378;&#35843;&#20102;&#20851;&#38190;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#33829;&#38144;&#25928;&#26524;&#12289;&#21453;&#24212;&#25935;&#24863;&#24230;&#21644;&#34892;&#20026;&#25935;&#24863;&#24230;&#65289;&#23545;&#24433;&#21709;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24191;&#21578;&#21830;&#21644;&#33829;&#38144;&#20154;&#21592;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02175v1 Announce Type: cross  Abstract: Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#22810;&#26465;&#20214;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Multi-Conditional Ranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#19968;&#32452;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#24050;&#25104;&#20026;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#35752;&#20102;&#22810;&#26465;&#20214;&#25490;&#24207;&#30340;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MCRank&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36328;&#19981;&#21516;&#39033;&#30446;&#31867;&#22411;&#21644;&#26465;&#20214;&#36827;&#34892;&#22810;&#26465;&#20214;&#25490;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;MCRank&#23545;LLMs&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#39033;&#30446;&#21644;&#26465;&#20214;&#25968;&#37327;&#20197;&#21450;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25490;&#24207;&#26465;&#20214;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#23545;&#26465;&#20214;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19578</link><description>&lt;p&gt;
&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#23454;&#29616;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19578
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#25104;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#23601;&#21487;&#20197;&#25191;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#20869;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#35266;&#27979;&#65288;&#36755;&#20837;&#65289;&#21644;&#21160;&#20316;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#21487;&#20197;&#34987;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#25509;&#25910;&#21644;&#29983;&#25104;&#65292;&#36890;&#36807;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#20165;&#22312;&#35821;&#35328;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#20123;&#21464;&#24418;&#22120;&#25797;&#38271;&#23558;&#26631;&#35760;&#21270;&#30340;&#35270;&#35273;&#20851;&#38190;&#28857;&#35266;&#23519;&#32763;&#35793;&#20026;&#34892;&#20026;&#36712;&#36857;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#22871;&#20214;&#20013;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;&#25193;&#25955;&#31574;&#30053;&#65289;&#12290;KAT&#19981;&#21516;&#20110;&#36890;&#24120;&#22312;&#35821;&#35328;&#39046;&#22495;&#25805;&#20316;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#22312;&#35270;&#35273;&#21644;&#21160;&#20316;&#39046;&#22495;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19578v1 Announce Type: cross  Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06402</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#19981;&#36866;&#29992;&#65306;&#23398;&#20064;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#22810;&#23569;&#20363;&#20026;&#20102;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032; Abstract: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#24050;&#32463;&#20174;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#21040;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24494;&#35843;&#30340;&#26497;&#31471;&#24418;&#24335;&#28041;&#21450;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#65288;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#65289;&#21482;&#21463;&#21040;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#21464;&#21270;&#65288;&#31216;&#20026;&#25351;&#20196;&#25110;&#25552;&#31034;&#65289;&#30340;&#25511;&#21046;&#12290;ICL&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23454;&#20363;&#20316;&#20026;&#31034;&#20363;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20026;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20351;&#29992;&#38745;&#24577;&#25968;&#37327;&#30340;&#31034;&#20363;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;ICL&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#20351;&#29992;&#30340;&#28436;&#31034;&#25968;&#37327;&#26159;&#21160;&#24577;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
&lt;/p&gt;</description></item><item><title>WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04808</link><description>&lt;p&gt;
WaterMax: &#25171;&#30772;LLM&#27700;&#21360;&#21487;&#26816;&#27979;&#24615;-&#31283;&#20581;&#24615;-&#36136;&#37327;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04808
&lt;/p&gt;
&lt;p&gt;
WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#26159;&#38459;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;WaterMax&#30340;&#26032;&#39062;&#27700;&#21360;&#26041;&#26696;&#65292;&#20855;&#26377;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20854;&#26032;&#35774;&#35745;&#19981;&#20250;&#23545;LLM&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#65288;&#19981;&#35843;&#25972;&#26435;&#37325;&#12289;&#23545;&#25968;&#12289;&#28201;&#24230;&#25110;&#37319;&#26679;&#25216;&#26415;&#65289;&#12290;WaterMax&#24179;&#34913;&#20102;&#31283;&#20581;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#27700;&#21360;&#25216;&#26415;&#30456;&#21453;&#65292;&#20174;&#26681;&#26412;&#19978;&#24341;&#21457;&#20102;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20854;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#26368;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19979;&#65292;&#23427;&#32988;&#36807;&#25152;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04808v1 Announce Type: cross  Abstract: Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16346</link><description>&lt;p&gt;
&#29992;&#25345;&#20037;&#21516;&#35843;&#22686;&#24378;&#22270;&#27744;&#21270;
&lt;/p&gt;
&lt;p&gt;
Boosting Graph Pooling with Persistent Homology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#30340;&#26426;&#21046;&#26174;&#33879;&#25552;&#21319;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#25345;&#20037;&#21516;&#35843;&#65288;PH&#65289;&#32435;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20197;&#20016;&#23500;&#34920;&#36798;&#33021;&#21147;&#30340;&#36235;&#21183;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;PH&#29305;&#24449;&#25554;&#20837;GNN&#23618;&#24635;&#26159;&#24102;&#26469;&#36739;&#20302;&#21487;&#35299;&#37322;&#24615;&#30340;&#36793;&#38469;&#25913;&#36827;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;PH&#21521;&#27744;&#21270;&#23618;&#27880;&#20837;&#20840;&#23616;&#25299;&#25169;&#19981;&#21464;&#24615;&#65292;&#28789;&#24863;&#26469;&#33258;PH&#20013;&#30340;&#36807;&#28388;&#25805;&#20316;&#33258;&#28982;&#22320;&#20351;&#22270;&#27744;&#21270;&#20197;&#25130;&#26029;&#26041;&#24335;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#24335;&#19979;&#65292;&#31895;&#21270;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#27839;&#30528;&#25345;&#20037;&#27744;&#21270;&#25299;&#25169;&#36827;&#34892;&#65292;&#20174;&#32780;&#25552;&#21319;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20960;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16346v1 Announce Type: new  Abstract: Recently, there has been an emerging trend to integrate persistent homology (PH) into graph neural networks (GNNs) to enrich expressive power. However, naively plugging PH features into GNN layers always results in marginal improvement with low interpretability. In this paper, we investigate a novel mechanism for injecting global topological invariance into pooling layers using PH, motivated by the observation that filtration operation in PH naturally aligns graph pooling in a cut-off manner. In this fashion, message passing in the coarsened graph acts along persistent pooled topology, leading to improved performance. Experimentally, we apply our mechanism to a collection of graph pooling methods and observe consistent and substantial performance gain over several popular datasets, demonstrating its wide applicability and flexibility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;</title><link>https://arxiv.org/abs/2402.13728</link><description>&lt;p&gt;
&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#22349;&#22604;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Average gradient outer product as a mechanism for deep neural collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Collapse (DNC)&#25351;&#30340;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26368;&#21518;&#20960;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#24778;&#20154;&#21018;&#24615;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#37117;&#24471;&#21040;&#20102;&#27979;&#37327;&#65292;&#20294;&#20854;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20805;&#20998;&#35777;&#25454;&#65292;&#34920;&#26126;DNC&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;(AGOP)&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#21457;&#29983;&#30340;&#12290;&#30456;&#27604;&#20110;&#35299;&#37322;&#31070;&#32463;&#22349;&#22604;&#30340;&#29305;&#24449;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#22914;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65292;&#36825;&#19968;&#36827;&#23637;&#26356;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#32487;&#32493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#26435;&#37325;&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#20540;&#26159;DNN&#20013;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36825;&#31181;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#30340;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AGOP&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#21457;&#31070;&#32463;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02368</link><description>&lt;p&gt;
&#35745;&#26102;&#22120;: &#29992;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Timer: Transformers for Time Series Analysis at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;GPT&#39118;&#26684;&#26550;&#26500;&#65292;&#20811;&#26381;&#28145;&#24230;&#27169;&#22411;&#22312;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#23454;&#29616;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#22823;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23567;&#26679;&#26412;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#27169;&#22411;&#21487;&#33021;&#36935;&#21040;&#24615;&#33021;&#29942;&#39048;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#20013;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#39281;&#21644;&#32780;&#38544;&#34109;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#65292;&#22823;&#27169;&#22411;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#33021;&#21147;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#21462;&#24471;&#20102;&#25345;&#32493;&#30340;&#36827;&#23637;&#65292;&#22312;&#23569;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20219;&#21153;&#26222;&#36866;&#24615;&#26041;&#38754;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#22312;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#25913;&#21464;&#30446;&#21069;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23567;&#27169;&#22411;&#30340;&#20570;&#27861;&#65292;&#26412;&#25991;&#26088;&#22312;&#26089;&#26399;&#24320;&#21457;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;LTSM&#65289;&#12290;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#21253;&#21547;10&#20159;&#20010;&#26102;&#38388;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#23558;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#32479;&#19968;&#20026;&#21333;&#24207;&#21015;&#24207;&#21015;&#65288;S3&#65289;&#26684;&#24335;&#65292;&#24182;&#24320;&#21457;&#20102;&#38754;&#21521;LTSM&#30340;GPT&#39118;&#26684;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet 
&lt;/p&gt;</description></item><item><title>CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00450</link><description>&lt;p&gt;
CPT: &#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#33021; &#21147;&#36882;&#36827;&#24335;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
CPT: Competence-progressive Training Strategy for Few-shot Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00450
&lt;/p&gt;
&lt;p&gt;
CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#25104;&#21151;&#20173;&#28982;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#33410;&#28857;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#26631;&#31614;&#31232;&#30095;&#65292;&#24378;&#35843;&#20102;GNN&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#24773;&#33410;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#38480;&#21046;&#65306;&#38543;&#26426;&#21644;&#22343;&#21248;&#20219;&#21153;&#20998;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#20803;&#23398;&#20064;&#22120;&#36807;&#26089;&#22320;&#38754;&#20020;&#22797;&#26434;&#20219;&#21153;&#65292;&#38459;&#30861;&#20102;&#27491;&#24120;&#30340;&#23398;&#20064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20803;&#23398;&#20064;&#22120;&#24212;&#35813;&#20174;&#31616;&#21333;&#27010;&#24565;&#24320;&#22987;&#65292;&#36880;&#28176;&#36827;&#20837;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#23601;&#20687;&#20154;&#31867;&#23398;&#20064;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#38590;&#24230;&#19982;&#20803;&#23398;&#20064;&#22120;&#30340;&#36882;&#36827;&#33021;&#21147;&#30456;&#21305;&#37197;&#65292;&#22686;&#24378;&#20102;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.02290</link><description>&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02290
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;Power-Link&#36890;&#36807;&#24341;&#20837;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#25512;&#21160;&#20102;&#27169;&#22411;&#36879;&#26126;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20132;&#20114;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#21364;&#27809;&#26377;&#24471;&#21040;&#24517;&#35201;&#30340;&#20851;&#27880;&#12290;&#23545;&#22522;&#20110;GNN&#30340;KGC&#27169;&#22411;&#32467;&#26524;&#36827;&#34892;&#36866;&#24403;&#35299;&#37322;&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#65292;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26356;&#21487;&#38752;&#30340;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;KGC&#35299;&#37322;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23454;&#20363;/&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#65292;&#36335;&#24452;&#21487;&#20197;&#25552;&#20379;&#26356;&#21451;&#22909;&#21644;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36824;&#27809;&#26377;&#23545;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Power-Link&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25506;&#32034;&#22522;&#20110;&#36335;&#24452;&#30340;KGC&#35299;&#37322;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21152;&#26435;&#25216;&#26415;&#65292;&#20351;&#24471;&#21487;&#20197;&#20197;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#35757;&#32451;&#26041;&#26696;&#29983;&#25104;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for 
&lt;/p&gt;</description></item><item><title>FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.01483</link><description>&lt;p&gt;
FedSN&#65306;&#19968;&#20010;&#36866;&#29992;&#20110;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#29992;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedSN: A General Federated Learning Framework over LEO Satellite Networks. (arXiv:2311.01483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01483
&lt;/p&gt;
&lt;p&gt;
FedSN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;LEO&#21355;&#26143;&#32593;&#32476;&#20013;&#30340;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#12289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#20197;&#21450;&#27169;&#22411;&#38472;&#26087;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#21355;&#26143;&#24050;&#32463;&#30001;&#21830;&#19994;&#20844;&#21496;&#25104;&#21151;&#22320;&#21457;&#23556;&#21644;&#37096;&#32626;&#21040;&#22826;&#31354;&#20013;&#65292;&#22914;SpaceX&#12290;&#30001;&#20110;LEO&#21355;&#26143;&#37197;&#22791;&#20102;&#22810;&#27169;&#20256;&#24863;&#22120;&#65292;&#23427;&#20204;&#19981;&#20165;&#29992;&#20110;&#36890;&#20449;&#65292;&#36824;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#22914;&#31354;&#38388;&#35843;&#21046;&#35782;&#21035;&#12289;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19982;LEO&#21355;&#26143;&#30340;&#26377;&#38480;&#25509;&#35302;&#26102;&#38388;&#65288;&#20363;&#22914;5&#20998;&#38047;&#65289;&#65292;&#22320;&#38754;&#31449;&#65288;GS&#65289;&#21487;&#33021;&#26080;&#27861;&#19979;&#36733;&#22914;&#27492;&#22823;&#37327;&#30340;&#21407;&#22987;&#24863;&#27979;&#25968;&#25454;&#36827;&#34892;&#38598;&#20013;&#27169;&#22411;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35201;&#22312;LEO&#21355;&#26143;&#19978;&#20351;&#29992;FL&#65292;&#25105;&#20204;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;i&#65289;&#24322;&#26500;&#35745;&#31639;&#21644;&#23384;&#20648;&#33021;&#21147;&#65292;ii&#65289;&#26377;&#38480;&#30340;&#19978;&#34892;&#36895;&#29575;&#65292;&#20197;&#21450;iii&#65289;&#27169;&#22411;&#38472;&#26087;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedSN&#30340;&#36890;&#29992;FL&#26694;&#26550;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#19968;
&lt;/p&gt;
&lt;p&gt;
Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space by commercial companies, such as SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communication but also for various machine learning applications, such as space modulation recognition, remote sensing image classification, etc. However, the ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, to enable FL on LEO satellites, we still face three critical challenges that are i) heterogeneous computing and memory capabilities, ii) limited uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges, an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11244</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23454;&#20307;&#21305;&#37197;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21305;&#37197;&#26159;&#21028;&#26029;&#20004;&#20010;&#23454;&#20307;&#25551;&#36848;&#26159;&#21542;&#25351;&#30340;&#26159;&#21516;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#23454;&#20307;&#21305;&#37197;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#25104;&#27969;&#31243;&#20013;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#20063;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#24212;&#29992;&#38656;&#35201;&#23558;&#26469;&#33258;&#19981;&#21516;&#20379;&#24212;&#21830;&#30340;&#20135;&#21697;&#21305;&#37197;&#36215;&#26469;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#21305;&#37197;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22914;BERT&#25110;RoBERTa&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#20307;&#21305;&#37197;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;ii&#65289;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23454;&#20307;&#19981;&#22815;&#20581;&#22766;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22522;&#20110;PLMs&#30340;&#21305;&#37197;&#22120;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;LLMs&#23545;&#39046;&#22495;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#36739;&#23569;&#19988;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#25176;&#31649;&#30340;LLMs&#65292;&#22914;GPT3.5&#21644;GPT4&#65292;&#20197;&#21450;&#22522;&#20110;Llama2&#30340;&#24320;&#28304;LLMs&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2310.03546</link><description>&lt;p&gt;
&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#22312;&#19981;&#21305;&#37197;&#27979;&#37327;&#21644;&#20808;&#39564;&#27169;&#22411;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models. (arXiv:2310.03546v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#25300;&#24335;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#25104;&#20687;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#39564;&#35777;&#65292;&#37327;&#21270;&#20102;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#32467;&#26524;&#34920;&#26126;PnP-ULA&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#25104;&#20687;&#36870;&#38382;&#39064;&#30340;&#24378;&#22823;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25554;&#25300;&#24335;&#26410;&#35843;&#25972;&#26391;&#20043;&#19975;&#31639;&#27861;&#65288;PnP-ULA&#65289;&#36890;&#36807;&#23558;&#29289;&#29702;&#27979;&#37327;&#27169;&#22411;&#19982;&#20351;&#29992;&#22270;&#20687;&#21435;&#22122;&#22120;&#25351;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#65288;MMSE&#65289;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#19982;&#19981;&#21305;&#37197;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#21644;&#21435;&#22122;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#23578;&#26410;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21518;&#39564;-L2&#25311;&#24230;&#37327;&#24182;&#21033;&#29992;&#23427;&#26469;&#37327;&#21270;PnP-ULA&#22312;&#19981;&#21305;&#37197;&#30340;&#21518;&#39564;&#20998;&#24067;&#19979;&#30340;&#26174;&#24335;&#35823;&#24046;&#30028;&#38480;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#36870;&#38382;&#39064;&#19978;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#65292;&#22914;&#20174;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;&#22270;&#20687;&#21435;&#27169;&#31946;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PnP-ULA&#30340;&#37319;&#26679;&#20998;&#24067;&#23545;&#20110;&#27979;&#37327;&#27169;&#22411;&#21644;&#21435;&#22122;&#22120;&#30340;&#19981;&#21305;&#37197;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#21487;&#20197;&#31934;&#30830;&#22320;&#25551;&#36848;&#20854;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.17329</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#28857;&#22270;&#32593;&#32476;&#39640;&#25928;&#35299;&#21078;&#26631;&#35760;&#32954;&#37096;&#26641;&#29366;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#21644;&#21487;&#29992;&#30340;&#34920;&#38754;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32954;&#37096;&#30142;&#30149;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#12290;&#27835;&#24840;&#32954;&#37096;&#30142;&#30149;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#32954;&#37096;&#31995;&#32479;&#20869;&#30340;&#35768;&#22810;&#22797;&#26434;&#30340;3D&#26641;&#29366;&#32467;&#26500;&#65292;&#22914;&#27668;&#36947;&#12289;&#21160;&#33033;&#21644;&#38745;&#33033;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#22534;&#26632;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23494;&#38598;&#20307;&#32032;&#32593;&#26684;&#30340;&#26631;&#20934;CNN&#26041;&#27861;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#26641;&#39592;&#26550;&#30340;&#22270;&#36830;&#36890;&#24615;&#65292;&#24182;&#32467;&#21512;&#20102;&#38544;&#24335;&#34920;&#38754;&#34920;&#31034;&#12290;&#23427;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;SOTA&#20934;&#30830;&#24230;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#29992;&#30340;&#34920;&#38754;&#12290;&#30001;&#20110;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#25105;&#20204;&#36824;&#25972;&#29702;&#20102;&#19968;&#22871;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65288;LADO&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#33021;&#22815;&#22522;&#20110;&#26412;&#22320;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#31639;&#27861;&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;</title><link>http://arxiv.org/abs/2306.10158</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Decentralized Online Convex Optimization in Networks. (arXiv:2306.10158v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#31639;&#27861;&#65288;LADO&#65289;&#65292;&#35813;&#31639;&#27861;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#33021;&#22815;&#22522;&#20110;&#26412;&#22320;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#19981;&#21516;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#31639;&#27861;&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32593;&#32476;&#21270;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21363;&#23398;&#20064;&#22686;&#24378;&#30340;&#20998;&#24067;&#24335;&#22312;&#32447;&#20248;&#21270;&#65288;LADO&#65289;&#65292;&#29992;&#20110;&#20351;&#20010;&#20307;&#26234;&#33021;&#20307;&#20165;&#22522;&#20110;&#26412;&#22320;&#22312;&#32447;&#20449;&#24687;&#36873;&#25321;&#34892;&#21160;&#12290;LADO&#21033;&#29992;&#22522;&#32447;&#31574;&#30053;&#26469;&#20445;&#38556;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#25509;&#36817;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31574;&#30053;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#22686;&#24378;&#22312;&#32447;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#20851;&#27880;&#30340;&#26159;&#38598;&#20013;&#24335;&#35774;&#32622;&#65292;LADO&#22312;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#24378;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;LADO&#30340;&#24179;&#22343;&#25104;&#26412;&#30028;&#38480;&#65292;&#25581;&#31034;&#20102;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#26126;&#30830;&#32771;&#34385;&#40065;&#26834;&#24615;&#35201;&#27714;&#26469;&#35757;&#32451;ML&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies decentralized online convex optimization in a networked multi-agent system and proposes a novel algorithm, Learning-Augmented Decentralized Online optimization (LADO), for individual agents to select actions only based on local online information. LADO leverages a baseline policy to safeguard online actions for worst-case robustness guarantees, while staying close to the machine learning (ML) policy for average performance improvement. In stark contrast with the existing learning-augmented online algorithms that focus on centralized settings, LADO achieves strong robustness guarantees in a decentralized setting. We also prove the average cost bound for LADO, revealing the tradeoff between average performance and worst-case robustness and demonstrating the advantage of training the ML policy by explicitly considering the robustness requirement.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08670</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#22312;&#19968;&#20010;&#30001;$n$&#20010;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#33410;&#28857;&#32452;&#25104;&#30340;&#31181;&#32676;&#20013;&#65292;&#37319;&#29992;&#20102;&#35875;&#35328;&#27169;&#22411;&#65306;&#27599;&#36718;&#65292;&#27599;&#20010;&#33410;&#28857;&#26412;&#22320;&#37319;&#29992;$m$&#20010;&#33218;&#20043;&#19968;&#65292;&#35266;&#23519;&#20174;&#33218;&#30340;&#65288;&#23545;&#25239;&#36873;&#25321;&#30340;&#65289;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22870;&#21169;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#25277;&#21462;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#36718;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#65306;&#27599;&#20010;&#33410;&#28857;&#30340;&#20915;&#31574;&#23436;&#20840;&#26159;&#23616;&#37096;&#30340;&#65292;&#21482;&#20381;&#36182;&#20110;&#20854;&#26368;&#26032;&#33719;&#24471;&#30340;&#22870;&#21169;&#20197;&#21450;&#23427;&#25277;&#26679;&#30340;&#37051;&#23621;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#28436;&#21270;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#36825;&#20123;&#33258;&#28982;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#65288;&#21363;&#65292;&#31181;&#32676;&#30340;&#22823;&#23567;&#21644;nu&#30340;&#22823;&#23567;&#65289;&#19979;&#25512;&#23548;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
&lt;/p&gt;</description></item><item><title>FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05172</link><description>&lt;p&gt;
FLEdge&#65306;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05172
&lt;/p&gt;
&lt;p&gt;
FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#65288;FL&#65289;&#22791;&#21463;&#20851;&#27880;&#12290; FL&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#22312;&#27169;&#25311;&#31995;&#32479;&#25110;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#19982;&#36793;&#32536;&#35745;&#31639;&#23494;&#20999;&#30456;&#20851;&#30340;&#23454;&#38469;&#31995;&#32479;&#35774;&#32622;&#12290; &#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;FLEdge&#26469;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#25928;&#29575;&#20197;&#21450;&#21508;&#31181;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#20855;&#26377;&#39640;&#36798;50&#65285;&#22833;&#25928;&#29575;&#30340;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#12290; FLEdge&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;&#65292;&#22312;&#26087;GPU&#21152;&#36895;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#27604;&#22312;&#29616;&#20195;&#26381;&#21153;&#22120;&#32423;GPU&#19978;&#35757;&#32451;&#39640;&#36798;3&#20493;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.09063</link><description>&lt;p&gt;
&#26377;&#30028;KRnet&#21450;&#20854;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#36817;&#20284;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bounded KRnet and its applications to density estimation and approximation. (arXiv:2305.09063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#36870;&#26144;&#23556;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#65292;&#30001;&#20110;&#20854;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;&#19978;&#65292;&#22240;&#27492;&#27604;KRnet&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26377;&#30028;&#22495;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#36870;&#26144;&#23556;&#65292;&#31216;&#20026;B-KRnet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25968;&#25454;&#25110;PDE&#65288;&#20363;&#22914;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#21644;Keller-Segel&#26041;&#31243;&#65289;&#30340;&#23494;&#24230;&#20272;&#35745;/&#36817;&#20284;&#12290;&#19982;KRnet&#31867;&#20284;&#65292;B-KRnet&#30340;&#32467;&#26500;&#23558;Knothe-Rosenblatt&#37325;&#25490;&#30340;&#19977;&#35282;&#24418;&#24418;&#24335;&#36716;&#21270;&#20026;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#12290;B-KRnet&#21644;KRnet&#20043;&#38388;&#30340;&#20027;&#35201;&#21306;&#21035;&#26159;B-KRnet&#23450;&#20041;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#65292;&#32780;KRnet&#23450;&#20041;&#22312;&#25972;&#20010;&#31354;&#38388;&#19978;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#22312;B-KRnet&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#26469;&#20445;&#25345;&#31934;&#30830;&#30340;&#21487;&#36870;&#24615;&#12290;&#23558;B-KRnet&#29992;&#20316;&#20256;&#36755;&#26144;&#23556;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#24212;&#20110;&#20808;&#39564;&#65288;&#22343;&#21248;&#65289;&#20998;&#24067;&#22312;&#36229;&#31435;&#26041;&#20307;&#19978;&#30340;&#25512;&#31227;&#12290;&#20026;&#20102;&#36817;&#20284;&#35745;&#31639;&#22495;&#19978;&#23450;&#20041;&#30340;PDF&#65292;B-KRnet&#27604;KRnet&#26356;&#26377;&#25928;&#12290;&#36890;&#36807;&#32806;&#21512;KRnet&#21644;B-KRnet&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#22312;&#39640;&#32500;&#22495;&#19978;&#23450;&#20041;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop an invertible mapping, called B-KRnet, on a bounded domain and apply it to density estimation/approximation for data or the solutions of PDEs such as the Fokker-Planck equation and the Keller-Segel equation. Similar to KRnet, the structure of B-KRnet adapts the triangular form of the Knothe-Rosenblatt rearrangement into a normalizing flow model. The main difference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube while KRnet is defined on the whole space, in other words, we introduce a new mechanism in B-KRnet to maintain the exact invertibility. Using B-KRnet as a transport map, we obtain an explicit probability density function (PDF) model that corresponds to the pushforward of a prior (uniform) distribution on the hypercube. To approximate PDFs defined on a bounded computational domain, B-KRnet is more effective than KRnet. By coupling KRnet and B-KRnet, we can also define a deep generative model on a high-dimensional domain where some di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.01661</link><description>&lt;p&gt;
SIA-FTP: &#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SIA-FTP: A Spoken Instruction Aware Flight Trajectory Prediction Framework. (arXiv:2305.01661v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#38899;&#25351;&#20196;&#21644;&#39134;&#34892;&#36712;&#36857;&#30340;&#27169;&#24577;&#24046;&#36317;&#38382;&#39064;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#36890;&#35759;&#36827;&#34892;&#22320;&#31354;&#21327;&#21830;&#26159;&#30830;&#20445;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#65288;ATC&#65289;&#25805;&#20316;&#23433;&#20840;&#21644;&#25928;&#29575;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#20294;&#26159;&#65292;&#38543;&#30528;&#20132;&#36890;&#27969;&#37327;&#30340;&#22686;&#21152;&#65292;&#30001;&#20110;&#20154;&#20026;&#22240;&#32032;&#23548;&#33268;&#30340;&#38169;&#35823;&#25351;&#20196;&#32473;ATC&#23433;&#20840;&#24102;&#26469;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#65288;FTP&#65289;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21382;&#21490;&#36712;&#36857;&#30340;&#39134;&#34892;&#29366;&#24577;&#65292;&#22312;&#23454;&#26102;&#26426;&#21160;&#25351;&#20196;&#30340;&#39044;&#27979;&#19978;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#36825;&#19981;&#21033;&#20110;&#20914;&#31361;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIA-FTP&#30340;&#35821;&#38899;&#25351;&#20196;&#24863;&#30693;FTP&#26694;&#26550;&#65292;&#36890;&#36807;&#21253;&#21547;&#21363;&#26102;&#30340;&#35821;&#38899;&#25351;&#20196;&#26469;&#25903;&#25345;&#39640;&#26426;&#21160;FTP&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#27169;&#24577;&#24046;&#36317;&#24182;&#26368;&#23567;&#21270;&#25968;&#25454;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#27880;&#24847;&#26426;&#21046;&#26469;&#34701;&#21512;&#35821;&#38899;&#25351;&#20196;&#23884;&#20837;&#21644;&#39134;&#34892;&#36712;&#36857;&#34920;&#31034;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;SIA-FTP&#65292;&#19982;&#29616;&#26377;&#30340;FTP&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ground-air negotiation via speech communication is a vital prerequisite for ensuring safety and efficiency in air traffic control (ATC) operations. However, with the increase in traffic flow, incorrect instructions caused by human factors bring a great threat to ATC safety. Existing flight trajectory prediction (FTP) approaches primarily rely on the flight status of historical trajectory, leading to significant delays in the prediction of real-time maneuvering instruction, which is not conducive to conflict detection. A major reason is that spoken instructions and flight trajectories are presented in different modalities in the current air traffic control (ATC) system, bringing great challenges to considering the maneuvering instruction in the FTP tasks. In this paper, a spoken instruction-aware FTP framework, called SIA-FTP, is innovatively proposed to support high-maneuvering FTP tasks by incorporating instant spoken instruction. To address the modality gap and minimize the data requ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.13458</link><description>&lt;p&gt;
&#31561;&#21464;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38480;&#21046;&#26550;&#26500;&#31561;&#21464;&#21644;&#20351;&#29992;&#22686;&#24378;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#25439;&#22833;&#21644;&#38750;&#32447;&#24615;&#24615;&#36827;&#34892;&#33258;&#28982;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#23545;&#20110;&#36825;&#20004;&#31181;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#31561;&#21464;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#26159;&#31283;&#23450;&#30340;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12562</link><description>&lt;p&gt;
&#38024;&#23545;&#21487;&#25193;&#23637;&#24615;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31616;&#21270;&#20197;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Simplifying Subgraph Representation Learning for Scalable Link Prediction. (arXiv:2301.12562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21270;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#25805;&#20316;&#31526;&#20197;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23558;&#38142;&#25509;&#39044;&#27979;&#36716;&#21270;&#20026;&#22312;&#38142;&#25509;&#21608;&#22260;&#23376;&#22270;&#19978;&#30340;&#22270;&#20998;&#31867;&#26469;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#24182;&#19988;&#30001;&#20110;&#23376;&#22270;&#27700;&#24179;&#25805;&#20316;&#30340;&#20195;&#20215;&#32780;&#19981;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#31867;&#65292;&#31216;&#20026;&#21487;&#25193;&#23637;&#31616;&#21270;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;S3GRL&#65289;&#65292;&#26088;&#22312;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;S3GRL&#31616;&#21270;&#20102;&#27599;&#20010;&#38142;&#25509;&#23376;&#22270;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#20316;&#20026;&#21487;&#25193;&#23637;&#24615;&#26694;&#26550;&#65292;S3GRL&#36866;&#24212;&#21508;&#31181;&#23376;&#22270;&#37319;&#26679;&#31574;&#30053;&#21644;&#25193;&#25955;&#36816;&#31639;&#31526;&#26469;&#27169;&#25311;&#35745;&#31639;&#20195;&#20215;&#39640;&#30340;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20010;S3GRL&#23454;&#20363;&#65292;&#24182;&#22312;&#23567;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;S3GRL&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;SGRL&#32780;&#19981;&#20250;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call Scalable Simplified SGRL (S3GRL). Aimed at faster training and inference, S3GRL simplifies the message passing and aggregation operations in each link's subgraph. S3GRL, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of S3GRL and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed S3GRL models scale up SGRLs without significant performance 
&lt;/p&gt;</description></item></channel></rss>