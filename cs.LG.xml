<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17933</link><description>&lt;p&gt;
SLEDGE: &#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#39550;&#39542;&#26234;&#33021;&#20307;&#30340;&#27169;&#25311;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17933
&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#65292;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#20197;&#21450;Diffusion Transformer&#26469;&#29983;&#25104;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#25311;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SLEDGE&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#35760;&#24405;&#35757;&#32451;&#30340;&#36710;&#36742;&#36816;&#21160;&#35268;&#21010;&#29983;&#25104;&#27169;&#25311;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#19968;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#26234;&#33021;&#20307;&#36793;&#30028;&#26694;&#21644;&#36710;&#36947;&#22270;&#12290;&#35813;&#27169;&#22411;&#30340;&#36755;&#20986;&#20316;&#20026;&#20132;&#36890;&#27169;&#25311;&#30340;&#21021;&#22987;&#29366;&#24577;&#12290;&#38024;&#23545;SLEDGE&#24453;&#29983;&#25104;&#30340;&#23454;&#20307;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36830;&#25509;&#24615;&#21644;&#27599;&#20010;&#22330;&#26223;&#30340;&#21487;&#21464;&#25968;&#37327;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#21464;&#24471;&#19981;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38500;&#20102;&#23545;&#29616;&#26377;&#36710;&#36947;&#22270;&#34920;&#31034;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26629;&#26684;&#21040;&#30690;&#37327;&#33258;&#32534;&#30721;&#22120;&#65288;RVAE&#65289;&#12290;&#23427;&#23558;&#26234;&#33021;&#20307;&#21644;&#36710;&#36947;&#22270;&#32534;&#30721;&#20026;&#26629;&#26684;&#21270;&#28508;&#22312;&#26144;&#23556;&#20013;&#30340;&#19981;&#21516;&#36890;&#36947;&#12290;&#36825;&#26377;&#21161;&#20110;&#36710;&#36947;&#26465;&#20214;&#19979;&#30340;&#26234;&#33021;&#20307;&#29983;&#25104;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;&#21464;&#25442;&#22120;&#21516;&#26102;&#29983;&#25104;&#36710;&#36947;&#21644;&#26234;&#33021;&#20307;&#12290;&#22312;SLEDGE&#20013;&#20351;&#29992;&#29983;&#25104;&#30340;&#23454;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#27169;&#25311;&#65292;&#20363;&#22914;&#19978;&#37319;&#26679;&#36716;&#24367;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#20985;&#25104;&#26412;&#20989;&#25968;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.14067</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#30340;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Outlier Rectification via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#20985;&#25104;&#26412;&#20989;&#25968;&#26469;&#26816;&#27979;&#21644;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20351;&#29992;&#20855;&#26377;&#20985;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#36755;&#36816;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#20256;&#32479;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#20004;&#38454;&#27573;&#27969;&#31243;&#65306;&#39318;&#20808;&#26816;&#27979;&#24182;&#31227;&#38500;&#24322;&#24120;&#20540;&#65292;&#28982;&#21518;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#25191;&#34892;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#27809;&#26377;&#23558;&#24322;&#24120;&#20540;&#31227;&#38500;&#19982;&#20272;&#35745;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#30041;&#19979;&#20102;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#24322;&#24120;&#20540;&#30699;&#27491;&#26426;&#21046;&#65292;&#23558;&#30699;&#27491;&#21644;&#20272;&#35745;&#38598;&#25104;&#21040;&#19968;&#20010;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#20855;&#26377;&#20985;&#25104;&#26412;&#20989;&#25968;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#26469;&#26500;&#24314;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#30699;&#27491;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#22312;&#30699;&#27491;&#38598;&#21512;&#20013;&#30340;&#26368;&#20339;&#20998;&#24067;&#26469;&#25191;&#34892;&#20272;&#35745;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24341;&#20837;&#30340;&#20985;&#25104;&#26412;&#20989;&#25968;&#26159;&#20351;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20855;&#26377;&#20851;&#38190;&#24615;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14067v1 Announce Type: cross  Abstract: In this paper, we propose a novel conceptual framework to detect outliers using optimal transport with a concave cost function. Conventional outlier detection approaches typically use a two-stage procedure: first, outliers are detected and removed, and then estimation is performed on the cleaned data. However, this approach does not inform outlier removal with the estimation task, leaving room for improvement. To address this limitation, we propose an automatic outlier rectification mechanism that integrates rectification and estimation within a joint optimization framework. We take the first step to utilize an optimal transport distance with a concave cost function to construct a rectification set in the space of probability distributions. Then, we select the best distribution within the rectification set to perform the estimation task. Notably, the concave cost function we introduced in this paper is the key to making our estimator e
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.04221</link><description>&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20309;&#20855;&#26377;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Why Online Reinforcement Learning is Causal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04221
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#22240;&#26524;&#24314;&#27169;&#30456;&#20114;&#34917;&#20805;&#65292;&#35770;&#25991;&#20027;&#35201;&#25351;&#20986;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#26465;&#20214;&#27010;&#29575;&#20855;&#26377;&#22240;&#26524;&#24615;&#65292;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#28508;&#21147;&#26368;&#22823;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22240;&#26524;&#24314;&#27169;&#33258;&#28982;&#20114;&#34917;&#12290;&#22240;&#26524;&#24314;&#27169;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#22312;&#29615;&#22659;&#20013;&#36827;&#34892;&#24178;&#39044;&#30340;&#25928;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36873;&#25321;&#26368;&#22823;&#21270;&#20195;&#29702;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#30340;&#22870;&#21169;&#30340;&#24178;&#39044;&#12290;&#24378;&#21270;&#23398;&#20064;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#20004;&#20010;&#26368;&#24378;&#22823;&#20449;&#24687;&#28304;&#65306;&#26102;&#38388;&#39034;&#24207;&#21644;&#23545;&#29615;&#22659;&#36827;&#34892;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#21738;&#20123;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#20174;&#22240;&#26524;&#24314;&#27169;&#20013;&#21463;&#30410;&#65292;&#20197;&#21450;&#22914;&#20309;&#21463;&#30410;&#12290;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#26377;&#33021;&#21147;&#30452;&#25509;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#20174;&#25506;&#32034;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#65292;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#26465;&#20214;&#27010;&#29575;&#26159;&#22240;&#26524;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;RL&#26159;&#22240;&#26524;&#23398;&#20064;&#26377;&#28508;&#21147;&#20135;&#29983;&#24046;&#24322;&#30340;&#29615;&#22659;&#12290;&#22522;&#26412;&#19978;&#65292;&#21407;&#22240;&#22312;&#20110;&#24403;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#26102;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#26159;&#30001;&#20854;&#23545;&#29615;&#22659;&#30340;&#35748;&#35782;&#25152;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04221v1 Announce Type: cross  Abstract: Reinforcement learning (RL) and causal modelling naturally complement each other. The goal of causal modelling is to predict the effects of interventions in an environment, while the goal of reinforcement learning is to select interventions that maximize the rewards the agent receives from the environment. Reinforcement learning includes the two most powerful sources of information for estimating causal relationships: temporal ordering and the ability to act on an environment. This paper examines which reinforcement learning settings we can expect to benefit from causal modelling, and how. In online learning, the agent has the ability to interact directly with their environment, and learn from exploring it. Our main argument is that in online learning, conditional probabilities are causal, and therefore offline RL is the setting where causal learning has the most potential to make a difference. Essentially, the reason is that when an a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.02178</link><description>&lt;p&gt;
&#25513;&#38754;&#24605;&#24819;:&#31616;&#21333;&#22320;&#25513;&#30422;&#37096;&#20998;&#25512;&#29702;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02178
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#30422;&#24605;&#32500;&#38142;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#21363;&#20351;&#26159;&#19968;&#20010;&#36731;&#24494;&#30340;&#38169;&#35823;&#20063;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#39046;&#22495;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#24341;&#20837;&#23545;&#36755;&#20837;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#38543;&#26426;&#25513;&#30422;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#26576;&#20123;&#26631;&#35760;&#65292;&#36825;&#31181;&#25216;&#26415;&#23545;&#25512;&#29702;&#20219;&#21153;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00942</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of Entropy Model in Distributed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#31070;&#32463;&#32593;&#32476;&#20013;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#21644;&#26080;&#24847;&#24178;&#25200;&#30340;&#38887;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#32780;&#19981;&#38477;&#20302;&#24615;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#29109;&#32534;&#30721;&#34987;&#24341;&#20837;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#20998;&#24067;&#24335;DNN&#19982;&#29109;&#27169;&#22411;&#32852;&#21512;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#26102;&#38388;&#29992;&#20316;&#36793;&#20449;&#24687;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#28508;&#22312;&#34920;&#31034;&#32534;&#30721;&#20026;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#30340;&#27604;&#29305;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#24182;&#35843;&#26597;&#20102;&#29109;&#27169;&#22411;&#23545;&#26377;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#25915;&#20987;&#65289;&#21644;&#26080;&#24847;&#24178;&#25200;&#65288;&#20363;&#22914;&#65292;&#22825;&#27668;&#21464;&#21270;&#21644;&#36816;&#21160;&#27169;&#31946;&#65289;&#30340;&#38887;&#24615;&#12290;&#36890;&#36807;&#23545;3&#31181;&#19981;&#21516;DNN&#26550;&#26500;&#12289;2&#20010;&#29109;&#27169;&#22411;&#21644;4&#20010;&#36895;&#29575;&#22833;&#30495;&#26435;&#34913;&#22240;&#23376;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29109;&#27169;&#22411;&#30340;&#38887;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00942v1 Announce Type: cross  Abstract: Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., adversarial attacks) and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>BeTAIL&#32467;&#21512;&#20102;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#21644;&#22312;&#32447;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#65292;&#20197;&#23398;&#20064;&#20174;&#20154;&#31867;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#21040;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32416;&#27491;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14194</link><description>&lt;p&gt;
BeTAIL&#65306;&#20174;&#20154;&#31867;&#36187;&#36710;&#28216;&#25103;&#20013;&#23398;&#20064;&#30340;&#34892;&#20026;&#36716;&#25442;&#22120;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human Racing Gameplay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14194
&lt;/p&gt;
&lt;p&gt;
BeTAIL&#32467;&#21512;&#20102;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#21644;&#22312;&#32447;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#65292;&#20197;&#23398;&#20064;&#20174;&#20154;&#31867;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#21040;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#32416;&#27491;&#29615;&#22659;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#31574;&#30053;&#32780;&#26080;&#38656;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#22914;&#33258;&#20027;&#36187;&#36710;&#65292;&#34987;&#27169;&#20223;&#30340;&#31574;&#30053;&#24517;&#39035;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#20154;&#31867;&#20915;&#31574;&#24314;&#27169;&#12290;&#24207;&#21015;&#24314;&#27169;&#38750;&#24120;&#26377;&#25928;&#22320;&#25429;&#25417;&#36816;&#21160;&#24207;&#21015;&#30340;&#22797;&#26434;&#27169;&#24335;&#65292;&#20294;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#25110;&#20998;&#24067;&#36716;&#31227;&#26041;&#38754;&#21364;&#24456;&#38590;&#12290;&#30456;&#21453;&#65292;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#21487;&#20197;&#32531;&#35299;&#36825;&#31181;&#25928;&#24212;&#65292;&#20294;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#22788;&#29702;&#22797;&#26434;&#36816;&#21160;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BeTAIL&#65306;&#34892;&#20026;&#36716;&#25442;&#22120;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65292;&#23427;&#23558;&#26469;&#33258;&#20154;&#31867;&#31034;&#33539;&#30340;&#34892;&#20026;&#36716;&#25442;&#22120;&#65288;BeT&#65289;&#31574;&#30053;&#19982;&#22312;&#32447;AIL&#30456;&#32467;&#21512;&#12290;BeTAIL&#23558;&#19968;&#20010;AIL&#21097;&#20313;&#31574;&#30053;&#28155;&#21152;&#21040;BeT&#31574;&#30053;&#20013;&#65292;&#20197;&#24314;&#27169;&#20154;&#31867;&#19987;&#23478;&#30340;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#24182;&#32416;&#27491;&#20998;&#24067;&#22806;&#29366;&#24577;&#25110;&#29615;&#22659;&#20013;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14194v1 Announce Type: new  Abstract: Imitation learning learns a policy from demonstrations without requiring hand-designed reward functions. In many robotic tasks, such as autonomous racing, imitated policies must model complex environment dynamics and human decision-making. Sequence modeling is highly effective in capturing intricate patterns of motion sequences but struggles to adapt to new environments or distribution shifts that are common in real-world robotics tasks. In contrast, Adversarial Imitation Learning (AIL) can mitigate this effect, but struggles with sample inefficiency and handling complex motion patterns. Thus, we propose BeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a Behavior Transformer (BeT) policy from human demonstrations with online AIL. BeTAIL adds an AIL residual policy to the BeT policy to model the sequential decision-making process of human experts and correct for out-of-distribution states or shifts in environmen
&lt;/p&gt;</description></item><item><title>gadjid&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26694;&#26550;&#26469;&#35745;&#31639;&#22240;&#26524;&#36317;&#31163;&#65292;&#36825;&#20123;&#36317;&#31163;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08616</link><description>&lt;p&gt;
Adjustment Identification Distance: &#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Adjustment Identification Distance: A gadjid for Causal Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08616
&lt;/p&gt;
&lt;p&gt;
gadjid&#36719;&#20214;&#21253;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#35843;&#25972;&#35782;&#21035;&#36317;&#31163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26694;&#26550;&#26469;&#35745;&#31639;&#22240;&#26524;&#36317;&#31163;&#65292;&#36825;&#20123;&#36317;&#31163;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#26102;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#23398;&#20064;&#30340;&#22270;&#24418;&#30340;&#35780;&#20272;&#26159;&#22256;&#38590;&#30340;&#65306;&#20004;&#20010;&#22270;&#24418;&#20043;&#38388;&#19981;&#21516;&#30340;&#36793;&#30340;&#25968;&#37327;&#19981;&#33021;&#21453;&#26144;&#20986;&#23427;&#20204;&#22312;&#24314;&#35758;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#20844;&#24335;&#26041;&#38754;&#26377;&#20309;&#19981;&#21516;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#24320;&#21457;&#22270;&#24418;&#20043;&#38388;&#30340;&#22240;&#26524;&#36317;&#31163;&#65292;&#20854;&#20013;&#21253;&#25324;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#24320;&#21457;&#20102;&#25913;&#36827;&#30340;&#22522;&#20110;&#35843;&#25972;&#30340;&#36317;&#31163;&#65292;&#20197;&#21450;&#23545;&#23436;&#25104;&#30340;&#37096;&#20998;&#26377;&#21521;&#26080;&#29615;&#22270;&#21644;&#22240;&#26524;&#24207;&#21015;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#36798;&#24615;&#31639;&#27861;&#26469;&#39640;&#25928;&#35745;&#31639;&#36317;&#31163;&#12290;&#22312;&#25105;&#20204;&#30340;gadjid&#36719;&#20214;&#21253;&#20013;&#65288;&#22312;https://github.com/CausalDisco/gadjid&#19978;&#24320;&#28304;&#65289;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#36317;&#31163;&#23454;&#29616;&#65307;&#23427;&#20204;&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;&#32467;&#26500;&#24178;&#39044;&#36317;&#31163;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20174;&#32780;&#20026;&#20197;&#21069;&#26080;&#27861;&#25193;&#23637;&#30340;&#22270;&#24418;&#23610;&#23544;&#25552;&#20379;&#20102;&#19968;&#20010;&#22240;&#26524;&#21457;&#29616;&#30340;&#25104;&#21151;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating graphs learned by causal discovery algorithms is difficult: The number of edges that differ between two graphs does not reflect how the graphs differ with respect to the identifying formulas they suggest for causal effects. We introduce a framework for developing causal distances between graphs which includes the structural intervention distance for directed acyclic graphs as a special case. We use this framework to develop improved adjustment-based distances as well as extensions to completed partially directed acyclic graphs and causal orders. We develop polynomial-time reachability algorithms to compute the distances efficiently. In our package gadjid (open source at https://github.com/CausalDisco/gadjid), we provide implementations of our distances; they are orders of magnitude faster than the structural intervention distance and thereby provide a success metric for causal discovery that scales to graph sizes that were previously prohibitive.
&lt;/p&gt;</description></item><item><title>&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;</title><link>https://arxiv.org/abs/2402.06590</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#65306;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Predictive representations: building blocks of intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06590
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#34892;&#20026;&#36890;&#24120;&#38656;&#35201;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#12290;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#35268;&#23450;&#20102;&#20160;&#20040;&#26679;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#26159;&#26377;&#29992;&#30340;&#20197;&#21450;&#22914;&#20309;&#35745;&#31639;&#23427;&#20204;&#12290;&#26412;&#25991;&#23558;&#36825;&#20123;&#29702;&#35770;&#35266;&#28857;&#19982;&#35748;&#30693;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#32487;&#20219;&#32773;&#34920;&#24449;&#65288;SR&#65289;&#21450;&#20854;&#24191;&#20041;&#24418;&#24335;&#65292;&#23427;&#20204;&#19981;&#20165;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#24037;&#20855;&#65292;&#20063;&#20316;&#20026;&#22823;&#33041;&#21151;&#33021;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#34701;&#21512;&#34920;&#26126;&#29305;&#23450;&#31867;&#22411;&#30340;&#39044;&#27979;&#24615;&#34920;&#24449;&#21487;&#33021;&#26159;&#26234;&#33021;&#30340;&#22810;&#21151;&#33021;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive behavior often requires predicting future events. The theory of reinforcement learning prescribes what kinds of predictive representations are useful and how to compute them. This paper integrates these theoretical ideas with work on cognition and neuroscience. We pay special attention to the successor representation (SR) and its generalizations, which have been widely applied both as engineering tools and models of brain function. This convergence suggests that particular kinds of predictive representations may function as versatile building blocks of intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2312.06837</link><description>&lt;p&gt;
&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Spectral State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#23454;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#21644;&#22266;&#23450;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#30340;&#39044;&#27979;&#20219;&#21153;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#20855;&#26377;&#20809;&#35889;&#28388;&#27874;&#31639;&#27861;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#26032;&#24418;&#24335;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31216;&#20026;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#26032;&#39062;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#12290;&#20809;&#35889;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#23646;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#26082;&#19981;&#20381;&#36182;&#20110;&#24213;&#23618;&#21160;&#21147;&#23398;&#30340;&#39057;&#35889;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#38382;&#39064;&#30340;&#32500;&#24230;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#36890;&#36807;&#22266;&#23450;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#26500;&#24314;&#30340;&#65292;&#19981;&#38656;&#35201;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#20173;&#28982;&#20248;&#20110;SSMs&#12290;&#22522;&#20110;&#20809;&#35889;&#36807;&#28388;&#31639;&#27861;&#30340;Spectral state space models&#22312;&#21512;&#25104;&#21160;&#24577;&#31995;&#32479;&#21644;&#21508;&#31181;&#27169;&#24577;&#30340;&#38271;&#31243;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#35780;&#20272;&#25903;&#25345;&#20102;&#20809;&#35889;&#28388;&#27874;&#22312;&#38656;&#35201;&#38750;&#24120;&#38271;&#31243;&#35760;&#24518;&#30340;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model.   Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.   The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2305.08752</link><description>&lt;p&gt;
&#27880;&#37322;&#38382;&#39064;&#65306;&#26469;&#33258;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#21407;&#20301;&#21644;&#33258;&#25105;&#22238;&#24518;&#27963;&#21160;&#27880;&#37322;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Matter of Annotation: An Empirical Study on In Situ and Self-Recall Activity Annotations from Wearable Sensors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08752
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#23545;&#25968;&#25454;&#36136;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#20294;&#26356;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#20174;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20013;&#26816;&#27979;&#20154;&#31867;&#27963;&#21160;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#20351;&#35768;&#22810;&#24212;&#29992;&#21463;&#30410;&#65292;&#20174;&#36890;&#36807;&#20581;&#24247;&#25252;&#29702;&#24739;&#32773;&#30340;&#27493;&#34892;&#30417;&#27979;&#21040;&#20581;&#36523;&#25351;&#23548;&#20877;&#21040;&#31616;&#21270;&#25163;&#24037;&#20316;&#19994;&#27969;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#22312;&#37326;&#22806;&#25968;&#25454;&#29992;&#25143;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;4&#31181;&#19981;&#21516;&#24120;&#29992;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#29992;&#25143;&#39537;&#21160;&#30340;&#12289;&#21407;&#20301;&#27880;&#37322;-&#21363;&#22312;&#35760;&#24405;&#27963;&#21160;&#20043;&#21069;&#25110;&#26399;&#38388;&#25191;&#34892;&#30340;&#27880;&#37322;-&#21644;&#22238;&#24518;&#26041;&#27861;-&#21442;&#19982;&#32773;&#22312;&#24403;&#22825;&#32467;&#26463;&#26102;&#36861;&#28335;&#22320;&#23545;&#20854;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#26631;&#35760;&#26041;&#27861;&#30452;&#25509;&#24433;&#21709;&#27880;&#37322;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#30456;&#24212;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#21407;&#20301;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#23569;&#65292;&#20294;&#26356;&#31934;&#30830;&#65292;&#32780;&#22238;&#24518;&#26041;&#27861;&#20135;&#29983;&#30340;&#26631;&#31614;&#36739;&#22810;&#65292;&#20294;&#19981;&#22815;&#31934;&#30830;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#26412;&#27963;&#21160;&#26085;&#35760;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08752v2 Announce Type: replace-cross  Abstract: Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes. We present an empirical study that compares 4 different commonly used annotation methods utilized in user studies that focus on in-the-wild data. These methods can be grouped in user-driven, in situ annotations - which are performed before or during the activity is recorded - and recall methods - where participants annotate their data in hindsight at the end of the day. Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data respectively. We noticed that in situ methods produce less but more precise labels than recall methods. Furthermore, we combined an activity diary
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13537</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65306;&#36208;&#21521;&#33258;&#30417;&#30563;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Masked Particle Modeling on Sets: Towards Self-Supervised High Energy Physics Foundation Models. (arXiv:2401.13537v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#39044;&#35757;&#32451;&#23398;&#20064;&#32622;&#25442;&#19981;&#21464;&#30340;&#20989;&#25968;&#65292;&#22312;&#26500;&#24314;&#36866;&#29992;&#20110;&#22810;&#31181;&#20219;&#21153;&#30340;&#39640;&#33021;&#29289;&#29702;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;"&#36974;&#34109;&#31890;&#23376;&#24314;&#27169;"&#65288;MPM&#65289;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#33021;&#29289;&#29702;&#65288;HEP&#65289;&#31185;&#23398;&#25968;&#25454;&#20013;&#26080;&#24207;&#36755;&#20837;&#30340;&#36890;&#29992;&#12289;&#21487;&#36716;&#31227;&#21644;&#21487;&#37325;&#29992;&#34920;&#31034;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#22522;&#20110;&#36974;&#34109;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#38598;&#21512;&#19978;&#30340;&#32622;&#25442;&#19981;&#21464;&#20989;&#25968;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#36825;&#39033;&#24037;&#20316;&#22312;&#26500;&#24314;&#21487;&#20197;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#36890;&#29992;&#39044;&#35757;&#32451;&#24182;&#31245;&#21518;&#31934;&#35843;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;HEP&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#22312;MPM&#20013;&#65292;&#38598;&#21512;&#20013;&#30340;&#31890;&#23376;&#34987;&#36974;&#34109;&#65292;&#35757;&#32451;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#23427;&#20204;&#30340;&#36523;&#20221;&#65292;&#36523;&#20221;&#30001;&#39044;&#35757;&#32451;&#30340;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31163;&#25955;&#21270;&#26631;&#35760;&#34920;&#31034;&#23450;&#20041;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#23545;&#25758;&#26426;&#29289;&#29702;&#23454;&#39564;&#20013;&#39640;&#33021;&#21943;&#27880;&#26679;&#26412;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#31163;&#25955;&#21270;&#12289;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#25490;&#24207;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose \textit{masked particle modeling} (MPM) as a self-supervised method for learning generic, transferable, and reusable representations on unordered sets of inputs for use in high energy physics (HEP) scientific data. This work provides a novel scheme to perform masked modeling based pre-training to learn permutation invariant functions on sets. More generally, this work provides a step towards building large foundation models for HEP that can be generically pre-trained with self-supervised learning and later fine-tuned for a variety of down-stream tasks. In MPM, particles in a set are masked and the training objective is to recover their identity, as defined by a discretized token representation of a pre-trained vector quantized variational autoencoder. We study the efficacy of the method in samples of high energy jets at collider physics experiments, including studies on the impact of discretization, permutation invariance, and ordering. We also study the fine-tuning capabili
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.13098</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#21147;&#20449;&#24687;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#39044;&#27979;&#38750;&#26412;&#22320;&#29289;&#31181;&#33337;&#33334;&#20132;&#36890;&#27969;&#37327;&#21644;&#20837;&#20405;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Gravity-Informed Deep Learning Framework for Predicting Ship Traffic Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge. (arXiv:2401.13098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13098
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32771;&#34385;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#31561;&#22240;&#32032;&#65292;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#27969;&#37327;&#65292;&#24182;&#29992;&#20110;&#25351;&#23548;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20013;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#20013;&#30340;&#20837;&#20405;&#29289;&#31181;&#23545;&#20840;&#29699;&#29615;&#22659;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#30001;&#20110;&#20132;&#36890;&#21644;&#36152;&#26131;&#22686;&#21152;&#65292;&#38750;&#26412;&#22303;&#29289;&#31181;&#24050;&#32463;&#24341;&#20837;&#20102;&#26032;&#30340;&#29615;&#22659;&#65292;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#30772;&#22351;&#65292;&#24182;&#23548;&#33268;&#20892;&#19994;&#12289;&#26519;&#19994;&#21644;&#28180;&#19994;&#26041;&#38754;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#25216;&#26415;&#20197;&#20943;&#36731;&#36825;&#20123;&#20837;&#20405;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#28023;&#20107;&#33322;&#36816;&#20132;&#36890;&#27969;&#37327;&#65292;&#24182;&#20197;&#27492;&#25351;&#23548;&#36890;&#36807;&#20840;&#29699;&#20132;&#36890;&#32593;&#32476;&#20256;&#25773;&#30340;&#20837;&#20405;&#29289;&#31181;&#39118;&#38505;&#35780;&#20272;&#12290;&#21463;&#22269;&#38469;&#36152;&#26131;&#37325;&#21147;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24433;&#21709;&#33337;&#33334;&#27963;&#21160;&#21487;&#33021;&#24615;&#21644;&#24433;&#21709;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#22914;&#33322;&#36816;&#36890;&#37327;&#23494;&#24230;&#12289;&#28207;&#21475;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#36152;&#26131;&#27969;&#37327;&#21644;&#20132;&#36890;&#26530;&#32445;&#30340;&#20013;&#24515;&#24615;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20998;&#26512;&#20837;&#20405;&#29289;&#31181;&#30340;&#39118;&#38505;&#32593;&#32476;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#20837;&#20405;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invasive species in water bodies pose a major threat to the environment and biodiversity globally. Due to increased transportation and trade, non-native species have been introduced to new environments, causing damage to ecosystems and leading to economic losses in agriculture, forestry, and fisheries. Therefore, there is a pressing need for risk assessment and management techniques to mitigate the impact of these invasions. This study aims to develop a new physics-inspired model to forecast maritime shipping traffic and thus inform risk assessment of invasive species spread through global transportation networks. Inspired by the gravity model for international trades, our model considers various factors that influence the likelihood and impact of vessel activities, such as shipping flux density, distance between ports, trade flow, and centrality measures of transportation hubs. Additionally, by analyzing the risk network of invasive species, we provide a comprehensive framework for as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05363</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;&#22495;&#23545;&#40784;&#23454;&#29616;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#30561;&#30496;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22495;&#27867;&#21270;&#27010;&#24565;&#21040;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#24449;&#23545;&#40784;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepDG&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#21644;&#26102;&#24207;&#29305;&#24449;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#37117;&#24456;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26102;&#20195;&#32423;&#21644;&#24207;&#21015;&#32423;&#29305;&#24449;&#23545;&#40784;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#20195;&#32423;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#30561;&#30496;&#26102;&#20195;&#30340;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07338</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#25903;&#25745;&#30528;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26377;&#25928;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#30452;&#25509;&#25351;&#20196;&#36319;&#38543;&#26032;&#20219;&#21153;&#30340;&#25903;&#25345;&#65292;&#35201;&#20040;&#24573;&#35270;&#20174;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#22522;&#30784;&#30693;&#35782;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;TabFMs&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32463;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#30446;&#26631;&#22312;&#22823;&#33539;&#22260;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;TabFM&#30340;&#26377;&#25928;&#24615;&#65306;&#23427;&#19981;&#20165;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#25512;&#29702;&#31561;&#36981;&#24490;&#25351;&#20196;&#30340;&#20219;&#21153;&#20013;&#26126;&#26174;&#20986;&#33394;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.03716</link><description>&lt;p&gt;
&#19968;&#26465;&#28459;&#38271;&#20043;&#36335;&#65306;&#25506;&#31350;RLHF&#20013;&#30340;&#38271;&#24230;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03716
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;RLHF&#20013;&#22870;&#21169;&#21644;&#38271;&#24230;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20248;&#21270;&#21709;&#24212;&#38271;&#24230;&#26159;RLHF&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#22870;&#21169;&#27169;&#22411;&#20351;&#24471;&#22312;&#36890;&#29992;&#32842;&#22825;&#35774;&#32622;&#20043;&#22806;&#36827;&#34892;&#26356;&#24191;&#27867;&#30340;&#23454;&#39564;&#25104;&#20026;&#21487;&#33021;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#20351;&#31995;&#32479;&#22312;&#32593;&#39029;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#22810;&#36718;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#26356;&#21152;&#8220;&#26377;&#29992;&#8221;&#12290;&#24403;&#20248;&#21270;&#26377;&#29992;&#24615;&#26102;&#65292;&#25105;&#20204;&#19968;&#30452;&#35266;&#23519;&#21040;RLHF&#20250;&#39537;&#20351;&#27169;&#22411;&#20135;&#29983;&#26356;&#38271;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#21709;&#24212;&#38271;&#24230;&#36827;&#34892;&#20248;&#21270;&#26159;RLHF&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#21462;&#24471;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38024;&#23545;&#26377;&#29992;&#24615;&#35757;&#32451;&#30340;&#19977;&#20010;&#24320;&#28304;&#20559;&#22909;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#19982;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#38271;&#24230;&#19982;&#22870;&#21169;&#24378;&#28872;&#30456;&#20851;&#65292;&#22870;&#21169;&#20998;&#25968;&#30340;&#25552;&#39640;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#25913;&#21464;&#36755;&#20986;&#38271;&#24230;&#30340;&#20998;&#24067;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;RL&#21644;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#24178;&#39044;&#65292;&#30475;&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can ach
&lt;/p&gt;</description></item><item><title>&#20998;&#22359;&#26159;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21344;&#25454;&#23454;&#39564;&#20013;&#31163;&#32447;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#32422;&#19968;&#21322;&#12290;&#24403;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#20998;&#22359;&#38382;&#39064;&#65292;&#21482;&#26377;&#22312;&#25968;&#25454;&#20998;&#24067;&#27809;&#26377;&#21464;&#21270;&#26102;&#34920;&#29616;&#19982;&#26222;&#36890;SGD&#35757;&#32451;&#30456;&#20223;&#12290;</title><link>http://arxiv.org/abs/2310.02206</link><description>&lt;p&gt;
&#20998;&#22359;&#65306;&#21363;&#20351;&#22312;&#19981;&#25913;&#21464;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#36951;&#24536;&#20063;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Chunking: Forgetting Matters in Continual Learning even without Changing Tasks. (arXiv:2310.02206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02206
&lt;/p&gt;
&lt;p&gt;
&#20998;&#22359;&#26159;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21344;&#25454;&#23454;&#39564;&#20013;&#31163;&#32447;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#32422;&#19968;&#21322;&#12290;&#24403;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#20998;&#22359;&#38382;&#39064;&#65292;&#21482;&#26377;&#22312;&#25968;&#25454;&#20998;&#24067;&#27809;&#26377;&#21464;&#21270;&#26102;&#34920;&#29616;&#19982;&#26222;&#36890;SGD&#35757;&#32451;&#30456;&#20223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30740;&#31350;&#20013;&#65292;&#20027;&#35201;&#20851;&#27880;&#21160;&#24577;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#25152;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;CL&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65306;&#65288;a&#65289;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#65288;b&#65289;&#22788;&#29702;&#25968;&#25454;&#34987;&#20998;&#25104;&#22359;&#30340;&#20107;&#23454;&#65292;&#22240;&#27492;&#22312;&#20219;&#20309;&#26102;&#38388;&#28857;&#19978;&#21482;&#26377;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#21518;&#32773;&#30340;&#23376;&#38382;&#39064;--&#25968;&#25454;&#30340;&#20998;&#22359;--&#24182;&#27880;&#24847;&#21040;&#20197;&#21069;&#23545;CL&#25991;&#29486;&#20013;&#20851;&#20110;&#20998;&#22359;&#30340;&#20998;&#26512;&#24456;&#23569;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#20998;&#22359;&#26159;CL&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#21344;&#25454;&#20102;&#31163;&#32447;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#32422;&#19968;&#21322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;CL&#31639;&#27861;&#27809;&#26377;&#35299;&#20915;&#20998;&#22359;&#23376;&#38382;&#39064;&#65292;&#21482;&#26377;&#22312;&#25968;&#25454;&#20998;&#24067;&#27809;&#26377;&#21464;&#21270;&#26102;&#25165;&#33021;&#34920;&#29616;&#20986;&#19982;&#26222;&#36890;SGD&#35757;&#32451;&#19968;&#26679;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#22312;&#25968;&#25454;&#22359;&#19978;&#36827;&#34892;&#23398;&#20064;&#26102;&#24615;&#33021;&#20250;&#19979;&#38477;&#65292;&#24182;&#21457;&#29616;&#36951;&#24536;&#26159;&#19968;&#20010;&#32463;&#24120;&#34987;&#30475;&#20316;&#26159;&#38382;&#39064;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work on continual learning (CL) has largely focused on the problems arising from the dynamically-changing data distribution. However, CL can be decomposed into two sub-problems: (a) shifts in the data distribution, and (b) dealing with the fact that the data is split into chunks and so only a part of the data is available to be trained on at any point in time. In this work, we look at the latter sub-problem -- the chunking of data -- and note that previous analysis of chunking in the CL literature is sparse. We show that chunking is an important part of CL, accounting for around half of the performance drop from offline learning in our experiments. Furthermore, our results reveal that current CL algorithms do not address the chunking sub-problem, only performing as well as plain SGD training when there is no shift in the data distribution. We analyse why performance drops when learning occurs on chunks of data, and find that forgetting, which is often seen to be a problem due to distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00927</link><description>&lt;p&gt;
&#29702;&#35299;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Understanding Transferable Representation Learning and Zero-shot Transfer in CLIP. (arXiv:2310.00927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28304;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#32780;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;CLIP&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#39044;&#35757;&#32451;&#26469;&#23398;&#20064;&#32852;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#25991;&#26412;&#24341;&#23548;&#30340;&#33258;&#28982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;CLIP&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;CLIP&#20013;&#30340;&#21487;&#36716;&#31227;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#19981;&#21516;&#27169;&#24577;&#30340;&#29305;&#24449;&#22914;&#20309;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#20256;&#36882;&#24615;&#33021;&#12290;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CLIP&#31867;&#22411;&#26041;&#27861;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;CLIP&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, CLIP has emerged as an effective approach that employs vision-language contrastive pretraining to learn joint image and text representations and exhibits remarkable performance in zero-shot learning and text-guided natural image generation. Despite the huge practical success of CLIP, its theoretical understanding remains elusive. In this paper, we formally study transferrable representation learning underlying CLIP and demonstrate how features from different modalities get aligned. We also analyze its zero-shot transfer performance on the downstream tasks. Inspired by our analysis, we propose a new CLIP-type approach, which achieves better performance than CLIP and other state-of-the-art methods on benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03774</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Safety Concerns in Automated Driving Perception. (arXiv:2309.03774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#32467;&#26500;&#20803;&#32032;&#65292;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#19968;&#27010;&#24565;&#19981;&#20165;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#26631;&#20934;&#30456;&#22865;&#21512;&#65292;&#36824;&#20026;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#26631;&#20934;&#25552;&#20379;&#20102;&#26032;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#24863;&#30693;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#23548;&#33268;&#20102;&#23545;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#22686;&#21152;&#38656;&#27714;&#12290;&#36825;&#31867;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;DNNs&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;&#20026;&#20102;&#20197;&#31995;&#32479;&#32508;&#21512;&#30340;&#26041;&#24335;&#30830;&#20445;&#22522;&#20110;DNNs&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#23433;&#20840;&#32771;&#34385;&#20316;&#20026;&#36866;&#24403;&#30340;&#32467;&#26500;&#20803;&#32032;&#12290;&#19968;&#26041;&#38754;&#65292;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#35774;&#35745;&#19982;&#29616;&#26377;&#30340;&#19982;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#23433;&#20840;&#30456;&#20851;&#30340;&#26631;&#20934;&#22914;ISO 21448&#65288;SOTIF&#65289;&#38750;&#24120;&#22865;&#21512;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24050;&#32463;&#28608;&#21457;&#20102;&#20960;&#31687;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#21363;&#23558;&#20986;&#21488;&#30340;&#20851;&#20110;AI&#23433;&#20840;&#30340;&#26631;&#20934;&#65292;&#22914;ISO PAS 8800&#12290;&#34429;&#28982;&#23433;&#20840;&#32771;&#34385;&#30340;&#27010;&#24565;&#20197;&#21069;&#24050;&#32463;&#34987;&#20171;&#32461;&#36807;&#65292;&#20294;&#26412;&#25991;&#23545;&#20854;&#36827;&#34892;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#65292;&#20511;&#37492;&#20102;&#21508;&#20010;&#39046;&#22495;&#21644;&#23433;&#20840;&#19987;&#23478;&#30340;&#21453;&#39304;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the field of deep learning and impressive performance of deep neural networks (DNNs) for perception have resulted in an increased demand for their use in automated driving (AD) systems. The safety of such systems is of utmost importance and thus requires to consider the unique properties of DNNs.  In order to achieve safety of AD systems with DNN-based perception components in a systematic and comprehensive approach, so-called safety concerns have been introduced as a suitable structuring element. On the one hand, the concept of safety concerns is -- by design -- well aligned to existing standards relevant for safety of AD systems such as ISO 21448 (SOTIF). On the other hand, it has already inspired several academic publications and upcoming standards on AI safety such as ISO PAS 8800.  While the concept of safety concerns has been previously introduced, this paper extends and refines it, leveraging feedback from various domain and safety experts in the field. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.06094</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;&#39537;&#21160;&#30340;&#22270;&#20687;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding. (arXiv:2306.06094v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#24615;&#26041;&#27861;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#20351;&#29992;&#21487;&#20280;&#32553;&#30690;&#37327;&#22270;(SVG)&#26684;&#24335;&#22788;&#29702;&#22270;&#20687;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;XML&#30340;SVG&#34920;&#36848;&#30340;&#25991;&#26412;&#25551;&#36848;&#32780;&#19981;&#26159;&#20809;&#26629;&#22270;&#20687;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;LLMs&#33021;&#22815;&#30452;&#25509;&#29702;&#35299;&#21644;&#25805;&#20316;&#22270;&#20687;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#35270;&#35273;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#31616;&#21333;&#30340;&#22270;&#20687;&#20998;&#31867;&#12289;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26041;&#27861;&#22312;&#21028;&#21035;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#23427;(i)&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#40065;&#26834;&#24615;&#65292;(ii)&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#20197;&#21450;(iii)&#22270;&#20687;&#26434;&#20081;&#31243;&#24230;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have made significant advancements in natural language understanding and generation. However, their potential in computer vision remains largely unexplored. In this paper, we introduce a new, exploratory approach that enables LLMs to process images using the Scalable Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions of SVG representations instead of raster images, we aim to bridge the gap between the visual and textual modalities, allowing LLMs to directly understand and manipulate images without the need for parameterized visual components. Our method facilitates simple image classification, generation, and in-context learning using only LLM capabilities. We demonstrate the promise of our approach across discriminative and generative tasks, highlighting its (i) robustness against distribution shift, (ii) substantial improvements achieved by tapping into the in-context learning abilities of LLMs, and (iii) image unders
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.13939</link><description>&lt;p&gt;
SpikeGPT&#65306;&#24102;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#28608;&#27963;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#30340;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#24050;&#32463;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;SNN&#30340;&#35757;&#32451;&#20063;&#34987;&#35777;&#26126;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#33853;&#21518;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#23578;&#26410;&#30475;&#21040;SNN&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;Receptance Weighted Key Value&#65288;RWKV&#65289;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#8220;SpikeGPT&#8221;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27169;&#22411;&#21464;&#20307;&#19978;&#35757;&#32451;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65306;45M&#21644;216M&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SpikeGPT&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNN&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#33033;&#20914;&#27169;&#22411;&#36890;&#24120;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2302.03460</link><description>&lt;p&gt;
&#27880;&#24847;&#30041;&#19979;&#31354;&#38553;&#65281;&#29992;&#40065;&#26364;&#21151;&#33021;&#29702;&#35770;&#26500;&#24314;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication. (arXiv:2302.03460v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21019;&#24314;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24050;&#20174;&#19968;&#31181;&#20027;&#35201;&#30340;&#25216;&#26415;&#23398;&#31185;&#21457;&#23637;&#25104;&#19982;&#31038;&#20250;&#31185;&#23398;&#32039;&#23494;&#30456;&#20132;&#30340;&#39046;&#22495;&#12290;&#20154;&#31867;&#20559;&#22909;&#23545;&#27604;&#30340;&#35299;&#37322;&#65292;&#30830;&#20999;&#32780;&#35328;&#26159;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#23545;&#20110;&#36825;&#31181;&#36716;&#21464;&#36215;&#21040;&#20102;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21551;&#21457;&#21644;&#24341;&#39046;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#21516;&#26679;&#37325;&#35201;&#30340;&#35266;&#23519;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#20154;&#31867;&#35299;&#37322;&#32773;&#24076;&#26395;&#36890;&#36807;&#23545;&#35805;&#24335;&#30340;&#20132;&#20114;&#19982;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#32773;&#36827;&#34892;&#20132;&#27969;&#30340;&#24895;&#26395;&#22312;&#31038;&#21306;&#20013;&#22522;&#26412;&#34987;&#24573;&#35270;&#12290;&#36825;&#32473;&#36825;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#21644;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#24456;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25552;&#20379;&#21333;&#19968;&#30340;&#20248;&#21270;&#35299;&#37322;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#33021;&#28385;&#36275;&#20854;&#25509;&#25910;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#37492;&#20110;&#20154;&#31867;&#30693;&#35782;&#21644;&#24847;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#21033;&#29992;&#23612;&#20811;&#25289;&#26031;&#183;&#40065;&#26364;&#21644;&#20854;&#20182;&#20132;&#27969;&#23398;&#32773;&#38416;&#36848;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#21521;&#26356;&#23545;&#35805;&#24335;&#30340;&#35299;&#37322;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#20854;&#20013;&#35299;&#37322;&#32773;&#21644;&#34987;&#35299;&#37322;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#25345;&#32493;&#20132;&#27969;&#26159;&#26680;&#24515;&#12290;&#36890;&#36807;&#36825;&#31181;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#24314;&#31435;&#26356;&#26377;&#25928;&#21644;&#24191;&#27867;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive -- more precisely, counterfactual -- explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have received much less attention. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2301.13803</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Vision Transformer via Debiased Self-Attention. (arXiv:2301.13803v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13803
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#20559;&#33258;&#27880;&#24847;&#21147;&#30340;&#20844;&#24179;&#24863;&#30693;&#35270;&#35273;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#26469;&#20943;&#36731;&#20559;&#35265;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#21644;&#36890;&#36807;&#33258;&#25105;&#20851;&#27880;&#26426;&#21046;&#24314;&#27169;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#22312;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;ViT&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;ViT&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#21253;&#25324;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#21478;&#19968;&#20010;&#38656;&#27714;&#65292;&#20844;&#24179;&#24615;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#20844;&#24179;&#24863;&#30693;&#31639;&#27861;&#65288;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;CNN&#65289;&#22312;ViT&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#23601;&#38656;&#35201;&#25105;&#20204;&#36890;&#36807;&#21435;&#20559;&#33258;&#27880;&#24847;&#65288;DSA&#65289;&#24320;&#21457;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;&#12290;DSA&#26159;&#19968;&#31181;&#36890;&#36807;&#30450;&#30446;&#26041;&#27861;&#26469;&#24378;&#21046;ViT&#28040;&#38500;&#19982;&#25935;&#24863;&#23646;&#24615;&#30456;&#20851;&#30340;&#34394;&#20551;&#29305;&#24449;&#20197;&#20943;&#36731;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#34987;&#29992;&#26469;&#23450;&#20301;&#21644;&#23631;&#34109;&#36755;&#20837;&#22270;&#20687;&#22359;&#20013;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has recently gained significant interest in solving computer vision (CV) problems due to its capability of extracting informative features and modeling long-range dependencies through the self-attention mechanism. To fully realize the advantages of ViT in real-world applications, recent works have explored the trustworthiness of ViT, including its robustness and explainability. However, another desiderata, fairness has not yet been adequately addressed in the literature. We establish that the existing fairness-aware algorithms (primarily designed for CNNs) do not perform well on ViT. This necessitates the need for developing our novel framework via Debiased Self-Attention (DSA). DSA is a fairness-through-blindness approach that enforces ViT to eliminate spurious features correlated with the sensitive attributes for bias mitigation. Notably, adversarial examples are leveraged to locate and mask the spurious features in the input image patches. In addition, DSA u
&lt;/p&gt;</description></item></channel></rss>