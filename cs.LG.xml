<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.13107</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#25991;&#26412;&#30340;&#22810;&#32423;&#24635;&#32467;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#22242;&#38431;SCaLAR&#22312;SemEval-2024&#20219;&#21153;5&#19978;&#30340;&#24037;&#20316;&#65306;&#27665;&#20107;&#31243;&#24207;&#20013;&#30340;&#27861;&#24459;&#35770;&#35777;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#30340;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#32780;&#20196;&#20154;&#26395;&#32780;&#21364;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#29983;&#25104;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#29305;&#24449;&#65288;&#21253;&#25324;CNN&#12289;GRU&#21644;LSTM&#65289;&#30340;&#22810;&#32423;Legal-Bert&#23884;&#20837;&#30340;&#34701;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#27861;&#24459;&#35299;&#37322;&#30340;&#20887;&#38271;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;T5&#30340;&#20998;&#27573;&#25688;&#35201;&#65292;&#25104;&#21151;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#22312;&#24320;&#21457;&#38598;&#19978;&#35265;&#35777;&#20102;macro F1&#20998;&#25968;&#22686;&#21152;&#20102;20&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#22686;&#21152;&#20102;10&#20010;&#30334;&#20998;&#28857;&#65292;&#32771;&#34385;&#21040;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13107v1 Announce Type: new  Abstract: This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based unsupervised approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including CNN, GRU, and LSTM. To address the lengthy nature of Legal explanation in the dataset, we introduce T5-based segment-wise summarization, which successfully retained crucial information, enhancing the model's performance. Our unsupervised system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08627</link><description>&lt;p&gt;
&#22810;&#20449;&#24230;&#32447;&#24615;&#22238;&#24402;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multifidelity linear regression for scientific machine learning from scarce data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#32473;&#23450;&#21442;&#25968;&#21270;&#27169;&#22411;&#31867;&#30340;&#21442;&#25968;&#26469;&#36866;&#24212;&#25968;&#25454;&#65292;&#20316;&#20026;&#23398;&#20064;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#22312;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#29615;&#22659;&#20013;&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#31639;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#21508;&#31181;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#21487;&#29992;&#30340;&#31185;&#23398;&#32972;&#26223;&#65307;&#20363;&#22914;&#65292;&#39640;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#30001;&#26114;&#36149;&#30340;&#20840;&#38754;&#35299;&#26512;&#30340;&#29289;&#29702;&#27169;&#25311;&#29983;&#25104;&#65292;&#32780;&#20302;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#26469;&#33258;&#22522;&#20110;&#31616;&#21270;&#30340;&#26356;&#20415;&#23452;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08627v1 Announce Type: cross  Abstract: Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.06659</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#27979;&#35797;&#26102;&#20020;&#24202;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06659
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#26469;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#23454;&#36341;&#20013;&#29992;&#20110;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#30142;&#30149;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#22312;&#26410;&#32463;&#27880;&#37322;&#30340;ECG&#25968;&#25454;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;eSSL&#65289;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#34920;&#24449;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#21487;&#20197;&#22312;&#25253;&#21578;&#20013;&#25214;&#21040;&#30340;&#20020;&#24202;&#30693;&#35782;&#12290;&#26412;&#25991;&#36890;&#36807;&#22810;&#27169;&#24577;&#23398;&#20064;ECG&#35760;&#24405;&#21644;&#30456;&#20851;&#25253;&#21578;&#65292;&#25552;&#20986;&#20102;Multimodal ECG Representation Learning (MERL)&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;ECG&#20998;&#31867;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Knowledge Enhanced Prompt Engineering (CKEPE)&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#21033;&#29992;&#22806;&#37096;&#19987;&#23478;&#39564;&#35777;&#30340;&#20020;&#24202;&#30693;&#35782;&#25968;&#25454;&#24211;&#65292;&#29983;&#25104;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#30149;&#21490;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06659v1 Announce Type: cross  Abstract: Electrocardiograms (ECGs) are non-invasive diagnostic tools crucial for detecting cardiac arrhythmic diseases in clinical practice. While ECG Self-supervised Learning (eSSL) methods show promise in representation learning from unannotated ECG data, they often overlook the clinical knowledge that can be found in reports. This oversight and the requirement for annotated samples for downstream tasks limit eSSL's versatility. In this work, we address these issues with the Multimodal ECG Representation Learning (MERL}) framework. Through multimodal learning on ECG records and associated reports, MERL is capable of performing zero-shot ECG classification with text prompts, eliminating the need for training data in downstream tasks. At test time, we propose the Clinical Knowledge Enhanced Prompt Engineering (CKEPE) approach, which uses Large Language Models (LLMs) to exploit external expert-verified clinical knowledge databases, generating mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.17570</link><description>&lt;p&gt;
&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sparse Variational Contaminated Noise Gaussian Process Regression for Forecasting Geomagnetic Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#21464;&#20998;&#21463;&#24178;&#25200;&#22122;&#22768;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26694;&#26550;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#65292;&#24212;&#29992;&#20110;&#22320;&#30913;&#25200;&#21160;&#39044;&#27979;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#21644;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22797;&#26434;&#21327;&#26041;&#24046;&#32467;&#26500;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;GP&#26694;&#26550;&#25193;&#23637;&#65292;&#20351;&#29992;&#21463;&#24178;&#25200;&#30340;&#27491;&#24577;&#20284;&#28982;&#20989;&#25968;&#26356;&#22909;&#22320;&#32771;&#34385;&#24322;&#26041;&#24046;&#26041;&#24046;&#21644;&#31163;&#32676;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31232;&#30095;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;&#65288;SVGP&#65289;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#21512;&#20855;&#26377;&#21463;&#24178;&#25200;&#27491;&#24577;&#22122;&#22768;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#27169;&#22411;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#22320;&#30913;&#22320;&#38754;&#25200;&#21160;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#23494;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#39044;&#27979;&#38388;&#38548;&#65292;&#20294;&#20855;&#26377;&#30456;&#20284;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17570v1 Announce Type: new  Abstract: Gaussian Processes (GP) have become popular machine learning methods for kernel based learning on datasets with complicated covariance structures. In this paper, we present a novel extension to the GP framework using a contaminated normal likelihood function to better account for heteroscedastic variance and outlier noise. We propose a scalable inference algorithm based on the Sparse Variational Gaussian Process (SVGP) method for fitting sparse Gaussian process regression models with contaminated normal noise on large datasets. We examine an application to geomagnetic ground perturbations, where the state-of-art prediction model is based on neural networks. We show that our approach yields shorter predictions intervals for similar coverage and accuracy when compared to an artificial dense neural network baseline.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;FairGrad&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#25439;&#22833;&#36882;&#20943;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15638</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Fair Resource Allocation in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15638
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;FairGrad&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#20219;&#21153;&#25439;&#22833;&#36882;&#20943;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21487;&#20197;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;MTL&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23384;&#22312;&#20914;&#31361;&#30340;&#26799;&#24230;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#26576;&#20123;&#20219;&#21153;&#30340;&#20844;&#24179;&#20248;&#21270;&#65292;&#20174;&#32780;&#22952;&#30861;MTL&#23454;&#29616;&#26356;&#22909;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#21463;&#36890;&#20449;&#32593;&#32476;&#20013;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;MTL&#30340;&#20248;&#21270;&#23450;&#24335;&#20026;&#19968;&#20010;&#25928;&#29992;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#30340;&#20844;&#24179;&#24230;&#37327;&#19979;&#26368;&#22823;&#21270;&#36328;&#20219;&#21153;&#30340;&#25439;&#22833;&#36882;&#20943;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MTL&#20248;&#21270;&#26041;&#27861;FairGrad&#12290;FairGrad&#19981;&#20165;&#21487;&#20197;&#28789;&#27963;&#22320;&#24378;&#35843;&#26576;&#20123;&#20219;&#21153;&#65292;&#32780;&#19988;&#21487;&#20197;&#23454;&#29616;&#29702;&#35770;&#25910;&#25947;&#20445;&#35777;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#26799;&#24230;&#35843;&#25972;&#26041;&#27861;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15638v1 Announce Type: new  Abstract: By jointly learning multiple tasks, multi-task learning (MTL) can leverage the shared knowledge across tasks, resulting in improved data efficiency and generalization performance. However, a major challenge in MTL lies in the presence of conflicting gradients, which can hinder the fair optimization of some tasks and subsequently impede MTL's ability to achieve better overall performance. Inspired by fair resource allocation in communication networks, we formulate the optimization of MTL as a utility maximization problem, where the loss decreases across tasks are maximized under different fairness measurements. To solve this problem, we propose FairGrad, a novel MTL optimization method. FairGrad not only enables flexible emphasis on certain tasks but also achieves a theoretical convergence guarantee. Extensive experiments demonstrate that our method can achieve state-of-the-art performance among gradient manipulation methods on a suite of
&lt;/p&gt;</description></item><item><title>Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.15350</link><description>&lt;p&gt;
Farsight&#65306;&#22312;AI&#24212;&#29992;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#22521;&#20859;&#36127;&#36131;&#20219;&#30340;AI&#24847;&#35782;
&lt;/p&gt;
&lt;p&gt;
Farsight: Fostering Responsible AI Awareness During AI Application Prototyping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15350
&lt;/p&gt;
&lt;p&gt;
Farsight&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#35774;&#35745;AI&#24212;&#29992;&#21407;&#22411;&#26102;&#35782;&#21035;&#28508;&#22312;&#21361;&#23475;&#65292;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Farsight&#21518;&#65292;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#39537;&#21160;&#30028;&#38754;&#20351;&#24471;&#21407;&#22411;&#35774;&#35745;&#21644;&#26500;&#24314;AI&#24212;&#29992;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#21487;&#33021;&#22312;AI&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#21407;&#22411;&#35774;&#35745;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#22320;&#20132;&#20114;&#24037;&#20855;Farsight&#65292;&#24110;&#21161;&#20154;&#20204;&#35782;&#21035;&#20182;&#20204;&#27491;&#22312;&#35774;&#35745;&#21407;&#22411;&#30340;AI&#24212;&#29992;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;&#26681;&#25454;&#29992;&#25143;&#30340;&#25552;&#31034;&#65292;Farsight&#31361;&#20986;&#26174;&#31034;&#20102;&#19982;&#30456;&#20851;AI&#20107;&#20214;&#26377;&#20851;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25506;&#32034;&#21644;&#32534;&#36753;LLM&#29983;&#25104;&#30340;&#29992;&#20363;&#12289;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#21361;&#23475;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19982;10&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#20849;&#21516;&#35774;&#35745;&#30740;&#31350;&#30340;&#35774;&#35745;&#35265;&#35299;&#65292;&#20197;&#21450;&#19982;42&#20301;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#32467;&#26524;&#12290;&#22312;&#20351;&#29992;Farsight&#21518;&#65292;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#20013;&#30340;AI&#21407;&#22411;&#35774;&#35745;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#29420;&#31435;&#35782;&#21035;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#24037;&#20855;&#27604;&#29616;&#26377;&#36164;&#28304;&#26356;&#26377;&#29992;&#19988;&#26356;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15350v1 Announce Type: cross  Abstract: Prompt-based interfaces for Large Language Models (LLMs) have made prototyping and building AI-powered applications easier than ever before. However, identifying potential harms that may arise from AI applications remains a challenge, particularly during prompt-based prototyping. To address this, we present Farsight, a novel in situ interactive tool that helps people identify potential harms from the AI applications they are prototyping. Based on a user's prompt, Farsight highlights news articles about relevant AI incidents and allows users to explore and edit LLM-generated use cases, stakeholders, and harms. We report design insights from a co-design study with 10 AI prototypers and findings from a user study with 42 AI prototypers. After using Farsight, AI prototypers in our user study are better able to independently identify potential harms associated with a prompt and find our tool more useful and usable than existing resources. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11469</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#35757;&#32451;&#25968;&#25454;&#19982;Transformer&#25991;&#26412;&#27169;&#22411;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#25991;&#26412;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25991;&#26412;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#35780;&#20272;&#36890;&#24120;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20043;&#21518;&#25165;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#20063;&#23384;&#22312;&#30528;&#24378;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20195;&#34920;&#24191;&#27867;&#36755;&#20837;&#24494;&#35843;&#35821;&#26009;&#24211;&#23646;&#24615;&#30340;13&#31181;&#19981;&#21516;&#29305;&#24449;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;BERT&#21644;RoBERTa&#65292;&#24182;&#38468;&#21152;&#20102;BART&#12289;ELECTRA&#21644;GPT2&#30340;&#20854;&#20182;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#35770;&#28857;&#25552;&#20379;&#22810;&#26679;&#30340;&#35777;&#25454;&#12290;&#39318;&#20808;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;(a)&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#19982;&#36731;&#37327;&#32423;&#20998;&#31867;&#22120;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#19968;&#36215;&#26377;&#25928;&#22320;&#39044;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>LtU-ILI&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#30340;&#20840;&#33021;&#26694;&#26550;&#65292;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#21644;&#26131;&#20110;&#24182;&#34892;&#21270;&#30340;&#35774;&#35745;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#12289;&#25512;&#26029;&#23431;&#23449;&#23398;&#12289;&#34920;&#24449;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#21644;&#33719;&#21462;&#38134;&#27827;&#31995;&#20013;&#30340;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.05137</link><description>&lt;p&gt;
LtU-ILI:&#19968;&#31181;&#38754;&#21521;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#30340;&#38544;&#24335;&#25512;&#29702;&#20840;&#33021;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics and Cosmology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05137
&lt;/p&gt;
&lt;p&gt;
LtU-ILI&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#30340;&#20840;&#33021;&#26694;&#26550;&#65292;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#24182;&#20855;&#26377;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#21644;&#26131;&#20110;&#24182;&#34892;&#21270;&#30340;&#35774;&#35745;&#12290;&#23454;&#38469;&#24212;&#29992;&#21253;&#25324;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#12289;&#25512;&#26029;&#23431;&#23449;&#23398;&#12289;&#34920;&#24449;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#21644;&#33719;&#21462;&#38134;&#27827;&#31995;&#20013;&#30340;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23398;&#20064;&#23431;&#23449;&#38544;&#21547;&#20284;&#28982;&#25512;&#29702;&#65288;LtU-ILI&#65289;&#27969;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#20013;&#24555;&#36895;&#12289;&#29992;&#25143;&#21451;&#22909;&#21644;&#21069;&#27839;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25512;&#29702;&#30340;&#20195;&#30721;&#24211;&#12290;&#35813;&#27969;&#31243;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#26550;&#26500;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#36719;&#20214;&#23454;&#29616;&#65292;&#21487;&#36731;&#26494;&#36866;&#24212;&#20219;&#20309;&#30740;&#31350;&#24037;&#20316;&#27969;&#31243;&#12290;&#23427;&#21253;&#25324;&#20840;&#38754;&#30340;&#39564;&#35777;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#21518;&#39564;&#20272;&#35745;&#35206;&#30422;&#33539;&#22260;&#65292;&#25552;&#39640;&#25512;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#27969;&#31243;&#23481;&#26131;&#23454;&#29616;&#24182;&#34892;&#21270;&#65292;&#24182;&#35774;&#35745;&#29992;&#20110;&#39640;&#25928;&#25506;&#32034;&#24314;&#27169;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#24212;&#29992;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#22825;&#20307;&#29289;&#29702;&#23398;&#21644;&#23431;&#23449;&#23398;&#30340;&#21508;&#20010;&#38382;&#39064;&#65292;&#22914;&#20174;X&#23556;&#32447;&#20809;&#24230;&#27861;&#20272;&#35745;&#26143;&#31995;&#22242;&#36136;&#37327;&#65307;&#20174;&#29289;&#36136;&#21151;&#29575;&#35889;&#21644;&#26263;&#29289;&#36136;&#28857;&#20113;&#20013;&#25512;&#26029;&#23431;&#23449;&#23398;&#65307;&#23545;&#24341;&#21147;&#27874;&#20449;&#21495;&#20013;&#30340;&#28304;&#30340;&#29305;&#24615;&#36827;&#34892;&#34920;&#24449;&#65307;&#20174;&#38134;&#27827;&#31995;&#20013;&#33719;&#21462;&#29289;&#29702;&#23576;&#22467;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Learning the Universe Implicit Likelihood Inference (LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge machine learning (ML) inference in astrophysics and cosmology. The pipeline includes software for implementing various neural architectures, training schema, priors, and density estimators in a manner easily adaptable to any research workflow. It includes comprehensive validation metrics to assess posterior estimate coverage, enhancing the reliability of inferred results. Additionally, the pipeline is easily parallelizable, designed for efficient exploration of modeling hyperparameters. To demonstrate its capabilities, we present real applications across a range of astrophysics and cosmology problems, such as: estimating galaxy cluster masses from X-ray photometry; inferring cosmology from matter power spectra and halo point clouds; characterising progenitors in gravitational wave signals; capturing physical dust parameters from galaxy co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21521;&#37327;&#22330;&#20844;&#24335;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#65292;&#24182;&#22312;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#26102;&#23637;&#31034;&#20102;&#26356;&#23567;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03232</link><description>&lt;p&gt;
&#26234;&#33021;&#27969;&#21305;&#37197;&#65306;&#20851;&#20110;&#27969;&#21305;&#37197;&#31639;&#27861;&#30340;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Smart Flow Matching: On The Theory of Flow Matching Algorithms with Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#27969;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#21521;&#37327;&#22330;&#20844;&#24335;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#65292;&#24182;&#22312;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#26102;&#23637;&#31034;&#20102;&#26356;&#23567;&#30340;&#26041;&#24046;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#20844;&#24335;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#26631;&#20934;&#27969;&#30340;&#25439;&#22833;&#30340;&#21521;&#37327;&#22330;&#12290;&#35813;&#20844;&#24335;&#22312;&#32473;&#23450;&#30340;&#20998;&#24067;&#961;&#8320;&#21644;&#26410;&#30693;&#30340;&#20998;&#24067;&#961;&#8321;&#20043;&#38388;&#36827;&#34892;&#20998;&#26512;&#24615;&#20381;&#36182;&#12290;&#22522;&#20110;&#36825;&#20010;&#20844;&#24335;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#20197;&#26465;&#20214;&#27969;&#21305;&#37197;&#30340;&#26041;&#24335;&#35757;&#32451;&#21521;&#37327;&#22330;&#27169;&#22411;&#12290;&#19982;&#26631;&#20934;&#30340;&#26465;&#20214;&#27969;&#21305;&#37197;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25439;&#22833;&#22312;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#26041;&#27861;&#35780;&#20272;&#26102;&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#26041;&#24046;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#27169;&#22411;&#21644;&#22823;&#32500;&#24230;&#34920;&#26684;&#25968;&#25454;&#27169;&#22411;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents the exact formula for the vector field that minimizes the loss for the standard flow. This formula depends analytically on a given distribution \rho_0 and an unknown one \rho_1. Based on the presented formula, a new loss and algorithm for training a vector field model in the style of Conditional Flow Matching are provided. Our loss, in comparison to the standard Conditional Flow Matching approach, exhibits smaller variance when evaluated through Monte Carlo sampling methods. Numerical experiments on synthetic models and models on tabular data of large dimensions demonstrate better learning results with the use of the presented algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2311.18237</link><description>&lt;p&gt;
&#20174;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36824;&#23637;&#31034;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#26377;&#38480;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25512;&#29702;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#23548;&#21521;&#30340;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#35299;&#20915;&#22914;&#20309;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#35757;&#32451;&#23567;&#22411;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#36807;Task-Agnostic VFM&#33976;&#39311;&#12289;Web-Scale CLIP&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24335;ImageNet&#39044;&#35757;&#32451;&#21644;&#33258;&#30417;&#30563;DINO&#39044;&#35757;&#32451;29.8%&#12289;22.1%&#12289;13.7%&#21644;11.6%&#30340;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#23637;&#29616;&#20986;&#20102;&#39640;&#36798;9&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18237v2 Announce Type: replace-cross  Abstract: Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, "How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2309.14277</link><description>&lt;p&gt;
SINCERE: &#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;
&lt;/p&gt;
&lt;p&gt;
SINCERE: Supervised Information Noise-Contrastive Estimation REvisited
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.14277
&lt;/p&gt;
&lt;p&gt;
SINCERE&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#36991;&#20813;&#20102;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#22312;&#20445;&#25345;&#31454;&#20105;&#24615;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;InfoNCE&#65289;&#25439;&#22833;&#20989;&#25968;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#21160;&#26426;&#65292;&#20026;&#35768;&#22810;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#30417;&#30563;&#23545;&#27604;&#65288;SupCon&#65289;&#25439;&#22833;&#21487;&#25193;&#23637;InfoNCE&#20197;&#20174;&#21487;&#29992;&#31867;&#26631;&#31614;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;SupCon&#25439;&#22833;&#20844;&#24335;&#23384;&#22312;&#30097;&#38382;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#20419;&#20351;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#26576;&#20123;&#22270;&#20687;&#22312;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30456;&#20114;&#25490;&#26021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#30417;&#30563;&#20449;&#24687;&#22122;&#22768;-&#23545;&#27604;&#20272;&#35745;&#20877;&#23457;&#65288;SINCERE&#65289;&#25439;&#22833;&#65292;&#20316;&#20026;&#20449;&#24687;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#30417;&#30563;&#25193;&#23637;&#65292;&#23427;&#27704;&#36828;&#19981;&#20250;&#23548;&#33268;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#22270;&#20687;&#30456;&#20114;&#25490;&#26021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SINCERE&#23548;&#33268;&#19981;&#21516;&#31867;&#21035;&#30340;&#23884;&#20837;&#26356;&#22909;&#22320;&#20998;&#31163;&#65292;&#21516;&#26102;&#23545;&#20110;&#30417;&#30563;&#21644;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#19978;&#30340;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.14277v2 Announce Type: replace-cross  Abstract: The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic boun
&lt;/p&gt;</description></item><item><title>AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.12963</link><description>&lt;p&gt;
AutoRT&#65306;&#22823;&#35268;&#27169;&#32534;&#25490;&#26426;&#22120;&#20154;&#20195;&#29702;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12963
&lt;/p&gt;
&lt;p&gt;
AutoRT&#26159;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#24182;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#31561;&#21151;&#33021;&#30340;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21033;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#26469;&#25512;&#29702;&#26377;&#29992;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20855;&#36523;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;&#22522;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoRT&#65292;&#19968;&#20010;&#21033;&#29992;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#25193;&#23637;&#23436;&#20840;&#26410;&#30693;&#22330;&#26223;&#20013;&#25805;&#20316;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#30340;&#31995;&#32479;&#65292;&#21482;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#24037;&#30417;&#30563;&#12290;AutoRT&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#23454;&#29616;&#22330;&#26223;&#29702;&#35299;&#21644;&#22522;&#30784;&#32465;&#23450;&#65292;&#24182;&#36827;&#19968;&#27493;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25552;&#20986;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#25351;&#20196;&#65292;&#20379;&#19968;&#32452;&#26426;&#22120;&#20154;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#26469;&#25351;&#23548;&#25968;&#25454;&#25910;&#38598;&#65292;AutoRT&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#33258;&#20027;&#26435;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#26174;&#33879;&#25193;&#22823;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;AutoRT&#21521;20&#22810;&#20010;&#26426;&#22120;&#20154;&#25552;&#35758;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models that incorporate language, vision, and more recently actions have revolutionized the ability to harness internet scale data to reason about useful tasks. However, one of the key challenges of training embodied foundation models is the lack of data grounded in the physical world. In this paper, we propose AutoRT, a system that leverages existing foundation models to scale up the deployment of operational robots in completely unseen scenarios with minimal human supervision. AutoRT leverages vision-language models (VLMs) for scene understanding and grounding, and further uses large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. Guiding data collection by tapping into the knowledge of foundation models enables AutoRT to effectively reason about autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. We demonstrate AutoRT proposing instructions to over 20 robots across multi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10800</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;AMOC&#36716;&#25442;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Estimation of AMOC transition probabilities using a machine learning based rare-event algorithm. (arXiv:2401.10800v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;TAMS&#21644;Next-Generation Reservoir Computing&#25216;&#26415;&#65292;&#21033;&#29992;&#31232;&#20107;&#20214;&#31639;&#27861;&#20272;&#35745;&#26469;&#28304;&#20110;&#25968;&#25454;&#30340;&#30830;&#23450;&#20989;&#25968;&#65292;&#26469;&#35745;&#31639;&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35199;&#27915;&#32463;&#24230;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#26159;&#20840;&#29699;&#27668;&#20505;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20020;&#30028;&#22240;&#32032;&#65292;&#21487;&#20197;&#22312;&#20840;&#29699;&#21464;&#26262;&#19979;&#23849;&#28291;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20351;&#29992;&#19968;&#31181;&#31232;&#20107;&#20214;&#31639;&#27861;&#65288;Trajectory-Adaptive Multilevel Splitting&#65292;TAMS&#65289;&#35745;&#31639;AMOC&#22312;&#25351;&#23450;&#26102;&#38388;&#31383;&#21475;&#20869;&#23849;&#28291;&#30340;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;TAMS&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#24050;&#30693;&#26368;&#20339;&#24471;&#20998;&#20989;&#25968;&#30340;&#23450;&#20041;&#65292;&#31216;&#20026;&#8220;&#30830;&#23450;&#20989;&#25968;&#8221;&#65292;&#20294;&#36890;&#24120;&#26080;&#27861;&#20808;&#39564;&#22320;&#35745;&#31639;&#23427;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;TAMS&#19982;&#19979;&#19968;&#20195;&#27700;&#24211;&#35745;&#31639;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20174;&#31232;&#20107;&#20214;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#30830;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;AMOC&#30340;&#38543;&#26426;&#30418;&#27169;&#22411;&#20013;&#27979;&#35797;&#20102;&#36825;&#31181;&#25216;&#26415;&#65292;&#35813;&#27169;&#22411;&#23384;&#22312;&#20004;&#31181;&#36716;&#21464;&#31867;&#22411;&#65292;&#31216;&#20026;F&#65288;&#24555;&#36895;&#65289;&#36716;&#21464;&#21644;S&#65288;&#32531;&#24930;&#65289;&#36716;&#21464;&#12290;F&#36716;&#21464;&#30340;&#32467;&#26524;&#19982;&#37027;&#20123;&#36827;&#34892;&#20102;&#26377;&#21033;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Atlantic Meridional Overturning Circulation (AMOC) is an important component of the global climate, known to be a tipping element, as it could collapse under global warming. The main objective of this study is to compute the probability that the AMOC collapses within a specified time window, using a rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS). However, the efficiency and accuracy of TAMS depend on the choice of the score function. Although the definition of the optimal score function, called ``committor function" is known, it is impossible in general to compute it a priori. Here, we combine TAMS with a Next-Generation Reservoir Computing technique that estimates the committor function from the data generated by the rare-event algorithm. We test this technique in a stochastic box model of the AMOC for which two types of transition exist, the so-called F(ast)-transitions and S(low)-transitions. Results for the F-transtions compare favorably with those 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10958</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction. (arXiv:2310.10958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#39044;&#27979;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#36890;&#36807;&#37319;&#29992;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;DNN&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#20248;&#21270;DNN&#35757;&#32451;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;DNN&#21442;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36981;&#24490;&#26576;&#31181;&#35268;&#24459;&#30340;&#35266;&#23519;&#65292;&#21457;&#29616;&#20102;&#21442;&#25968;&#39044;&#27979;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;DNN&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#32423;&#12289;&#30828;&#20214;&#38480;&#21046;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23545;&#22122;&#22768;&#23481;&#24525;&#24230;&#30340;&#29305;&#24615;&#65292;&#37319;&#29992;&#21442;&#25968;&#32447;&#24615;&#39044;&#27979;&#65288;PLP&#65289;&#26041;&#27861;&#26469;&#36827;&#34892;DNN&#21442;&#25968;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#22312;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#39592;&#26550;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26465;&#20214;&#21644;&#26102;&#26399;&#19979;&#65292;&#19982;&#27491;&#24120;&#30340;&#35757;&#32451;&#26041;&#24335;&#30456;&#27604;&#65292;&#36890;&#36807;&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01210</link><description>&lt;p&gt;
&#23454;&#29616;&#31283;&#20581;&#30340;&#24515;&#33039;&#20998;&#21106;&#65306;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Cardiac Segmentation using Graph Convolutional Networks. (arXiv:2310.01210v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#23454;&#29616;&#24515;&#33039;&#20998;&#21106;&#65292;&#36890;&#36807;&#39044;&#27979;&#36718;&#24275;&#28857;&#32780;&#19981;&#26159;&#26631;&#35760;&#27599;&#20010;&#20687;&#32032;&#65292;&#28040;&#38500;&#20102;&#24515;&#33039;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#21516;&#26102;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33258;&#21160;&#21270;&#30340;&#24515;&#33039;&#20998;&#21106;&#21487;&#20197;&#24555;&#36895;&#12289;&#21487;&#37325;&#22797;&#22320;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#26816;&#26597;&#20013;&#25552;&#21462;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;U-Net&#32467;&#26500;&#26159;&#30446;&#21069;&#21307;&#23398;&#20998;&#21106;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#26102;&#20998;&#21106;&#24515;&#33039;&#32467;&#26500;&#65292;&#24182;&#19988;&#24179;&#22343;&#35823;&#24046;&#21487;&#19982;&#35266;&#27979;&#32773;&#38388;&#21464;&#24322;&#24615;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35813;&#26550;&#26500;&#20173;&#28982;&#20250;&#29983;&#25104;&#35768;&#22810;&#35299;&#31163;&#24322;&#24120;&#30340;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#39044;&#27979;&#20986;&#24863;&#20852;&#36259;&#32467;&#26500;&#30340;&#36718;&#24275;&#28857;&#65292;&#32780;&#19981;&#26159;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#33039;&#35299;&#21078;&#23398;&#30340;&#22270;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#36825;&#28040;&#38500;&#20102;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;CAMUS&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#32467;&#26500;&#20998;&#21106;&#20013;&#30340;&#35299;&#21078;&#23398;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#22312;&#20020;&#24202;HUNT4&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20020;&#24202;&#27979;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully automatic cardiac segmentation can be a fast and reproducible method to extract clinical measurements from an echocardiography examination. The U-Net architecture is the current state-of-the-art deep learning architecture for medical segmentation and can segment cardiac structures in real-time with average errors comparable to inter-observer variability. However, this architecture still generates large outliers that are often anatomically incorrect. This work uses the concept of graph convolutional neural networks that predict the contour points of the structures of interest instead of labeling each pixel. We propose a graph architecture that uses two convolutional rings based on cardiac anatomy and show that this eliminates anatomical incorrect multi-structure segmentations on the publicly available CAMUS dataset. Additionally, this work contributes with an ablation study on the graph convolutional architecture and an evaluation of clinical measurements on the clinical HUNT4 dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;</title><link>http://arxiv.org/abs/2308.11038</link><description>&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#20301;&#32622;&#20248;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;K-Means&#21644;P-Median&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#26469;&#20248;&#21270;&#22312;&#22478;&#24066;&#29615;&#22659;&#19979;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#24067;&#32622;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#36317;&#31163;&#21644;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#27969;&#38598;&#25955;&#22320;&#22312;&#26368;&#21518;&#19968;&#20844;&#37324;&#37197;&#36865;&#36317;&#31163;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65307;&#21363;&#20351;&#36317;&#31163;&#24494;&#23567;&#22686;&#21152;&#20063;&#20250;&#23545;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#30340;&#19994;&#21153;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#20250;&#22686;&#21152;&#20854;&#30899;&#36275;&#36857;&#12290;&#29305;&#21035;&#26159;&#22312;Covid-19&#20043;&#21518;&#65292;&#35813;&#34892;&#19994;&#30340;&#22686;&#38271;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#22312;&#22478;&#24066;&#29615;&#22659;&#20013;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#20248;&#21270;&#29289;&#27969;&#38598;&#25955;&#22320;&#30340;&#24067;&#32622;&#12290;&#35813;&#26041;&#27861;&#20381;&#27425;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#31354;&#38388;&#20301;&#32622;&#65292;&#20351;&#29992;K-Means&#23545;&#20132;&#20184;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#32858;&#31867;&#26041;&#27861;&#20351;&#29992;&#36947;&#36335;&#32593;&#32476;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#12290;&#36991;&#20813;&#20351;&#29992;&#38750;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;P-Median&#26041;&#27861;&#30830;&#23450;&#38598;&#25955;&#22320;&#30340;&#20301;&#32622;&#12290;P-Median&#26041;&#27861;&#36824;&#23558;&#20132;&#20184;&#25968;&#37327;&#21644;&#20154;&#21475;&#20316;&#20026;&#26435;&#37325;&#32771;&#34385;&#22312;&#20869;&#12290;&#20351;&#29992;Muller&#21644;Phipps&#65288;M&#65286;P&#65289;&#30340;&#23454;&#38469;&#20132;&#20184;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&amp;P) is used to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2308.05564</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#21246;&#32467;&#30340;&#39640;&#25928;&#21464;&#20998;&#25512;&#29702;&#21450;&#20854;&#22312;&#32929;&#31080;&#25910;&#30410;&#29575;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#23545;&#37329;&#34701;&#25968;&#25454;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#30340;&#23614;&#37096;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Azzalini&#21644;Capitanio&#65288;2003&#65289;&#25152;&#38544;&#21547;&#30340;&#20044;&#40486;&#21246;&#32467;&#22312;&#25104;&#23545;&#38750;&#23545;&#31216;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#20004;&#31181;&#27969;&#34892;&#30340;&#20044;&#40486;&#21246;&#32467;&#26356;&#39640;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23545;&#35813;&#20044;&#40486;&#21246;&#32467;&#30340;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#39640;&#26031;&#29983;&#25104;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#36817;&#20284;&#30340;&#38468;&#21152;&#21518;&#39564;&#12290;&#20351;&#29992;&#24555;&#36895;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;2017&#24180;&#33267;2021&#24180;&#38388;93&#20010;&#32654;&#22269;&#32929;&#31080;&#30340;&#32929;&#31080;&#25910;&#30410;&#29575;&#30340;&#21246;&#32467;&#27169;&#22411;&#12290;&#38500;&#20102;&#25104;&#23545;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#22806;&#65292;&#35813;&#21246;&#32467;&#36824;&#25429;&#25417;&#21040;&#20102;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20381;&#36182;&#30340;&#22823;&#37327;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2308.01118</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#35752;&#35770;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#27969;&#34892;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#27969;&#34892;&#20559;&#24046;&#12290;&#23427;&#21516;&#26102;&#25552;&#20379;&#20102;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#21644;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20197;&#20010;&#24615;&#21270;&#30340;&#26041;&#24335;&#24110;&#21161;&#20154;&#20204;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#20027;&#35201;&#25215;&#35834;&#26159;&#33021;&#22815;&#22686;&#21152;&#30446;&#24405;&#20013;&#36739;&#23569;&#30693;&#21517;&#30340;&#29289;&#21697;&#30340;&#21487;&#35265;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#29616;&#20170;&#30340;&#25512;&#33616;&#31639;&#27861;&#21453;&#32780;&#34920;&#29616;&#20986;&#27969;&#34892;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#22312;&#25512;&#33616;&#20013;&#32463;&#24120;&#20851;&#27880;&#30456;&#24403;&#27969;&#34892;&#30340;&#29289;&#21697;&#12290;&#36825;&#31181;&#20559;&#24046;&#19981;&#20165;&#21487;&#33021;&#23548;&#33268;&#30701;&#26399;&#20869;&#23545;&#28040;&#36153;&#32773;&#21644;&#25552;&#20379;&#32773;&#30340;&#25512;&#33616;&#20215;&#20540;&#26377;&#38480;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#24341;&#36215;&#19981;&#24076;&#26395;&#30340;&#24378;&#21270;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#27969;&#34892;&#20559;&#24046;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#26816;&#27979;&#12289;&#37327;&#21270;&#21644;&#20943;&#23569;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20559;&#24046;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#26082;&#21253;&#25324;&#20102;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#35745;&#31639;&#24230;&#37327;&#30340;&#27010;&#36848;&#65292;&#20063;&#21253;&#25324;&#20102;&#20943;&#23569;&#20559;&#24046;&#30340;&#20027;&#35201;&#25216;&#26415;&#26041;&#27861;&#30340;&#22238;&#39038;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems help people find relevant content in a personalized way. One main promise of such systems is that they are able to increase the visibility of items in the long tail, i.e., the lesser-known items in a catalogue. Existing research, however, suggests that in many situations today's recommendation algorithms instead exhibit a popularity bias, meaning that they often focus on rather popular items in their recommendations. Such a bias may not only lead to limited value of the recommendations for consumers and providers in the short run, but it may also cause undesired reinforcement effects over time. In this paper, we discuss the potential reasons for popularity bias and we review existing approaches to detect, quantify and mitigate popularity bias in recommender systems. Our survey therefore includes both an overview of the computational metrics used in the literature as well as a review of the main technical approaches to reduce the bias. We furthermore critically discu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#26041;&#27861;&#22312;&#24515;&#30005;&#22270;(ECG)&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#26816;&#26597;&#25514;&#26045;&#20197;&#30830;&#23450;&#21512;&#29702;&#30340;&#24402;&#22240;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#24739;&#32773;&#20122;&#32452;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;XAI&#25216;&#26415;&#22914;&#20309;&#34987;&#29992;&#20110;&#30693;&#35782;&#21457;&#29616;&#65292;&#22914;&#35782;&#21035;&#24515;&#32908;&#26775;&#27515;&#20122;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17043</link><description>&lt;p&gt;
&#35299;&#26512;&#24515;&#30005;&#22270;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#23457;&#35745;&#21644;&#30693;&#35782;&#21457;&#29616;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Explaining Deep Learning for ECG Analysis: Building Blocks for Auditing and Knowledge Discovery. (arXiv:2305.17043v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;(XAI)&#26041;&#27861;&#22312;&#24515;&#30005;&#22270;(ECG)&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#22871;&#26816;&#26597;&#25514;&#26045;&#20197;&#30830;&#23450;&#21512;&#29702;&#30340;&#24402;&#22240;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#24739;&#32773;&#20122;&#32452;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;XAI&#25216;&#26415;&#22914;&#20309;&#34987;&#29992;&#20110;&#30693;&#35782;&#21457;&#29616;&#65292;&#22914;&#35782;&#21035;&#24515;&#32908;&#26775;&#27515;&#20122;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#24515;&#33039;&#30142;&#30149;&#21644;&#38544;&#34255;&#30340;&#20020;&#24202;&#22240;&#32032;&#65292;&#22240;&#27492;&#23427;&#20204;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#22320;&#29992;&#20110;&#20998;&#26512;&#24515;&#30005;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21518;&#20107;&#35299;&#37322;(XAI)&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23616;&#37096;(&#27599;&#20010;&#26679;&#26412;&#30340;&#36129;&#29486;&#20540;)&#21644;&#20840;&#23616;(&#22522;&#20110;&#39046;&#22495;&#19987;&#23478;&#27010;&#24565;)&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#22871;&#26816;&#26597;&#25514;&#26045;&#65292;&#20197;&#30830;&#23450;&#21512;&#29702;&#30340;&#24402;&#22240;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#31526;&#21512;&#19987;&#23478;&#35268;&#21017;&#30340;&#23450;&#37327;&#35777;&#25454;&#12290;&#36825;&#31181;&#25968;&#25454;&#38598;&#33539;&#22260;&#30340;&#20998;&#26512;&#36229;&#20986;&#20102;&#20010;&#26696;&#32463;&#39564;&#35777;&#25454;&#65292;&#36890;&#36807;&#27719;&#24635;&#24739;&#32773;&#20122;&#32452;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;XAI&#25216;&#26415;&#22914;&#20309;&#34987;&#29992;&#20110;&#30693;&#35782;&#21457;&#29616;&#65292;&#22914;&#35782;&#21035;&#24515;&#32908;&#26775;&#27515;&#30340;&#20122;&#22411;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20123;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#23457;&#35745;&#21644;&#30693;&#35782;&#21457;&#29616;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become increasingly popular for analyzing ECG data because of their ability to accurately identify cardiac conditions and hidden clinical factors. However, the lack of transparency due to the black box nature of these models is a common concern. To address this issue, explainable AI (XAI) methods can be employed. In this study, we present a comprehensive analysis of post-hoc XAI methods, investigating the local (attributions per sample) and global (based on domain expert concepts) perspectives. We have established a set of sanity checks to identify sensible attribution methods, and we provide quantitative evidence in accordance with expert rules. This dataset-wide analysis goes beyond anecdotal evidence by aggregating data across patient subgroups. Furthermore, we demonstrate how these XAI techniques can be utilized for knowledge discovery, such as identifying subtypes of myocardial infarction. We believe that these proposed methods can serve as building block
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.16791</link><description>&lt;p&gt;
&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#31163;&#25955;&#21270;&#20559;&#24046;&#21644;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#65292;&#24471;&#21040;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#30340;&#27867;&#21270;&#24046;&#36317;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Kidger&#65292;Morrill&#31561;&#65292;2020&#65289;&#20174;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20013;&#39044;&#27979;&#32467;&#26524;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#26159;&#19968;&#20010;&#26410;&#35266;&#23519;&#21040;&#30340;&#36830;&#32493;&#36335;&#24452;&#30340;&#31163;&#25955;&#21270;&#65292;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#20855;&#26377;&#26410;&#30693;&#21521;&#37327;&#22330;&#30340;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#20381;&#36182;&#20110;&#36825;&#20010;&#36335;&#24452;&#12290;&#20351;&#29992;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#20250;&#24341;&#20837;&#31163;&#25955;&#20559;&#24046;&#65292;&#25105;&#20204;&#31934;&#30830;&#22320;&#37327;&#21270;&#20102;&#36825;&#31181;&#20559;&#24046;&#12290;&#36890;&#36807;&#20351;&#29992;&#20851;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27969;&#30340;&#36830;&#32493;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36924;&#36817;&#20559;&#24046;&#30452;&#25509;&#19982;&#30001;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#29983;&#25104;&#27169;&#22411;&#30340;&#21033;&#26222;&#24076;&#33576;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#30456;&#20851;&#12290;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#19978;&#30028;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#36798;&#21040;&#30340;&#26399;&#26395;&#25439;&#22833;&#19982;&#36125;&#21494;&#26031;&#26368;&#20248;&#39118;&#38505;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13865</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#24494;&#35843;&#30340;&#26377;&#36873;&#25321;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Selective Pre-training for Private Fine-tuning. (arXiv:2305.13865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#24819;&#22312;&#30005;&#23376;&#37038;&#20214;&#23458;&#25143;&#31471;&#25110;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#35757;&#32451;&#25991;&#26412;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#36981;&#23432;&#29305;&#23450;&#30340;&#22266;&#23450;&#22823;&#23567;&#65292;&#20197;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;D_pub&#21644;&#19968;&#20010;&#23545;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;T&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;D_priv&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;D_pub&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;M&#65292;&#24182;&#22312;D_priv&#19978;&#24494;&#35843;&#23427;&#65292;&#20351;&#24471;M&#30456;&#23545;&#20110;T&#30340;&#24615;&#33021;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;M&#30456;&#23545;&#20110;D_priv&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;D_pub&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#23558;&#20844;&#20849;&#20998;&#24067;&#19982;&#31169;&#26377;&#20998;&#24067;&#38752;&#36817;&#65292;&#26159;&#26368;&#22823;&#21270;M&#39044;&#35757;&#32451;&#21518;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#38500;&#20102;&#24615;&#33021;&#25913;&#36827;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we want to train text prediction models in email clients or word processors. The models must preserve the privacy of user data and adhere to a specific fixed size to meet memory and inference time requirements. We introduce a generic framework to solve this problem. Specifically, we are given a public dataset $D_\text{pub}$ and a private dataset $D_\text{priv}$ corresponding to a downstream task $T$. How should we pre-train a fixed-size model $M$ on $D_\text{pub}$ and fine-tune it on $D_\text{priv}$ such that performance of $M$ with respect to $T$ is maximized and $M$ satisfies differential privacy with respect to $D_\text{priv}$? We show that pre-training on a {\em subset} of dataset $D_\text{pub}$ that brings the public distribution closer to the private distribution is a crucial ingredient to maximize the transfer learning abilities of $M$ after pre-training, especially in the regimes where model sizes are relatively small. Besides performance improvements, our framework als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.13093</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27010;&#29575;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#31283;&#23450;&#24615;&#26469;&#22238;&#31572;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#23450;&#20041;&#21644;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#25509;&#36817;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#12290;&#20256;&#32479;&#25991;&#29486;&#20381;&#36182;&#20110;&#21442;&#25968;&#32479;&#35745;&#30697;&#65292;&#29305;&#21035;&#26159;&#21442;&#25968;&#26041;&#24046;&#30340;&#25910;&#25947;&#26469;&#37327;&#21270;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;\textit{&#27010;&#29575;&#25910;&#25947;}&#26465;&#20214;&#26469;&#23450;&#20041;SGD&#30340;\textit{&#27010;&#29575;&#31283;&#23450;&#24615;}&#12290;&#25552;&#20986;&#30340;&#31283;&#23450;&#24615;&#30452;&#25509;&#22238;&#31572;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;SGD&#22914;&#20309;&#20174;&#25968;&#37327;&#24222;&#22823;&#30340;&#21487;&#33021;&#20005;&#37325;&#36807;&#25311;&#21512;&#30340;&#35299;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#22312;&#27010;&#29575;&#24615;&#31283;&#23450;&#24615;&#30340;&#38236;&#22836;&#19979;&#65292;SGD&#25165;&#34920;&#29616;&#20986;&#20016;&#23500;&#32780;&#23454;&#38469;&#30456;&#20851;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#22914;&#23436;&#20840;&#22833;&#21435;&#31283;&#23450;&#24615;&#38454;&#27573;&#12289;&#19981;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12289;&#25910;&#25947;&#21040;&#20302;&#31209;&#38797;&#28857;&#38454;&#27573;&#21644;&#27491;&#30830;&#23398;&#20064;&#38454;&#27573;&#12290;&#24403;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36825;&#20123;&#30456;&#22270;&#24847;&#21619;&#30528;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#31283;&#23450;&#21644;&#19981;&#31283;&#23450;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
&lt;/p&gt;</description></item><item><title>AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.09475</link><description>&lt;p&gt;
AMPNet: &#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09475
&lt;/p&gt;
&lt;p&gt;
AMPNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#33021;&#22815;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#22312;&#23454;&#38469;&#29983;&#29289;&#31995;&#32479;&#19978;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;AMPNet&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#20256;&#32479;GNNs&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#29305;&#24449;&#21521;&#37327;&#65292;&#21487;&#33021;&#24573;&#35270;&#20102;&#20851;&#20110;&#20010;&#20307;&#33410;&#28857;&#29305;&#24449;&#30340;&#22797;&#26434;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;AMPNet&#65292;&#29992;&#20110;GNNs&#65292;&#23427;&#23545;&#33410;&#28857;&#36827;&#34892;&#36880;&#20010;&#29305;&#24449;&#32534;&#30721;&#65292;&#24182;&#22312;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#20013;&#36890;&#36807;&#36328;&#33410;&#28857;&#27880;&#24847;&#21147;&#27169;&#22411;&#29305;&#24449;&#32423;&#21035;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#30495;&#23454;&#29983;&#29289;&#31995;&#32479;&#65288;&#22914;fMRI&#33041;&#27963;&#21160;&#35760;&#24405;&#21644;&#31354;&#38388;&#22522;&#22240;&#32452;&#25968;&#25454;&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;AMPNet&#30340;&#33021;&#21147;&#65292;&#23427;&#22312;fMRI&#20449;&#21495;&#37325;&#24314;&#26041;&#38754;&#30456;&#27604;&#29616;&#26377;&#22522;&#20934;&#25552;&#39640;&#20102;20&#65285;&#65292;&#22312;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#21518;&#21448;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;8&#65285;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#29983;&#29289;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;AMPNet&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#32423;&#21035;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39044;&#35745;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#24577;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#20013;&#23398;&#20064;&#21040;&#19982;&#20854;&#34892;&#20026;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#25429;&#33719;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.12067</link><description>&lt;p&gt;
&#21516;&#24577;&#33258;&#32534;&#30721;&#22120; - &#20174;&#35266;&#23519;&#21040;&#36716;&#21270;&#23398;&#20064;&#32676;&#32452;&#32467;&#26500;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions. (arXiv:2207.12067v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#24577;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#20013;&#23398;&#20064;&#21040;&#19982;&#20854;&#34892;&#20026;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#25429;&#33719;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35753;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23398;&#20064;&#21040;&#20934;&#30830;&#34920;&#31034;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#30340;&#20869;&#22312;&#27169;&#22411;&#26159;&#19968;&#20010;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#19981;&#20165;&#21253;&#21547;&#35266;&#23519;&#24615;&#30693;&#35782;&#65292;&#20063;&#21253;&#21547;&#24178;&#39044;&#24615;&#30693;&#35782;&#30340;&#34920;&#29616;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#34920;&#31034;&#23398;&#20064;&#21644;&#32676;&#35770;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#33021;&#22815;&#22312;&#34892;&#21160;&#36807;&#31243;&#20013;&#23398;&#20064;&#21040;&#19982;&#20043;&#30456;&#19968;&#33268;&#30340;&#24863;&#30693;&#20449;&#24687;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#32780;&#36825;&#20123;&#34892;&#21160;&#23454;&#38469;&#19978;&#26159;&#21464;&#25442;&#36825;&#20123;&#20449;&#24687;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#28508;&#22312;&#31354;&#38388;&#19978;&#24212;&#29992;&#32676;&#32452;&#34920;&#31034;&#65292;&#36890;&#36807;&#21033;&#29992;&#31561;&#21464;&#25439;&#22833;&#24378;&#21046;&#23454;&#26045;&#36866;&#24403;&#30340;&#21516;&#24577;&#24615;&#36136;&#20197;&#23436;&#25104;&#35757;&#32451;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20808;&#39564;&#32676;&#32452;&#30693;&#35782;&#65292;&#24182;&#19988;&#19981;&#38480;&#21046;&#20195;&#29702;&#21487;&#25191;&#34892;&#30340;&#34892;&#21160;&#38598;&#21512;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#23398;&#20064;&#21040;&#34892;&#21160;&#30340;&#32676;&#32452;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#33719;&#20102;&#29615;&#22659;&#20013;&#30340;&#36716;&#25442;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the str
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23454;&#20363;&#30456;&#20851;&#30340;&#23545;&#25968;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#35774;&#35745;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.11168</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#23545;&#25968;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11168
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25214;&#21040;&#20102;&#23454;&#20363;&#30456;&#20851;&#30340;&#23545;&#25968;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#35774;&#35745;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#36328;&#24230;&#12289;&#24179;&#22343;&#22870;&#21169;&#35774;&#23450;&#19979;&#30340;&#36830;&#32493;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#31163;&#25955;&#26102;&#38388;MDPs&#19981;&#21516;&#65292;&#36830;&#32493;&#26102;&#38388;&#36807;&#31243;&#22312;&#37319;&#21462;&#34892;&#21160;&#21518;&#20250;&#31227;&#21160;&#21040;&#19968;&#20010;&#29366;&#24577;&#24182;&#22312;&#27492;&#20572;&#30041;&#19968;&#20010;&#38543;&#26426;&#25345;&#32493;&#26102;&#38388;&#12290;&#22312;&#26410;&#30693;&#30340;&#36716;&#31227;&#27010;&#29575;&#21644;&#25351;&#25968;&#25345;&#32493;&#26102;&#38388;&#21464;&#21270;&#29575;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#19982;&#26102;&#38388;&#36328;&#24230;&#23545;&#25968;&#30456;&#20851;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#33021;&#22815;&#23454;&#29616;&#23545;&#25968;&#22686;&#38271;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#19978;&#38480;&#32622;&#20449;&#22686;&#24378;&#23398;&#20064;&#12289;&#22343;&#20540;&#25345;&#32493;&#26102;&#38388;&#30340;&#31934;&#32454;&#20272;&#35745;&#20197;&#21450;&#28857;&#36807;&#31243;&#30340;&#38543;&#26426;&#27604;&#36739;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.
&lt;/p&gt;</description></item></channel></rss>