<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01054</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#35760;&#24518;&#24739;&#32773;&#24433;&#20687;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Unconditional Latent Diffusion Models Memorize Patient Imaging Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#36890;&#36807;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#31243;&#24230;&#20197;&#21450;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#24212;&#29992;&#26159;&#36890;&#36807;&#25552;&#20986;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#24320;&#25918;&#25968;&#25454;&#20849;&#20139;&#12290;&#23613;&#31649;&#26377;&#36825;&#20010;&#24212;&#29992;&#30340;&#21069;&#26223;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#24739;&#32773;&#25968;&#25454;&#30340;&#35760;&#24518;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#24739;&#32773;&#25968;&#25454;&#30340;&#21103;&#26412;&#32780;&#19981;&#26159;&#26032;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#36825;&#30772;&#22351;&#20102;&#20445;&#25252;&#24739;&#32773;&#25968;&#25454;&#30340;&#25972;&#20010;&#30446;&#30340;&#65292;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#24739;&#32773;&#34987;&#37325;&#26032;&#35782;&#21035;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#30028;&#20013;&#36825;&#20010;&#38382;&#39064;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#30340;&#35760;&#24518;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;2D&#21644;3D&#30340;&#38544;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#29992;CT&#12289;MR&#21644;X&#20809;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#26469;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#34987;&#35760;&#24518;&#30340;&#31243;&#24230;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#33021;&#23548;&#33268;&#35760;&#24518;&#30340;&#21508;&#31181;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative latent diffusion models hold a wide range of applications in the medical imaging domain. A noteworthy application is privacy-preserved open-data sharing by proposing synthetic data as surrogates of real patient data. Despite the promise, these models are susceptible to patient data memorization, where models generate patient data copies instead of novel synthetic samples. This undermines the whole purpose of preserving patient data and may even result in patient re-identification. Considering the importance of the problem, surprisingly it has received relatively little attention in the medical imaging community. To this end, we assess memorization in latent diffusion models for medical image synthesis. We train 2D and 3D latent diffusion models on CT, MR, and X-ray datasets for synthetic data generation. Afterwards, we examine the amount of training data memorized utilizing self-supervised models and further investigate various factors that can possibly lead to memorization 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02649</link><description>&lt;p&gt;
&#35770;&#20915;&#31574;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
On the Importance of Uncertainty in Decision-Making with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#20915;&#31574;&#38382;&#39064;&#26102;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38598;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#24050;&#32463;&#25104;&#20026;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20219;&#20309;&#19968;&#20010;&#39069;&#22806;&#30340;&#38454;&#27573;&#29992;&#20110;&#20272;&#35745;&#20195;&#29702;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#23545;&#19990;&#30028;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#20197;&#33258;&#28982;&#35821;&#35328;&#20026;&#36755;&#20837;&#30340;&#22522;&#26412;&#20915;&#31574;&#26694;&#26550;&#20043;&#19968;&#65292;&#21363;&#19978;&#19979;&#25991;&#33218;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#20449;&#24687;&#21253;&#25324;&#25991;&#26412;&#12290;&#20316;&#20026;&#27809;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;&#20195;&#34920;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24102;&#26377;&#36138;&#23146;&#31574;&#30053;&#30340;LLM&#33218;&#65292;&#35813;&#31574;&#30053;&#36873;&#25321;&#23545;&#24212;&#20110;&#26368;&#22823;&#39044;&#27979;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23558;&#27492;&#22522;&#20934;&#19982;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24615;&#38598;&#25104;&#21040;&#27748;&#26222;&#26862;&#25277;&#26679;&#31574;&#30053;&#20013;&#31215;&#26497;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;LLM&#33218;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20363;&#22914;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#12289;&#36749;&#23398;&#21644;Epin&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02649v1 Announce Type: new  Abstract: We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epin
&lt;/p&gt;</description></item><item><title>Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00411</link><description>&lt;p&gt;
Aardvark Weather:&#31471;&#23545;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather: end-to-end data-driven weather forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00411
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20013;&#31243;&#22825;&#27668;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#34987;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#31649;&#36947;&#30340;&#29305;&#23450;&#21644;&#21333;&#20010;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26080;&#27861;&#22312;&#27809;&#26377;&#26469;&#33258;&#20256;&#32479;&#25805;&#20316;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#30340;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#65292;&#36825;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#25903;&#25345;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#20195;&#25972;&#20010;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31649;&#36947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Aardvark Weather&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#23427;&#25509;&#21463;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#39044;&#25253;&#12290;&#36825;&#20123;&#20840;&#29699;&#39044;&#25253;&#20197;&#19968;&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;24&#23567;&#26102;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;&#22810;&#20010;&#21387;&#21147;&#27700;&#24179;24&#20010;&#21464;&#37327;&#20135;&#29983;&#65292;&#24182;&#22312;&#20116;&#21040;&#19971;&#22825;&#30340;&#21069;&#26399;&#39046;&#20808;&#26102;&#27573;&#23545;&#27599;&#23567;&#26102;&#27668;&#20505;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#22320;&#39044;&#25253;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00411v1 Announce Type: cross  Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36793;&#35270;&#20026;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#32500;&#24230;&#26469;&#34920;&#31034;&#20989;&#25968;&#65292;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#19981;&#35268;&#21017;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20351;&#22522;&#30784;&#30697;&#38453;&#23545;&#31216;&#21270;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00218</link><description>&lt;p&gt;
&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Functional-Edged Network Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#36793;&#35270;&#20026;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#32500;&#24230;&#26469;&#34920;&#31034;&#20989;&#25968;&#65292;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#19981;&#35268;&#21017;&#35266;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20351;&#22522;&#30784;&#30697;&#38453;&#23545;&#31216;&#21270;&#65292;&#26368;&#32456;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#29616;&#26377;&#20316;&#21697;&#24418;&#25104;&#23545;&#27604;&#65292;&#29616;&#26377;&#20316;&#21697;&#37117;&#23558;&#33410;&#28857;&#35270;&#20026;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#36793;&#26469;&#34920;&#31034;&#19981;&#21516;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32593;&#32476;&#24314;&#27169;&#65292;&#20854;&#20013;&#36793;&#26159;&#21151;&#33021;&#25968;&#25454;&#65292;&#24182;&#23558;&#37051;&#25509;&#30697;&#38453;&#36716;&#25442;&#20026;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#24341;&#20837;&#19968;&#20010;&#39069;&#22806;&#30340;&#32500;&#24230;&#19987;&#38376;&#29992;&#20110;&#20989;&#25968;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;Tucker&#21151;&#33021;&#20998;&#35299;&#26469;&#22788;&#29702;&#21151;&#33021;&#37051;&#25509;&#24352;&#37327;&#65292;&#20026;&#36827;&#19968;&#27493;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#31038;&#21306;&#65292;&#23545;&#22522;&#30784;&#30697;&#38453;&#36827;&#34892;&#27491;&#21017;&#21270;&#20351;&#20854;&#23545;&#31216;&#21270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#21151;&#33021;&#36793;&#30340;&#19981;&#35268;&#21017;&#35266;&#27979;&#65292;&#25105;&#20204;&#36827;&#34892;&#27169;&#22411;&#25512;&#26029;&#20197;&#35299;&#20915;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#65292;&#36890;&#36807;Riemann&#20849;&#36717;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20960;&#20010;&#23450;&#29702;&#26469;&#23637;&#31034;&#21151;&#33021;&#36793;&#32536;&#32593;&#32476;&#27169;&#22411;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#21644;&#30495;&#23454;&#22320;&#38081;&#31995;&#32479;&#25968;&#25454;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00218v1 Announce Type: cross  Abstract: Contrasts with existing works which all consider nodes as functions and use edges to represent the relationships between different functions. We target at network modeling whose edges are functional data and transform the adjacency matrix into a functional adjacency tensor, introducing an additional dimension dedicated to function representation. Tucker functional decomposition is used for the functional adjacency tensor, and to further consider the community between nodes, we regularize the basis matrices to be symmetrical. Furthermore, to deal with irregular observations of the functional edges, we conduct model inference to solve a tensor completion problem. It is optimized by a Riemann conjugate gradient descent method. Besides these, we also derive several theorems to show the desirable properties of the functional edged network model. Finally, we evaluate the efficacy of our proposed model using simulation data and real metro sys
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.17993</link><description>&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#28982;&#26234;&#33021;&#30456;&#34701;&#21512;&#65306;&#20174;&#32479;&#35745;&#21147;&#23398;&#21040;&#20154;&#24037;&#26234;&#33021;&#20877;&#21040;&#28237;&#27969;
&lt;/p&gt;
&lt;p&gt;
Mixing Artificial and Natural Intelligence: From Statistical Mechanics to AI and Back to Turbulence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17993
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#36890;&#36807;&#21019;&#26032;&#24615;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#28237;&#27969;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20026;AI&#21644;&#28237;&#27969;&#30740;&#31350;&#20043;&#38388;&#32039;&#23494;&#20132;&#32455;&#30340;&#26410;&#26469;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#21453;&#24605;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#26410;&#26469;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#28237;&#27969;&#30740;&#31350;&#65292;&#24182;&#36890;&#36807;&#26681;&#26893;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#26816;&#39564;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20943;&#23569;&#30340;&#25289;&#26684;&#26391;&#26085;&#28237;&#27969;&#27169;&#22411;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#23457;&#26597;&#20102;&#28237;&#27969;&#30740;&#31350;&#20013;&#30340;&#21508;&#31181;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#32479;&#35745;&#27969;&#20307;&#21147;&#23398;&#30340;&#21516;&#26102;&#21457;&#23637;&#20013;&#30340;&#28508;&#22312;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17993v1 Announce Type: cross  Abstract: The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;</title><link>https://arxiv.org/abs/2403.17806</link><description>&lt;p&gt;
&#22362;&#20449;&#24544;&#23454;&#65306;&#22312;&#25214;&#21040;&#27169;&#22411;&#26426;&#21046;&#26102;&#36229;&#36234;&#30005;&#36335;&#37325;&#21472;
&lt;/p&gt;
&lt;p&gt;
Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17806
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;EAP-IG&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#24050;&#37319;&#29992;&#30005;&#36335;&#26694;&#26550;&#65292;&#26088;&#22312;&#25214;&#21040;&#35299;&#37322;LM&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#34892;&#20026;&#30340;&#26368;&#23567;&#35745;&#31639;&#23376;&#22270;&#25110;&#30005;&#36335;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#23545;&#27599;&#20010;&#36793;&#25191;&#34892;&#22240;&#26524;&#24178;&#39044;&#26469;&#30830;&#23450;&#21738;&#20123;&#36793;&#23646;&#20110;LM&#30340;&#30005;&#36335;&#65292;&#20294;&#36825;&#22312;&#27169;&#22411;&#35268;&#27169;&#36739;&#22823;&#26102;&#25928;&#29575;&#20302;&#19979;&#12290;&#36793;&#32536;&#24402;&#22240;&#20462;&#34917;&#65288;EAP&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36817;&#20284;&#24178;&#39044;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#20294;&#19981;&#23436;&#32654;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; - &#24102;&#26377;&#38598;&#25104;&#26799;&#24230;&#30340;EAP&#65288;EAP-IG&#65289;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20445;&#25345;&#30005;&#36335;&#30340;&#26680;&#24515;&#23646;&#24615;&#65306;&#24544;&#23454;&#12290;&#22914;&#26524;&#30005;&#36335;&#26159;&#24544;&#23454;&#30340;&#65292;&#21017;&#21487;&#20197;&#21435;&#25481;&#30005;&#36335;&#20043;&#22806;&#30340;&#25152;&#26377;&#27169;&#22411;&#36793;&#32780;&#19981;&#20250;&#25913;&#21464;&#27169;&#22411;&#22312;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#24544;&#23454;&#24615;&#26159;&#30740;&#31350;&#30005;&#36335;&#32780;&#19981;&#26159;&#23436;&#25972;&#27169;&#22411;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;EAP&#25214;&#21040;&#30340;&#30005;&#36335;&#19981;&#22826;&#24544;&#23454;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17775</link><description>&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#38750;&#31169;&#23494;&#30340;
&lt;/p&gt;
&lt;p&gt;
Secure Aggregation is Not Private Against Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23433;&#20840;&#32858;&#21512;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#38754;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26102;&#24182;&#19981;&#20855;&#22791;&#36275;&#22815;&#30340;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#65288;SecAgg&#65289;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#38544;&#31169;&#22686;&#24378;&#26426;&#21046;&#65292;&#20165;&#20801;&#35768;&#26381;&#21153;&#22120;&#35775;&#38382;&#27169;&#22411;&#26356;&#26032;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26356;&#26032;&#30340;&#26426;&#23494;&#24615;&#12290;&#23613;&#31649;&#26377;&#20851;SecAgg&#20445;&#25252;&#38544;&#31169;&#33021;&#21147;&#30340;&#24191;&#27867;&#22768;&#26126;&#65292;&#20294;&#32570;&#20047;&#23545;&#20854;&#38544;&#31169;&#24615;&#30340;&#27491;&#24335;&#20998;&#26512;&#65292;&#22240;&#27492;&#36825;&#20123;&#20551;&#35774;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;SecAgg&#35270;&#20026;&#27599;&#20010;&#23616;&#37096;&#26356;&#26032;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#26426;&#21046;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;SecAgg&#30340;&#38544;&#31169;&#24433;&#21709;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#25915;&#20987;&#26041;&#24335;&#65292;&#20854;&#20013;&#23545;&#25163;&#26381;&#21153;&#22120;&#35797;&#22270;&#22312;SecAgg&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#19968;&#35757;&#32451;&#36718;&#20013;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#25552;&#20132;&#30340;&#26356;&#26032;&#21521;&#37327;&#26159;&#20004;&#20010;&#21487;&#33021;&#21521;&#37327;&#20013;&#30340;&#21738;&#19968;&#20010;&#12290;&#36890;&#36807;&#36827;&#34892;&#38544;&#31169;&#23457;&#26680;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#25915;&#20987;&#30340;&#25104;&#21151;&#27010;&#29575;&#65292;&#24182;&#37327;&#21270;&#20102;SecAgg&#25552;&#20379;&#30340;LDP&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#32467;&#26524;&#25581;&#31034;&#20102;&#65292;&#19982;&#26222;&#36941;&#22768;&#26126;&#30456;&#21453;&#65292;SecAgg&#24182;&#27809;&#26377;&#25552;&#20379;&#31169;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17775v1 Announce Type: new  Abstract: Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offer
&lt;/p&gt;</description></item><item><title>PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.16497</link><description>&lt;p&gt;
PathoTune: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#33267;&#30149;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
PathoTune: Adapting Visual Foundation Model to Pathological Specialists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16497
&lt;/p&gt;
&lt;p&gt;
PathoTune&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#39640;&#25928;&#22320;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#65292;&#20174;&#32780;&#32531;&#35299;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#22270;&#20687;&#29702;&#35299;&#36208;&#21521;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#26102;&#20195;&#30340;&#21516;&#26102;&#65292;&#30149;&#29702;&#24433;&#20687;&#30340;&#30740;&#31350;&#20063;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#23613;&#31649;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#30149;&#29702;&#22522;&#30784;&#27169;&#22411;&#65292;&#20294;&#22914;&#20309;&#23558;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#20026;&#20102;&#19979;&#28216;&#35843;&#25972;&#65292;&#25105;&#20204;&#25552;&#20986;&#23384;&#22312;&#20004;&#20010;&#22495;&#24046;&#36317;&#65292;&#21363;&#22522;&#30784;-&#20219;&#21153;&#24046;&#36317;&#21644;&#20219;&#21153;-&#23454;&#20363;&#24046;&#36317;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PathoTune&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#22810;&#27169;&#24577;&#25552;&#31034;&#24494;&#35843;&#65292;&#39640;&#25928;&#22320;&#23558;&#30149;&#29702;&#29978;&#33267;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#35843;&#25972;&#21040;&#30149;&#29702;&#29305;&#23450;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#35782;&#21035;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;&#23454;&#20363;&#29305;&#23450;&#30340;&#35270;&#35273;&#25552;&#31034;&#26469;&#32534;&#30721;&#21333;&#20010;&#30149;&#29702;&#22270;&#20687;&#29305;&#24449;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20197;&#34917;&#19969;&#32423;&#21035;&#21644;WSI&#32423;&#21035;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#21333;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16497v1 Announce Type: cross  Abstract: As natural image understanding moves towards the pretrain-finetune era, research in pathology imaging is concurrently evolving. Despite the predominant focus on pretraining pathological foundation models, how to adapt foundation models to downstream tasks is little explored. For downstream adaptation, we propose the existence of two domain gaps, i.e., the Foundation-Task Gap and the Task-Instance Gap. To mitigate these gaps, we introduce PathoTune, a framework designed to efficiently adapt pathological or even visual foundation models to pathology-specific tasks via multi-modal prompt tuning. The proposed framework leverages Task-specific Visual Prompts and Task-specific Textual Prompts to identify task-relevant features, along with Instance-specific Visual Prompts for encoding single pathological image features. Results across multiple datasets at both patch-level and WSI-level demonstrate its superior performance over single-modality
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;</title><link>https://arxiv.org/abs/2403.15498</link><description>&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#19990;&#30028;&#27169;&#22411;&#21644;&#28508;&#21464;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Emergent World Models and Latent Variable Estimation in Chess-Playing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15498
&lt;/p&gt;
&lt;p&gt;
&#26827;&#31867;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#35757;&#32451;&#65292;&#20173;&#33021;&#23398;&#20064;&#20986;&#20869;&#37096;&#34920;&#31034;&#30340;&#26827;&#30424;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#24615;&#33021;&#26469;&#28304;&#30340;&#35752;&#35770;&#12290;&#26159;&#20165;&#20165;&#23398;&#20064;&#21477;&#27861;&#27169;&#24335;&#21644;&#34920;&#38754;&#32479;&#35745;&#32467;&#26524;&#65292;&#36824;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35821;&#20041;&#21644;&#19990;&#30028;&#27169;&#22411;&#65311;&#25105;&#20204;&#22312;&#35937;&#26827;&#36825;&#20010;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36890;&#36807;&#22312;&#30495;&#23454;&#28216;&#25103;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#21644;&#23545;&#27604;&#28608;&#27963;&#26469;&#30740;&#31350;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#23613;&#31649;&#27169;&#22411;&#27809;&#26377;&#20808;&#39564;&#30340;&#28216;&#25103;&#30693;&#35782;&#65292;&#20165;&#20165;&#36890;&#36807;&#19979;&#19968;&#20010;&#23383;&#31526;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20851;&#20110;&#26827;&#30424;&#29366;&#24577;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15498v1 Announce Type: cross  Abstract: Language models have shown unprecedented capabilities, sparking debate over the source of their performance. Is it merely the outcome of learning syntactic patterns and surface level statistics, or do they extract semantics and a world model from the text? Prior work by Li et al. investigated this by training a GPT model on synthetic, randomly generated Othello games and found that the model learned an internal representation of the board state. We extend this work into the more complex domain of chess, training on real games and investigating our model's internal representations using linear probes and contrastive activations. The model is given no a priori knowledge of the game and is solely trained on next character prediction, yet we find evidence of internal representations of board state. We validate these internal representations by using them to make interventions on the model's activations and edit its internal board state. Un
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14797</link><description>&lt;p&gt;
&#36890;&#36807;&#35760;&#24518;&#32593;&#32476;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Preventing Catastrophic Forgetting through Memory Networks in Continuous Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#32593;&#32476;&#21644;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#36825;&#39033;&#24037;&#20316;&#33268;&#21147;&#20110;&#22312;&#36830;&#32493;&#26816;&#27979;&#20013;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#20915;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#30340;&#32972;&#26223;&#36140;&#20302;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#35757;&#32451;&#26550;&#26500;&#22312;&#25345;&#32493;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#26102;&#24456;&#38590;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#22312;&#25345;&#32493;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#26816;&#27979;&#25110;&#20998;&#21106;&#65289;&#35774;&#35745;&#30340;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#33719;&#24471;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26816;&#27979;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#20197;&#20351;&#39044;&#35757;&#32451;&#30340;DETR&#39118;&#26684;&#26816;&#27979;&#22120;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23616;&#37096;&#26597;&#35810;&#20989;&#25968;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#35760;&#24518;&#21333;&#20803;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25345;&#32493;&#26816;&#27979;&#20013;&#19968;&#20010;&#31216;&#20026;&#32972;&#26223;&#36140;&#20302;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#24403;&#26469;&#33258;&#20808;&#21069;&#20219;&#21153;&#30340;&#23545;&#35937;&#31867;&#21035;&#22312;&#26410;&#26469;&#20219;&#21153;&#20013;&#37325;&#26032;&#20986;&#29616;&#26102;&#65292;&#21487;&#33021;&#27809;&#26377;&#26631;&#31614;&#65292;&#23548;&#33268;&#23427;&#20204;&#34987;&#38544;&#24335;&#35270;&#20026;&#32972;&#26223;&#12290;&#36825;&#26159;&#25345;&#32493;&#26816;&#27979;&#25110;&#20998;&#21106;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14797v1 Announce Type: cross  Abstract: Modern pre-trained architectures struggle to retain previous information while undergoing continuous fine-tuning on new tasks. Despite notable progress in continual classification, systems designed for complex vision tasks such as detection or segmentation still struggle to attain satisfactory performance. In this work, we introduce a memory-based detection transformer architecture to adapt a pre-trained DETR-style detector to new tasks while preserving knowledge from previous tasks. We propose a novel localized query function for efficient information retrieval from memory units, aiming to minimize forgetting. Furthermore, we identify a fundamental challenge in continual detection referred to as background relegation. This arises when object categories from earlier tasks reappear in future tasks, potentially without labels, leading them to be implicitly treated as background. This is an inevitable issue in continual detection or segme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13872</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#25112;&#26415;&#32593;&#32476;&#26410;&#26469;&#29366;&#24577;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph Representation Learning for Tactical Networks Future State Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#26469;&#29366;&#24577;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
tbd:&#25112;&#26415;&#33258;&#32452;&#32455;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#21160;&#24577;&#21644;&#22810;&#36339;&#29305;&#24615;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#32593;&#32476;&#36830;&#25509;&#23545;&#20110;&#26377;&#25928;&#30340;&#36164;&#28304;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;STGED&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25112;&#26415;&#36890;&#20449;&#32593;&#32476;&#65292;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#29366;&#24577;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#26469;&#23398;&#20064;&#28508;&#22312;&#30340;&#25112;&#26415;&#34892;&#20026;&#12290;STGED&#23618;&#27425;&#22320;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27880;&#24847;&#26426;&#21046;&#23545;&#19968;&#31995;&#21015;&#36890;&#20449;&#32593;&#32476;&#29366;&#24577;&#36827;&#34892;&#31354;&#38388;&#32534;&#30721;&#65292;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#29366;&#24577;&#30340;&#28436;&#21464;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#21069;&#39304;&#32593;&#32476;&#26469;&#35299;&#30721;&#26410;&#26469;&#29366;&#24577;&#19979;&#30340;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;STGED&#22312;&#19981;&#21516;&#26102;&#38388;&#27493;&#36755;&#20837;&#19979;&#19968;&#30452;&#27604;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#33719;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13872v1 Announce Type: new  Abstract: Resource allocation in tactical ad-hoc networks presents unique challenges due to their dynamic and multi-hop nature. Accurate prediction of future network connectivity is essential for effective resource allocation in such environments. In this paper, we introduce the Spatial-Temporal Graph Encoder-Decoder (STGED) framework for Tactical Communication Networks that leverages both spatial and temporal features of network states to learn latent tactical behaviors effectively. STGED hierarchically utilizes graph-based attention mechanism to spatially encode a series of communication network states, leverages a recurrent neural network to temporally encode the evolution of states, and a fully-connected feed-forward network to decode the connectivity in the future state. Through extensive experiments, we demonstrate that STGED consistently outperforms baseline models by large margins across different time-steps input, achieving an accuracy of
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12326</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#25552;&#31034;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#32463;&#31579;&#36873;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23398;&#20064;&#21644;&#38543;&#21518;&#20256;&#25773;&#19981;&#33391;&#27010;&#24565;&#65288;&#22914;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65289;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#32467;&#21512;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#12290;&#36825;&#21487;&#23398;&#20064;&#25552;&#31034;&#20805;&#24403;&#38468;&#21152;&#20869;&#23384;&#65292;&#23558;&#19981;&#33391;&#27010;&#24565;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20013;&#65292;&#24182;&#20943;&#23569;&#36825;&#20123;&#27010;&#24565;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#30456;&#24212;&#25991;&#26412;&#36755;&#20837;&#30340;&#20381;&#36182;&#12290;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#25552;&#31034;&#20013;&#65292;&#28040;&#38500;&#36825;&#20123;&#19981;&#33391;&#27010;&#24565;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#20182;&#27010;&#24565;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11299</link><description>&lt;p&gt;
SQ-LLaVA&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#27169;&#22411;&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#22312;&#32463;&#36807;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26174;&#30528;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#40511;&#27807;&#25104;&#20026;&#25972;&#20010;&#32593;&#32476;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#25913;&#21892;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#32771;&#34385;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35270;&#35273;&#20219;&#21153;&#33539;&#22260;&#30340;&#26356;&#22810;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#29992;&#20110;&#38382;&#31572;&#65292;&#20294;&#36825;&#31181;&#25805;&#20316;&#25104;&#26412;&#36739;&#39640;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#21253;&#21547;&#22823;&#37327;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20294;&#36825;&#19968;&#26041;&#38754;&#19968;&#30452;&#40092;&#26377;&#20154;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#35270;&#35273;&#25351;&#23548;&#25968;&#25454;&#20869;&#37096;&#34987;&#24573;&#35270;&#30340;&#19978;&#19979;&#25991;&#65292;&#35757;&#32451;&#27169;&#22411;&#33258;&#25105;&#35757;&#32451;'&#23398;&#20064;'&#22914;&#20309;&#25552;&#20986;&#39640;&#36136;&#37327;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SQ-LLaVA&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;&#33258;&#38382;&#33258;&#31572;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#21161;&#25163;&#12290;SQ-LLaVA&#22312;&#29983;&#25104;&#28789;&#27963;&#19988;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11299v1 Announce Type: cross  Abstract: Recent advancements in the vision-language model have shown notable generalization in vision-language tasks after visual instruction tuning. However, bridging the gap between the pre-trained vision encoder and the large language models becomes the whole network's bottleneck. To improve cross-modality alignment, existing works usually consider more visual instruction data covering a broader range of vision tasks to fine-tune the model for question-answering, which are costly to obtain. However, the image contains rich contextual information that has been largely under-explored. This paper first attempts to harness this overlooked context within visual instruction data, training the model to self-supervised `learning' how to ask high-quality questions. In this way, we introduce a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant. SQ-LLaVA exhibits proficiency in generating flexible and meaningful image-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10707</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#38598;&#25104;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#30340;&#28508;&#22312;&#20027;&#39064;&#65306;&#27668;&#20505;&#36816;&#21160;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Themes of Messaging on Social Media by Integrating LLMs: A Case Study on Climate Campaigns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21151;&#33021;&#65292;&#20197;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#65292;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25581;&#31034;&#21644;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#20027;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#37492;&#20110;&#20256;&#32479;&#20027;&#39064;&#32423;&#20998;&#26512;&#30340;&#23616;&#38480;&#24615;&#65292;&#24448;&#24448;&#21482;&#25429;&#25417;&#21040;&#25972;&#20307;&#27169;&#24335;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#26356;&#31934;&#32454;&#12289;&#20027;&#39064;&#32858;&#28966;&#30340;&#25506;&#32034;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#28041;&#21450;&#25163;&#21160;&#27969;&#31243;&#21644;&#20154;&#22312;&#24490;&#29615;&#20013;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#22312;&#20280;&#32553;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36164;&#28304;&#24378;&#24230;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20808;&#36827;&#21151;&#33021;&#30340;&#26426;&#22120;&#22312;&#24490;&#29615;&#20013;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#26356;&#28145;&#20837;&#22320;&#35843;&#26597;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#30340;&#20027;&#39064;&#26041;&#38754;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#22810;&#26679;&#30340;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#26356;&#24191;&#27867;&#20027;&#39064;&#20869;&#26377;&#30340;&#24494;&#22937;&#32454;&#33410;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10707v1 Announce Type: cross  Abstract: This paper introduces a novel approach to uncovering and analyzing themes in social media messaging. Recognizing the limitations of traditional topic-level analysis, which tends to capture only the overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Conventional methods of theme discovery, involving manual processes and a human-in-the-loop approach, are valuable but face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach allows for a deeper investigation into the thematic aspects of social media discourse, enabling us to uncover a diverse array of themes, each with unique characteristics and relevance, thereby offering a comprehensive understanding of the nuances present within broader topics.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10348</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Difficulty-based Curriculum for Training Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10348
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20840;&#38754;&#30740;&#31350;&#20219;&#21153;&#38590;&#24230;&#65292;&#21457;&#29616;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21435;&#22122;&#20219;&#21153;&#38590;&#24230;&#30340;&#28176;&#36827;&#24335;&#35838;&#31243;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#23545;&#21508;&#20010;&#26102;&#38388;&#27493;&#38271;&#21644;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#21435;&#22122;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21435;&#22122;&#20219;&#21153;&#30340;&#30456;&#23545;&#38590;&#24230;&#20173;&#23384;&#22312;&#20105;&#35758;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20219;&#21153;&#38590;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#25910;&#25947;&#34892;&#20026;&#21644;&#26102;&#38388;&#27493;&#38271;&#38388;&#36830;&#32493;&#27010;&#29575;&#20998;&#24067;&#30340;&#30456;&#23545;&#29109;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#26174;&#31034;&#65292;&#36739;&#26089;&#26102;&#38388;&#27493;&#38271;&#30340;&#21435;&#22122;&#23384;&#22312;&#25910;&#25947;&#32531;&#24930;&#21644;&#36739;&#39640;&#30340;&#30456;&#23545;&#29109;&#65292;&#34920;&#26126;&#22312;&#36825;&#20123;&#36739;&#20302;&#26102;&#38388;&#27493;&#38271;&#19978;&#20219;&#21153;&#38590;&#24230;&#22686;&#21152;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30001;&#26131;&#21040;&#38590;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#20511;&#37492;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10348v1 Announce Type: cross  Abstract: Diffusion-based generative models have emerged as powerful tools in the realm of generative modeling. Despite extensive research on denoising across various timesteps and noise levels, a conflict persists regarding the relative difficulties of the denoising tasks. While various studies argue that lower timesteps present more challenging tasks, others contend that higher timesteps are more difficult. To address this conflict, our study undertakes a comprehensive examination of task difficulties, focusing on convergence behavior and changes in relative entropy between consecutive probability distributions across timesteps. Our observational study reveals that denoising at earlier timesteps poses challenges characterized by slower convergence and higher relative entropy, indicating increased task difficulty at these lower timesteps. Building on these observations, we introduce an easy-to-hard learning scheme, drawing from curriculum learn
&lt;/p&gt;</description></item><item><title>eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.10153</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#19987;&#23478;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Multi-modal Contrastive Learning with Expert Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10153
&lt;/p&gt;
&lt;p&gt;
eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;CLIP&#27169;&#22411;&#8212;&#8212;eCLIP&#65292;&#23427;&#38598;&#25104;&#20102;&#25918;&#23556;&#31185;&#21307;&#29983;&#30524;&#29699;&#27880;&#35270;&#28909;&#22270;&#24418;&#24335;&#30340;&#19987;&#23478;&#27880;&#37322;&#12290;&#23427;&#35299;&#20915;&#20102;&#23545;&#27604;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#31232;&#32570;&#21644;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#8212;&#8212;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#38477;&#20302;&#20102;&#34920;&#31034;&#30340;&#36136;&#37327;&#24182;&#38459;&#30861;&#20102;&#36328;&#27169;&#24577;&#20114;&#25805;&#20316;&#24615;&#12290;eCLIP&#38598;&#25104;&#20102;&#19968;&#20010;&#28909;&#22270;&#22788;&#29702;&#22120;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#22686;&#24378;&#26469;&#26377;&#25928;&#21033;&#29992;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;eCLIP&#35774;&#35745;&#20026;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;CLIP&#21464;&#20307;&#65292;&#26080;&#38656;&#20462;&#25913;&#26680;&#24515;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#35814;&#32454;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#25512;&#26029;&#12289;&#32447;&#24615;&#25506;&#38024;&#12289;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#21450;&#20351;&#29992;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#65292;eCLIP&#23637;&#31034;&#20102;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.04884</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Optimizing Retinal Prosthetic Stimuli with Conditional Invertible Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04884
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#26080;&#30417;&#30563;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#65292;&#25552;&#39640;&#20102;&#30005;&#26497;&#38453;&#21015;&#30340;&#21050;&#28608;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#26893;&#20837;&#30340;&#35270;&#32593;&#33180;&#20551;&#20307;&#20026;&#36890;&#36807;&#32469;&#36807;&#35270;&#32593;&#33180;&#20013;&#25439;&#22351;&#30340;&#20809;&#24863;&#21463;&#32454;&#32990;&#24182;&#30452;&#25509;&#21050;&#28608;&#21097;&#20313;&#21151;&#33021;&#24615;&#35270;&#32593;&#33180;&#32454;&#32990;&#26469;&#24674;&#22797;&#37096;&#20998;&#35270;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#22836;&#21644;&#35270;&#32593;&#33180;&#32454;&#32990;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#36755;&#36890;&#24120;&#21463;&#38480;&#20110;&#30005;&#26497;&#38453;&#21015;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#23545;&#19981;&#21516;&#33410;&#32454;&#32990;&#31867;&#22411;&#30340;&#29305;&#24322;&#24615;&#19981;&#36275;&#65292;&#23548;&#33268;&#21050;&#28608;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#26377;&#26465;&#20214;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#20248;&#21270;&#35270;&#32593;&#33180;&#20551;&#20307;&#21050;&#28608;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#21487;&#36870;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#23427;&#20204;&#29992;&#20316;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#27169;&#22411;&#30340;&#26367;&#20195;&#65292;&#24182;&#23558;&#36755;&#20837;&#25668;&#20687;&#22836;&#20449;&#21495;&#32534;&#30721;&#20026;&#30005;&#26497;&#38453;&#21015;&#19978;&#20248;&#21270;&#30340;&#30005;&#21050;&#28608;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#22914;&#31616;&#21333;&#30340;&#38477;&#37319;&#26679;&#12289;&#32447;&#24615;&#27169;&#22411;&#21644;&#21069;&#39304;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04884v1 Announce Type: cross  Abstract: Implantable retinal prostheses offer a promising solution to restore partial vision by circumventing damaged photoreceptor cells in the retina and directly stimulating the remaining functional retinal cells. However, the information transmission between the camera and retinal cells is often limited by the low resolution of the electrode array and the lack of specificity for different ganglion cell types, resulting in suboptimal stimulations. In this work, we propose to utilize normalizing flow-based conditional invertible neural networks to optimize retinal implant stimulation in an unsupervised manner. The invertibility of these networks allows us to use them as a surrogate for the computational model of the visual system, while also encoding input camera signals into optimized electrical stimuli on the electrode array. Compared to other methods, such as trivial downsampling, linear models, and feed-forward convolutional neural networ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#27861;&#22686;&#24378;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving LLM Code Generation with Grammar Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01632
&lt;/p&gt;
&lt;p&gt;
SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SynCode&#65292;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#21644;&#36890;&#29992;&#22320;&#35299;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#30721;&#30340;&#26032;&#26694;&#26550;&#12290;SynCode&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#21033;&#29992;&#31163;&#32447;&#26500;&#24314;&#30340;&#22522;&#20110;&#35821;&#35328;&#35821;&#27861;&#32456;&#32467;&#31526;&#30340;&#39640;&#25928;&#26597;&#25214;&#34920;DFA mask store&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SynCode&#22312;&#32473;&#23450;&#32534;&#31243;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#23637;&#31034;&#20854;&#22312;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#25928;&#20196;&#29260;&#30340;&#21516;&#26102;&#25298;&#32477;&#26080;&#25928;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19982;&#30001;CFG&#23450;&#20041;&#30340;&#20219;&#20309;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#39564;&#35777;&#20102;&#38024;&#23545;Python&#21644;Go&#30340;CFG&#23454;&#39564;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24403;SynCode&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#32467;&#21512;&#26102;&#65292;&#35821;&#27861;&#38169;&#35823;&#20943;&#23569;96.07%&#65292;&#24432;&#26174;&#20102;&#20854;&#23545;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15429</link><description>&lt;p&gt;
ProTIP&#65306;&#38024;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#25239;&#38543;&#26426;&#25200;&#21160;&#30340;&#27010;&#29575;&#40065;&#26834;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion Models against Stochastic Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#27010;&#29575;&#27010;&#24565;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#29992;&#20110;&#35780;&#20272;&#20854;&#32479;&#35745;&#20445;&#35777;&#65292;&#35299;&#20915;&#20102;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#21028;&#26029;&#22256;&#38590;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#23637;&#29616;&#20102;&#22312;&#31616;&#21333;&#25991;&#26412;&#25551;&#36848;&#22522;&#30784;&#19978;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19982;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#19968;&#26679;&#65292;DMs&#23384;&#22312;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;T2I DMs&#30340;&#40065;&#26834;&#24615;&#26102;&#65292;&#23384;&#22312;&#20197;&#20108;&#20803;&#25110;&#26368;&#22351;&#24773;&#20917;&#38382;&#39064;&#35299;&#26041;&#38754;&#30340;&#23581;&#35797;&#65292;&#20294;&#26080;&#27861;&#22238;&#31572;&#27169;&#22411;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AE&#65289;&#26102;&#30340;&#24635;&#20307;&#40065;&#26834;&#24615;&#22914;&#20309;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;T2I DMs&#40065;&#26834;&#24615;&#30340;&#27010;&#29575;&#27010;&#24565;&#65307;&#28982;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;ProTIP&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#30340;&#35780;&#20272;&#12290;&#20027;&#35201;&#25361;&#25112;&#28304;&#33258;&#65306;i&#65289;&#29983;&#25104;&#36807;&#31243;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65307;&#21644;ii&#65289;&#30830;&#23450;&#25200;&#21160;&#36755;&#20837;&#26159;&#21542;&#20026;AE&#28041;&#21450;&#27604;&#36739;&#20004;&#20010;&#36755;&#20986;&#20998;&#24067;&#65292;&#36825;&#19982;&#20854;&#20182;DL&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#20854;&#20013;AE&#26159;&#22312;&#26631;&#31614;&#38169;&#35823;&#39044;&#27979;&#26102;&#34987;&#35782;&#21035;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15429v1 Announce Type: cross  Abstract: Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#27492;&#23454;&#29616;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14213</link><description>&lt;p&gt;
&#20010;&#20307;&#38388;&#20849;&#20139;&#33041;&#30005;&#22270;&#26102;&#31354;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#31070;&#32463;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Shared Spatiotemporal EEG Representations Across Individuals for Naturalistic Neuroscience
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14213
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#27492;&#23454;&#29616;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#21050;&#28608;&#35825;&#23548;&#30340;&#31070;&#32463;&#34920;&#24449;&#25581;&#31034;&#20102;&#20154;&#31867;&#22914;&#20309;&#23545;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#22806;&#22260;&#21050;&#28608;&#20570;&#20986;&#21453;&#24212;&#12290;&#29702;&#35299;&#33258;&#28982;&#21050;&#28608;&#22788;&#29702;&#30340;&#19968;&#33324;&#31070;&#32463;&#26426;&#21046;&#30340;&#20851;&#38190;&#22312;&#20110;&#23545;&#40784;&#21508;&#20010;&#20010;&#20307;&#30340;&#31070;&#32463;&#27963;&#21160;&#24182;&#25552;&#21462;&#20010;&#20307;&#38388;&#30340;&#20849;&#20139;&#31070;&#32463;&#34920;&#24449;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#20197;&#20854;&#20016;&#23500;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#32780;&#38395;&#21517;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20010;&#20307;&#38388;&#20849;&#20139;&#26102;&#31354;&#33041;&#30005;&#22270;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#36890;&#29992;&#26694;&#26550;&#65288;CL-SSTER&#65289;&#12290;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;CL-SSTER&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26368;&#22823;&#21270;&#30456;&#21516;&#21050;&#28608;&#19979;&#21508;&#20010;&#20010;&#20307;&#30340;EEG&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#65292;&#19982;&#19981;&#21516;&#21050;&#28608;&#30340;&#30456;&#23545;&#24212;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#21367;&#31215;&#21516;&#26102;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14213v1 Announce Type: cross  Abstract: Neural representations induced by naturalistic stimuli offer insights into how humans respond to peripheral stimuli in daily life. The key to understanding the general neural mechanisms underlying naturalistic stimuli processing involves aligning neural activities across individuals and extracting inter-subject shared neural representations. Targeting the Electroencephalogram (EEG) technique, known for its rich spatial and temporal information, this study presents a general framework for Contrastive Learning of Shared SpatioTemporal EEG Representations across individuals (CL-SSTER). Harnessing the representational capabilities of contrastive learning, CL-SSTER utilizes a neural network to maximize the similarity of EEG representations across individuals for identical stimuli, contrasting with those for varied stimuli. The network employed spatial and temporal convolutions to simultaneously learn the spatial and temporal patterns inhere
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14047</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Effective Transfer Learning for Neuro-Symbolic Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21644;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#23545;NeSy&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#24863;&#30693;&#26144;&#23556;&#21040;&#31526;&#21495;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#25512;&#29702;&#32773;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14047v1 Announce Type: cross  Abstract: Deep Learning (DL) techniques have achieved remarkable successes in recent years. However, their ability to generalize and execute reasoning tasks remains a challenge. A potential solution to this issue is Neuro-Symbolic Integration (NeSy), where neural approaches are combined with symbolic reasoning. Most of these methods exploit a neural network to map perceptions to symbols and a logical reasoner to predict the output of the downstream task. These methods exhibit superior generalization capacity compared to fully neural architectures. However, they suffer from several issues, including slow convergence, learning difficulties with complex perception tasks, and convergence to local minima. This paper proposes a simple yet effective method to ameliorate these problems. The key idea involves pretraining a neural model on the downstream task. Then, a NeSy model is trained on the same task via transfer learning, where the weights of the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.13654</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#27604;&#20363;&#31215;&#20998;&#25511;&#21046;&#22120;&#22312;&#33410;&#27969;&#38400;&#22522;&#20934;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#65292;&#35813;&#33410;&#27969;&#38400;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#30913;&#28382;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#30340;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#24320;&#22987;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#24341;&#23548;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20174;&#19982;&#38400;&#38376;&#30340;&#39069;&#22806;&#20132;&#20114;&#20013;&#23398;&#20064;&#26469;&#25913;&#36827;&#38381;&#29615;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38400;&#38376;&#19978;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#31361;&#26174;&#20102;&#23558;PI&#21644;RL&#26694;&#26550;&#32467;&#21512;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#25511;&#21046;&#24615;&#33021;&#30340;&#22909;&#22788;&#12290;&#22312;&#25152;&#26377;&#23454;&#39564;&#27979;&#35797;&#26696;&#20363;&#20013;&#65292;&#32467;&#26524;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#37117;&#20248;&#20110;&#20256;&#32479;RL&#20195;&#29702;&#65292;&#24182;&#19988;&#20248;&#20110;PI&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13654v1 Announce Type: cross  Abstract: This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;</title><link>https://arxiv.org/abs/2402.12231</link><description>&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#25913;&#21892;&#27010;&#29575;&#31215;&#20998;&#22120;&#23545;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12231
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25551;&#36848;&#31185;&#23398;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20294;&#30830;&#23450;&#35299;&#37322;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#30340;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#22238;&#28779;&#36825;&#19968;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#38024;&#23545;ODEs&#20013;&#30340;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#26799;&#24230;&#20248;&#21270;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#20943;&#23569;&#27010;&#29575;&#31215;&#20998;&#22120;&#30340;&#19968;&#20010;&#22122;&#22768;&#21442;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#20110;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#21442;&#25968;&#25968;&#37327;&#30340;Hodgkin-Huxley&#27169;&#22411;&#33719;&#24471;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11816</link><description>&lt;p&gt;
&#36991;&#20813;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#65306;&#23398;&#20064;&#20197;&#21069;&#26410;&#26366;&#23398;&#21040;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11816
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;&#22914;SimCLR&#12289;CLIP&#20013;&#65289;&#20013;&#21457;&#29616;&#20102;&#29305;&#24449;&#25233;&#21046;&#65306;&#22312;&#21333;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#38454;&#27573;&#65292;&#23545;&#27604;&#27169;&#22411;&#20165;&#25429;&#33719;&#23545;&#27604;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#37096;&#20998;&#20849;&#20139;&#20449;&#24687;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20855;&#26377;&#29305;&#24449;&#25233;&#21046;&#65292;&#23545;&#27604;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#23398;&#20064;&#36275;&#22815;&#36866;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20943;&#36731;&#29305;&#24449;&#25233;&#21046;&#38382;&#39064;&#24182;&#30830;&#20445;&#23545;&#27604;&#27169;&#22411;&#23398;&#20064;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#23545;&#27604;&#23398;&#20064;&#65288;MCL&#65289;&#26694;&#26550;&#12290;&#19982;&#36890;&#24120;&#20250;&#23548;&#33268;&#29305;&#24449;&#25233;&#21046;&#30340;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#19981;&#21516;&#65292;MCL&#36880;&#28176;&#23398;&#20064;&#20197;&#21069;&#26410;&#25506;&#32034;&#36807;&#30340;&#26032;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#25345;&#24050;&#32463;&#23398;&#21040;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
&lt;/p&gt;</description></item><item><title>Alt-GDA&#31639;&#27861;&#34987;&#35777;&#26126;&#26356;&#24555;&#65292;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10475</link><description>&lt;p&gt;
&#26497;&#23567;&#21270;&#20248;&#21270;&#20013;&#20132;&#26367;&#26356;&#26032;&#30340;&#22522;&#26412;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Fundamental Benefit of Alternating Updates in Minimax Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10475
&lt;/p&gt;
&lt;p&gt;
Alt-GDA&#31639;&#27861;&#34987;&#35777;&#26126;&#26356;&#24555;&#65292;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gradient Descent-Ascent&#65288;GDA&#65289;&#31639;&#27861;&#26088;&#22312;&#35299;&#20915;&#26497;&#23567;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#19979;&#38477;&#21644;&#19978;&#21319;&#27493;&#39588;&#65292;&#20998;&#20026;&#21516;&#26102;&#36827;&#34892;&#65288;Sim-GDA&#65289;&#25110;&#20132;&#26367;&#36827;&#34892;&#65288;Alt-GDA&#65289;&#12290; Alt-GDA&#36890;&#24120;&#25910;&#25947;&#26356;&#24555;&#65292;&#20294;&#20004;&#32773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#23578;&#26410;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#38024;&#23545;&#24378;&#20984;&#24378;&#20985;&#21644;Lipschitz&#26799;&#24230;&#30446;&#26631;&#25552;&#20986;&#20102;&#23545;&#20004;&#31181;&#31639;&#27861;&#30340;&#32454;&#31890;&#24230;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;Alt-GDA&#30340;&#26032;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#30028;&#20005;&#26684;&#23567;&#20110;Sim-GDA&#30340;&#19979;&#30028;&#65307;&#21363;Alt-GDA&#34987;&#35777;&#26126;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;Sim-GDA&#21644;Alt-GDA&#30340;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Alex-GDA&#28385;&#36275;&#26356;&#23567;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10342</link><description>&lt;p&gt;
&#22312;RLHF&#20013;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;&#20851;&#20110;&#26377;&#25928;&#25968;&#25454;&#21033;&#29992;&#30340;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#20381;&#36182;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#32463;&#39564;&#25104;&#21151;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#20173;&#20391;&#37325;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65288;PO-RLHF&#65289;&#30340;RLHF&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27969;&#34892;&#30340;&#31574;&#30053;&#35206;&#30422;-&#31574;&#30053;&#26799;&#24230;&#65288;PC-PG&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20551;&#35774;&#23545;&#22870;&#21169;&#20989;&#25968;&#26377;&#30693;&#35782;&#12290;&#22312;PO-RLHF&#20013;&#65292;&#19981;&#20551;&#35774;&#30693;&#36947;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#27604;&#36739;&#21453;&#39304;&#26469;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;PO-RLHF&#25552;&#20379;&#20102;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#20026;&#35299;&#37322;&#20026;&#20160;&#20040;&#23569;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#33021;&#36275;&#20197;&#22312;RLHF&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#26159;&#25105;&#20204;&#30340;&#36712;&#36857;&#32423;el
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.09821</link><description>&lt;p&gt;
&#38899;&#39057;&#24674;&#22797;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Audio Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#39057;&#25773;&#25918;&#35774;&#22791;&#21644;&#24555;&#36895;&#25968;&#25454;&#20256;&#36755;&#30340;&#21457;&#23637;&#65292;&#23545;&#39640;&#38899;&#36136;&#30340;&#38656;&#27714;&#22312;&#23089;&#20048;&#21644;&#36890;&#20449;&#39046;&#22495;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24405;&#21046;&#36807;&#31243;&#20013;&#30340;&#22833;&#30495;&#21644;&#24178;&#25200;&#65292;&#25110;&#32773;&#30001;&#20110;&#19981;&#23436;&#21892;&#30340;&#20256;&#36755;&#31649;&#36947;&#65292;&#38899;&#39057;&#36136;&#37327;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38899;&#39057;&#24674;&#22797;&#26041;&#27861;&#26088;&#22312;&#20174;&#25439;&#22351;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#24674;&#22797;&#20986;&#28165;&#26224;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#25163;&#24037;&#35268;&#21017;&#21644;&#32479;&#35745;&#21551;&#21457;&#27861;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#25105;&#20204;&#23545;&#38899;&#39057;&#20449;&#21495;&#30340;&#35748;&#35782;&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#36716;&#21521;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24314;&#27169;&#33021;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09821v1 Announce Type: cross  Abstract: With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08384</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#23398;&#20064;&#65306;&#23454;&#29616;&#21160;&#24577;&#27491;&#21017;&#21270;&#30340;&#40065;&#26834;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Selective Learning: Towards Robust Calibration with Dynamic Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#21644;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35823;&#26657;&#20934;&#25351;&#30340;&#26159;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#26159;&#30001;&#36807;&#25311;&#21512;&#38382;&#39064;&#24341;&#36215;&#30340;&#65292;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#29305;&#28857;&#26159;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#21576;&#29616;&#20986;&#30340;&#25152;&#26377;&#20869;&#23481;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36827;&#34892;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#28155;&#21152;&#26368;&#22823;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#24182;&#32531;&#35299;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;&#36825;&#20010;&#30446;&#26631;&#21487;&#20197;&#29702;&#35299;&#20026;&#23547;&#25214;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#22686;&#21152;&#21487;&#20449;&#24230;&#26469;&#36866;&#24212;&#23454;&#38469;&#26631;&#31614;&#65292;&#21516;&#26102;&#36890;&#36807;&#38477;&#20302;&#21487;&#20449;&#24230;&#26469;&#26368;&#22823;&#21270;&#39044;&#27979;&#27010;&#29575;&#30340;&#29109;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26126;&#30830;&#25351;&#23548;&#65292;&#23548;&#33268;&#30446;&#26631;&#20914;&#31361;&#65288;&#22686;&#21152;&#20294;&#20063;&#38477;&#20302;&#21487;&#20449;&#24230;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#27491;&#21017;&#21270;&#65288;DReg&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#23398;&#20064;&#24212;&#35813;&#23398;&#21040;&#20160;&#20040;&#65292;&#20174;&#32780;&#36991;&#20813;&#21487;&#20449;&#24230;&#35843;&#25972;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration in deep learning refers to there is a discrepancy between the predicted confidence and performance. This problem usually arises due to the overfitting problem, which is characterized by learning everything presented in the training set, resulting in overconfident predictions during testing. Existing methods typically address overfitting and mitigate the miscalibration by adding a maximum-entropy regularizer to the objective function. The objective can be understood as seeking a model that fits the ground-truth labels by increasing the confidence while also maximizing the entropy of predicted probabilities by decreasing the confidence. However, previous methods lack clear guidance on confidence adjustment, leading to conflicting objectives (increasing but also decreasing confidence). Therefore, we introduce a method called Dynamic Regularization (DReg), which aims to learn what should be learned during training thereby circumventing the confidence adjusting trade-off. At 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03495</link><description>&lt;p&gt;
&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Partially Stochastic Infinitely Deep Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#37096;&#20998;&#38543;&#26426;&#24615;&#25972;&#21512;&#21040;&#26080;&#38480;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#26550;&#26500;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#38543;&#26426;&#24615;&#22312;&#26080;&#38480;&#28145;&#24230;&#26497;&#38480;&#19979;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20840;&#38543;&#26426;&#24615;&#30340;&#22909;&#22788;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#37197;&#32622;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26435;&#37325;&#21010;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30830;&#31435;&#25105;&#20204;&#30340;&#32593;&#32476;&#23478;&#26063;&#31526;&#21512;&#36890;&#29992;&#26465;&#20214;&#20998;&#24067;&#36817;&#20284;&#22120;&#30340;&#25968;&#23398;&#20445;&#35777;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03317</link><description>&lt;p&gt;
SpecFormer&#65306;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#20445;&#25252;&#35270;&#35273;Transformer&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#38754;&#23545;&#24694;&#24847;&#25915;&#20987;&#26102;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#35843;&#25972;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SpecFormer&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;ViTs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#65292;&#24182;&#24471;&#21040;&#20102;&#20180;&#32454;&#25512;&#23548;&#30340;&#29702;&#35770;&#20445;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#20026;&#33258;&#27880;&#24847;&#23618;&#24314;&#31435;&#20102;&#26412;&#22320;Lipschitz&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#65288;MSVP&#65289;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#36825;&#20123;&#36793;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#24130;&#36845;&#20195;&#26041;&#27861;&#23558;MSVP&#26080;&#32541;&#38598;&#25104;&#21040;ViTs&#30340;&#27880;&#24847;&#21147;&#23618;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#20462;&#25913;&#21518;&#30340;&#27169;&#22411;SpecFormer&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have gained prominence as a preferred choice for a wide range of computer vision tasks due to their exceptional performance. However, their widespread adoption has raised concerns about security in the face of malicious attacks. Most existing methods rely on empirical adjustments during the training process, lacking a clear theoretical foundation. In this study, we address this gap by introducing SpecFormer, specifically designed to enhance ViTs' resilience against adversarial attacks, with support from carefully derived theoretical guarantees. We establish local Lipschitz bounds for the self-attention layer and introduce a novel approach, Maximum Singular Value Penalization (MSVP), to attain precise control over these bounds. We seamlessly integrate MSVP into ViTs' attention layers, using the power iteration method for enhanced computational efficiency. The modified model, SpecFormer, effectively reduces the spectral norms of attention weight matrices, there
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861; DEFT&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#26469;&#20419;&#36827;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#27969;&#30340;PEFT&#25216;&#26415;&#65292;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01911</link><description>&lt;p&gt;
&#20174;PEFT&#21040;DEFT&#65306;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
From PEFT to DEFT: Parameter Efficient Finetuning for Reducing Activation Density in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#28608;&#27963;&#23494;&#24230;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861; DEFT&#12290;&#30740;&#31350;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23384;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#26469;&#20419;&#36827;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#36890;&#36807;&#24212;&#29992;&#20027;&#27969;&#30340;PEFT&#25216;&#26415;&#65292;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#25104;&#20026;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#30340;&#20107;&#23454;&#19978;&#30340;&#36215;&#28857;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#36866;&#24212;PLMs&#30340;&#25163;&#27573;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#20013;&#22810;&#23618;&#24863;&#30693;&#65288;MLP&#65289;&#27169;&#22359;&#30340;&#20013;&#38388;&#36755;&#20986;&#20013;&#23384;&#22312;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#12290;&#20302;&#28608;&#27963;&#23494;&#24230;&#33021;&#22815;&#22312;&#25903;&#25345;&#31232;&#30095;&#24863;&#30693;&#30828;&#20214;&#19978;&#23454;&#29616;&#39640;&#25928;&#27169;&#22411;&#25512;&#26029;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#22312;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23494;&#24230;&#25439;&#22833;&#65292;&#40723;&#21169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65288;&#31561;&#20215;&#20110;&#26356;&#20302;&#30340;&#28608;&#27963;&#23494;&#24230;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#21253;&#25324;QLoRA&#12289;LoRA&#12289;Adapter&#12289;Prompt/Prefix Tuning&#22312;&#20869;&#30340;&#20027;&#27969;PEFT&#25216;&#26415;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20419;&#36827;&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models (PLMs) have become the de facto starting point for fine-tuning on downstream tasks. However, as model sizes continue to increase, traditional fine-tuning of all parameters becomes challenging. To address this, parameter-efficient fine-tuning (PEFT) methods have gained popularity as a means to adapt PLMs effectively. In parallel, recent studies have revealed the presence of activation sparsity within the intermediate outputs of the multilayer perception (MLP) blocks in transformers. Low activation density enables efficient model inference on sparsity-aware hardware. Building upon this insight, in this work, we propose a novel density loss that encourages higher activation sparsity (equivalently, lower activation density) in the pre-trained models. We demonstrate the effectiveness of our approach by utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter, Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse downstream tasks. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2401.17695</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#20809;&#35889;&#32858;&#31867;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Datacube segmentation via Deep Spectral Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#28145;&#24230;&#32858;&#31867;&#31639;&#27861;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#32479;&#35745;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#23637;&#35270;&#35273;&#25216;&#26415;&#22312;&#29289;&#29702;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#31867;&#20998;&#26512;&#20135;&#29983;&#30340;&#25968;&#25454;&#31435;&#26041;&#20307;&#22312;&#35299;&#37322;&#19978;&#24448;&#24448;&#20855;&#26377;&#25361;&#25112;&#65292;&#22240;&#20026;&#24456;&#38590;&#20174;&#32452;&#25104;&#25968;&#25454;&#31435;&#26041;&#20307;&#30340;&#20809;&#35889;&#20013;&#36776;&#21035;&#20986;&#30456;&#20851;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#30340;&#24040;&#22823;&#32500;&#24230;&#23545;&#20110;&#32479;&#35745;&#35299;&#37322;&#26469;&#35828;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65307;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#21253;&#21547;&#20102;&#22823;&#37327;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21033;&#29992;&#65292;&#20197;&#25551;&#32472;&#20986;&#25152;&#30740;&#31350;&#26696;&#20363;&#30340;&#19968;&#20123;&#22522;&#26412;&#29305;&#24615;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#36866;&#24403;&#23450;&#20041;&#30340;&#20302;&#32500;&#23884;&#20837;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20809;&#35889;&#36827;&#34892;&#65288;&#28145;&#24230;&#65289;&#32858;&#31867;&#26469;&#33719;&#24471;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#32534;&#30721;&#31354;&#38388;&#20013;&#24212;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#23545;&#25968;&#25454;&#31435;&#26041;&#20307;&#20687;&#32032;&#30340;&#20809;&#35889;&#23646;&#24615;&#36827;&#34892;&#28145;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#19987;&#38376;&#35757;&#32451;&#30340;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;&#22120;&#36827;&#34892;&#32479;&#35745;&#32500;&#24230;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Extended Vision techniques are ubiquitous in physics. However, the data cubes steaming from such analysis often pose a challenge in their interpretation, due to the intrinsic difficulty in discerning the relevant information from the spectra composing the data cube.   Furthermore, the huge dimensionality of data cube spectra poses a complex task in its statistical interpretation; nevertheless, this complexity contains a massive amount of statistical information that can be exploited in an unsupervised manner to outline some essential properties of the case study at hand, e.g.~it is possible to obtain an image segmentation via (deep) clustering of data-cube's spectra, performed in a suitably defined low-dimensional embedding space.   To tackle this topic, we explore the possibility of applying unsupervised clustering methods in encoded space, i.e. perform deep clustering on the spectral properties of datacube pixels. A statistical dimensional reduction is performed by an ad hoc trained 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#36807;&#31243;&#20013;&#24341;&#20837;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2312.15474</link><description>&lt;p&gt;
&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#30340;&#20445;&#23432;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15474
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#36807;&#31243;&#20013;&#24341;&#20837;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#65288;ODRL&#65289;&#26088;&#22312;&#23558;&#31574;&#30053;&#20174;&#28304;&#29615;&#22659;&#36716;&#31227;&#21040;&#20855;&#26377;&#19981;&#21516;&#20294;&#30456;&#20284;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#30446;&#26631;&#29615;&#22659;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;RL&#20195;&#29702;&#36807;&#24230;&#20381;&#36182;&#28304;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#65292;&#23548;&#33268;&#21457;&#29616;&#22312;&#35813;&#29615;&#22659;&#20013;&#34920;&#29616;&#21331;&#36234;&#30340;&#31574;&#30053;&#65292;&#20294;&#22312;&#30446;&#26631;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#23569;&#26679;&#26412;&#26694;&#26550;&#20013;&#65292;&#24341;&#20837;&#20102;&#26469;&#33258;&#30446;&#26631;&#29615;&#22659;&#30340;&#26377;&#38480;&#25968;&#37327;&#36716;&#25442;&#20197;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#19981;&#21516;&#31163;&#32447;&#21160;&#21147;&#23398;&#26465;&#20214;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#35775;&#38382;&#30446;&#26631;&#29615;&#22659;&#26159;&#26497;&#31471;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;CLIP&#31995;&#21015;&#27169;&#22411;&#27966;&#29983;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#30340;&#22870;&#21169;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#33021;&#22815;&#23454;&#29616;&#22810;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2312.09187</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22870;&#21169;&#30340;&#26469;&#28304;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models as a Source of Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09187
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;CLIP&#31995;&#21015;&#27169;&#22411;&#27966;&#29983;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#30340;&#22870;&#21169;&#65292;&#20174;&#32780;&#35757;&#32451;&#20986;&#33021;&#22815;&#23454;&#29616;&#22810;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#21487;&#20197;&#22312;&#20016;&#23500;&#22810;&#26679;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#23454;&#29616;&#35768;&#22810;&#30446;&#26631;&#30340;&#36890;&#29992;&#20195;&#29702;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#21069;&#27839;&#20043;&#19968;&#12290;&#24314;&#31435;&#20855;&#26377;RL&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#20851;&#38190;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#23454;&#29616;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#29616;&#25104;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#28304;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;CLIP&#31995;&#21015;&#27169;&#22411;&#20013;&#27966;&#29983;&#35270;&#35273;&#23454;&#29616;&#21508;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;&#22870;&#21169;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#35821;&#35328;&#30446;&#26631;&#30340;RL&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#35268;&#27169;&#21270;&#36235;&#21183;&#65292;&#26174;&#31034;&#26356;&#22823;&#30340;VLM&#20250;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#35270;&#35273;&#30446;&#26631;&#23454;&#29616;&#22870;&#21169;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#26377;&#33021;&#21147;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09187v2 Announce Type: replace  Abstract: Building generalist agents that can accomplish many goals in rich open-ended environments is one of the research frontiers for reinforcement learning. A key limiting factor for building generalist agents with RL has been the need for a large number of reward functions for achieving different goals. We investigate the feasibility of using off-the-shelf vision-language models, or VLMs, as sources of rewards for reinforcement learning agents. We show how rewards for visual achievement of a variety of language goals can be derived from the CLIP family of models, and used to train RL agents that can achieve a variety of language goals. We showcase this approach in two distinct visual domains and present a scaling trend showing how larger VLMs lead to more accurate rewards for visual goal achievement, which in turn produces more capable RL agents.
&lt;/p&gt;</description></item><item><title>SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04985</link><description>&lt;p&gt;
SparQ&#27880;&#24847;&#21147;&#65306;&#39640;&#25928;&#24102;&#23485;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SparQ Attention: Bandwidth-Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04985
&lt;/p&gt;
&lt;p&gt;
SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21019;&#20102;&#35768;&#22810;&#26032;&#21487;&#33021;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#20351;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparQ&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#24615;&#33719;&#21462;&#32531;&#23384;&#21382;&#21490;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;LLMs&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.13958</link><description>&lt;p&gt;
&#22788;&#29702;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#20013;&#30340;&#38750;&#20809;&#28369;&#25361;&#25112;&#65306;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective Tensor Recovery Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#21487;&#23398;&#20064;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#22411;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#24341;&#20837;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#20248;&#21270;&#31639;&#27861;&#65292;&#35299;&#20915;&#22788;&#29702;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35768;&#22810;&#22522;&#20110;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#30340;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#22312;&#22788;&#29702;&#35270;&#35273;&#25968;&#25454;&#65288;&#22914;&#24425;&#33394;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26174;&#31034;&#20986;&#38750;&#20809;&#28369;&#21464;&#21270;&#30340;&#24352;&#37327;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#36973;&#21463;&#20005;&#37325;&#30340;&#24615;&#33021;&#36864;&#21270;&#12290;&#34429;&#28982;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;t-SVD&#30340;&#26041;&#27861;&#21364;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#24674;&#22797;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#24352;&#37327;&#26680;&#33539;&#25968;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20132;&#26367;&#36817;&#31471;&#20056;&#23376;&#26041;&#27861;&#65288;APMM&#65289;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#36845;&#20195;&#22320;&#35299;&#20915;&#25552;&#20986;&#30340;&#24352;&#37327;&#34917;&#20840;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;APMM&#25910;&#25947;&#21040;&#20248;&#21270;&#38382;&#39064;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;APMM&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#24352;&#37327;&#24674;&#22797;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#21327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13958v2 Announce Type: replace-cross  Abstract: Recently, numerous tensor singular value decomposition (t-SVD)-based tensor recovery methods have shown promise in processing visual data, such as color images and videos. However, these methods often suffer from severe performance degradation when confronted with tensor data exhibiting non-smooth changes. It has been commonly observed in real-world scenarios but ignored by the traditional t-SVD-based methods. In this work, we introduce a novel tensor recovery model with a learnable tensor nuclear norm to address such a challenge. We develop a new optimization algorithm named the Alternating Proximal Multiplier Method (APMM) to iteratively solve the proposed tensor completion model. Theoretical analysis demonstrates the convergence of the proposed APMM to the Karush-Kuhn-Tucker (KKT) point of the optimization problem. In addition, we propose a multi-objective tensor recovery framework based on APMM to efficiently explore the co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2309.00770</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#19982;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#25552;&#20986;&#20102;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#23450;&#20041;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20998;&#31867;&#20307;&#31995;&#65292;&#20197;&#21327;&#21161;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#36827;&#34892;&#20844;&#24179;&#24615;&#20998;&#26512;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#20154;&#20204;&#33021;&#22815;&#22788;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#65292;&#36880;&#28176;&#34701;&#20837;&#35302;&#21450;&#25105;&#20204;&#31038;&#20132;&#39046;&#22495;&#30340;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#23398;&#20064;&#12289;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20559;&#35265;&#35780;&#20272;&#21644;&#32531;&#35299;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#12289;&#24418;&#24335;&#21270;&#21644;&#25193;&#23637;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#30340;&#27010;&#24565;&#65292;&#23450;&#20041;&#20102;&#20260;&#23475;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#20960;&#20010;&#23454;&#29616;LLMs&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#30452;&#35266;&#30340;&#20998;&#31867;&#20307;&#31995;&#32479;&#19968;&#20102;&#25991;&#29486;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#29992;&#20110;&#20559;&#35265;&#35780;&#20272;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21363;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#32531;&#35299;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00770v2 Announce Type: replace-cross  Abstract: Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.13425</link><description>&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#35843;&#26597;&#65306;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Survey on Uncertainty Quantification for Deep Learning: An Uncertainty Source Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20174;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#30340;&#35282;&#24230;&#20998;&#26512;&#19981;&#21516;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20197;&#21450;&#31185;&#23398;&#19982;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#35748;&#35782;&#21040;DNNs&#26377;&#26102;&#20250;&#20570;&#20986;&#24847;&#22806;&#12289;&#38169;&#35823;&#20294;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#28798;&#38590;&#21709;&#24212;&#31561;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#20986;&#29616;&#20005;&#37325;&#21518;&#26524;&#12290;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26088;&#22312;&#20272;&#35745;DNN&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#65292;&#36229;&#36234;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#38024;&#23545;DNNs&#30340;UQ&#26041;&#27861;&#12290;&#31995;&#32479;&#22320;&#23545;&#36825;&#20123;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#24182;&#27604;&#36739;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#20855;&#26377;&#26497;&#22823;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35843;&#26597;&#22823;&#22810;&#38598;&#20013;&#22312;&#20174;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35282;&#24230;&#25110;&#36125;&#21494;&#26031;&#35282;&#24230;&#23545;UQ&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#27599;&#31181;&#26041;&#27861;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.13425v3 Announce Type: replace  Abstract: Deep neural networks (DNNs) have achieved tremendous success in making accurate predictions for computer vision, natural language processing, as well as science and engineering domains. However, it is also well-recognized that DNNs sometimes make unexpected, incorrect, but overconfident predictions. This can cause serious consequences in high-stake applications, such as autonomous driving, medical diagnosis, and disaster response. Uncertainty quantification (UQ) aims to estimate the confidence of DNN predictions beyond prediction accuracy. In recent years, many UQ methods have been developed for DNNs. It is of great practical value to systematically categorize these UQ methods and compare their advantages and disadvantages. However, existing surveys mostly focus on categorizing UQ methodologies from a neural network architecture perspective or a Bayesian perspective and ignore the source of uncertainty that each methodology can incor
&lt;/p&gt;</description></item><item><title>&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11694</link><description>&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11694
&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#65292;&#24182;&#19988;&#20854;&#35774;&#35745;&#21463;&#21040;&#20102;&#29992;&#20110;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#30340;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#30340;&#25928;&#29575;&#21551;&#21457;&#12290;&#20381;&#36182;&#21464;&#37327;&#21487;&#20197;&#38544;&#24335;&#25110;&#26174;&#24335;&#23450;&#20041;&#65292;&#24182;&#19988;&#26041;&#31243;&#21487;&#20197;&#20351;&#29992;&#20195;&#25968;&#12289;&#24494;&#20998;&#25110;&#31215;&#20998;&#20851;&#31995;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#35745;&#31639;&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#65292;&#20294;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#22312;&#20171;&#32461;&#22522;&#30784;&#29702;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#22312;&#36825;&#37324;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22312;&#20801;&#35768;&#35745;&#31639;&#30340;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05735</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#32534;&#36753;&#24050;&#32463;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#32534;&#36753;&#25552;&#31034;&#26469;&#36716;&#25442;&#35270;&#39057;&#30340;&#20840;&#23616;&#39118;&#26684;&#12289;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20855;&#26377;&#26102;&#24207;&#19968;&#33268;&#24615;&#30340;&#24103;&#65292;&#21487;&#33021;&#28041;&#21450;&#25193;&#25955;&#21453;&#28436;&#21644;/&#25110;&#36328;&#24103;&#27880;&#24847;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20302;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65288;OCD&#65289;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26356;&#22810;&#22320;&#20998;&#37197;&#32473;&#23545;&#24863;&#30693;&#36136;&#37327;&#26356;&#37325;&#35201;&#30340;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25552;&#26696;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65306;i&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;&#37319;&#26679;&#65292;&#23558;&#29992;&#20110;&#26174;&#33879;&#21306;&#22495;&#25110;&#32972;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#19982;&#29992;&#20110;&#21069;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#20998;&#31163;&#24320;&#26469;&#65292;&#23558;&#22823;&#37096;&#20998;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#32473;&#21069;&#32773;&#65307;ii&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;3D&#20196;&#29260;&#21512;&#24182;&#65292;&#29992;&#20110;&#25913;&#21892;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04372</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#36827;&#34892;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#20165;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#24471;&#21040;&#30340;&#26410;&#30693;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35774;&#32622;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#26144;&#23556;&#21644;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25193;&#25955;&#26144;&#23556;&#29992;&#20110;&#20174;&#21487;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#36817;&#20284;&#24471;&#21040;&#28418;&#31227;&#39033;&#65292;&#28982;&#21518;&#22312;&#31163;&#25955;&#26102;&#38388;&#30340;&#26391;&#20043;&#19975;&#37319;&#26679;&#22120;&#20013;&#23454;&#29616;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#26680;&#24102;&#23485;&#35774;&#32622;&#20026;&#19982;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#27493;&#38271;&#21305;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36890;&#24120;&#19982;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35010;&#27493;&#39588;&#26041;&#26696;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#26679;&#26412;&#20445;&#25345;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#20984;&#21253;&#20869;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#20026;&#29983;&#25104;&#26465;&#20214;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#20132;&#27969;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#21644;&#22242;&#38431;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.15600</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Context-aware Communication for Multi-agent Reinforcement Learning. (arXiv:2312.15600v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15600
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#20132;&#27969;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#21512;&#20316;&#21644;&#22242;&#38431;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#26377;&#25928;&#30340;&#36890;&#20449;&#21327;&#35758;&#23545;&#20110;&#20419;&#36827;&#21512;&#20316;&#21644;&#25552;&#39640;&#22242;&#38431;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#21033;&#29992;&#36890;&#20449;&#65292;&#35768;&#22810;&#20197;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#26412;&#22320;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#26465;&#28040;&#24687;&#24182;&#24191;&#25773;&#32473;&#25152;&#26377;&#21487;&#36798;&#30340;&#26234;&#33021;&#20307;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#20026;&#20010;&#20307;&#26234;&#33021;&#20307;&#25552;&#20379;&#36275;&#22815;&#12289;&#20851;&#38190;&#21644;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#22312;&#24102;&#23485;&#20005;&#37325;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#20026;MARL&#24320;&#21457;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#26088;&#22312;&#21521;&#19981;&#21516;&#30340;&#26234;&#33021;&#20307;&#21457;&#36865;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#36890;&#20449;&#21327;&#35758;&#21517;&#20026;CACOM&#65292;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#20013;&#65292;&#26234;&#33021;&#20307;&#20197;&#24191;&#25773;&#26041;&#24335;&#20132;&#25442;&#31895;&#30053;&#34920;&#31034;&#65292;&#20026;&#31532;&#20108;&#20010;&#38454;&#27573;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#32039;&#38543;&#20854;&#21518;&#65292;&#26234;&#33021;&#20307;&#22312;&#31532;&#20108;&#20010;&#38454;&#27573;&#20013;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20026;&#25509;&#25910;&#32773;&#36873;&#25321;&#24615;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#28040;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#23398;&#20064;&#30340;&#27493;&#38271;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication protocols in multi-agent reinforcement learning (MARL) are critical to fostering cooperation and enhancing team performance. To leverage communication, many previous works have proposed to compress local information into a single message and broadcast it to all reachable agents. This simplistic messaging mechanism, however, may fail to provide adequate, critical, and relevant information to individual agents, especially in severely bandwidth-limited scenarios. This motivates us to develop context-aware communication schemes for MARL, aiming to deliver personalized messages to different agents. Our communication protocol, named CACOM, consists of two stages. In the first stage, agents exchange coarse representations in a broadcast fashion, providing context for the second stage. Following this, agents utilize attention mechanisms in the second stage to selectively generate messages personalized for the receivers. Furthermore, we employ the learned step size quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#21319;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19919</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#22522;&#20110;&#20215;&#20540;&#26368;&#22823;&#21270;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Strategies through Value Maximization in Neural Networks. (arXiv:2310.19919v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#21319;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#23398;&#20064;&#20195;&#29702;&#38754;&#20020;&#35832;&#22810;&#23398;&#20064;&#36873;&#25321;&#65292;&#21253;&#25324;&#36229;&#21442;&#25968;&#36873;&#25321;&#21644;&#20219;&#21153;&#20998;&#24067;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#22914;&#35838;&#31243;&#12290;&#20102;&#35299;&#22914;&#20309;&#36827;&#34892;&#36825;&#20123;&#20803;&#23398;&#20064;&#36873;&#25321;&#21487;&#20197;&#25552;&#20379;&#23545;&#29983;&#29289;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#25511;&#21046;&#21151;&#33021;&#30340;&#35268;&#33539;&#35299;&#37322;&#65292;&#24182;&#25913;&#36827;&#24037;&#31243;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20248;&#21270;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#65292;&#30446;&#21069;&#20173;&#28982;&#25361;&#25112;&#30528;&#35745;&#31639;&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#29615;&#22659;&#20013;&#20174;&#29702;&#35770;&#19978;&#30740;&#31350;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#21162;&#21147;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#23436;&#20840;&#35268;&#33539;&#21270;&#30340;&#30446;&#26631;&#19978;&#39640;&#25928;&#22320;&#20248;&#21270;&#25511;&#21046;&#20449;&#21495;&#65306;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#25240;&#29616;&#32047;&#31215;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#20272;&#35745;&#26799;&#24230;&#19979;&#38477;&#30340;&#24179;&#22343;&#21160;&#21147;&#26041;&#31243;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35745;&#31639;&#30340;&#21487;&#34892;&#24615;&#65292;&#35813;&#26041;&#31243;&#36866;&#29992;&#20110;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#23481;&#20102;&#19968;&#31995;&#21015;&#20803;&#23398;&#20064;&#21644;&#33258;&#21160;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24418;&#25104;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial learning agents face numerous choices about how to learn, ranging from hyperparameter selection to aspects of task distributions like curricula. Understanding how to make these meta-learning choices could offer normative accounts of cognitive control functions in biological learners and improve engineered systems. Yet optimal strategies remain challenging to compute in modern deep networks due to the complexity of optimizing through the entire learning process. Here we theoretically investigate optimal strategies in a tractable setting. We present a learning effort framework capable of efficiently optimizing control signals on a fully normative objective: discounted cumulative performance throughout learning. We obtain computational tractability by using average dynamical equations for gradient descent, available for simple neural network architectures. Our framework accommodates a range of meta-learning and automatic curriculum learning methods in a unified n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#22312;&#20302;&#31934;&#24230;&#37319;&#26679;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.16320</link><description>&lt;p&gt;
&#22686;&#24378;&#20302;&#31934;&#24230;&#37319;&#26679;&#65306;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Precision Sampling via Stochastic Gradient Hamiltonian Monte Carlo. (arXiv:2310.16320v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#22312;&#20302;&#31934;&#24230;&#37319;&#26679;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31934;&#24230;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20302;&#25104;&#26412;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#22826;&#22810;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#23545;&#25968;&#20985;&#21644;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#65292;&#20351;&#29992;&#20302;&#31934;&#24230;&#21644;&#20840;&#31934;&#24230;&#26799;&#24230;&#32047;&#21152;&#22120;&#30340;&#38543;&#26426;&#26799;&#24230;Hamiltonian Monte Carlo (SGHMC)&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#23454;&#29616;2-Wasserstein&#36317;&#31163;&#30340;&#949;&#35823;&#24046;&#65292;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;&#20302;&#31934;&#24230;&#37319;&#26679;&#22120;&#65288;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#65292;SGLD&#65289;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#65288;$\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$ vs $\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$&#65289;&#12290;&#21478;&#22806;&#65292;&#22522;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20302;&#31934;&#24230;SGHMC&#30456;&#23545;&#20110;SGLD&#22312;&#38750;&#23545;&#25968;&#20985;&#20998;&#24067;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-precision training has emerged as a promising low-cost technique to enhance the training efficiency of deep neural networks without sacrificing much accuracy. Its Bayesian counterpart can further provide uncertainty quantification and improved generalization accuracy. This paper investigates low-precision sampling via Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) with low-precision and full-precision gradient accumulators for both strongly log-concave and non-log-concave distributions. Theoretically, our results show that, to achieve $\epsilon$-error in the 2-Wasserstein distance for non-log-concave distributions, low-precision SGHMC achieves quadratic improvement ($\widetilde{\mathbf{O}}\left({\epsilon^{-2}{\mu^*}^{-2}\log^2\left({\epsilon^{-1}}\right)}\right)$) compared to the state-of-the-art low-precision sampler, Stochastic Gradient Langevin Dynamics (SGLD) ($\widetilde{\mathbf{O}}\left({{\epsilon}^{-4}{\lambda^{*}}^{-1}\log^5\left({\epsilon^{-1}}\right)}\right)$). Moreo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.15047</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20803;-&#65288;&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#65289;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#23637;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;&#36825;&#31181;&#23398;&#20064;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26356;&#22909;&#22320;&#21560;&#25910;&#24191;&#27867;&#36866;&#29992;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20851;&#20110;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20135;&#29983;&#30340;&#20004;&#31181;&#20551;&#35774;&#65292;&#24182;&#23601;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#28508;&#22312;&#39118;&#38505;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Brown&#31561;&#20154;&#65288;2020&#65289;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#23454;&#39564;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#31216;&#20026;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65288;meta-OCL&#65289;&#30340;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#20351;LLMs&#26356;&#23481;&#26131;&#8220;&#20869;&#21270;&#8221;&#25991;&#26412;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#35813;&#25991;&#26412;&#24191;&#27867;&#36866;&#29992;&#65288;&#20363;&#22914;&#30495;&#23454;&#38472;&#36848;&#25110;&#26435;&#23041;&#26469;&#28304;&#30340;&#25991;&#26412;&#65289;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23427;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#21512;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20551;&#35774;&#65292;&#35299;&#37322;&#20102;&#20803;-&#36229;&#25991;&#26412;&#22806;&#35821;&#22659;&#23398;&#20064;&#30340;&#20986;&#29616;&#65306;&#19968;&#31181;&#26159;&#20381;&#36182;&#20110;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#65292;&#21478;&#19968;&#31181;&#26159;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#22120;&#30340;&#38544;&#21547;&#26799;&#24230;&#23545;&#40784;&#20559;&#24046;&#21487;&#33021;&#36127;&#36131;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#33021;&#24847;&#21619;&#30528;&#26410;&#26469;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#24605;&#32771;&#65292;&#24182;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/krasheni&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
&lt;/p&gt;</description></item><item><title>DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.12128</link><description>&lt;p&gt;
DiagrammerGPT: &#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning. (arXiv:2310.12128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12128
&lt;/p&gt;
&lt;p&gt;
DiagrammerGPT&#26159;&#19968;&#20010;&#36890;&#36807;LLM&#35268;&#21010;&#29983;&#25104;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#30340;&#26694;&#26550;&#65292;&#22635;&#34917;&#20102;T2I&#27169;&#22411;&#22312;&#22270;&#34920;&#29983;&#25104;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#20351;&#29992;T2I&#27169;&#22411;&#29983;&#25104;&#22270;&#34920;&#26041;&#38754;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22270;&#34920;&#26159;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#20016;&#23500;&#21644;&#31354;&#38388;&#22797;&#26434;&#30340;&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#20449;&#24687;&#30340;&#31526;&#21495;/&#31034;&#24847;&#24615;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#19968;&#31181;&#23494;&#38598;&#30340;&#30456;&#20851;&#23545;&#35937;&#12289;&#25991;&#26412;&#26631;&#31614;&#12289;&#26041;&#21521;&#31661;&#22836;&#12289;&#36830;&#25509;&#32447;&#31561;&#32452;&#21512;&#65289;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#34920;&#26102;&#32463;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35768;&#22810;&#23545;&#35937;&#36890;&#36807;&#22797;&#26434;&#30340;&#20851;&#31995;&#65288;&#22914;&#31661;&#22836;/&#32447;&#65289;&#23494;&#38598;&#36830;&#25509;&#26102;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#23545;&#35937;&#24067;&#23616;&#25511;&#21046;&#65292;&#24182;&#19988;&#32463;&#24120;&#19981;&#33021;&#28210;&#26579;&#20986;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#26631;&#31614;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiagrammerGPT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#22270;&#34920;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24067;&#23616;&#24341;&#23548;&#33021;&#21147;&#26469;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#24320;&#25918;&#39046;&#22495;&#12289;&#24320;&#25918;&#24179;&#21488;&#30340;&#22270;&#34920;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;LLM&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#8220;&#22270;&#34920;&#35268;&#21010;&#8221;&#65288;&#22312;&#19968;&#20010;&#35268;&#21010;&#26041;&#26696;&#20013;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image (T2I) generation has seen significant growth over the past few years. Despite this, there has been little work on generating diagrams with T2I models. A diagram is a symbolic/schematic representation that explains information using structurally rich and spatially complex visualizations (e.g., a dense combination of related objects, text labels, directional arrows, connection lines, etc.). Existing state-of-the-art T2I models often fail at diagram generation because they lack fine-grained object layout control when many objects are densely connected via complex relations such as arrows/lines and also often fail to render comprehensible text labels. To address this gap, we present DiagrammerGPT, a novel two-stage text-to-diagram generation framework that leverages the layout guidance capabilities of LLMs (e.g., GPT-4) to generate more accurate open-domain, open-platform diagrams. In the first stage, we use LLMs to generate and iteratively refine 'diagram plans' (in a planne
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00149</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#65306;&#21521;&#33021;&#22815;&#35757;&#32451;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#21333;&#19968;&#22270;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00149
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#22270;&#23398;&#20064;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#19981;&#21516;&#23646;&#24615;&#21644;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#21333;&#19968;&#27169;&#22411;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#35821;&#35328;&#39046;&#22495;&#20869;&#25972;&#21512;&#21644;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#30340;&#24322;&#24120;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#32479;&#19968;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22270;&#23398;&#20064;&#39046;&#22495;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#36981;&#24490;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;&#36825;&#31181;&#24046;&#24322;&#20351;&#24471;&#24456;&#38590;&#23558;&#22270;&#34920;&#31034;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#20854;&#27425;&#65292;&#22270;&#19978;&#30340;&#20219;&#21153;&#20998;&#21270;&#20026;&#33410;&#28857;&#12289;&#38142;&#25509;&#21644;&#22270;&#20219;&#21153;&#65292;&#38656;&#35201;&#19981;&#21516;&#30340;&#23884;&#20837;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36866;&#24403;&#22270;&#25552;&#31034;&#33539;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#19968;&#20992;&#20999;"&#65288;OFA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#21333;&#19968;&#22270;&#27169;&#22411;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing a single model that addresses multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in integrating and solving different tasks within the language domain. However, a unified model for various tasks on graphs remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. Striving to handle all the aforementioned challenges, we propose One for All (OFA), the first general framework that can use a single graph model to address the above challenges. Specifically, OFA proposes text-at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#25104;&#21151;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#20943;&#23569;&#20102;&#20998;&#24067;&#22806;&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2309.17230</link><description>&lt;p&gt;
&#20266;&#29305;&#24449;&#22810;&#26679;&#24615;&#25913;&#21892;&#20102;&#23545;&#20998;&#24067;&#22806;&#27867;&#21270;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Spurious Feature Diversification Improves Out-of-distribution Generalization. (arXiv:2309.17230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#22312;&#20998;&#24067;&#22806;&#27867;&#21270;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#20854;&#25104;&#21151;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#20943;&#23569;&#20102;&#20998;&#24067;&#22806;&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#30340;&#27867;&#21270;&#26159;&#19968;&#20010;&#20851;&#38190;&#24615;&#25361;&#25112;&#12290;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#22914;&#22312;&#27169;&#22411;&#21442;&#25968;&#19978;&#36827;&#34892;&#25554;&#20540;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;OOD&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#30340;&#22522;&#26412;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#31181;&#24120;&#29992;&#30340;&#26435;&#37325;&#31354;&#38388;&#38598;&#25104;&#26041;&#27861;WiSE-FT&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#24494;&#35843;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#21363;WiSE-FT&#25104;&#21151;&#22320;&#32416;&#27491;&#20102;&#35768;&#22810;&#20010;&#20307;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#30340;&#24773;&#20917;&#65292;&#36825;&#23545;&#20110;&#20854;OOD&#30340;&#26377;&#25928;&#24615;&#36129;&#29486;&#37325;&#22823;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20102;&#35299;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22823;&#37327;&#20266;&#29305;&#24449;&#30340;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#39044;&#27979;&#20102;&#19978;&#36848;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22522;&#20110;&#38598;&#25104;&#30340;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#20266;&#29305;&#24449;&#65292;&#20943;&#23569;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#39044;&#27979;&#38169;&#35823;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;</title><link>http://arxiv.org/abs/2309.15091</link><description>&lt;p&gt;
VideoDirectorGPT: &#36890;&#36807;LLM&#24341;&#23548;&#30340;&#35268;&#21010;&#23454;&#29616;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning. (arXiv:2309.15091v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#23454;&#29616;&#19968;&#33268;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#26469;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#24037;&#20316;&#38598;&#20013;&#22312;&#29983;&#25104;&#21333;&#20010;&#20107;&#20214;&#21644;&#21333;&#19968;&#32972;&#26223;&#30340;&#30701;&#35270;&#39057;&#29255;&#27573;&#65288;&#21363;&#21333;&#22330;&#26223;&#35270;&#39057;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#24067;&#23616;&#21644;&#25511;&#21046;&#19979;&#28216;&#35270;&#35273;&#27169;&#22359;&#65288;&#22914;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65289;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#36825;&#20123;LLMs&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#29992;&#20110;&#29983;&#25104;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#38271;&#35270;&#39057;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VideoDirectorGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19968;&#33268;&#30340;&#22810;&#22330;&#26223;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;LLMs&#30340;&#30693;&#35782;&#36827;&#34892;&#35270;&#39057;&#20869;&#23481;&#35268;&#21010;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#21333;&#20010;&#25991;&#26412;&#25552;&#31034;&#36755;&#20837;&#25105;&#20204;&#30340;&#35270;&#39057;&#35268;&#21010;&#22120;LLM&#65288;GPT-4&#65289;&#20013;&#65292;&#23558;&#20854;&#25193;&#23637;&#20026;&#8220;&#35270;&#39057;&#35745;&#21010;&#8221;&#65292;&#20854;&#20013;&#21253;&#25324;&#29983;&#25104;&#22330;&#26223;&#25551;&#36848;&#12289;&#23454;&#20307;&#21450;&#20854;&#24067;&#23616;&#12289;&#27599;&#20010;&#22330;&#26223;&#30340;&#32972;&#26223;&#20197;&#21450;&#20445;&#25345;&#19968;&#33268;&#24615;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although recent text-to-video (T2V) generation methods have seen significant advancements, most of these works focus on producing short video clips of a single event with a single background (i.e., single-scene videos). Meanwhile, recent large language models (LLMs) have demonstrated their capability in generating layouts and programs to control downstream visual modules such as image generation models. This raises an important question: can we leverage the knowledge embedded in these LLMs for temporally consistent long video generation? In this paper, we propose VideoDirectorGPT, a novel framework for consistent multi-scene video generation that uses the knowledge of LLMs for video content planning and grounded video generation. Specifically, given a single text prompt, we first ask our video planner LLM (GPT-4) to expand it into a 'video plan', which involves generating the scene descriptions, the entities with their respective layouts, the background for each scene, and consistency 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12112</link><description>&lt;p&gt;
&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Continual Category Discovery. (arXiv:2308.12112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20013;&#21516;&#26102;&#22788;&#29702;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#21033;&#29992;&#25345;&#32493;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#25512;&#21160;&#30528;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30340;&#26497;&#38480;&#65292;&#20854;&#20013;&#19968;&#20010;&#26234;&#33021;&#20307;&#26399;&#26395;&#23398;&#20064;&#26032;&#30340;&#26631;&#35760;&#20219;&#21153;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35774;&#32622;&#19982;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#19981;&#22826;&#21563;&#21512;&#65292;&#20854;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#21253;&#25324;&#20840;&#26032;&#65288;&#23436;&#20840;&#26080;&#26631;&#35760;&#65289;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#30340;&#31034;&#20363;&#12290;&#21463;&#21040;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#25918;&#26494;&#36825;&#20010;&#20551;&#35774;&#12290;&#30830;&#20999;&#22320;&#35828;&#65292;&#22312;&#20219;&#20309;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23384;&#22312;&#26032;&#30340;&#21644;&#24050;&#30693;&#30340;&#31867;&#21035;&#65292;&#24182;&#19988;&#24517;&#39035;&#20351;&#29992;&#25345;&#32493;&#29256;&#26412;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#21457;&#29616;&#23427;&#20204;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#35774;&#32622;&#20026;&#24191;&#20041;&#25345;&#32493;&#31867;&#21035;&#21457;&#29616;&#65288;GCCD&#65289;&#12290;&#23427;&#32479;&#19968;&#20102;CL&#21644;GCD&#65292;&#24357;&#21512;&#20102;&#21512;&#25104;&#22522;&#20934;&#21644;&#29616;&#23454;&#29983;&#27963;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#20174;&#21518;&#32493;&#20219;&#21153;&#20013;&#31215;&#32047;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#21547;&#26080;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of Continual Learning (CL) methods push the limit of supervised learning settings, where an agent is expected to learn new labeled tasks and not forget previous knowledge. However, these settings are not well aligned with real-life scenarios, where a learning agent has access to a vast amount of unlabeled data encompassing both novel (entirely unlabeled) classes and examples from known classes. Drawing inspiration from Generalized Category Discovery (GCD), we introduce a novel framework that relaxes this assumption. Precisely, in any task, we allow for the existence of novel and known classes, and one must use continual version of unsupervised learning methods to discover them. We call this setting Generalized Continual Category Discovery (GCCD). It unifies CL and GCD, bridging the gap between synthetic benchmarks and real-life scenarios. With a series of experiments, we present that existing methods fail to accumulate knowledge from subsequent tasks in which unlabeled samples of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.12499</link><description>&lt;p&gt;
AdvDiff:&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#32469;&#36807;&#38450;&#24481;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#29702;&#35770;&#19978;&#26080;&#27861;&#35777;&#26126;&#65292;&#22240;&#27492;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#19978;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#30446;&#26631;&#29983;&#25104;&#30340;&#20363;&#23376;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AdvDiff&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#26799;&#24230;&#38598;&#25104;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#26377;&#25928;&#21644;&#31283;&#23450;&#12290;&#22312;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvDiff&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10864</link><description>&lt;p&gt;
&#23558;&#27880;&#24847;&#21147;&#20998;&#21106;&#19982;&#32465;&#23450;&#29992;&#20110;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Divide &amp; Bind Your Attention for Improved Generative Semantic Nursing. (arXiv:2307.10864v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#20998;&#21106;&#19982;&#32465;&#23450;"&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#36827;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#30340;&#25439;&#22833;&#30446;&#26631;&#65292;&#21253;&#25324;&#20851;&#27880;&#20002;&#22833;&#21644;&#32465;&#23450;&#20002;&#22833;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#25552;&#31034;&#21644;&#19981;&#36866;&#24403;&#23646;&#24615;&#32465;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#36924;&#30495;&#30340;&#21387;&#20498;&#24615;&#32467;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#23436;&#20840;&#20381;&#29031;&#36755;&#20837;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#8212;&#8212;&#20851;&#27880;&#19982;&#28608;&#21457;&#65292;&#24341;&#20837;&#20102;&#29983;&#25104;&#35821;&#20041;&#25252;&#29702;&#65288;GSN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#26029;&#26102;&#20248;&#21270;&#36328;&#27880;&#24847;&#21147;&#20197;&#26356;&#22909;&#22320;&#34701;&#20837;&#35821;&#20041;&#12290;&#23427;&#22312;&#29983;&#25104;&#31616;&#21333;&#25552;&#31034;&#65292;&#22914;&#8220;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#8221;&#65292;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;&#20197;&#21450;&#35299;&#20915;&#19981;&#36866;&#24403;&#30340;&#23646;&#24615;&#32465;&#23450;&#38382;&#39064;&#26041;&#38754;&#30340;&#21151;&#25928;&#26377;&#25152;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#22797;&#26434;&#25552;&#31034;&#25110;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#22330;&#26223;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#25913;&#36827;&#30340;&#23646;&#24615;&#32465;&#23450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#21106;&#19982;&#32465;&#23450;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;GSN&#25439;&#22833;&#30446;&#26631;&#65306;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#20002;&#22833;&#21644;&#19968;&#31181;&#32465;&#23450;&#20002;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20854;&#33021;&#22815;&#26356;&#22909;&#22320;&#23558;&#35821;&#20041;&#32435;&#20837;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#29305;&#28857;&#19978;&#33073;&#39062;&#32780;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging large-scale text-to-image generative models, e.g., Stable Diffusion (SD), have exhibited overwhelming results with high fidelity. Despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. Prior work, Attend &amp; Excite, has introduced the concept of Generative Semantic Nursing (GSN), aiming to optimize cross-attention during inference time to better incorporate the semantics. It demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. However, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. To address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose Divide &amp; Bind. We introduce two novel loss objectives for GSN: a novel attendance loss and a binding loss. Our approach stands out in its ability to fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;</title><link>http://arxiv.org/abs/2307.07091</link><description>&lt;p&gt;
&#31163;&#32447;&#32452;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#20102;&#22235;&#20010;&#21253;&#21547;256&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#30001;&#24615;&#33021;&#19981;&#21516;&#30340;&#20195;&#29702;&#37319;&#38598;&#65292;&#21253;&#21547;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21487;&#20197;&#35753;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36991;&#20813;&#26114;&#36149;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#37325;&#22797;&#12290;&#20026;&#20102;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#32452;&#21512;&#24335;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#29983;&#25104;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23588;&#20026;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20801;&#35768;&#20174;&#23569;&#37327;&#32452;&#20214;&#20013;&#21019;&#24314;&#22810;&#20010;&#20219;&#21153;&#65292;2&#65289;&#20219;&#21153;&#32467;&#26500;&#21487;&#20197;&#35753;&#35757;&#32451;&#22909;&#30340;&#20195;&#29702;&#36890;&#36807;&#32452;&#21512;&#30456;&#20851;&#30340;&#23398;&#20064;&#32452;&#20214;&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#24182;&#19988;3&#65289;&#32452;&#21512;&#32500;&#24230;&#25552;&#20379;&#20102;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#22235;&#20010;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#20102;&#26469;&#33258;CompoSuite [Mendez et al., 2022a]&#30340;256&#20010;&#20219;&#21153;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#24615;&#33021;&#31561;&#32423;&#30340;&#20195;&#29702;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#20102;2.56&#20159;&#26465;&#36716;&#25442;&#35760;&#24405;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#23398;&#20064;&#32452;&#21512;&#20219;&#21153;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#35774;&#32622;&#19978;&#36827;&#34892;&#30340;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) is a promising direction that allows RL agents to pre-train on large datasets, avoiding the recurrence of expensive data collection. To advance the field, it is crucial to generate large-scale datasets. Compositional RL is particularly appealing for generating such large datasets, since 1) it permits creating many tasks from few components, 2) the task structure may enable trained agents to solve new tasks by combining relevant learned components, and 3) the compositional dimensions provide a notion of task relatedness. This paper provides four offline RL datasets for simulated robotic manipulation created using the 256 tasks from CompoSuite [Mendez et al., 2022a]. Each dataset is collected from an agent with a different degree of performance, and consists of 256 million transitions. We provide training and evaluation settings for assessing an agent's ability to learn compositional task policies. Our benchmarking experiments on each setting show that
&lt;/p&gt;</description></item><item><title>&#36229;&#32593;&#32476;&#26159;&#19968;&#31181;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#20248;&#28857;&#12290;&#23427;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2306.06955</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36229;&#32593;&#32476;&#31616;&#35201;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Brief Review of Hypernetworks in Deep Learning. (arXiv:2306.06955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06955
&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#26159;&#19968;&#31181;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#20248;&#28857;&#12290;&#23427;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#25345;&#32493;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#32593;&#32476;&#65288;Hypernetworks&#65289;&#26159;&#29983;&#25104;&#21478;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65288;&#30446;&#26631;&#32593;&#32476;&#65289;&#26435;&#37325;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#20204;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20986;&#29616;&#65292;&#33021;&#22815;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21160;&#24577;&#24615;&#12289;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#12289;&#20449;&#24687;&#20849;&#20139;&#21644;&#27169;&#22411;&#21387;&#32553;&#31561;&#12290;&#36229;&#32593;&#32476;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#20013;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;&#25345;&#32493;&#23398;&#20064;&#12289;&#22240;&#26524;&#25512;&#26029;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#26435;&#37325;&#21098;&#26525;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#38646;&#26679;&#26412;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#12290;&#23613;&#31649;&#36229;&#32593;&#32476;&#22312;&#19981;&#21516;&#30340;&#38382;&#39064;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#21487;&#29992;&#30340;&#32508;&#36848;&#26469;&#21578;&#30693;&#30740;&#31350;&#20154;&#21592;&#26377;&#20851;&#20854;&#21457;&#23637;&#24773;&#20917;&#24182;&#24110;&#21161;&#21033;&#29992;&#36229;&#32593;&#32476;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#36229;&#32593;&#32476;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20363;&#23376;&#26469;&#35828;&#26126;&#22914;&#20309;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20116;&#20010;&#35774;&#35745;&#20934;&#21017;&#23545;&#36229;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variabi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#38548;&#26085;&#38656;&#27714;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.06194</link><description>&lt;p&gt;
&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#20844;&#20849;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#65306;&#29616;&#26377;&#27169;&#22411;&#30340;&#20803;&#20998;&#26512;&#21644;&#24320;&#28304;&#22522;&#20934;&#35774;&#26045;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Public Transit Demand Prediction During Highly Dynamic Conditions: A Meta-Analysis of State-of-the-Art Models and Open-Source Benchmarking Infrastructure. (arXiv:2306.06194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#38548;&#26085;&#38656;&#27714;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#24182;&#35780;&#20272;&#20102;&#20116;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#38656;&#27714;&#39044;&#27979;&#26159;&#21160;&#24577;&#20844;&#20132;&#36335;&#32447;&#35268;&#21010;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30701;&#26399;&#20844;&#20849;&#20132;&#36890;&#38656;&#27714;&#65292;&#20294;&#24212;&#29992;&#33539;&#22260;&#20165;&#38480;&#20110;&#30701;&#26102;&#38388;&#12289;&#31283;&#23450;&#30340;&#26102;&#38388;&#33539;&#22260;&#21644;&#23569;&#25968;&#36710;&#31449;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#36824;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20063;&#27809;&#26377;&#31995;&#32479;&#22320;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#35745;&#37327;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#31283;&#23450;&#21644;&#39640;&#24230;&#21160;&#24577;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#26234;&#33021;&#21345;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21733;&#20262;&#27604;&#20122;&#27874;&#21733;&#22823;BRT&#31995;&#32479;&#22312;&#38548;&#26085;&#30340;&#38656;&#27714;&#12290;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#26465;&#20214;&#21253;&#25324;&#19968;&#20010;&#38271;&#36798;&#19968;&#20010;&#26376;&#30340;&#25239;&#35758;&#21644;COVID-19&#22823;&#27969;&#34892;&#12290;&#36825;&#20004;&#20010;&#26465;&#20214;&#37117;&#24341;&#21457;&#20102;&#38656;&#27714;&#30340;&#24040;&#22823;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31283;&#23450;&#26465;&#20214;&#19979;&#65292;&#22823;&#22810;&#25968;&#27979;&#35797;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#20284;&#65292;MAAPE&#20174;0.08&#21040;0.12&#19981;&#31561;&#12290; &#22522;&#20934;&#35774;&#26045;&#30340;
&lt;/p&gt;
&lt;p&gt;
Real-time demand prediction is a critical input for dynamic bus routing. While many researchers have developed numerous complex methods to predict short-term transit demand, the applications have been limited to short, stable time frames and a few stations. How these methods perform in highly dynamic environments has not been studied, nor has their performance been systematically compared. We built an open-source infrastructure with five common methodologies, including econometric and deep learning approaches, and assessed their performance under stable and highly dynamic conditions. We used a time series from smartcard data to predict demand for the following day for the BRT system in Bogota, Colombia. The dynamic conditions in the time series include a month-long protest and the COVID-19 pandemic. Both conditions triggered drastic shifts in demand. The results reveal that most tested models perform similarly in stable conditions, with MAAPE varying from 0.08 to 0.12. The benchmark de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2306.05553</link><description>&lt;p&gt;
&#31561;&#21464;&#23618;&#19982;&#19981;&#21464;&#23618;&#30340;&#23545;&#27604;&#65306;&#28857;&#20113;&#20998;&#31867;&#20013;&#39592;&#24178;&#32593;&#32476;&#21644;&#27744;&#21270;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Equivariant vs. Invariant Layers: A Comparison of Backbone and Pooling for Point Cloud Classification. (arXiv:2306.05553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#20351;&#29992;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#21363;&#20351;&#26159;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#12289;&#26126;&#30830;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#26159;&#33719;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28857;&#20113;&#31561;&#38598;&#21512;&#32467;&#26500;&#25968;&#25454;&#24050;&#21463;&#21040;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#25972;&#21512;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20026;&#35774;&#35745;&#26377;&#25928;&#30340;&#28857;&#20113;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#34013;&#26412;&#12290;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#26159;&#32622;&#25442;&#19981;&#21464;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#30001;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#12289;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#21644;&#22238;&#24402;/&#20998;&#31867;&#22836;&#32452;&#25104;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20391;&#37325;&#20110;&#25913;&#21892;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#65292;&#20294;&#20840;&#23616;&#27744;&#21270;&#30340;&#24433;&#21709;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#25442;&#31561;&#21464;&#39592;&#24178;&#21644;&#32622;&#25442;&#19981;&#21464;&#20840;&#23616;&#27744;&#21270;&#22312;&#19977;&#20010;&#22522;&#20934;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;&#35832;&#22914;&#22522;&#20110;&#20256;&#36755;&#25110;&#27880;&#24847;&#21147;&#30340;&#22797;&#26434;&#27744;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31616;&#21333;&#39592;&#24178;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#39592;&#24178;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#30410;&#20250;&#20943;&#24369;&#12290;2&#65289;&#29978;&#33267;&#22797;&#26434;&#30340;&#39592;&#24178;&#20063;&#21487;&#20197;&#21463;&#30410;&#20110;&#26356;&#22797;&#26434;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#26126;&#30830;&#22320;&#32534;&#30721;&#32622;&#25442;&#19981;&#21464;&#24615;&#12290;3&#65289;&#20351;&#29992;&#32622;&#25442;&#19981;&#21464;&#27744;&#21270;&#23545;&#20110;&#22312;&#28857;&#20113;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from set-structured data, such as point clouds, has gained significant attention from the community. Geometric deep learning provides a blueprint for designing effective set neural networks by incorporating permutation symmetry. Of our interest are permutation invariant networks, which are composed of a permutation equivariant backbone, permutation invariant global pooling, and regression/classification head. While existing literature has focused on improving permutation equivariant backbones, the impact of global pooling is often overlooked. In this paper, we examine the interplay between permutation equivariant backbones and permutation invariant global pooling on three benchmark point cloud classification datasets. Our findings reveal that: 1) complex pooling methods, such as transport-based or attention-based poolings, can significantly boost the performance of simple backbones, but the benefits diminish for more complex backbones, 2) even complex backbones can benefit fro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25361;&#25112;&#20102;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#20551;&#35774;&#65292;&#24182;&#24378;&#35843;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05300</link><description>&lt;p&gt;
&#22522;&#20110;Epoch&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30456;&#20851;&#22122;&#22768;&#65306;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances. (arXiv:2306.05300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05300
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25361;&#25112;&#20102;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#20551;&#35774;&#65292;&#24182;&#24378;&#35843;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#26435;&#37325;&#26041;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#24050;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#30340;&#22522;&#30707;&#65292;&#20294;&#35748;&#20026;SGD&#24341;&#20837;&#30340;&#22122;&#22768;&#22312;&#26102;&#38388;&#19978;&#26159;&#19981;&#30456;&#20851;&#30340;&#65292;&#23613;&#31649;epoch-based&#35757;&#32451;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#25361;&#25112;&#65292;&#24182;&#35843;&#26597;&#20102;epoch-based&#22122;&#22768;&#30456;&#20851;&#24615;&#23545;&#31163;&#25955;&#26102;&#38388;&#24102;&#21160;&#37327;&#30340;SGD&#30340;&#31283;&#24577;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#38480;&#20110;&#20108;&#27425;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#35745;&#31639;&#35757;&#32451;epoch&#26102;&#22122;&#22768;&#30340;&#31934;&#30830;&#33258;&#30456;&#20851;&#24615;&#65292;&#20551;&#35774;&#35813;&#22122;&#22768;&#29420;&#31435;&#20110;&#26435;&#37325;&#21521;&#37327;&#20013;&#30340;&#23567;&#27874;&#21160;;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;epoch-based&#23398;&#20064;&#26041;&#26696;&#24341;&#20837;&#30340;&#30456;&#20851;&#24615;&#23545;SGD&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26354;&#29575;&#22823;&#20110;&#19968;&#20010;&#36229;&#21442;&#25968;&#30456;&#20851;&#20540;&#30340;&#26041;&#21521;&#19978;&#65292;&#36824;&#21407;&#20102;&#19981;&#30456;&#20851;&#22122;&#22768;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#23545;&#24179;&#22374;&#30340;&#26041;&#21521;&#19978;&#65292;&#26435;&#37325;&#26041;&#24046;&#26174;&#30528;&#20943;&#23567;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#20108;&#32500;&#22270;&#20363;&#23545;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#20102;&#30452;&#35266;&#35299;&#37322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#20851;&#20110;epoch-based SGD&#20013;&#30456;&#20851;&#22122;&#22768;&#24433;&#21709;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#25351;&#23548;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent (SGD) has become a cornerstone of neural network optimization, yet the noise introduced by SGD is often assumed to be uncorrelated over time, despite the ubiquity of epoch-based training. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum, limited to a quadratic loss. Our main contributions are twofold: first, we calculate the exact autocorrelation of the noise for training in epochs under the assumption that the noise is independent of small fluctuations in the weight vector; second, we explore the influence of correlations introduced by the epoch-based learning scheme on SGD dynamics. We find that for directions with a curvature greater than a hyperparameter-dependent crossover value, the results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced. We provide an intui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04621</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23545;&#40784;&#12289;&#25552;&#28860;&#21644;&#25193;&#20805;&#25152;&#26377;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#65292;&#38656;&#38754;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#24050;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#36793;&#32536;&#20998;&#24067;&#30340;&#21306;&#21035;&#65292;&#21069;&#32773;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#19988;&#21487;&#33021;&#19982;&#21518;&#32773;&#19981;&#21516;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20351;&#20266;&#26631;&#31614;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#20559;&#20506;&#65292;&#22914;&#24050;&#26631;&#35760;&#25968;&#25454;&#25110;&#24179;&#34913;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#30830;&#20445;&#25512;&#29702;&#26102;&#30340;&#24179;&#34913;&#26410;&#26631;&#35760;&#20998;&#24067;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#36880;&#28176;&#23558;&#20998;&#31867;&#22120;&#20174;&#21160;&#24577;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#21040;&#24179;&#34913;&#20998;&#24067;&#65307;&#21033;&#29992;&#34987;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#33293;&#24323;&#30340;&#20302;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30340;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65307;&#20197;&#21450;&#19968;&#31181;&#23558;&#26631;&#35760;&#37096;&#20998;&#30340;&#36755;&#20837;&#25968;&#25454;&#25193;&#23637;&#21040;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15742</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#22788;&#29702;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#26159;&#27979;&#35797;&#26032;&#30103;&#27861;&#30340;&#24120;&#29992;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#25928;&#24212;&#20250;&#25513;&#30422;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#37325;&#35201;&#30340;&#20010;&#20307;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26102;&#38388;&#35774;&#32622;&#20013;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#22788;&#29702;&#26159;&#26102;&#24207;&#30340;&#21644;&#26102;&#21464;&#30340;&#65292;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#20135;&#29983;&#20102;&#38169;&#32508;&#22797;&#26434;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20801;&#35768;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#36793;&#38469;&#32467;&#26500;&#27169;&#22411;&#35880;&#24910;&#22320;&#35299;&#20915;&#20102;&#35266;&#23519;&#25968;&#25454;&#21644;&#30446;&#26631;&#21453;&#20107;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11247</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22312;&#22797;&#26434;&#24418;&#29366;&#20013;&#27169;&#25311;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Quantum physics-informed neural networks for simulating computational fluid dynamics in complex shapes. (arXiv:2304.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#27969;&#21160;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24418;&#29366;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;21%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#27969;&#20307;&#30340;&#36895;&#24230;&#21644;&#21387;&#21147;&#20998;&#24067;&#65288;&#36890;&#36807;&#35299;&#20915;&#32435;&#32500;&#23572;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65289;&#26159;&#21270;&#23398;&#12289;&#33021;&#28304;&#12289;&#21046;&#33647;&#24037;&#19994;&#20197;&#21450;&#26426;&#26800;&#24037;&#31243;&#21644;&#31649;&#36947;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#65288;&#22914;OpenFOAM&#21644;Ansys&#65289;&#22312;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#37325;&#26032;&#27169;&#25311;&#27599;&#24403;&#20960;&#20309;&#21442;&#25968;&#25110;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#34987;&#25913;&#21464;&#12290;&#29289;&#29702;&#23398;&#20449;&#36182;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#27169;&#25311;&#22797;&#26434;&#20960;&#20309;&#24418;&#29366;&#20013;&#27969;&#20307;&#27969;&#21160;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36866;&#24212;&#20960;&#20309;&#24418;&#29366;&#21644;&#32593;&#26684;&#23450;&#20041;&#30340;&#21464;&#21270;&#65292;&#20801;&#35768;&#36328;&#19981;&#21516;&#24418;&#29366;&#36827;&#34892;&#27010;&#25324;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#28151;&#21512;&#37327;&#23376;&#29289;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#27169;&#25311;&#19977;&#32500; Y &#22411;&#28151;&#21512;&#22120;&#20013;&#30340;&#23618;&#27969;&#27969;&#20307;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#37327;&#23376;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982; PINN &#30340;&#28789;&#27963;&#24615;&#30456;&#32467;&#21512;&#65292;&#31934;&#24230;&#27604;&#26222;&#36890; PINN &#25552;&#39640;&#20102; 21&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the distribution of the velocities and pressures of a fluid (by solving the Navier-Stokes equations) is a principal task in the chemical, energy, and pharmaceutical industries, as well as in mechanical engineering and the design of pipeline systems. With existing solvers, such as OpenFOAM and Ansys, simulations of fluid dynamics in intricate geometries are computationally expensive and require re-simulation whenever the geometric parameters or the initial and boundary conditions are altered. Physics-informed neural networks (PINNs) are a promising tool for simulating fluid flows in complex geometries, as they can adapt to changes in the geometry and mesh definitions, allowing for generalization across different shapes. We present a hybrid quantum physics-informed neural network that simulates laminar fluid flows in 3D Y-shaped mixers. Our approach combines the expressive power of a quantum model with the flexibility of a PINN, resulting in a 21% higher accuracy compared to a pu
&lt;/p&gt;</description></item><item><title>MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10494</link><description>&lt;p&gt;
MaskedKD&#65306;&#20351;&#29992;&#36974;&#34109;&#22270;&#20687;&#30340;&#39640;&#25928;Vision Transformer&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MaskedKD: Efficient Distillation of Vision Transformers with Masked Images. (arXiv:2302.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10494
&lt;/p&gt;
&lt;p&gt;
MaskedKD&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#22359;&#26469;&#26174;&#33879;&#38477;&#20302;Vision Transformer (ViT)&#33976;&#39311;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#24433;&#21709;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#35757;&#32451;&#36731;&#37327;&#32423;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20250;&#22312;&#35757;&#32451;&#25104;&#26412;&#20013;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#20026;&#35813;&#26041;&#27861;&#38656;&#35201;&#22312;&#35757;&#32451;&#26679;&#26412;&#19978;&#33719;&#21462;&#25945;&#24072;&#30417;&#30563;&#12290;&#24403;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;Vision Transformer&#65288;ViTs&#65289;&#31561;&#25945;&#24072;&#27169;&#22411;&#26102;&#65292;&#36825;&#31181;&#38468;&#21152;&#25104;&#26412;&#8212;&#8212;&#33976;&#39311;&#25104;&#26412;&#8212;&#8212;&#26368;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MaskedKD&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#30528;&#38477;&#20302;&#33976;&#39311;ViTs&#30340;&#25104;&#26412;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#23398;&#29983;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MaskedKD&#36890;&#36807;&#36974;&#34109;&#19968;&#37096;&#20998;&#36755;&#20837;&#21040;&#25945;&#24072;&#27169;&#22411;&#30340;&#22270;&#20687;&#22359;&#20196;&#25945;&#24072;&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#20943;&#23569;&#65292;&#22240;&#27492;&#21487;&#20197;&#36339;&#36807;&#22788;&#29702;&#36825;&#20123;&#22359;&#25152;&#38656;&#30340;&#35745;&#31639;&#12290;&#25152;&#36873;&#30340;&#36974;&#32617;&#20301;&#32622;&#26088;&#22312;&#38450;&#27490;&#23631;&#34109;&#23398;&#29983;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#30340;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#35813;&#36974;&#32617;&#36873;&#25321;&#26426;&#21046;&#22522;&#20110;&#23398;&#29983;&#27169;&#22411;&#30340;&#26576;&#20123;&#27880;&#24847;&#21147;&#20998;&#25968;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is an effective method for training lightweight models, but it introduces a significant amount of computational overhead to the training cost, as the method requires acquiring teacher supervisions on training samples. This additional cost -- called distillation cost -- is most pronounced when we employ large-scale teacher models such as vision transformers (ViTs). We present MaskedKD, a simple yet effective strategy that can significantly reduce the cost of distilling ViTs without sacrificing the prediction accuracy of the student model. Specifically, MaskedKD diminishes the cost of running teacher at inference by masking a fraction of image patch tokens fed to the teacher, and therefore skipping the computations required to process those patches. The mask locations are selected to prevent masking away the core features of an image that the student model uses for prediction. This mask selection mechanism operates based on some attention score of the student model
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.05712</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#32508;&#36848;: &#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26679;&#26412;&#26469;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36807;&#22810;&#30340;&#26679;&#26412;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#20316;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23376;&#38598;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#12290;SSL&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#30456;&#20851;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#32508;&#36848;&#30740;&#31350;&#26469;&#35299;&#37322;&#19981;&#21516;&#30340;SSL&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#28436;&#21464;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#12289;&#19977;&#20010;&#20027;&#35201;&#36235;&#21183;&#21644;&#24453;&#35299;&#38382;&#39064;&#30340;&#35270;&#35282;&#32508;&#36848;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#22823;&#22810;&#25968;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#27010;&#36848;&#20102;SSL&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12581</link><description>&lt;p&gt;
&#36890;&#36807;Monte Carlo Forest Search&#23454;&#29616;UNSAT&#27714;&#35299;&#22120;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12581
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Monte Carlo Forest Search&#65288;MCFS&#65289;&#65292;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20915;&#31574;&#26641;MDP&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#31034;&#20363;&#21253;&#25324;&#35777;&#26126;SAT&#20844;&#24335;&#30340;&#19981;&#21487;&#28385;&#36275;&#24615;&#65307;&#35745;&#31639;&#21487;&#28385;&#36275;&#30340;SAT&#20844;&#24335;&#30340;&#35299;&#30340;&#25968;&#37327;&#65307;&#20197;&#21450;&#25214;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#12290;MCFS&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;Monte Carlo Tree Search&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#22312;&#20505;&#36873;&#26641;&#30340;&#26862;&#26519;&#20013;&#23547;&#25214;&#19968;&#20010;&#23567;&#26641;&#65292;&#32780;&#19981;&#26159;&#22312;&#26641;&#20013;&#25214;&#21040;&#19968;&#20010;&#22909;&#36335;&#24452;&#65288;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#20102;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#31216;&#20043;&#20026;Knuth Synthesis&#65292;&#36825;&#26159;&#19968;&#20010;MCFS&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;DPLL&#20998;&#25903;&#31574;&#30053;&#26469;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#12290;&#36825;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#20197;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#27744;&#20013;&#38543;&#26426;&#36873;&#25321;&#8220;&#22909;&#8221;&#30340;&#26641;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26356;&#22823;&#30340;&#26862;&#26519;&#26469;&#36880;&#27493;&#26500;&#24314;&#26862;&#26519;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21512;&#25104;MDP&#31867;&#65292;&#29992;&#20316;&#30495;&#23454;&#26641;MDP&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#33410;&#28857;&#38388;&#36716;&#25442;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
&lt;/p&gt;</description></item></channel></rss>