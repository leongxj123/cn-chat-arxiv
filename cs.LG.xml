<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2404.02545</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02545
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#39046;&#22495;&#30340;&#26032;&#30340;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24212;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#24809;&#32602;Q&#20540;&#30340;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#19981;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36825;&#30830;&#20445;&#20102;&#23433;&#20840;&#24615;&#24182;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#26420;&#32032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#22833;&#36133;&#65292;&#22240;&#20026;&#30001;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#34892;&#20026;&#24341;&#36215;&#30340;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#31639;&#27861;&#20027;&#35201;&#24809;&#32602;OOD&#34892;&#20026;&#30340;Q&#20540;&#65292;&#20854;&#32422;&#26463;&#30340;&#36136;&#37327;&#20063;&#24456;&#37325;&#35201;&#12290;&#19981;&#31934;&#30830;&#30340;&#32422;&#26463;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#32780;&#31934;&#30830;&#30340;&#32422;&#26463;&#21017;&#38656;&#35201;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#39046;&#22495;&#35745;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#26684;&#28857;&#26144;&#23556;&#20266;&#35745;&#25968;&#26041;&#27861;&#65288;GPC&#65289;&#65292;&#20197;&#36866;&#24403;&#22320;&#24809;&#32602;Q&#20540;&#24182;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26144;&#23556;&#21040;&#31163;&#25955;&#31354;&#38388;&#65292;&#24182;&#36890;&#36807;&#20266;&#35745;&#25968;&#32422;&#26463;&#23427;&#20204;&#30340;Q&#20540;&#12290;&#36825;&#26159;&#19968;&#20010;&#29702;&#35770;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02545v1 Announce Type: cross  Abstract: Offline reinforcement learning learns from a static dataset without interacting with the environment, which ensures security and thus owns a good prospect of application. However, directly applying naive reinforcement learning methods usually fails in an offline environment due to function approximation errors caused by out-of-distribution(OOD) actions. To solve this problem, existing algorithms mainly penalize the Q-value of OOD actions, the quality of whose constraints also matter. Imprecise constraints may lead to suboptimal solutions, while precise constraints require significant computational costs. In this paper, we propose a novel count-based method for continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize the Q-value appropriately and reduce the computational cost. The proposed method maps the state and action space to discrete space and constrains their Q-values through the pseudo-count. It is theoretic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#26032;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;$r$-$\ell{}$WL&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#33021;&#22815;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13749</link><description>&lt;p&gt;
Weisfeiler&#21644;Leman&#21464;&#24471;&#30127;&#29378;&#65306;&#19968;&#31181;&#26032;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13749
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#26032;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;$r$-$\ell{}$WL&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#33021;&#22815;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;$r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21516;&#26500;&#27979;&#35797;&#23618;&#27425;&#32467;&#26500;&#20197;&#21450;&#30456;&#24212;&#30340;GNN&#26694;&#26550;$r$-$\ell{}$MPNN&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35745;&#25968;&#38271;&#24230;&#26368;&#22810;&#20026;$r + 2$&#30340;&#24490;&#29615;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$r$-$\ell{}$WL&#21487;&#20197;&#35745;&#25968;&#20185;&#20154;&#25484;&#22270;&#30340;&#21516;&#24577;&#12290;&#36825;&#20005;&#26684;&#22320;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;1-WL&#65292;&#21518;&#32773;&#21482;&#33021;&#35745;&#25968;&#26641;&#30340;&#21516;&#24577;&#65292;&#23454;&#38469;&#19978;&#19982;&#20219;&#24847;&#22266;&#23450;&#30340;$k$-WL&#26159;&#19981;&#21487;&#27604;&#30340;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;$r$-$\ell{}$MPNN&#30340;&#34920;&#36798;&#21644;&#35745;&#25968;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RPaolino/loopy&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13749v1 Announce Type: new  Abstract: We introduce $r$-loopy Weisfeiler-Leman ($r$-$\ell{}$WL), a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, $r$-$\ell{}$MPNN, that can count cycles up to length $r + 2$. Most notably, we show that $r$-$\ell{}$WL can count homomorphisms of cactus graphs. This strictly extends classical 1-WL, which can only count homomorphisms of trees and, in fact, is incomparable to $k$-WL for any fixed $k$. We empirically validate the expressive and counting power of the proposed $r$-$\ell{}$MPNN on several synthetic datasets and present state-of-the-art predictive performance on various real-world datasets. The code is available at https://github.com/RPaolino/loopy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.06672</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#20174;&#32852;&#37030;&#23398;&#20064;&#20013;&#26377;&#21487;&#35777;&#26126;&#30340;&#20114;&#24800;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Provable Mutual Benefits from Federated Learning in Privacy-Sensitive Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#39046;&#22495;&#20013;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;FL&#21327;&#35758;&#65292;&#26082;&#33021;&#20445;&#35777;&#38544;&#31169;&#65292;&#21448;&#33021;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#35774;&#35745;&#20986;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#30410;&#22788;&#30340;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#25968;&#25454;&#25152;&#26377;&#32773;&#36890;&#36807;&#20174;&#24444;&#27492;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#30410;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26381;&#21153;&#22120;&#21487;&#20197;&#35774;&#35745;&#19968;&#31181;&#23545;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#26377;&#21033;&#30340;FL&#21327;&#35758;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#20984;&#38543;&#26426;&#20248;&#21270;&#32972;&#26223;&#19979;&#23384;&#22312;&#30456;&#20114;&#26377;&#21033;&#21327;&#35758;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23545;&#31216;&#38544;&#31169;&#20559;&#22909;&#19979;&#65292;&#26368;&#22823;&#21270;&#24635;&#23458;&#25143;&#25928;&#29992;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26368;&#22823;&#21270;&#26368;&#32456;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21327;&#35758;&#65292;&#24182;&#22312;&#21512;&#25104;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06672v1 Announce Type: cross  Abstract: Cross-silo federated learning (FL) allows data owners to train accurate machine learning models by benefiting from each others private datasets. Unfortunately, the model accuracy benefits of collaboration are often undermined by privacy defenses. Therefore, to incentivize client participation in privacy-sensitive domains, a FL protocol should strike a delicate balance between privacy guarantees and end-model accuracy. In this paper, we study the question of when and how a server could design a FL protocol provably beneficial for all participants. First, we provide necessary and sufficient conditions for the existence of mutually beneficial protocols in the context of mean estimation and convex stochastic optimization. We also derive protocols that maximize the total clients' utility, given symmetric privacy preferences. Finally, we design protocols maximizing end-model accuracy and demonstrate their benefits in synthetic experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#23398;&#20064;&#26497;&#38480;&#30340;&#22270;&#35770;&#37327; $\beta_M(G)$&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.18591</link><description>&lt;p&gt;
&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#19978;&#19979;&#25991;&#36172;&#21338;&#65306;&#20174;&#29420;&#31435;&#25968;&#21040;MAS&#25968;
&lt;/p&gt;
&lt;p&gt;
Stochastic contextual bandits with graph feedback: from independence number to MAS number
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21051;&#30011;&#23398;&#20064;&#26497;&#38480;&#30340;&#22270;&#35770;&#37327; $\beta_M(G)$&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#65292;&#22312;&#36825;&#31867;&#20114;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#27604;&#26222;&#36890;&#19978;&#19979;&#25991;&#36172;&#21338;&#26356;&#20016;&#23500;&#32467;&#26500;&#65292;&#20854;&#20013;&#37319;&#21462;&#19968;&#20010;&#34892;&#21160;&#23558;&#22312;&#25152;&#26377;&#24773;&#22659;&#19979;&#25581;&#31034;&#25152;&#26377;&#30456;&#37051;&#34892;&#21160;&#30340;&#22870;&#21169;&#12290;&#19982;&#22810;&#33218;&#36172;&#21338;&#35774;&#32622;&#19981;&#21516;&#65292;&#22810;&#25991;&#29486;&#24050;&#32463;&#23545;&#22270;&#21453;&#39304;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#35752;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#36172;&#21338;&#23545;&#24212;&#37096;&#20998;&#20173;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#35752;&#30340;&#22320;&#26041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#36951;&#25022;&#19979;&#38480; $\Omega(\sqrt{\beta_M(G) T})$ &#25506;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013; $M$ &#26159;&#24773;&#22659;&#25968;&#65292;$G$ &#26159;&#21453;&#39304;&#22270;&#65292;$\beta_M(G)$ &#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#24449;&#35813;&#38382;&#39064;&#31867;&#30340;&#22522;&#30784;&#23398;&#20064;&#38480;&#21046;&#30340;&#22270;&#35770;&#37327;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;$\beta_M(G)$ &#22312; $\alpha(G)$ (&#22270;&#30340;&#29420;&#31435;&#25968;) &#21644; $\mathsf{m}(G)$ (&#22270;&#30340;&#26368;&#22823;&#26080;&#29615;&#23376;&#22270;&#65288;MAS&#65289;&#25968;) &#20043;&#38388;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18591v1 Announce Type: new  Abstract: We consider contextual bandits with graph feedback, a class of interactive learning problems with richer structures than vanilla contextual bandits, where taking an action reveals the rewards for all neighboring actions in the feedback graph under all contexts. Unlike the multi-armed bandits setting where a growing literature has painted a near-complete understanding of graph feedback, much remains unexplored in the contextual bandits counterpart. In this paper, we make inroads into this inquiry by establishing a regret lower bound $\Omega(\sqrt{\beta_M(G) T})$, where $M$ is the number of contexts, $G$ is the feedback graph, and $\beta_M(G)$ is our proposed graph-theoretical quantity that characterizes the fundamental learning limit for this class of problems. Interestingly, $\beta_M(G)$ interpolates between $\alpha(G)$ (the independence number of the graph) and $\mathsf{m}(G)$ (the maximum acyclic subgraph (MAS) number of the graph) as 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07248</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#65306;&#23558;&#32500;&#24230;&#19982;&#20934;&#30830;&#24230;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Depth Separations in Neural Networks: Separating the Dimension from the Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#19968;&#20010;$\mathcal{O}(1)$-Lipschitz&#30446;&#26631;&#20989;&#25968;&#33267;&#24120;&#25968;&#31934;&#24230;&#26102;&#30340;&#25351;&#25968;&#20998;&#31163;&#65292;&#23545;&#20110;&#25903;&#25345;&#22312;$[0,1]^{d}$&#19978;&#30340;&#20998;&#24067;&#65292;&#20551;&#35774;&#26435;&#37325;&#25351;&#25968;&#26377;&#30028;&#12290;&#36825;&#35299;&#20915;&#20102;&#22312;\citet{safran2019depth}&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#30340;&#23384;&#22312;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#65292;&#23558;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#20998;&#31163;&#30340;&#19979;&#30028;&#35201;&#27714;&#33267;&#23569;&#26377;&#19968;&#20010;Lipschitz&#21442;&#25968;&#12289;&#30446;&#26631;&#20934;&#30830;&#24230;&#25110;&#36924;&#36817;&#22495;&#30340;&#22823;&#23567;&#65288;&#26576;&#31181;&#24230;&#37327;&#65289;&#19982;&#36755;&#20837;&#32500;&#24230;&#22810;&#39033;&#24335;&#22320;&#32553;&#25918;&#65292;&#32780;&#25105;&#20204;&#20445;&#25345;&#21069;&#20004;&#32773;&#19981;&#21464;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22495;&#38480;&#21046;&#22312;&#21333;&#20301;&#36229;&#31435;&#26041;&#20307;&#19978;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#36866;&#29992;&#20110;&#21508;&#31181;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#24179;&#22343;&#24773;&#20917;&#21040;&#26368;&#22351;&#24773;&#20917;&#30340;&#38543;&#26426;&#33258;&#32422;&#21270;&#35770;&#35777;&#30340;&#24212;&#29992;&#65292;&#20197;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07193</link><description>&lt;p&gt;
&#26799;&#24230;&#22122;&#22768;&#30340;&#38544;&#24615;&#20559;&#35265;&#65306;&#20174;&#23545;&#31216;&#24615;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Gradient Noise: A Symmetry Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#26102;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35828;&#26126;&#20102;SGD&#21644;&#26799;&#24230;&#19979;&#38477;&#20043;&#38388;&#30340;&#20998;&#27495;&#26159;&#22810;&#20040;&#24040;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#23545;&#31216;&#24615;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#24433;&#21709;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19968;&#26063;&#23545;&#31216;&#24615;&#20998;&#20026;&#20004;&#31867;&#12290;&#23545;&#20110;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#33258;&#28982;&#22320;&#25910;&#25947;&#21040;&#20855;&#26377;&#24179;&#34913;&#21644;&#23545;&#40784;&#26799;&#24230;&#22122;&#22768;&#30340;&#35299;&#12290;&#23545;&#20110;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#28982;&#36866;&#29992;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#26222;&#36941;&#30340;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#32780;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#32454;&#33410;&#26080;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#23545;&#20110;&#36880;&#27493;&#21464;&#24418;&#21644;&#24179;&#22374;&#21270;&#25552;&#20379;&#20102;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#34920;&#31034;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;&#30340;&#23616;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07066</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Range Queries with Correlated Input Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;&#30340;&#23616;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32447;&#24615;&#26597;&#35810;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#29305;&#21035;&#26159;&#33539;&#22260;&#26597;&#35810;&#65292;&#21033;&#29992;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#21516;&#26102;&#23454;&#29616;&#26080;&#20559;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32479;&#35745;&#36879;&#26126;&#24615;&#21644;&#23545;&#31934;&#24230;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#26080;&#35770;&#26159;&#22312;&#26576;&#20123;&#26597;&#35810;&#36793;&#32536;&#19978;&#36824;&#26159;&#22312;&#23618;&#27425;&#25968;&#25454;&#24211;&#32467;&#26500;&#25152;&#26263;&#31034;&#30340;&#31934;&#24230;&#35201;&#27714;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#20934;&#30830;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#35813;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a class of locally differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our bounds show that we obtain near-optimal utility while being empirically competitive against output perturbation methods.
&lt;/p&gt;</description></item><item><title>CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04620</link><description>&lt;p&gt;
CataractBot&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04620
&lt;/p&gt;
&lt;p&gt;
CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#34892;&#19994;&#30340;&#21457;&#23637;&#65292;&#24739;&#32773;&#36234;&#26469;&#36234;&#36861;&#27714;&#26356;&#21487;&#38752;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20449;&#24687;&#26469;&#28304;&#65292;&#20294;&#25968;&#23383;&#26102;&#20195;&#21364;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#36807;&#22810;&#19988;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#24739;&#32773;&#20027;&#35201;&#20449;&#20219;&#21307;&#29983;&#21644;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#65292;&#31361;&#26174;&#20102;&#19987;&#23478;&#35748;&#21487;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19987;&#23478;&#38754;&#20020;&#30340;&#21387;&#21147;&#23548;&#33268;&#20102;&#27807;&#36890;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CataractBot&#65292;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#19982;&#21360;&#24230;&#19968;&#23478;&#19977;&#32423;&#30524;&#31185;&#21307;&#38498;&#21512;&#20316;&#24320;&#21457;&#30340;CataractBot&#36890;&#36807;&#26597;&#35810;&#31574;&#21010;&#30340;&#30693;&#35782;&#24211;&#65292;&#21363;&#26102;&#22238;&#31572;&#30333;&#20869;&#38556;&#25163;&#26415;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#24322;&#27493;&#25552;&#20379;&#19987;&#23478;&#39564;&#35777;&#30340;&#31572;&#22797;&#12290;CataractBot&#20855;&#22791;&#22810;&#27169;&#24335;&#25903;&#25345;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#19982;49&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#65292;CataractBot&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03478</link><description>&lt;p&gt;
&#36229;&#25193;&#25955;&#65306;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#24433;&#20687;&#21644;&#22825;&#27668;&#39044;&#25253;&#65289;&#26102;&#65292;&#20934;&#30830;&#20272;&#35745;&#21644;&#21306;&#20998;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65288;&#21487;&#20197;&#36890;&#36807;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#38477;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#19982;&#24403;&#21069;&#20219;&#21153;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#20934;&#30830;&#26377;&#25928;&#22320;&#20174;&#25968;&#25454;&#38598;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#29616;&#22312;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20174;&#27010;&#24565;&#19978;&#21464;&#24471;&#31616;&#21333;&#26126;&#20102;&#65306;&#21482;&#38656;&#35201;&#35757;&#32451;&#21644;&#20174;&#19968;&#20010;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#38598;&#21512;&#20013;&#37319;&#26679;&#21363;&#21487;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#38598;&#21512;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting. Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models. Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model. Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.09980</link><description>&lt;p&gt;
&#24515;&#23460;&#20998;&#21106;&#65306;U-Net&#34893;&#29983;&#27169;&#22411;&#30340;&#31616;&#35201;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Ventricular Segmentation: A Brief Comparison of U-Net Derivatives. (arXiv:2401.09980v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#26045;&#20102;&#22810;&#20010;U-Net&#34893;&#29983;&#27169;&#22411;&#20197;&#23454;&#29616;&#23545;&#24515;&#33039;&#29305;&#23450;&#37096;&#20301;&#30340;&#20840;&#38754;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26159;&#25351;&#29992;&#20110;&#35266;&#23519;&#20154;&#20307;&#21450;&#20854;&#20869;&#37096;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20197;&#35786;&#26029;&#12289;&#30417;&#27979;&#29978;&#33267;&#27835;&#30103;&#21307;&#23398;&#30142;&#30149;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24515;&#33039;&#30701;&#36724;&#30913;&#20849;&#25391;&#25104;&#20687;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#25552;&#39640;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#21307;&#23398;&#30142;&#30149;&#30340;&#35786;&#26029;&#12289;&#30417;&#27979;&#21644;&#27835;&#30103;&#12290;&#37325;&#28857;&#26159;&#23454;&#26045;&#21508;&#31181;U-Net&#30340;&#34893;&#29983;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#22320;&#20998;&#31163;&#24515;&#33039;&#30340;&#29305;&#23450;&#37096;&#20998;&#65292;&#36827;&#34892;&#20840;&#38754;&#30340;&#35299;&#21078;&#21644;&#21151;&#33021;&#20998;&#26512;&#12290;&#36890;&#36807;&#22270;&#20687;&#12289;&#22270;&#34920;&#21644;&#23450;&#37327;&#25351;&#26631;&#30340;&#32452;&#21512;&#23637;&#31034;&#20102;&#27169;&#22411;&#21450;&#20854;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#25913;&#36827;&#30340;&#31574;&#30053;&#12290;&#26412;&#25688;&#35201;&#31616;&#35201;&#27010;&#36848;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24515;&#33039;&#22270;&#20687;&#20998;&#21106;&#30340;&#24037;&#20316;&#65292;&#24378;&#35843;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging refers to the technologies and methods utilized to view the human body and its inside, in order to diagnose, monitor, or even treat medical disorders. This paper aims to explore the application of deep learning techniques in the semantic segmentation of Cardiac short-axis MRI (Magnetic Resonance Imaging) images, aiming to enhance the diagnosis, monitoring, and treatment of medical disorders related to the heart. The focus centers on implementing various architectures that are derivatives of U-Net, to effectively isolate specific parts of the heart for comprehensive anatomical and functional analysis. Through a combination of images, graphs, and quantitative metrics, the efficacy of the models and their predictions are showcased. Additionally, this paper addresses encountered challenges and outline strategies for future improvements. This abstract provides a concise overview of the efforts in utilizing deep learning for cardiac image segmentation, emphasizing both the ac
&lt;/p&gt;</description></item><item><title>MeTA&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#36866;&#24212;&#22810;&#20010;&#28304;&#27169;&#22411;&#21040;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2401.02561</link><description>&lt;p&gt;
MeTA: &#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MeTA: Multi-source Test Time Adaptation. (arXiv:2401.02561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02561
&lt;/p&gt;
&lt;p&gt;
MeTA&#26159;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#36866;&#24212;&#22810;&#20010;&#28304;&#27169;&#22411;&#21040;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#36866;&#24212;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#36807;&#31243;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#36866;&#24212;&#21040;&#27599;&#20010;&#36827;&#20837;&#30340;&#27979;&#35797;&#25968;&#25454;&#25209;&#27425;&#20013;&#65288;&#21363;&#65292;&#26080;&#38656;&#22823;&#37327;&#30340;&#27979;&#35797;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20687;&#20256;&#32479;&#39046;&#22495;&#36866;&#24212;&#20013;&#37027;&#26679;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#26435;&#38480;&#12290;&#30001;&#20110;&#23427;&#19982;&#27599;&#20010;&#27979;&#35797;&#25968;&#25454;&#25209;&#27425;&#19968;&#36215;&#24037;&#20316;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#21512;&#38656;&#35201;&#22312;&#25968;&#25454;&#27969;&#20837;&#26102;&#36827;&#34892;&#20915;&#31574;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;&#24403;&#21069;&#30340;&#27979;&#35797;&#26102;&#36866;&#24212;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#28304;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#22810;&#28304;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;MeTA&#65289;&#26694;&#26550;&#65292;&#23427;&#22788;&#29702;&#22810;&#20010;&#28304;&#27169;&#22411;&#24182;&#23558;&#23427;&#20204;&#26368;&#20339;&#32452;&#21512;&#20197;&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#12290;MeTA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#12290;&#39318;&#20808;&#65292;&#23427;&#26377;&#25928;&#22320;&#33719;&#24471;&#26368;&#20339;&#32452;&#21512;&#26435;&#37325;&#65292;&#20197;&#23558;&#28304;&#27169;&#22411;&#32452;&#21512;&#20197;&#36866;&#24212;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;&#23427;&#30830;&#23450;&#35201;&#26356;&#26032;&#21738;&#20010;&#28304;&#27169;&#22411;&#21442;&#25968;&#65292;&#20197;&#20415;&#21482;&#26356;&#26032;&#19982;&#27979;&#35797;&#25968;&#25454;&#26368;&#30456;&#20851;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test time adaptation is the process of adapting, in an unsupervised manner, a pre-trained source model to each incoming batch of the test data (i.e., without requiring a substantial portion of the test data to be available, as in traditional domain adaptation) and without access to the source data. Since it works with each batch of test data, it is well-suited for dynamic environments where decisions need to be made as the data is streaming in. Current test time adaptation methods are primarily focused on a single source model. We propose the first completely unsupervised Multi-source Test Time Adaptation (MeTA) framework that handles multiple source models and optimally combines them to adapt to the test data. MeTA has two distinguishing features. First, it efficiently obtains the optimal combination weights to combine the source models to adapt to the test data distribution. Second, it identifies which of the source model parameters to update so that only the model which is most corr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;</title><link>http://arxiv.org/abs/2401.00828</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#37327;&#23376;&#22330;&#35770;&#22810;&#26684;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Lattice Sampling of Quantum Field Theories via Neural Operator-based Flows. (arXiv:2401.00828v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31639;&#23376;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#22330;&#35770;&#20013;&#20174;&#24213;&#23618;&#33258;&#30001;&#29702;&#35770;&#21040;&#30446;&#26631;&#29702;&#35770;&#30340;&#31163;&#25955;-&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20174;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20013;&#37319;&#26679;&#31163;&#25955;&#22330;&#37197;&#32622;$\phi$&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;$S$&#26159;&#26576;&#20010;&#37327;&#23376;&#22330;&#35770;&#36830;&#32493;&#27431;&#20960;&#37324;&#24471;&#20316;&#29992;$\mathcal S$&#30340;&#26684;&#28857;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#23558;&#35813;&#23494;&#24230;&#36817;&#20284;&#35270;&#20026;&#24213;&#23618;&#20989;&#25968;&#23494;&#24230;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#30340;&#23398;&#20064;&#31639;&#23376;&#23454;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#26102;&#38388;&#30456;&#20851;&#31639;&#23376;$\mathcal V_t$&#30340;&#26041;&#27861;&#65292;&#20854;&#26102;&#38388;&#31215;&#20998;&#25552;&#20379;&#20102;&#33258;&#30001;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z_0^{-1}e^{-\mathcal S_{0}[\phi(x)]}$&#30340;&#20989;&#25968;&#20998;&#24067;&#19982;&#30446;&#26631;&#29702;&#35770;$[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#24403;&#36873;&#25321;&#29305;&#23450;&#30340;&#26684;&#28857;&#26102;&#65292;&#31639;&#23376;$\mathcal V_t$&#21487;&#20197;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#32500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30690;&#37327;&#22330;$V_t$&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#26684;&#28857;&#19978;&#23454;&#29616;&#20102;&#36830;&#32493;&#30340;&#24402;&#19968;&#21270;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling discrete field configurations $\phi$ from the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the lattice-discretization of the continuous Euclidean action $\mathcal S$ of some quantum field theory. Since such densities arise as the approximation of the underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal S[\phi(x)]}$, we frame the task as an instance of operator learning. In particular, we propose to approximate a time-dependent operator $\mathcal V_t$ whose time integral provides a mapping between the functional distributions of the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the operator $\mathcal V_t$ can be discretized to a finite dimensional, time-dependent vector field $V_t$ which in turn induces a continuous normalizing flow between fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01968</link><description>&lt;p&gt;
&#26465;&#20214;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Latent Diffusion Model for Conditional Reservoir Facies Generation. (arXiv:2311.01968v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#20648;&#23618;&#30456;&#29983;&#25104;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#20445;&#30041;&#26465;&#20214;&#25968;&#25454;&#65292;&#29983;&#25104;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#12290;&#23427;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27833;&#27668;&#39046;&#22495;&#30340;&#30000;&#22320;&#24320;&#21457;&#21644;&#20648;&#23618;&#31649;&#29702;&#20013;&#65292;&#22522;&#20110;&#26377;&#38480;&#27979;&#37327;&#25968;&#25454;&#21019;&#24314;&#20934;&#30830;&#19988;&#22320;&#36136;&#30495;&#23454;&#30340;&#20648;&#23618;&#30456;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#20004;&#28857;&#22320;&#36136;&#32479;&#35745;&#26041;&#27861;&#34429;&#28982;&#22522;&#30784;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#30340;&#22320;&#36136;&#27169;&#24335;&#12290;&#22810;&#28857;&#32479;&#35745;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#20294;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#38543;&#30528;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#20852;&#36215;&#21644;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#20154;&#20204;&#24320;&#22987;&#20542;&#21521;&#20110;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30456;&#36739;&#20110;GANs&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#26465;&#20214;&#19979;&#30340;&#20648;&#23618;&#30456;&#29983;&#25104;&#12290;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#20648;&#23618;&#30456;&#65292;&#20005;&#26684;&#20445;&#30041;&#20102;&#26465;&#20214;&#25968;&#25454;&#12290;&#23427;&#26126;&#26174;&#20248;&#20110;&#22522;&#20110;GANs&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating accurate and geologically realistic reservoir facies based on limited measurements is crucial for field development and reservoir management, especially in the oil and gas sector. Traditional two-point geostatistics, while foundational, often struggle to capture complex geological patterns. Multi-point statistics offers more flexibility, but comes with its own challenges. With the rise of Generative Adversarial Networks (GANs) and their success in various fields, there has been a shift towards using them for facies generation. However, recent advances in the computer vision domain have shown the superiority of diffusion models over GANs. Motivated by this, a novel Latent Diffusion Model is proposed, which is specifically designed for conditional generation of reservoir facies. The proposed model produces high-fidelity facies realizations that rigorously preserve conditioning data. It significantly outperforms a GAN-based alternative.
&lt;/p&gt;</description></item><item><title>&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13040</link><description>&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20855;&#26377;&#24322;&#24120;&#29305;&#24449;&#24182;&#32534;&#30721;&#26356;&#22810;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Robust multimodal models have outlier features and encode more concepts. (arXiv:2310.13040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13040
&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23637;&#31034;&#20102;&#24322;&#24120;&#29305;&#24449;&#21644;&#26356;&#22810;&#27010;&#24565;&#30340;&#32534;&#30721;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#21306;&#20998;&#20581;&#22766;&#27169;&#22411;&#19982;&#38750;&#20581;&#22766;&#27169;&#22411;&#65311;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#21464;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20581;&#22766;&#24615;&#12290;&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;&#20581;&#22766;&#24615;&#30340;&#24046;&#24322;&#21487;&#20197;&#36861;&#28335;&#21040;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36824;&#19981;&#28165;&#26970;&#36825;&#23545;&#20110;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#20160;&#20040;&#24847;&#21619;&#30528;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;12&#20010;&#20855;&#26377;&#19981;&#21516;&#39592;&#24178;&#65288;ResNets&#21644;ViTs&#65289;&#21644;&#39044;&#35757;&#32451;&#38598;&#65288;OpenAI&#65292;LAION-400M&#65292;LAION-2B&#65292;YFCC15M&#65292;CC12M&#21644;DataComp&#65289;&#30340;&#20581;&#22766;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#23384;&#22312;&#20004;&#20010;&#20581;&#22766;&#24615;&#30340;&#29305;&#24449;&#65306;&#65288;1&#65289;&#20581;&#22766;&#27169;&#22411;&#20855;&#26377;&#30001;&#20854;&#28608;&#27963;&#29305;&#24449;&#34920;&#24449;&#30340;&#24322;&#24120;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#29305;&#24449;&#20540;&#27604;&#24179;&#22343;&#20540;&#39640;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#24322;&#24120;&#29305;&#24449;&#22312;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#29305;&#26435;&#26041;&#21521;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
What distinguishes robust models from non-robust ones? This question has gained traction with the appearance of large-scale multimodal models, such as CLIP. These models have demonstrated unprecedented robustness with respect to natural distribution shifts. While it has been shown that such differences in robustness can be traced back to differences in training data, so far it is not known what that translates to in terms of what the model has learned. In this work, we bridge this gap by probing the representation spaces of 12 robust multimodal models with various backbones (ResNets and ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12M and DataComp). We find two signatures of robustness in the representation spaces of these models: (1) Robust models exhibit outlier features characterized by their activations, with some being several orders of magnitude above average. These outlier features induce privileged directions in the model's representation space. We demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2308.12970</link><description>&lt;p&gt;
NeuralClothSim: &#31070;&#32463;&#21464;&#24418;&#22330;&#19982;Kirchhoff-Love&#34180;&#22771;&#29702;&#35770;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin Shell Theory. (arXiv:2308.12970v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861; NeuralClothSim&#65292;&#20351;&#29992;&#34180;&#22771;&#29702;&#35770;&#21644;&#31070;&#32463;&#21464;&#24418;&#22330;&#36827;&#34892;&#34920;&#38754;&#28436;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25361;&#25112;&#65292;&#20026;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#26009;&#27169;&#25311;&#26159;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25991;&#29486;&#20013;&#26377;&#22823;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#24067;&#26009;&#27169;&#25311;&#22120;&#20135;&#29983;&#31526;&#21512;&#19981;&#21516;&#31867;&#22411;&#36793;&#30028;&#26465;&#20214;&#30340;&#36924;&#30495;&#24067;&#26009;&#21464;&#24418;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#22312;&#20960;&#20010;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65306;&#23427;&#20204;&#22312;&#20855;&#26377;&#22266;&#23450;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#26174;&#24335;&#34920;&#38754;&#34920;&#31034;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#25191;&#34892;&#19968;&#31995;&#21015;&#31163;&#25955;&#21270;&#30340;&#26356;&#26032;&#65288;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#29616;&#26377;&#30340;&#27714;&#35299;&#22120;&#36827;&#34892;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#24182;&#19981;&#30452;&#35266;&#65292;&#36825;&#22312;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#20195;&#31070;&#32463;&#26550;&#26500;&#20013;&#26102;&#36896;&#25104;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#19978;&#36848;&#38480;&#21046;&#65292;&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#20197;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#35270;&#35282;&#26469;&#32771;&#34385;&#29289;&#29702;&#21512;&#29702;&#30340;&#24067;&#26009;&#27169;&#25311;&#65292;&#24182;&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralClothSim&#65292;&#21363;&#19968;&#31181;&#20351;&#29992;&#34180;&#22771;&#30340;&#26032;&#24067;&#26009;&#27169;&#25311;&#26041;&#27861;&#65292;&#20854;&#20013;&#34920;&#38754;&#28436;&#21270;&#36890;&#36807;&#31070;&#32463;&#21464;&#24418;&#22330;&#31561;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloth simulation is an extensively studied problem, with a plethora of solutions available in computer graphics literature. Existing cloth simulators produce realistic cloth deformations that obey different types of boundary conditions. Nevertheless, their operational principle remains limited in several ways: They operate on explicit surface representations with a fixed spatial resolution, perform a series of discretised updates (which bounds their temporal resolution), and require comparably large amounts of storage. Moreover, back-propagating gradients through the existing solvers is often not straightforward, which poses additional challenges when integrating them into modern neural architectures. In response to the limitations mentioned above, this paper takes a fundamentally different perspective on physically-plausible cloth simulation and re-thinks this long-standing problem: We propose NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in which surface ev
&lt;/p&gt;</description></item><item><title>&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10869</link><description>&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#19982;&#20851;&#31995;-&#26102;&#38388;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10869
&lt;/p&gt;
&lt;p&gt;
&#20113;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#35782;&#21035;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#29420;&#31435;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#30340;&#24322;&#24120;&#26080;&#27861;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#38382;&#39064;&#22312;&#22823;&#35268;&#27169;&#20113;&#26381;&#21153;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#21487;&#33021;&#23548;&#33268;&#24040;&#39069;&#25910;&#20837;&#25439;&#22833;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#26381;&#21153;&#30417;&#25511;&#25351;&#26631;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#23450;&#20301;&#36825;&#20123;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#37492;&#20110;&#29616;&#20195;&#20113;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#65292;&#36825;&#39033;&#20219;&#21153;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#21487;&#33021;&#38656;&#35201;&#36229;&#20986;&#20010;&#20154;&#33021;&#21147;&#30340;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#21644;&#36164;&#28304;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#27599;&#20010;&#25351;&#26631;&#29420;&#31435;&#22320;&#26816;&#27979;&#24322;&#24120;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#38590;&#20197;&#30001;&#24037;&#31243;&#24072;&#25163;&#21160;&#35786;&#26029;&#30340;&#21387;&#20498;&#24615;&#35686;&#25253;&#39118;&#26292;&#12290;&#20026;&#20102;&#36861;&#27714;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#20165;&#24212;&#32771;&#34385;&#25351;&#26631;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#36824;&#24212;&#32771;&#34385;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#22810;&#21464;&#37327;&#25351;&#26631;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#22312;&#26126;&#30830;&#25552;&#21462;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#26631;&#35760;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies m
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.05872</link><description>&lt;p&gt;
&#22312;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#23398;&#20064;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate and Collaborate in a Competitive Multi-Agent Setup to Clean the Ocean from Macroplastics. (arXiv:2304.05872v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#12289;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24335;&#28023;&#27915;&#24223;&#24323;&#29289;&#28165;&#29702;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#20351;&#24471;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21487;&#20197;&#21327;&#20316;&#31454;&#20105;&#24182;&#23454;&#29616;&#25910;&#38598;&#24223;&#24323;&#29289;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24314;&#31435;&#22312;&#19968;&#20010;&#39640;&#24433;&#21709;&#38382;&#39064;&#19978;&#65292;&#36890;&#36807;&#23545;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#30340;&#25910;&#38598;&#23454;&#29616;&#20102;&#21327;&#20316;&#19982;&#31454;&#20105;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#23427;&#22686;&#21152;&#20102;&#20195;&#29702;&#30340;&#35266;&#23519;&#31354;&#38388;&#12290;&#22312;&#25105;&#20204;&#33258;&#23450;&#20041;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#25511;&#21046;&#30528;&#25910;&#38598;&#22609;&#26009;&#30340;&#33337;&#21482;&#12290;&#36825;&#31181;&#36890;&#20449;&#26426;&#21046;&#20351;&#20195;&#29702;&#33021;&#22815;&#20351;&#29992;&#20108;&#36827;&#21046;&#20449;&#21495;&#26469;&#24320;&#21457;&#36890;&#20449;&#21327;&#35758;&#12290;&#34429;&#28982;&#20195;&#29702;&#30340;&#38598;&#20307;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#28165;&#29702;&#28023;&#27915;&#24223;&#24323;&#22609;&#26009;&#65292;&#20294;&#20195;&#29702;&#20250;&#22240;&#20010;&#20154;&#25910;&#38598;&#21040;&#30340;&#24223;&#24323;&#22609;&#26009;&#25968;&#37327;&#32780;&#33719;&#24471;&#22870;&#21169;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24517;&#39035;&#23398;&#20250;&#26377;&#25928;&#22320;&#27807;&#36890;&#24182;&#20445;&#25345;&#31454;&#20105;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a balance between collaboration and competition is crucial for artificial agents in many real-world applications. We investigate this using a Multi-Agent Reinforcement Learning (MARL) setup on the back of a high-impact problem. The accumulation and yearly growth of plastic in the ocean cause irreparable damage to many aspects of oceanic health and the marina system. To prevent further damage, we need to find ways to reduce macroplastics from known plastic patches in the ocean. Here we propose a Graph Neural Network (GNN) based communication mechanism that increases the agents' observation space. In our custom environment, agents control a plastic collecting vessel. The communication mechanism enables agents to develop a communication protocol using a binary signal. While the goal of the agent collective is to clean up as much as possible, agents are rewarded for the individual amount of macroplastics collected. Hence agents have to learn to communicate effectively while maintai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.12094</link><description>&lt;p&gt;
&#35780;&#20272;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating explainability for machine learning predictions using model-agnostic metrics. (arXiv:2302.12094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#22411;&#26080;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#23558;&#21508;&#20010;&#35299;&#37322;&#33021;&#21147;&#26041;&#38754;&#24635;&#32467;&#25104;&#26631;&#37327;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#31649;&#29702;&#21644;&#30417;&#31649;&#26041;&#38754;&#30340;&#20247;&#22810;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#34892;&#19994;&#21644;&#39046;&#22495;&#65292;&#20915;&#31574;&#32773;&#38656;&#20840;&#38754;&#32454;&#33268;&#22320;&#20102;&#35299;&#36825;&#20123;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#36825;&#20010;&#38656;&#27714;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#33021;&#22815;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20449;&#20219;&#24230;&#20197;&#21450;&#24110;&#21161;&#27169;&#22411;&#22312;&#36947;&#24503;&#19978;&#36827;&#34892;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#37327;&#21270;AI&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#20854;&#29305;&#24449;&#36827;&#34892;&#26131;&#20110;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#23558;&#35299;&#37322;&#33021;&#21147;&#30340;&#19981;&#21516;&#26041;&#38754;&#24635;&#32467;&#20026;&#26631;&#37327;&#65292;&#25552;&#20379;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#20419;&#36827;&#20915;&#31574;&#32773;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid advancements in artificial intelligence (AI) technology have brought about a plethora of new challenges in terms of governance and regulation. AI systems are being integrated into various industries and sectors, creating a demand from decision-makers to possess a comprehensive and nuanced understanding of the capabilities and limitations of these systems. One critical aspect of this demand is the ability to explain the results of machine learning models, which is crucial to promoting transparency and trust in AI systems, as well as fundamental in helping machine learning models to be trained ethically. In this paper, we present novel metrics to quantify the degree of which AI model predictions can be easily explainable by its features. Our metrics summarize different aspects of explainability into scalars, providing a more comprehensive understanding of model predictions and facilitating communication between decision-makers and stakeholders, thereby increasing the overall transp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2110.14427</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#28176;&#36817;&#32479;&#35745;&#30340;ODE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;$$\theta_{n+1}=\theta_n+\alpha_{n+1}f(\theta_n, \Phi_{n+1})$$&#20854;&#20013;$\Phi$&#26159;&#19968;&#20010;&#22312;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;$\textsf{X}$&#19978;&#20855;&#26377;&#24179;&#31283;&#20998;&#24067;$\pi$&#30340;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;$f&#65306;\Re^d\times\textsf{X}\to\Re^d$&#12290;&#22312;&#31216;&#20026;&#65288;DV3&#65289;&#30340;Donsker-Varadhan Lyapunov&#28418;&#31227;&#26465;&#20214;&#30340;&#19968;&#31181;&#29256;&#26412;&#21644;&#23545;&#20855;&#26377;&#21521;&#37327;&#22330;$\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$&#20197;&#21450;$\Phi\sim\pi$&#30340;&#22343;&#20540;&#27969;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20027;&#35201;&#32467;&#26524;&#12290;(i) $\{\theta_n\}$&#20197;&#27010;&#29575;1&#21644;$L_4$&#25910;&#25947;&#20110;$\bar{f}(\theta)$&#30340;&#21807;&#19968;&#26681;$\theta^*$&#12290;(ii) &#24314;&#31435;&#20102;&#27867;&#20989;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#35823;&#24046;&#19968;&#32500;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;(iii) &#23545;&#20110;&#24402;&#19968;&#21270;&#29256;&#26412;$z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$&#30340;&#24179;&#22343;&#21442;&#25968;$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$ &#65292;&#22312;&#27493;&#38271;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ in which $\Phi$ is a geometrically ergodic Markov chain on a general state space $\textsf{X}$ with stationary distribution $\pi$, and $f:\Re^d\times\textsf{X}\to\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$, with $\Phi\sim\pi$.  (i) $\{ \theta_n\}$ is convergent a.s. and in $L_4$ to the unique root $\theta^*$ of $\bar{f}(\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$, of the averaged parameters, $\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard assumptions on the step-s
&lt;/p&gt;</description></item></channel></rss>