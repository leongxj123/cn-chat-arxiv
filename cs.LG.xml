<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.09303</link><description>&lt;p&gt;
&#29992;&#29702;&#35770;&#35270;&#35282;&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09303
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20174;&#29702;&#35770;&#35282;&#24230;&#20026;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#20013;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#37325;&#24314;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#20165;&#20351;&#29992;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#35782;&#21035;&#24322;&#24120;&#21457;&#29616;&#65292;&#23545;&#20581;&#24247;&#31579;&#26597;&#21644;&#35782;&#21035;&#32597;&#35265;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#33258;&#32534;&#30721;&#22120;&#65288;AEs&#65289;&#30340;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#23427;&#20204;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#24037;&#20316;&#65306;&#20165;&#20351;&#29992;&#27491;&#24120;&#25968;&#25454;&#35757;&#32451;&#30340;AEs&#19981;&#33021;&#24456;&#22909;&#22320;&#37325;&#24314;&#30475;&#19981;&#35265;&#30340;&#24322;&#24120;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#37325;&#24314;&#38169;&#35823;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#24314;&#35757;&#32451;&#30446;&#26631;&#19982;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#36825;&#19968;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#19981;&#22815;&#21512;&#29702;&#12290;&#35813;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#22522;&#20110;AE&#30340;&#37325;&#24314;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#24182;&#25581;&#31034;&#20102;&#25913;&#36827;AE&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#22312;&#20110;&#26368;&#23567;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
&lt;/p&gt;</description></item><item><title>MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.08040</link><description>&lt;p&gt;
MicroT&#65306;&#29992;&#20110;MCUs&#30340;&#20302;&#33021;&#32791;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MicroT: Low-Energy and Adaptive Models for MCUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08040
&lt;/p&gt;
&lt;p&gt;
MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MicroT&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;MCUs&#30340;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#27169;&#22411;&#21010;&#20998;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#36890;&#36807;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#32852;&#21512;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#37096;&#20998;&#27169;&#22411;&#21644;&#23436;&#25972;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22312;MCUs&#19978;&#65292;&#22686;&#21152;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25191;&#34892;&#20851;&#33410;&#25512;&#29702;&#30340;&#38454;&#27573;&#20915;&#31574;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#37096;&#20998;&#27169;&#22411;&#26368;&#21021;&#22788;&#29702;&#26679;&#26412;&#65292;&#22914;&#26524;&#32622;&#20449;&#24230;&#24471;&#20998;&#20302;&#20110;&#35774;&#23450;&#30340;&#38408;&#20540;&#65292;&#23436;&#25972;&#27169;&#22411;&#23558;&#24674;&#22797;&#24182;&#32487;&#32493;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#12289;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;MCU&#26495;&#19978;&#35780;&#20272;&#20102;MicroT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#26412;&#22320;&#20219;&#21153;&#26102;&#65292;MicroT&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#33021;&#32791;&#12290;&#19982;&#26410;&#32463;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30456;&#27604;&#65292;MicroT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05171</link><description>&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#20811;&#26381;&#20102;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Reward Overoptimization via Adversarial Policy Optimization with Lightweight Uncertainty Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37327;&#21270;&#22870;&#21169;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#65292;&#20174;&#32780;&#26377;&#25928;&#32531;&#35299;&#20102;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#25239;&#31574;&#30053;&#20248;&#21270;&#65288;AdvPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;AdvPO&#22260;&#32469;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#21306;&#38388;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#24067;&#40065;&#26834;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#25913;&#36827;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;Anthropic HH&#21644;TL;DR&#25688;&#35201;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AdvPO&#22312;&#20943;&#36731;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05171v1 Announce Type: cross  Abstract: We introduce Adversarial Policy Optimization (AdvPO), a novel solution to the pervasive issue of reward over-optimization in Reinforcement Learning from Human Feedback (RLHF) for Large Language Models (LLMs). Over-optimization occurs when a reward model serves as an imperfect proxy for human preference, and RL-driven policy optimization erroneously exploits reward inaccuracies. In this paper, we begin by introducing a lightweight way to quantify uncertainties in rewards, relying solely on the last layer embeddings of the reward model, without the need for computationally expensive reward ensembles. AdvPO then addresses a distributionally robust optimization problem centred around the confidence interval of the reward model's predictions for policy improvement. Through comprehensive experiments on the Anthropic HH and TL;DR summarization datasets, we illustrate the efficacy of AdvPO in mitigating the overoptimization issue, consequently
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15020</link><description>&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20581;&#22766;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistically-sound beam search with masked language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#26463;&#25628;&#32034;&#23384;&#22312;&#25361;&#25112;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24207;&#21015;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;readily available&#12290;&#28982;&#32780;&#65292;&#20272;&#31639;&#36825;&#26679;&#30340;&#20998;&#24067;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#65292;&#21253;&#25324;&#34507;&#30333;&#24037;&#31243;&#21644;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20351;&#29992;MLMs&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20351;&#29992;&#26631;&#20934;&#26463;&#25628;&#32034;&#23545;MLMs&#25191;&#34892;&#25991;&#26412;&#22635;&#20805;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#24403;&#36825;&#20123;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20462;&#25913;&#65292;&#32780;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#39044;&#26399;&#26465;&#20214;&#19979;&#23427;&#20248;&#20110;&#21069;&#36848;&#30340;&#26463;&#25628;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27604;&#36739;&#22810;&#20010;&#39046;&#22495;&#20013;&#20960;&#31181;&#20351;&#29992;MLMs&#36827;&#34892;&#22635;&#20805;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.14982</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#22312;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#23637;&#29616;&#20986;&#19981;&#21516;&#27169;&#24335;&#65306;&#21021;&#27493;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Human Brain Exhibits Distinct Patterns When Listening to Fake Versus Real Audio: Preliminary Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14982
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#23545;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26377;&#19981;&#21516;&#30340;&#21453;&#24212;&#27169;&#24335;&#65292;&#19982;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#19981;&#21516;&#65292;&#36825;&#20026;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#21548;&#21462;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#26102;&#22823;&#33041;&#27963;&#21160;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31639;&#27861;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#24182;&#27809;&#26377;&#26174;&#31034;&#20986;&#30495;&#23454;&#21644;&#34394;&#20551;&#38899;&#39057;&#20043;&#38388;&#30340;&#28165;&#26224;&#19981;&#21516;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#65292;&#36890;&#36807; EEG &#27979;&#37327;&#65292;&#22312;&#20010;&#20307;&#25509;&#35302;&#34394;&#20551;&#19982;&#30495;&#23454;&#38899;&#39057;&#26102;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#36825;&#20123;&#21021;&#27493;&#35777;&#25454;&#20026;&#26410;&#26469;&#22312;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#26816;&#27979;&#31561;&#39046;&#22495;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14982v1 Announce Type: cross  Abstract: In this paper we study the variations in human brain activity when listening to real and fake audio. Our preliminary results suggest that the representations learned by a state-of-the-art deepfake audio detection algorithm, do not exhibit clear distinct patterns between real and fake audio. In contrast, human brain activity, as measured by EEG, displays distinct patterns when individuals are exposed to fake versus real audio. This preliminary evidence enables future research directions in areas such as deepfake audio detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12072</link><description>&lt;p&gt;
&#21464;&#20998;&#26041;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25506;&#32034;&#65306;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#22312;&#20110;&#28857;&#20272;&#35745;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#23376;&#31354;&#38388;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35797;&#22270;&#27010;&#36848;&#20351;&#29992;&#21464;&#20998;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#25104;&#20687;&#20013;&#36870;&#38382;&#39064;&#30340;&#24403;&#21069;&#26041;&#27861;&#12290;&#37325;&#28857;&#20851;&#27880;&#28857;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#25239;&#24615;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#32500;&#31034;&#20363;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#24182;&#22312;&#32463;&#39564;&#19978;&#39564;&#35777;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#35813;&#32508;&#36848;&#30340;&#21478;&#19968;&#20010;&#37325;&#28857;&#26159;&#36890;&#36807;&#26126;&#30830;&#25351;&#23548;&#26469;&#25506;&#32034;&#25968;&#25454;&#19968;&#33268;&#35299;&#30340;&#23376;&#31354;&#38388;&#65292;&#20197;&#28385;&#36275;&#29305;&#23450;&#35821;&#20041;&#25110;&#32441;&#29702;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12072v1 Announce Type: cross  Abstract: This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65292;&#24182;&#25581;&#31034;&#20102;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#22240;&#32032;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#65292;&#26368;&#32456;&#20351;&#24471;&#32463;&#39564;&#37325;&#25918;&#21487;&#24212;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10374</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Revisiting Experience Replayable Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65292;&#24182;&#25581;&#31034;&#20102;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#22240;&#32032;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#65292;&#26368;&#32456;&#20351;&#24471;&#32463;&#39564;&#37325;&#25918;&#21487;&#24212;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#32463;&#39564;&#37325;&#25918;&#65288;ER&#65289;&#34987;&#35748;&#20026;&#20165;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#24773;&#20917;&#19979;&#23558;ER&#24212;&#29992;&#20110;&#22312;&#31574;&#30053;&#31639;&#27861;&#20013;&#65292;&#36825;&#34920;&#26126;&#31163;&#31574;&#30053;&#24615;&#21487;&#33021;&#26159;&#24212;&#29992;ER&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65288;ERC&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#28385;&#36275;ERC&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#20551;&#35774;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;ERC&#30340;&#20851;&#38190;&#12290;&#20174;&#24230;&#37327;&#23398;&#20064;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#19981;&#31283;&#23450;&#22240;&#32032;&#65292;&#21253;&#25324;i&#65289;&#26469;&#33258;&#36127;&#26679;&#26412;&#30340;&#25490;&#26021;&#21147;&#21644;ii&#65289;&#19981;&#24403;&#32463;&#39564;&#30340;&#37325;&#25918;&#12290;&#22240;&#27492;&#65292;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#65292;&#25152;&#25552;&#20986;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#20351;&#24471;ER&#36866;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#19968;&#31181;&#22312;&#31574;&#30053;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20854;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict "experience replayable conditions" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning 
&lt;/p&gt;</description></item><item><title>EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.09288</link><description>&lt;p&gt;
EcoVal:&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EcoVal: An Efficient Data Valuation Framework for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09288
&lt;/p&gt;
&lt;p&gt;
EcoVal&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#65292;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23454;&#29992;&#22320;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20013;&#37327;&#21270;&#25968;&#25454;&#30340;&#20215;&#20540;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#20513;&#35758;&#20013;&#20570;&#20986;&#26356;&#20855;&#25112;&#30053;&#24847;&#20041;&#30340;&#20915;&#31574;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Shapley&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;&#22312;&#35745;&#31639;&#26041;&#38754;&#38750;&#24120;&#26114;&#36149;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#27169;&#22411;&#25165;&#33021;&#33719;&#24471;Shapley&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26694;&#26550;EcoVal&#65292;&#20197;&#24555;&#36895;&#23454;&#29992;&#30340;&#26041;&#24335;&#20272;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#19981;&#30452;&#25509;&#22788;&#29702;&#29420;&#31435;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;&#26159;&#30830;&#23450;&#31867;&#20284;&#30340;&#25968;&#25454;&#28857;&#31751;&#30340;&#20215;&#20540;&#12290;&#36825;&#20010;&#20215;&#20540;&#36827;&#19968;&#27493;&#22312;&#25152;&#26377;&#25104;&#21592;&#31751;&#28857;&#20043;&#38388;&#20256;&#25773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#25968;&#25454;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#20215;&#20540;&#26469;&#30830;&#23450;&#25972;&#20307;&#25968;&#25454;&#20215;&#20540;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#24314;&#27169;&#20026;&#8220;&#29983;&#20135;&#20989;&#25968;&#8221;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.15022</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based analysis of glioma tissue sections: a review. (arXiv:2401.15022v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15022
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#20998;&#26512;&#20013;&#20855;&#26377;&#35786;&#26029;&#21644;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#24403;&#21069;&#30740;&#31350;&#32858;&#28966;&#20110;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#65292;&#20197;&#21450;&#23545;&#35813;&#30142;&#30149;&#30340;&#20998;&#31867;&#12289;&#20998;&#32423;&#12289;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#21644;&#29983;&#23384;&#39044;&#27979;&#31561;&#20020;&#24202;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33014;&#36136;&#30244;&#30340;&#35786;&#26029;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#20351;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#33014;&#36136;&#30244;&#32452;&#32455;&#36827;&#34892;&#32452;&#32455;&#23398;&#35780;&#20272;&#65292;&#20026;&#35786;&#26029;&#21644;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#29616;&#29366;&#36827;&#34892;&#27010;&#36848;&#65292;&#26412;&#32508;&#36848;&#23545;70&#20010;&#20844;&#24320;&#21487;&#24471;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#35770;&#25991;&#20851;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#26579;&#33394;&#30340;&#33014;&#36136;&#30244;&#32452;&#32455;&#20999;&#29255;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#65288;16/70&#65289;&#65292;&#20998;&#32423;&#65288;23/70&#65289;&#65292;&#20998;&#23376;&#26631;&#35760;&#39044;&#27979;&#65288;13/70&#65289;&#21644;&#29983;&#23384;&#39044;&#27979;&#65288;27/70&#65289;&#31561;&#35786;&#26029;&#20219;&#21153;&#12290;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#22312;&#26041;&#27861;&#23398;&#26041;&#38754;&#21450;&#20854;&#20020;&#24202;&#36866;&#29992;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#23545;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#32452;&#32455;&#20999;&#29255;&#36827;&#34892;&#35780;&#20272;&#12290;&#22810;&#25968;&#30740;&#31350;&#65288;49/70&#65289;&#22522;&#20110;&#20844;&#24320;&#30340;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#21644;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#25968;&#25454;&#38598;&#65292;&#20165;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the diagnosis of gliomas has become increasingly complex. Histological assessment of glioma tissue using modern machine learning techniques offers new opportunities to support diagnosis and outcome prediction. To give an overview of the current state of research, this review examines 70 publicly available research studies on machine learning-based analysis of stained human glioma tissue sections, covering the diagnostic tasks of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and survival prediction (27/70). All studies were reviewed with regard to methodological aspects as well as clinical applicability. It was found that the focus of current research is the assessment of hematoxylin and eosin-stained tissue sections of adult-type diffuse gliomas. The majority of studies (49/70) are based on the publicly available glioblastoma and low-grade glioma datasets from The Cancer Genome Atlas (TCGA) and only a few studies employed other datasets in is
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.12800</link><description>&lt;p&gt;
&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#21450;&#20854;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications. (arXiv:2401.12800v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#29289;&#29702;&#23618;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#25152;&#25903;&#25345;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#20123;&#31995;&#32479;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#20248;&#21270;&#29289;&#29702;&#23618;&#23454;&#29616;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#31471;&#21040;&#31471;&#31995;&#32479;&#22312;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#65292;&#28385;&#36275;&#20102;5G&#21450;&#20854;&#20197;&#19978;&#32593;&#32476;&#31995;&#32479;&#21644;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#22312;&#29289;&#29702;&#23618;&#30340;&#21457;&#23637;&#20351;&#24471;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#22810;&#27169;&#24577;&#20256;&#36755;&#31561;&#21508;&#31181;&#27169;&#24577;&#19979;&#23454;&#29616;&#20102;&#39640;&#32423;&#30340;&#35821;&#20041;&#24212;&#29992;&#12290;&#36825;&#20123;&#24212;&#29992;&#20174;&#20256;&#32479;&#30340;&#27604;&#29305;&#32423;&#36890;&#20449;&#36716;&#21464;&#20026;&#35821;&#20041;&#32423;&#26234;&#33021;&#36890;&#20449;&#31995;&#32479;&#65292;&#33021;&#22815;&#29702;&#35299;&#21644;&#36866;&#24212;&#25968;&#25454;&#20256;&#36755;&#30340;&#19978;&#19979;&#25991;&#21644;&#24847;&#22270;&#12290;&#34429;&#28982;&#29289;&#29702;&#23618;&#20316;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#36890;&#20449;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26159;&#23454;&#29616;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36817;&#24180;&#26469;&#26377;&#21508;&#31181;&#30740;&#31350;&#20998;&#21035;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has enabled a paradigm shift in wireless communication system with data driven end-to-end (E2E) learning and optimization of the Physical Layer (PHY). By leveraging the representation learning of DL, E2E systems exhibit enhanced adaptability and performance in complex wireless environments, fulfilling the demands of 5G and beyond network systems and applications. The evolution of data-driven techniques in the PHY has enabled advanced semantic applications across various modalities including text, image, audio, video, and multi-modal transmissions. These applications transcend from traditional bit-level communication to semantic-level intelligent communication systems, which are capable of understanding and adapting to the context and intent of the data transmission. Although PHY as a DL architecture for data-driven E2E communication is a key factor in enabling semantic communication systems (SemCom), and various studies in recent years have surveyed them separately, 
&lt;/p&gt;</description></item><item><title>&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11694</link><description>&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11694
&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#65292;&#24182;&#19988;&#20854;&#35774;&#35745;&#21463;&#21040;&#20102;&#29992;&#20110;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#30340;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#30340;&#25928;&#29575;&#21551;&#21457;&#12290;&#20381;&#36182;&#21464;&#37327;&#21487;&#20197;&#38544;&#24335;&#25110;&#26174;&#24335;&#23450;&#20041;&#65292;&#24182;&#19988;&#26041;&#31243;&#21487;&#20197;&#20351;&#29992;&#20195;&#25968;&#12289;&#24494;&#20998;&#25110;&#31215;&#20998;&#20851;&#31995;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#35745;&#31639;&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#65292;&#20294;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#22312;&#20171;&#32461;&#22522;&#30784;&#29702;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#22312;&#36825;&#37324;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22312;&#20801;&#35768;&#35745;&#31639;&#30340;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
&lt;/p&gt;</description></item><item><title>AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.10652</link><description>&lt;p&gt;
AutoChunk: &#33258;&#21160;&#28608;&#27963;&#22359;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10652
&lt;/p&gt;
&lt;p&gt;
AutoChunk&#26159;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20869;&#23384;&#30340;&#22823;&#37327;&#38656;&#27714;&#65292;&#21253;&#25324;&#21442;&#25968;&#20869;&#23384;&#21644;&#28608;&#27963;&#20869;&#23384;&#65292;&#24050;&#32463;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#21442;&#25968;&#20869;&#23384;&#65292;&#23545;&#28608;&#27963;&#20869;&#23384;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#28608;&#27963;&#20869;&#23384;&#39044;&#35745;&#20250;&#32463;&#21382;&#26174;&#33879;&#30340;&#25351;&#25968;&#22686;&#38271;&#12290;&#22312;&#36825;&#20010;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoChunk&#65292;&#19968;&#31181;&#33258;&#21160;&#21644;&#33258;&#36866;&#24212;&#30340;&#32534;&#35793;&#22120;&#31995;&#32479;&#65292;&#36890;&#36807;&#22359;&#31574;&#30053;&#26377;&#25928;&#22320;&#20943;&#23569;&#38271;&#24207;&#21015;&#25512;&#26029;&#30340;&#28608;&#27963;&#20869;&#23384;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#30340;&#20248;&#21270;&#29983;&#25104;&#22359;&#35745;&#21010;&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#22359;&#25628;&#32034;&#36890;&#36807;&#25506;&#32034;&#25152;&#26377;&#21487;&#33021;&#30340;&#22359;&#20505;&#36873;&#39033;&#65292;&#22359;&#36873;&#25321;&#36890;&#36807;&#35782;&#21035;&#26368;&#20339;&#22359;&#36827;&#34892;&#12290;&#36816;&#34892;&#26102;&#65292;AutoChunk&#37319;&#29992;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#24212;&#29992;&#22359;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15690</link><description>&lt;p&gt;
&#20351;&#29992;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#36827;&#34892;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed with Power-Enhanced Residual Network for Interpolation and Inverse Problems. (arXiv:2310.15690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#25552;&#21319;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#24615;&#33021;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22312;&#35299;&#20915;&#21453;&#38382;&#39064;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#31216;&#20026;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;2D&#21644;3D&#29615;&#22659;&#19979;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#30340;&#25554;&#20540;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#27531;&#24046;&#20803;&#32032;&#20013;&#28155;&#21152;&#24130;&#27425;&#39033;&#65292;&#35813;&#32593;&#32476;&#32467;&#26500;&#22686;&#24378;&#20102;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;&#32593;&#32476;&#28145;&#24230;&#12289;&#23485;&#24230;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#20248;&#21183;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#24179;&#28369;&#20989;&#25968;&#26041;&#38754;&#20855;&#26377;&#24322;&#24120;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#38469;&#31034;&#20363;&#20063;&#35777;&#23454;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30456;&#23545;&#20110;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#36234;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#36824;&#24212;&#29992;&#20110;&#35299;&#20915;&#21453;Burgers&#26041;&#31243;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#24635;&#20043;&#65292;&#22686;&#24378;&#22411;&#27531;&#24046;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26126;&#26174;&#25552;&#21319;&#20102;&#25554;&#20540;&#21644;&#21453;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel neural network structure called the Power-Enhancing residual network, designed to improve interpolation capabilities for both smooth and non-smooth functions in 2D and 3D settings. By adding power terms to residual elements, the architecture boosts the network's expressive power. The study explores network depth, width, and optimization methods, showing the architecture's adaptability and performance advantages. Consistently, the results emphasize the exceptional accuracy of the proposed Power-Enhancing residual network, particularly for non-smooth functions. Real-world examples also confirm its superiority over plain neural network in terms of accuracy, convergence, and efficiency. The study also looks at the impact of deeper network. Moreover, the proposed architecture is also applied to solving the inverse Burgers' equation, demonstrating superior performance. In conclusion, the Power-Enhancing residual network offers a versatile solution that significa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;&#27169;&#22411;&#12290;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#28508;&#22312;&#21442;&#25968;&#65292;&#21487;&#20197;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#22312;&#32771;&#34385;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#33539;&#22260;&#20869;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10717</link><description>&lt;p&gt;
&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;
&lt;/p&gt;
&lt;p&gt;
A representation learning approach to probe for dynamical dark energy in matter power spectra. (arXiv:2310.10717v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35266;&#27979;&#30740;&#31350;&#20013;&#25506;&#27979;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#20013;&#30340;&#21160;&#24577;&#26263;&#33021;&#37327;&#27169;&#22411;&#12290;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#28508;&#22312;&#21442;&#25968;&#65292;&#21487;&#20197;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#22312;&#32771;&#34385;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#33539;&#22260;&#20869;&#20855;&#26377;&#24456;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DE-VAE&#65292;&#19968;&#31181;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#35266;&#27979;&#30740;&#31350;&#20013;&#23547;&#25214;&#21160;&#24577;&#26263;&#33021;&#37327;&#65288;DE&#65289;&#27169;&#22411;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;DE-VAE&#22312;$.01-2.5 \ h/\rm{Mpc}$&#30340;&#27874;&#25968;$k$&#21644;&#22235;&#20010;&#32418;&#31227;&#20540;$z \in (0.1,0.48,0.78,1.5)$&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#26368;&#20856;&#22411;&#30340;&#21160;&#24577;DE&#21442;&#25968;&#21270;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#24182;&#19988;&#20351;&#29992;&#20004;&#20010;&#39069;&#22806;&#21442;&#25968;&#26469;&#25551;&#36848;&#28436;&#21270;&#30340;DE&#24577;&#26041;&#31243;&#12290;&#36825;&#20123;&#21387;&#32553;&#34920;&#31034;&#19982;&#26631;&#20934;&#30340;&#20919;&#26263;&#29289;&#36136;&#65288;CDM&#65289;&#21442;&#25968;&#36830;&#25509;&#65292;&#24182;&#26144;&#23556;&#22238;&#37325;&#26500;&#30340;&#25552;&#21319;&#65307;&#21387;&#32553;&#21644;&#37325;&#26500;&#32452;&#20214;&#37117;&#34987;&#21442;&#25968;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#21333;&#19968;&#30340;&#28508;&#22312;&#21442;&#25968;&#36275;&#20197;&#22312;&#24191;&#27867;&#30340;&#23431;&#23449;&#23398;&#21442;&#25968;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#21040;95%&#65288;99%&#65289;&#30340;DE&#21151;&#29575;&#35889;&#65292;&#24182;&#19988;&#22312;&#21253;&#25324;&#23431;&#23449;&#26041;&#24046;&#30340;&#39640;&#26031;&#35823;&#24046;&#30340;1$ \sigma$&#65288;2$ \sigma$&#65289;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.02011</link><description>&lt;p&gt;
&#35299;&#30721;&#20154;&#31867;&#34892;&#20026;&#65306;&#20998;&#26512;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#25968;&#25454;&#36827;&#34892;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Decoding Human Activities: Analyzing Wearable Accelerometer and Gyroscope Data for Activity Recognition. (arXiv:2310.02011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27963;&#21160;&#35782;&#21035;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#21033;&#29992;&#27531;&#24046;&#32593;&#32476;&#21644;&#27531;&#24046;MobileNet&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#26041;&#27861;&#36827;&#34892;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#36816;&#21160;&#25110;&#30456;&#23545;&#23450;&#20301;&#26377;&#25928;&#22320;&#20135;&#29983;&#20102;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#35835;&#21462;&#30340;&#21407;&#22987;&#30005;&#20449;&#21495;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25805;&#20316;&#25216;&#26415;&#26469;&#23545;&#19981;&#21516;&#30340;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27531;&#24046;&#32593;&#32476;&#19982;&#27531;&#24046;MobileNet&#36827;&#34892;&#21512;&#22863;&#30340;&#20998;&#23618;&#22810;&#32467;&#26500;&#26041;&#27861;&#65292;&#31216;&#20026;FusionActNet&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#27531;&#24046;&#22359;&#20998;&#21035;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#26126;&#26174;&#32780;&#29420;&#29305;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32593;&#32476;&#29420;&#31435;&#35757;&#32451;&#65292;&#24471;&#21040;&#20004;&#20010;&#19987;&#19994;&#30340;&#39640;&#31934;&#24230;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26550;&#26500;&#35843;&#25972;&#30340;&#31639;&#27861;&#20248;&#21183;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#36229;&#31867;&#20013;&#20248;&#31168;&#22320;&#35782;&#21035;&#27963;&#21160;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#27531;&#24046;&#32593;&#32476;&#36890;&#36807;&#21152;&#26435;&#21512;&#22863;&#30340;&#27531;&#24046;MobileNet&#36827;&#34892;&#20256;&#36882;&#12290;&#38543;&#21518;&#65292;&#36825;&#20010;&#21512;&#22863;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19968;&#20123;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's movement or relative positioning effectively generates raw electrical signals that can be read by computing machines to apply various manipulative techniques for the classification of different human activities. In this paper, a stratified multi-structural approach based on a Residual network ensembled with Residual MobileNet is proposed, termed as FusionActNet. The proposed method involves using carefully designed Residual blocks for classifying the static and dynamic activities separately because they have clear and distinct characteristics that set them apart. These networks are trained independently, resulting in two specialized and highly accurate models. These models excel at recognizing activities within a specific superclass by taking advantage of the unique algorithmic benefits of architectural adjustments. Afterward, these two ResNets are passed through a weighted ensemble-based Residual MobileNet. Subsequently, this ensemble proficiently discriminates between a sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.10485</link><description>&lt;p&gt;
Grid&#21644;&#33258;&#28982;&#35821;&#21477;&#23545;Normal-to-Lombard&#36716;&#25442;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study of Grid and Natural sentences effects on Normal-to-Lombard conversion. (arXiv:2309.10485v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;Grid&#21477;&#23376;&#21644;&#33258;&#28982;&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;Grid&#21477;&#23376;&#30340;alpha&#27604;&#20363;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22312;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#19968;&#33268;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grid&#21477;&#23376;&#24120;&#29992;&#20110;&#30740;&#31350;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#22522;&#20110;Grid&#21477;&#23376;&#35757;&#32451;&#30340;Normal-to-Lombard&#27169;&#22411;&#26159;&#21542;&#36275;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#38899;&#21487;&#25026;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24179;&#34892;&#30340;Lombard&#35821;&#26009;&#24211;&#65288;&#31216;&#20026;Lombard Chinese TIMIT&#65292;LCT&#65289;&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#20102;&#20013;&#25991;TIMIT&#30340;&#33258;&#28982;&#21477;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LCT&#21644;Enhanced Mandarin Lombard Grid&#35821;&#26009;&#24211;&#65288;EMALG&#65289;&#27604;&#36739;&#20102;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#22312;Lombard&#25928;&#24212;&#21644;Normal-to-Lombard&#36716;&#25442;&#26041;&#38754;&#12290;&#36890;&#36807;&#23545;Lombard&#25928;&#24212;&#30340;&#21442;&#25968;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;&#33258;&#28982;&#21477;&#23376;&#21644;Grid&#21477;&#23376;&#30340;&#21442;&#25968;&#21464;&#21270;&#30456;&#20284;&#65292;&#20294;&#22312;alpha&#27604;&#20363;&#22686;&#21152;&#26041;&#38754;&#65292;Grid&#21477;&#23376;&#30340;&#22686;&#21152;&#26356;&#22823;&#12290;&#22312;&#36328;&#24615;&#21035;&#21644;&#20449;&#22122;&#27604;&#30340;&#20027;&#35266;&#21487;&#25026;&#24230;&#35780;&#20272;&#20013;&#65292;&#22522;&#20110;EMALG&#35757;&#32451;&#30340;StarGAN&#27169;&#22411;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grid sentence is commonly used for studying the Lombard effect and Normal-to-Lombard conversion. However, it's unclear if Normal-to-Lombard models trained on grid sentences are sufficient for improving natural speech intelligibility in real-world applications. This paper presents the recording of a parallel Lombard corpus (called Lombard Chinese TIMIT, LCT) extracting natural sentences from Chinese TIMIT. Then We compare natural and grid sentences in terms of Lombard effect and Normal-to-Lombard conversion using LCT and Enhanced MAndarin Lombard Grid corpus (EMALG). Through a parametric analysis of the Lombard effect, We find that as the noise level increases, both natural sentences and grid sentences exhibit similar changes in parameters, but in terms of the increase of the alpha ratio, grid sentences show a greater increase. Following a subjective intelligibility assessment across genders and Signal-to-Noise Ratios, the StarGAN model trained on EMALG consistently outperforms the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65292;&#32467;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#26657;&#27491;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27700;&#25991;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.02040</link><description>&lt;p&gt;
&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients. (arXiv:2308.02040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20934;&#30830;&#30340;&#31354;&#38388;&#25104;&#26412;&#26799;&#24230;&#22312;&#21487;&#24494;&#30340;&#39640;&#20998;&#36776;&#29575;&#27700;&#25991;&#27169;&#22411;&#20013;&#23398;&#20064;&#21010;&#20998;&#21306;&#22495;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65292;&#32467;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#26657;&#27491;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#27700;&#25991;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#27969;&#37327;&#25968;&#25454;&#30340;&#26410;&#30417;&#27979;&#27969;&#22495;&#20013;&#20272;&#35745;&#31354;&#38388;&#20998;&#24067;&#30340;&#27700;&#25991;&#21442;&#25968;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21306;&#22495;&#21270;&#38382;&#39064;&#65292;&#38656;&#35201;&#32771;&#34385;&#27969;&#37327;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#24182;&#26045;&#21152;&#31354;&#38388;&#32422;&#26463;&#12290;&#19968;&#31181;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#23547;&#25214;&#19968;&#20010;&#23450;&#37327;&#23558;&#29289;&#29702;&#25351;&#26631;&#19982;&#27010;&#24565;&#27169;&#22411;&#21442;&#25968;&#32852;&#31995;&#36215;&#26469;&#30340;&#36716;&#31227;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25968;&#25454;&#21516;&#21270;&#21644;&#21442;&#25968;&#21306;&#22495;&#21270;&#65288;HDA-PR&#65289;&#26041;&#27861;&#65292;&#23558;&#21487;&#23398;&#20064;&#30340;&#21306;&#22495;&#21270;&#26144;&#23556;&#65288;&#22522;&#20110;&#22810;&#20803;&#22238;&#24402;&#25110;&#31070;&#32463;&#32593;&#32476;&#65289;&#24341;&#20837;&#21487;&#24494;&#30340;&#27700;&#25991;&#27169;&#22411;&#20013;&#12290;&#23427;&#21487;&#20197;&#21033;&#29992;&#20934;&#30830;&#30340;&#20276;&#38543;&#26799;&#24230;&#22312;&#39640;&#32500;&#30340;&#21306;&#22495;&#21270;&#32972;&#26223;&#19979;&#22312;&#24191;&#27867;&#30340;&#26102;&#31354;&#35745;&#31639;&#22495;&#20013;&#21033;&#29992;&#24322;&#36136;&#25968;&#25454;&#12290;&#36870;&#38382;&#39064;&#36890;&#36807;&#22810;&#20010;&#35266;&#27979;&#31449;&#28857;&#30340;&#20449;&#24687;&#35745;&#31639;&#26657;&#27491;&#20195;&#20215;&#20989;&#25968;&#26469;&#35299;&#20915;&#12290;HDA-PR&#22312;&#39640;&#20998;&#36776;&#29575;&#12289;&#23567;&#26102;&#32423;&#21644;&#20844;&#37324;&#32423;&#30340;&#21306;&#22495;&#27169;&#25311;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating spatially distributed hydrological parameters in ungauged catchments poses a challenging regionalization problem and requires imposing spatial constraints given the sparsity of discharge data. A possible approach is to search for a transfer function that quantitatively relates physical descriptors to conceptual model parameters. This paper introduces a Hybrid Data Assimilation and Parameter Regionalization (HDA-PR) approach incorporating learnable regionalization mappings, based on either multivariate regressions or neural networks, into a differentiable hydrological model. It enables the exploitation of heterogeneous datasets across extensive spatio-temporal computational domains within a high-dimensional regionalization context, using accurate adjoint-based gradients. The inverse problem is tackled with a multi-gauge calibration cost function accounting for information from multiple observation sites. HDA-PR was tested on high-resolution, hourly and kilometric regional mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#20219;&#21153;&#30340;&#27010;&#29575;&#35823;&#24046;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.09397</link><description>&lt;p&gt;
&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#28176;&#36827;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-Asymptotic Performance of Social Machine Learning Under Limited Data. (arXiv:2306.09397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38480;&#21046;&#25968;&#25454;&#19979;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#29575;&#35823;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#20219;&#21153;&#30340;&#27010;&#29575;&#35823;&#24046;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#19982;&#38169;&#35823;&#27010;&#29575;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#28041;&#21450;&#29420;&#31435;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#38543;&#21518;&#22312;&#22270;&#19978;&#36827;&#34892;&#21327;&#20316;&#20915;&#31574;&#38454;&#27573;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#20998;&#31867;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20998;&#31867;&#20219;&#21153;&#65292;&#20998;&#21035;&#20026;&#32479;&#35745;&#20998;&#31867;&#21644;&#21333;&#26679;&#26412;&#20998;&#31867;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#39044;&#27979;&#38454;&#27573;&#25968;&#25454;&#35266;&#27979;&#21463;&#38480;&#30340;&#24773;&#20917;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#35268;&#21017;&#65292;&#24182;&#30456;&#24212;&#20998;&#26512;&#20102;&#38169;&#35823;&#27010;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#35757;&#32451;&#26465;&#20214;&#65292;&#35813;&#26465;&#20214;&#28041;&#21450;&#35757;&#32451;&#20998;&#31867;&#22120;&#29983;&#25104;&#30340;&#36793;&#32536;&#20998;&#24067;&#12290;&#22522;&#20110;&#27492;&#26465;&#20214;&#65292;&#25105;&#20204;&#20026;&#20004;&#31181;&#20219;&#21153;&#25512;&#23548;&#20986;&#20102;&#27010;&#29575;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#21462;&#20915;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#29992;&#20110;&#32452;&#21512;&#20998;&#24067;&#24335;&#20998;&#31867;&#22120;&#30340;&#32452;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the probability of error associated with the social machine learning framework, which involves an independent training phase followed by a cooperative decision-making phase over a graph. This framework addresses the problem of classifying a stream of unlabeled data in a distributed manner. We consider two kinds of classification tasks with limited observations in the prediction phase, namely, the statistical classification task and the single-sample classification task. For each task, we describe the distributed learning rule and analyze the probability of error accordingly. To do so, we first introduce a stronger consistent training condition that involves the margin distributions generated by the trained classifiers. Based on this condition, we derive an upper bound on the probability of error for both tasks, which depends on the statistical properties of the data and the combination policy used to combine the distributed classifiers. For the statistical classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11567</link><description>&lt;p&gt;
TSGM&#65306;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series. (arXiv:2305.11567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11567
&lt;/p&gt;
&lt;p&gt;
TSGM&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#20063;&#24456;&#26377;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#25110;&#39640;&#24230;&#25935;&#24863;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#22312;&#30740;&#31350;&#32773;&#21644;&#24037;&#19994;&#32452;&#32455;&#20043;&#38388;&#30340;&#20849;&#20139;&#20197;&#21450;&#29616;&#26377;&#21644;&#26032;&#30340;&#25968;&#25454;&#23494;&#38598;&#22411; ML &#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#19968;&#38590;&#39064;&#30340;&#21487;&#33021;&#26041;&#27861;&#26159;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65288;TSGM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24320;&#28304;&#26694;&#26550;&#12290;TSGM&#21253;&#25324;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65306;&#29983;&#25104;&#27169;&#22411;&#12289;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#25311;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#35780;&#20272;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#65306;&#30456;&#20284;&#24615;&#12289;&#19979;&#28216;&#25928;&#26524;&#12289;&#39044;&#27979;&#19968;&#33268;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#38544;&#31169;&#12290;&#35813;&#26694;&#26550;&#26159;&#21487;&#25193;&#23637;&#30340;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#23454;&#29616;&#33258;&#24049;&#30340;&#26041;&#27861;&#24182;&#22312;&#21487;&#20849;&#20139;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;TSGM&#23558;&#26377;&#21161;&#20110;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporally indexed data are essential in a wide range of fields and of interest to machine learning researchers. Time series data, however, are often scarce or highly sensitive, which precludes the sharing of data between researchers and industrial organizations and the application of existing and new data-intensive ML methods. A possible solution to this bottleneck is to generate synthetic data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling of synthetic time series. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, and simulator-based approaches. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, and privacy. The framework is extensible, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. TSGM w
&lt;/p&gt;</description></item><item><title>SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13033</link><description>&lt;p&gt;
SmartChoices: &#23398;&#20064;&#23454;&#29616;&#22686;&#24378;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
SmartChoices: Augmenting Software with Learned Implementations. (arXiv:2304.13033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13033
&lt;/p&gt;
&lt;p&gt;
SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22788;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#12290;&#24378;&#22823;&#30340;&#27169;&#22411;&#27491;&#22312;&#35757;&#32451;&#20013;&#65292;&#36828;&#27604;&#20165;&#20351;&#29992;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#26041;&#27861;&#26356;&#22909;&#22320;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24320;&#21457;&#24182;&#37096;&#32626;&#21040;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#20013;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SmartChoices&#65292;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#21040;&#25104;&#29087;&#36719;&#20214;&#22534;&#26632;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#24635;&#20307;&#35774;&#35745;&#29702;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992; SmartChoices &#22312;&#22823;&#22411;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are living in a golden age of machine learning. Powerful models are being trained to perform many tasks far better than is possible using traditional software engineering approaches alone. However, developing and deploying those models in existing software systems remains difficult. In this paper we present SmartChoices, a novel approach to incorporating machine learning into mature software stacks easily, safely, and effectively. We explain the overall design philosophy and present case studies using SmartChoices within large scale industrial systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.05655</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102; H.Q. Minh&#12289;L. Bazzani &#21644; V. Murino &#22312;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12299;&#65288;Journal of Machine Learning Research&#65289;&#20013;&#20171;&#32461;&#30340;&#19968;&#31181;&#28041;&#21450;&#31639;&#23376;&#20540;&#27491;&#23450;&#26680;&#21450;&#20854;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#30340;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#12290;&#32467;&#26524;&#28041;&#21450;&#21040;&#32771;&#34385;&#20984;&#25110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#26377;&#38480;&#25110;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#19968;&#33324;&#26694;&#26550;&#20801;&#35768;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#12290;&#23545;&#23548;&#33268;&#37096;&#20998;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#25351;&#25968;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#35814;&#32454;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.02444</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#26657;&#20934;Transformer
&lt;/p&gt;
&lt;p&gt;
Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;Sparse Gaussian Process attention (SGPA)&#26469;&#26657;&#20934;Transformer&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;SGPA-based Transformers&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#24191;&#27867;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23558;Transformer&#30340;&#25104;&#21151;&#25193;&#23637;&#21040;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#38656;&#35201;&#20934;&#30830;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#27880;&#24847;&#21147;&#65288;SGPA&#65289;&#65292;&#23427;&#30452;&#25509;&#22312;Transformer&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#65288;MHA&#65289;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20197;&#26657;&#20934;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#23427;&#29992;&#19968;&#20010;&#26377;&#25928;&#30340;&#23545;&#31216;&#26680;&#26367;&#20195;&#20102;&#32553;&#25918;&#28857;&#31215;&#25805;&#20316;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;SGP&#65289;&#25216;&#26415;&#26469;&#36817;&#20284;MHA&#36755;&#20986;&#30340;&#21518;&#39564;&#36807;&#31243;&#12290;&#32463;&#39564;&#19978;&#65292;&#22312;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#30340;&#19968;&#31995;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;SGPA&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26174;&#33879;&#25913;&#21892;&#20102;&#20869;&#20998;&#24067;&#26657;&#20934;&#21644;&#22806;&#20998;&#24067;&#30340;&#40065;&#26834;&#24615;&#21644;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24037;&#20855;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;AUTOLYCUS&#12290;</title><link>http://arxiv.org/abs/2302.02162</link><description>&lt;p&gt;
AUTOLYCUS: &#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#23545;&#30333;&#30418;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models. (arXiv:2302.02162v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24037;&#20855;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;AUTOLYCUS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#38416;&#26126;AI&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#25216;&#26415;&#21644;&#31243;&#24207;&#12290;&#34429;&#28982;XAI&#23545;&#20110;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#24456;&#26377;&#20215;&#20540;&#65292;&#20294;&#29992;&#20110;&#36825;&#31181;&#25581;&#31034;&#30340;&#25968;&#25454;&#20250;&#24102;&#26469;&#28508;&#22312;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#28431;&#27934;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#30830;&#23450;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#21253;&#25324;&#25104;&#21592;&#25512;&#35770;&#12289;&#27169;&#22411;&#21453;&#28436;&#21644;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#26681;&#25454;&#28041;&#21450;&#30340;&#35774;&#32622;&#21644;&#21508;&#26041;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#33021;&#38024;&#23545;&#27169;&#22411;&#26412;&#36523;&#25110;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#25552;&#20379;XAI&#30340;&#24037;&#20855;&#29305;&#21035;&#20250;&#22686;&#21152;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#24403;AI&#27169;&#22411;&#30340;&#25152;&#26377;&#32773;&#20165;&#24895;&#25552;&#20379;&#40657;&#30418;&#35775;&#38382;&#32780;&#19981;&#19982;&#20854;&#20182;&#26041;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#32467;&#26500;&#26102;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#31181;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTOLYCUS&#65292;&#19968;&#31181;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.07341</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#30693;&#36947;&#25105;&#30340;&#33080;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Know My Face?. (arXiv:2209.07341v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;IDIA&#26469;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#65292;&#34920;&#26126;&#38656;&#35201;&#26356;&#22909;&#22320;&#35299;&#20915;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#24212;&#29992;&#20013;&#30340;&#26222;&#21450;&#65292;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#22810;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#29305;&#21035;&#26159;&#20687;CLIP&#36825;&#26679;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#36523;&#20221;&#25512;&#26029;&#25915;&#20987;(IDIA)&#36890;&#36807;&#29992;&#21516;&#19968;&#20154;&#30340;&#22270;&#29255;&#21521;&#27169;&#22411;&#26597;&#35810;&#65292;&#20174;&#32780;&#25581;&#31034;&#35813;&#20010;&#20154;&#26159;&#21542;&#34987;&#21253;&#21547;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#35753;&#27169;&#22411;&#20174;&#21508;&#31181;&#21487;&#33021;&#30340;&#25991;&#26412;&#26631;&#31614;&#20013;&#36873;&#25321;&#65292;&#27169;&#22411;&#20250;&#36879;&#38706;&#26159;&#21542;&#35782;&#21035;&#35813;&#20154;&#29289;&#65292;&#20174;&#32780;&#34920;&#26126;&#20854;&#34987;&#29992;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20110;&#35757;&#32451;&#30340;&#20010;&#20154;&#21487;&#20197;&#34987;&#38750;&#24120;&#39640;&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#30830;&#35748;&#35813;&#27169;&#22411;&#24050;&#32463;&#23398;&#20250;&#23558;&#21517;&#31216;&#19982;&#25551;&#32472;&#30340;&#20010;&#20154;&#30456;&#20851;&#32852;&#65292;&#36825;&#24847;&#21619;&#30528;&#25935;&#24863;&#20449;&#24687;&#23384;&#22312;&#20110;&#20854;&#20013;&#65292;&#21487;&#20197;&#34987;&#23545;&#25163;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#38656;&#35201;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26356;&#22909;&#22320;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of deep learning in various applications, privacy concerns around the protection of training data has become a critical area of research. Whereas prior studies have focused on privacy risks in single-modal models, we introduce a novel method to assess privacy for multi-modal models, specifically vision-language models like CLIP. The proposed Identity Inference Attack (IDIA) reveals whether an individual was included in the training data by querying the model with images of the same person. Letting the model choose from a wide variety of possible text labels, the model reveals whether it recognizes the person and, therefore, was used for training. Our large-scale experiments on CLIP demonstrate that individuals used for training can be identified with very high accuracy. We confirm that the model has learned to associate names with depicted individuals, implying the existence of sensitive information that can be extracted by adversaries. Our results highlight the need for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2208.07497</link><description>&lt;p&gt;
&#23398;&#20064;ACOPF&#30340;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Bucketized Active Sampling for Learning ACOPF. (arXiv:2208.07497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#26368;&#20248;&#28526;&#27969;&#65288;OPF&#65289;&#30340;&#20248;&#21270;&#20195;&#29702;&#65292;&#21363;&#36817;&#20284;OPF&#30340;&#36755;&#20837;/&#36755;&#20986;&#20851;&#31995;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#35777;&#26126;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#38656;&#35201;&#23545;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#36827;&#34892;OPF&#30340;&#65288;&#31163;&#32447;&#65289;&#27714;&#35299;&#12290;&#20026;&#20102;&#28385;&#36275;&#24066;&#22330;&#28165;&#31639;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#20998;&#26742;&#20027;&#21160;&#37319;&#26679;&#65288;BAS&#65289;&#65292;&#26088;&#22312;&#22312;&#26102;&#38388;&#38480;&#21046;&#20869;&#35757;&#32451;&#26368;&#20339;&#30340;OPF&#20195;&#29702;&#12290;BAS&#23558;&#36755;&#20837;&#20998;&#24067;&#20998;&#25104;&#26742;&#65292;&#24182;&#20351;&#29992;&#25910;&#38598;&#20989;&#25968;&#30830;&#23450;&#19979;&#19968;&#27425;&#37319;&#26679;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#23558;&#30456;&#21516;&#30340;&#20998;&#26742;&#24212;&#29992;&#20110;&#39564;&#35777;&#38598;&#65292;BAS&#21033;&#29992;&#26631;&#35760;&#30340;&#39564;&#35777;&#26679;&#26412;&#26469;&#36873;&#25321;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#12290;BAS&#36824;&#20381;&#36182;&#20110;&#38543;&#26102;&#38388;&#22686;&#21152;&#21644;&#20943;&#23569;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;BAS&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers optimization proxies for Optimal Power Flow (OPF), i.e., machine-learning models that approximate the input/output relationship of OPF. Recent work has focused on showing that such proxies can be of high fidelity. However, their training requires significant data, each instance necessitating the (offline) solving of an OPF for a sample of the input distribution. To meet the requirements of market-clearing applications, this paper proposes Bucketized Active Sampling (BAS), a novel active learning framework that aims at training the best possible OPF proxy within a time limit. BAS partitions the input distribution into buckets and uses an acquisition function to determine where to sample next. By applying the same partitioning to the validation set, BAS leverages labeled validation samples in the selection of unlabeled samples. BAS also relies on an adaptive learning rate that increases and decreases over time. Experimental results demonstrate the benefits of BAS.
&lt;/p&gt;</description></item></channel></rss>