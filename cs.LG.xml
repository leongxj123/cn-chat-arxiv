<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#22810;&#26465;&#20214;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Multi-Conditional Ranking with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;(MCRank)&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#25490;&#24207;&#20219;&#21153;&#20013;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#19968;&#32452;&#39033;&#30446;&#36827;&#34892;&#25490;&#24207;&#24050;&#25104;&#20026;&#25512;&#33616;&#21644;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#25506;&#35752;&#20102;&#22810;&#26465;&#20214;&#25490;&#24207;&#30340;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;MCRank&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#36328;&#19981;&#21516;&#39033;&#30446;&#31867;&#22411;&#21644;&#26465;&#20214;&#36827;&#34892;&#22810;&#26465;&#20214;&#25490;&#24207;&#12290;&#25105;&#20204;&#20351;&#29992;MCRank&#23545;LLMs&#36827;&#34892;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#30528;&#39033;&#30446;&#21644;&#26465;&#20214;&#25968;&#37327;&#20197;&#21450;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#21462;&#21644;&#25490;&#24207;&#26465;&#20214;&#65292;&#28982;&#21518;&#36845;&#20195;&#22320;&#23545;&#26465;&#20214;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00211v1 Announce Type: new  Abstract: Utilizing large language models (LLMs) to rank a set of items has become a common approach in recommendation and retrieval systems. Typically, these systems focus on ordering a substantial number of documents in a monotonic order based on a given query. However, real-world scenarios often present a different challenge: ranking a comparatively smaller set of items, but according to a variety of diverse and occasionally conflicting conditions. In this paper, we define and explore the task of multi-conditional ranking by introducing MCRank, a benchmark tailored for assessing multi-conditional ranking across various item types and conditions. Our analysis of LLMs using MCRank indicates a significant decrease in performance as the number and complexity of items and conditions grow. To overcome this limitation, we propose a novel decomposed reasoning method, consisting of EXtracting and Sorting the conditions, and then Iterativly Ranking the i
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07447</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#30456;&#20851;&#22810;&#30005;&#23376;Schr\"odinger&#26041;&#31243;&#30340;&#20174;&#22836;&#21464;&#20998;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Ab-initio variational wave functions for the time-dependent many-electron Schr\"odinger equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25551;&#36848;&#22810;&#30005;&#23376;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#23545;&#20110;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#30005;&#23376;&#32467;&#26500;&#12289;&#20957;&#32858;&#24577;&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#22797;&#26434;&#26448;&#26009;&#30340;&#34892;&#20026;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#23454;&#26102;&#28436;&#21270;&#23545;&#20110;&#29702;&#35770;&#21644;&#35745;&#31639;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#31995;&#32479;&#25506;&#32034;&#20102;&#24191;&#38420;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21442;&#25968;&#21270;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24577;&#28436;&#21270;&#30340;&#36817;&#20284;&#12290;&#20026;&#20102;&#32771;&#34385;&#30005;&#23376;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26102;&#38388;&#30456;&#20851;&#30340;Jastrow&#22240;&#23376;&#21644;&#22238;&#27969;&#21464;&#25442;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 Announce Type: cross  Abstract: Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The ti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02772</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24247;&#22797;&#38203;&#28860;&#36136;&#37327;&#35780;&#20272;&#65292;&#32467;&#21512;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Rehabilitation Exercise Quality Assessment through Supervised Contrastive Learning with Hard and Soft Negatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#30828;&#21644;&#36719;&#36127;&#26679;&#26412;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#20013;&#26679;&#26412;&#31232;&#32570;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38203;&#28860;&#30340;&#24247;&#22797;&#35745;&#21010;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#12289;&#38477;&#20302;&#27515;&#20129;&#29575;&#21644;&#20877;&#20303;&#38498;&#29575;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#34394;&#25311;&#24247;&#22797;&#65292;&#24739;&#32773;&#21487;&#20197;&#22312;&#23478;&#29420;&#31435;&#23436;&#25104;&#38203;&#28860;&#65292;&#21033;&#29992;AI&#31639;&#27861;&#20998;&#26512;&#38203;&#28860;&#25968;&#25454;&#65292;&#20026;&#24739;&#32773;&#25552;&#20379;&#21453;&#39304;&#65292;&#24182;&#21521;&#20020;&#24202;&#21307;&#29983;&#26356;&#26032;&#20182;&#20204;&#30340;&#36827;&#23637;&#24773;&#20917;&#12290;&#36825;&#20123;&#35745;&#21010;&#36890;&#24120;&#20250;&#25351;&#23450;&#21508;&#31181;&#38203;&#28860;&#31867;&#22411;&#65292;&#36825;&#23548;&#33268;&#24247;&#22797;&#38203;&#28860;&#35780;&#20272;&#25968;&#25454;&#38598;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65306;&#34429;&#28982;&#22312;&#25972;&#20307;&#35757;&#32451;&#26679;&#26412;&#20013;&#20016;&#23500;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#27599;&#31181;&#20855;&#20307;&#38203;&#32451;&#31867;&#22411;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#12290;&#36825;&#31181;&#24046;&#24322;&#24433;&#21709;&#20102;&#29616;&#26377;&#26041;&#27861;&#35757;&#32451;&#20855;&#26377;&#23567;&#26679;&#26412;&#37327;&#30340;&#27599;&#31181;&#38203;&#32451;&#30340;&#21487;&#27867;&#21270;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#30828;&#36127;&#26679;&#26412;&#21644;&#36719;&#36127;&#26679;&#26412;&#30340;&#26377;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#20102;&#25972;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02772v1 Announce Type: cross  Abstract: Exercise-based rehabilitation programs have proven to be effective in enhancing the quality of life and reducing mortality and rehospitalization rates. AI-driven virtual rehabilitation, which allows patients to independently complete exercises at home, utilizes AI algorithms to analyze exercise data, providing feedback to patients and updating clinicians on their progress. These programs commonly prescribe a variety of exercise types, leading to a distinct challenge in rehabilitation exercise assessment datasets: while abundant in overall training samples, these datasets often have a limited number of samples for each individual exercise type. This disparity hampers the ability of existing approaches to train generalizable models with such a small sample size per exercise. Addressing this issue, our paper introduces a novel supervised contrastive learning framework with hard and soft negative samples that effectively utilizes the entir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.11782</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#20196;&#20154;&#20449;&#26381;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Evidence Do Language Models Find Convincing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36171;&#20104;&#20027;&#35266;&#12289;&#26377;&#20105;&#35758;&#21644;&#30683;&#30462;&#30340;&#26597;&#35810;&#20219;&#21153;&#65292;&#22914;&#8220;&#38463;&#26031;&#24052;&#29980;&#26159;&#21542;&#19982;&#30284;&#30151;&#26377;&#20851;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27169;&#31946;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#24517;&#39035;&#25628;&#32034;&#22823;&#37327;&#32593;&#31449;&#65292;&#24182;&#32771;&#34385;&#8220;&#25105;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#26159;&#20196;&#20154;&#20449;&#26381;&#30340;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#22914;&#20309;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026; ConflictingQA &#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#26377;&#20105;&#35758;&#30340;&#26597;&#35810;&#19982;&#19968;&#31995;&#21015;&#21253;&#21547;&#19981;&#21516;&#20107;&#23454;&#65288;&#22914;&#23450;&#37327;&#32467;&#26524;&#65289;&#12289;&#35770;&#35777;&#39118;&#26684;&#65288;&#22914;&#26435;&#23041;&#21628;&#22768;&#65289;&#21644;&#31572;&#26696;&#65288;&#26159;&#25110;&#21542;&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#35777;&#25454;&#25991;&#26723;&#37197;&#23545;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25506;&#35752;&#21738;&#20123;&#25991;&#26412;&#29305;&#24449;&#26368;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#27604;&#22914;&#25991;&#26412;&#26159;&#21542;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#36890;&#36807;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06277</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#21487;&#25511;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable seismic velocity synthesis using generative diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#36890;&#36807;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22320;&#38663;&#36895;&#24230;&#20272;&#35745;&#23545;&#20110;&#29702;&#35299;&#22320;&#29699;&#30340;&#22320;&#19979;&#32467;&#26500;&#12289;&#35780;&#20272;&#33258;&#28982;&#36164;&#28304;&#21644;&#35780;&#20272;&#22320;&#38663;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21453;&#28436;&#31639;&#27861;&#22312;&#21306;&#22495;&#65288;&#20363;&#22914;&#21208;&#25506;&#65289;&#21644;&#20840;&#29699;&#36895;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#65292;&#20197;&#35206;&#30422;&#30446;&#26631;&#35299;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25552;&#39640;&#36895;&#24230;&#20272;&#35745;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#36824;&#38656;&#35201;&#32435;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#20363;&#22914;&#22320;&#36136;&#31867;&#21035;&#12289;&#38075;&#20117;&#35760;&#24405;&#21644;&#22320;&#19979;&#32467;&#26500;&#65292;&#20294;&#30446;&#21069;&#30340;&#32479;&#35745;&#25110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#19981;&#22815;&#28789;&#27963;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22320;&#38663;&#36895;&#24230;&#21512;&#25104;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21487;&#23481;&#26131;&#22320;&#32435;&#20837;&#36825;&#20123;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#23454;&#39564;&#25968;&#25454;&#23494;&#20999;&#21305;&#37197;&#30340;&#22320;&#38663;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate seismic velocity estimations are vital to understanding Earth's subsurface structures, assessing natural resources, and evaluating seismic hazards. Machine learning-based inversion algorithms have shown promising performance in regional (i.e., for exploration) and global velocity estimation, while their effectiveness hinges on access to large and diverse training datasets whose distributions generally cover the target solutions. Additionally, enhancing the precision and reliability of velocity estimation also requires incorporating prior information, e.g., geological classes, well logs, and subsurface structures, but current statistical or neural network-based methods are not flexible enough to handle such multi-modal information. To address both challenges, we propose to use conditional generative diffusion models for seismic velocity synthesis, in which we readily incorporate those priors. This approach enables the generation of seismic velocities that closely match the expe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05372</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#38477;&#38454;&#24314;&#27169;&#26041;&#27861;&#30740;&#31350;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Reduced-order modeling of unsteady fluid flow using neural network ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#38750;&#23450;&#24120;&#27969;&#20307;&#27969;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#12290;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#35299;&#20915;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#38477;&#38454;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20197;&#20415;&#33719;&#24471;&#23436;&#20840;&#27169;&#22411;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#22312;&#22788;&#29702;&#31354;&#38388;&#20998;&#24067;&#30340;&#25968;&#25454;&#65288;&#21253;&#25324;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65289;&#26102;&#65292;&#36890;&#24120;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;CAEs&#65289;&#12290;&#22312;&#24212;&#29992;&#20110;&#38750;&#23450;&#24120;&#29289;&#29702;&#38382;&#39064;&#26102;&#65292;&#38477;&#38454;&#27169;&#22411;&#36824;&#38656;&#35201;&#23545;&#20302;&#32500;&#28508;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#12290;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#20013;&#32463;&#24120;&#29992;&#20110;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#26410;&#30693;&#35774;&#35745;&#28857;&#19978;&#36827;&#34892;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#39044;&#27979;&#26102;&#65292;&#32463;&#24120;&#20250;&#36935;&#21040;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#65292;&#21363;&#26089;&#26399;&#38169;&#35823;&#21487;&#33021;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#32047;&#31215;&#24182;&#23548;&#33268;&#36739;&#22823;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#20013;&#24120;&#29992;&#30340;&#35013;&#34955;&#26041;&#27861;&#26469;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02468</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#25506;&#32034;&#30340;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#21516;&#20276;
&lt;/p&gt;
&lt;p&gt;
Fast Peer Adaptation with Context-aware Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#12290;&#36890;&#36807;&#22870;&#21169;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#26377;&#25928;&#35782;&#21035;&#21516;&#20276;&#34892;&#20026;&#27169;&#24335;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20419;&#36827;&#26234;&#33021;&#20307;&#31215;&#26497;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#22312;&#19981;&#30830;&#23450;&#21516;&#20276;&#31574;&#30053;&#26102;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#65292;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#31574;&#30053;&#30340;&#26410;&#30693;&#21516;&#20276;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#35782;&#21035;&#21516;&#20276;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26159;&#36866;&#24212;&#20013;&#36827;&#34892;&#26368;&#20339;&#21453;&#24212;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24403;&#28216;&#25103;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#19988;&#26102;&#38388;&#36328;&#24230;&#24456;&#38271;&#26102;&#65292;&#25506;&#32034;&#26410;&#30693;&#21516;&#20276;&#30340;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#20276;&#35782;&#21035;&#22870;&#21169;&#65292;&#26681;&#25454;&#26234;&#33021;&#20307;&#22312;&#21382;&#21490;&#29615;&#22659;&#19979;&#65288;&#20363;&#22914;&#22810;&#20010;&#22238;&#21512;&#30340;&#35266;&#23519;&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#35782;&#21035;&#21516;&#20276;&#30340;&#34892;&#20026;&#27169;&#24335;&#26469;&#22870;&#21169;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#36825;&#20010;&#22870;&#21169;&#28608;&#21169;&#26234;&#33021;&#20307;&#23398;&#20064;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#25506;&#32034;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#21363;&#22312;&#23545;&#21516;&#20276;&#31574;&#30053;&#19981;&#30830;&#23450;&#26102;&#31215;&#26497;&#23547;&#25214;&#21644;&#25910;&#38598;&#20449;&#24687;&#21453;&#39304;&#65292;&#24182;&#22312;&#26377;&#20449;&#24515;&#26102;&#21033;&#29992;&#19978;&#19979;&#25991;&#25191;&#34892;&#26368;&#20339;&#21453;&#24212;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast adapting to unknown peers (partners or opponents) with different strategies is a key challenge in multi-agent games. To do so, it is crucial for the agent to efficiently probe and identify the peer's strategy, as this is the prerequisite for carrying out the best response in adaptation. However, it is difficult to explore the strategies of unknown peers, especially when the games are partially observable and have a long horizon. In this paper, we propose a peer identification reward, which rewards the learning agent based on how well it can identify the behavior pattern of the peer over the historical context, such as the observation over multiple episodes. This reward motivates the agent to learn a context-aware policy for effective exploration and fast adaptation, i.e., to actively seek and collect informative feedback from peers when uncertain about their policies and to exploit the context to perform the best response when confident. We evaluate our method on diverse testbeds 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.14134</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Diffusion Reward: Learning Rewards via Conditional Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14134
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#23398;&#20064;&#22870;&#21169;&#65292;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#25351;&#23450;&#39044;&#26399;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion Reward&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#26465;&#20214;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20174;&#19987;&#23478;&#35270;&#39057;&#20013;&#23398;&#20064;&#22870;&#21169;&#20197;&#35299;&#20915;&#22797;&#26434;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#19987;&#23478;&#36712;&#36857;&#30340;&#26465;&#20214;&#19979;&#35266;&#23519;&#21040;&#36739;&#20302;&#30340;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;Diffusion Reward&#34987;&#24418;&#24335;&#21270;&#20026;&#36127;&#30340;&#26465;&#20214;&#29109;&#65292;&#40723;&#21169;&#19987;&#23478;&#24335;&#34892;&#20026;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MetaWorld&#21644;Adroit&#30340;10&#20010;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#20197;&#35270;&#35273;&#36755;&#20837;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;Diffusion Reward&#29978;&#33267;&#33021;&#22815;&#25104;&#21151;&#26377;&#25928;&#22320;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22823;&#22823;&#36229;&#36234;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;&#39033;&#30446;&#39029;&#38754;&#21644;&#20195;&#30721;&#65306;https://diffusion-reward.github.io/&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14134v2 Announce Type: replace  Abstract: Learning rewards from expert videos offers an affordable and effective solution to specify the intended behaviors for reinforcement learning tasks. In this work, we propose Diffusion Reward, a novel framework that learns rewards from expert videos via conditional video diffusion models for solving complex visual RL problems. Our key insight is that lower generative diversity is observed when conditioned on expert trajectories. Diffusion Reward is accordingly formalized by the negative of conditional entropy that encourages productive exploration of expert-like behaviors. We show the efficacy of our method over 10 robotic manipulation tasks from MetaWorld and Adroit with visual input and sparse reward. Moreover, Diffusion Reward could even solve unseen tasks successfully and effectively, largely surpassing baseline methods. Project page and code: https://diffusion-reward.github.io/.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.12731</link><description>&lt;p&gt;
SHAP&#35780;&#20998;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22312;&#26410;&#30693;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#19979;&#30340;SHAP&#35780;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#23646;&#20998;&#25968;&#21453;&#26144;&#20102;&#36755;&#20837;&#23454;&#20307;&#20013;&#30340;&#29305;&#24449;&#20540;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#20854;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#35780;&#20998;&#20043;&#19968;&#26159;SHAP&#35780;&#20998;&#65292;&#23427;&#26159;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;&#35813;&#35780;&#20998;&#30340;&#23450;&#20041;&#20381;&#36182;&#20110;&#23454;&#20307;&#32676;&#20307;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30001;&#20110;&#36890;&#24120;&#19981;&#30693;&#36947;&#31934;&#30830;&#30340;&#20998;&#24067;&#65292;&#22240;&#27492;&#38656;&#35201;&#20027;&#35266;&#22320;&#36827;&#34892;&#20998;&#37197;&#25110;&#20174;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#29305;&#24449;&#35780;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#19981;&#30693;&#36947;&#23454;&#20307;&#32676;&#20307;&#20998;&#24067;&#30340;SHAP&#35780;&#20998;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21253;&#21547;&#28508;&#22312;&#20998;&#24067;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#22495;&#65292;&#32780;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#25104;&#20026;&#22312;&#35813;&#21306;&#22495;&#19978;&#23450;&#20041;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25214;&#21040;&#35813;&#20989;&#25968;&#30340;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#25152;&#26377;&#29305;&#24449;&#30340;SHAP&#35780;&#20998;&#30340;&#32039;&#26463;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution scores reflect how important the feature values in an input entity are for the output of a machine learning model. One of the most popular attribution scores is the SHAP score, which is an instantiation of the general Shapley value used in coalition game theory. The definition of this score relies on a probability distribution on the entity population. Since the exact distribution is generally unknown, it needs to be assigned subjectively or be estimated from data, which may lead to misleading feature scores. In this paper, we propose a principled framework for reasoning on SHAP scores under unknown entity population distributions. In our framework, we consider an uncertainty region that contains the potential distributions, and the SHAP score of a feature becomes a function defined over this region. We study the basic problems of finding maxima and minima of this function, which allows us to determine tight ranges for the SHAP scores of all features. In particular, we pinp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12546</link><description>&lt;p&gt;
&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
On Building Myopic MPC Policies using Supervised Learning. (arXiv:2401.12546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#26500;&#24314;&#36817;&#35270;MPC&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#32780;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#20013;&#65292;&#32467;&#21512;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#22312;&#36817;&#20284;&#26174;&#24335;MPC&#39046;&#22495;&#65292;&#20854;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#36890;&#36807;&#31163;&#32447;&#29983;&#25104;&#30340;&#26368;&#20339;&#29366;&#24577;-&#21160;&#20316;&#23545;&#26469;&#23398;&#20064;MPC&#31574;&#30053;&#12290;&#34429;&#28982;&#36817;&#20284;&#26174;&#24335;MPC&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#22797;&#21046;MPC&#31574;&#30053;&#65292;&#29992;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#22312;&#32447;&#20248;&#21270;&#65292;&#20294;&#36890;&#24120;&#20250;&#22833;&#21435;&#35299;&#20915;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#65292;&#21363;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#26368;&#20248;&#20540;&#20989;&#25968;&#32780;&#19981;&#26159;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#22312;&#19968;&#20010;&#38750;&#24120;&#30701;&#30340;&#39044;&#27979;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#23558;&#20854;&#20316;&#20026;&#36817;&#35270;MPC&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#22312;&#32447;&#35745;&#31639;&#36127;&#25285;&#65292;&#19981;&#24433;&#21709;&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of supervised learning techniques in combination with model predictive control (MPC) has recently generated significant interest, particularly in the area of approximate explicit MPC, where function approximators like deep neural networks are used to learn the MPC policy via optimal state-action pairs generated offline. While the aim of approximate explicit MPC is to closely replicate the MPC policy, substituting online optimization with a trained neural network, the performance guarantees that come with solving the online optimization problem are typically lost. This paper considers an alternative strategy, where supervised learning is used to learn the optimal value function offline instead of learning the optimal policy. This can then be used as the cost-to-go function in a myopic MPC with a very short prediction horizon, such that the online computation burden reduces significantly without affecting the controller performance. This approach differs from existing wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10306</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#21453;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#31243;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#37117;&#26159;&#38750;&#32447;&#24615;&#19988;&#21464;&#21270;&#30340;&#12290;&#22312;&#31532;&#19968;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21463;&#31354;&#38388;&#21464;&#21270;&#30340;&#31995;&#32479;&#35823;&#24046;&#65288;&#21363;&#20559;&#24046;&#65292;&#20063;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65289;&#20559;&#31227;&#30340;&#25968;&#25454;&#12290;&#20219;&#21153;&#26159;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#30495;&#23454;&#29366;&#24577;&#65292;&#21363;PDE&#30340;&#35299;&#12290;&#22312;&#31532;&#20108;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;PDE&#35299;&#30340;&#31232;&#30095;&#20449;&#24687;&#12290;&#20219;&#21153;&#26159;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#31354;&#38388;&#20013;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PC-CNN&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#30340;&#26102;&#38388;&#31383;&#21475;&#26041;&#26696;&#32422;&#26463;PDE&#26469;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PC-CNN&#22312;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#35299;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#20197;&#21450;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#21518;&#32773;&#25551;&#36848;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#26102;&#31354;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
&lt;/p&gt;</description></item><item><title>&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.08919</link><description>&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#23545;&#27604;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08919
&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#26631;&#21270;&#22312;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#26412;&#21487;&#35835;&#24615;&#21644;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#26631;&#35760;&#27599;&#20010;&#31526;&#21512;&#26465;&#20214;&#30340;&#23383;&#31526;&#65288;&#20840;&#38899;&#26631;&#21270;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;PD&#65289;&#26159;&#36873;&#25321;&#26631;&#35760;&#23376;&#38598;&#20197;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24110;&#21161;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#22810;&#30340;&#38899;&#26631;&#31526;&#21495;&#20250;&#22952;&#30861;&#29087;&#32451;&#35835;&#32773;&#65292;&#38477;&#20302;&#38405;&#35835;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#37096;&#20998;&#26631;&#35760;&#30340;&#25991;&#26412;&#36890;&#24120;&#27604;&#23436;&#20840;&#26631;&#35760;&#30340;&#25991;&#26412;&#26356;&#23481;&#26131;&#38405;&#35835;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#32431;&#25991;&#26412;&#26356;&#23481;&#26131;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;-&#19968;&#31181;&#19982;&#29616;&#26377;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;CCPD&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#20004;&#27425;&#22788;&#29702;&#65292;&#19968;&#27425;&#26377;&#19978;&#19979;&#25991;&#65292;&#19968;&#27425;&#27809;&#26377;&#65292;&#24182;&#19988;&#21482;&#23545;&#20004;&#27425;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23383;&#31526;&#36827;&#34892;&#38899;&#26631;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;</title><link>http://arxiv.org/abs/2310.20609</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#35299;&#20915;&#22270;&#21305;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph Matching via convex relaxation to the simplex. (arXiv:2310.20609v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#20984;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#20855;&#26377;&#21807;&#19968;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#21305;&#37197;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#35813;&#38382;&#39064;&#21253;&#25324;&#22312;&#20004;&#20010;&#36755;&#20837;&#22270;&#20043;&#38388;&#25214;&#21040;&#26368;&#20339;&#23545;&#40784;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#32593;&#32476;&#21435;&#21311;&#21517;&#21270;&#21644;&#34507;&#30333;&#36136;&#23545;&#40784;&#31561;&#39046;&#22495;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;NP&#38590;&#38382;&#39064;&#8220;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#8221;&#65288;QAP&#65289;&#36827;&#34892;&#20984;&#26494;&#24347;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#21363;&#23545;&#21333;&#20301;&#21333;&#32431;&#24418;&#36827;&#34892;&#26494;&#24347;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#20855;&#26377;&#38381;&#21512;&#36845;&#20195;&#24418;&#24335;&#30340;&#39640;&#25928;&#38236;&#20687;&#19979;&#38477;&#26041;&#26696;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#22312;&#30456;&#20851;&#39640;&#26031;Wigner&#27169;&#22411;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#32431;&#24418;&#26494;&#24347;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#20855;&#26377;&#21807;&#19968;&#35299;&#12290;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#65292;&#36825;&#34987;&#35777;&#26126;&#21487;&#20197;&#31934;&#30830;&#24674;&#22797;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#36755;&#20837;&#30697;&#38453;&#20551;&#35774;&#26465;&#20214;&#65292;&#29992;&#20110;&#26631;&#20934;&#36138;&#24515;&#21462;&#25972;&#26041;&#27861;&#65292;&#24182;&#19988;&#36825;&#20010;&#26465;&#20214;&#27604;&#24120;&#29992;&#30340;&#8220;&#23545;&#35282;&#32447;&#20248;&#21183;&#8221;&#26465;&#20214;&#26356;&#23485;&#26494;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26465;&#20214;&#35777;&#26126;&#20102;&#22320;&#38754;&#30495;&#23454;&#25490;&#21015;&#30340;&#31934;&#30830;&#19968;&#27493;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the Graph Matching problem, which consists of finding the best possible alignment between two input graphs, and has many applications in computer vision, network deanonymization and protein alignment. A common approach to tackle this problem is through convex relaxations of the NP-hard \emph{Quadratic Assignment Problem} (QAP).  Here, we introduce a new convex relaxation onto the unit simplex and develop an efficient mirror descent scheme with closed-form iterations for solving this problem. Under the correlated Gaussian Wigner model, we show that the simplex relaxation admits a unique solution with high probability. In the noiseless case, this is shown to imply exact recovery of the ground truth permutation. Additionally, we establish a novel sufficiency condition for the input matrix in standard greedy rounding methods, which is less restrictive than the commonly used `diagonal dominance' condition. We use this condition to show exact one-step recovery of the gro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03720</link><description>&lt;p&gt;
HeaP: &#20351;&#29992;LLMs&#36827;&#34892;&#23618;&#27425;&#21270;Web&#21160;&#20316;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#37327;&#25968;&#25454;&#21644;&#38646;-shot&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25945;&#25480;LLMs&#22312;Web&#19978;&#25191;&#34892;&#20219;&#21153;&#38754;&#20020;&#30528;&#22522;&#26412;&#25361;&#25112; - &#32452;&#21512;&#24615;&#22823;&#30340;&#24320;&#25918;&#19990;&#30028;&#20219;&#21153;&#21644;Web&#25509;&#21475;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;LLMs&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#32423;&#30340;&#38381;&#29615;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31574;&#30053;&#26500;&#25104;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#27861;&#65292;&#21363;&#26032;&#30340;Web&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#31574;&#30053;&#30340;&#32452;&#21512;&#26469;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;LLMs&#30340;Hierarchical Policies for Web Actions&#65288;HeaP&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#19968;&#32452;&#23618;&#27425;&#21270;&#30340;LLM&#25552;&#31034;&#26469;&#35268;&#21010;&#39640;&#32423;&#20219;&#21153;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#31574;&#30053;&#25191;&#34892;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#22871;Web&#20219;&#21153;&#65292;&#21253;&#25324;MiniWoB++&#65292;WebArena&#65292;&#27169;&#25311;&#33322;&#31354;CRM&#20197;&#21450;&#23454;&#38469;&#32593;&#31449;&#26469;&#35780;&#20272;HeaP&#19982;&#19968;&#31995;&#21015;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website i
&lt;/p&gt;</description></item><item><title>DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;</title><link>http://arxiv.org/abs/2310.02027</link><description>&lt;p&gt;
DeepHGCN&#65306;&#26397;&#30528;&#26356;&#28145;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02027
&lt;/p&gt;
&lt;p&gt;
DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HGCN&#65289;&#22312;&#25552;&#21462;&#20998;&#23618;&#22270;&#20449;&#24687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#21452;&#26354;&#25805;&#20316;&#21644;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;HGCN&#21463;&#38480;&#20110;&#27973;&#23618;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;GCNs&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20294;&#26159;&#24320;&#21457;&#21452;&#26354;&#27835;&#30103;&#26041;&#27861;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25805;&#20316;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#20197;&#36866;&#24212;&#21452;&#26354;&#24615;&#36136;&#12290;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepHGCN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22823;&#22823;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#25928;&#26524;&#30340;&#28145;&#23618;&#22810;&#23618;HGCN&#26550;&#26500;&#12290;DeepHGCN&#20855;&#26377;&#20004;&#20010;&#28145;&#23618;HGCN&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#32447;&#24615;&#26144;&#23556;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26377;&#25928;&#30340;&#21452;&#26354;&#27531;&#24046;&#36830;&#25509;&#21644;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20419;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.10275</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#24378;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;CRAMP&#65292;&#19968;&#31181;&#22522;&#20110;&#22686;&#24378;&#24335;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#65292;&#26088;&#22312;&#20026;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31354;&#20013;&#32676;&#20307;&#12289;&#33258;&#21160;&#21270;&#20179;&#20648;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#21035;&#65306;&#38598;&#20013;&#24335;&#35268;&#21010;&#21644;&#20998;&#25955;&#24335;&#35268;&#21010;&#12290;&#38598;&#20013;&#24335;&#35268;&#21010;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#65292;&#22240;&#27492;&#22312;&#22823;&#22411;&#21644;&#22797;&#26434;&#29615;&#22659;&#20013;&#19981;&#20855;&#22791;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20998;&#25955;&#24335;&#35268;&#21010;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#26102;&#36335;&#24452;&#35268;&#21010;&#65292;&#23637;&#31034;&#20102;&#38544;&#24335;&#30340;&#21327;&#35843;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#23427;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#19988;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CRAMP&#30340;&#20247;&#21253;&#24863;&#30693;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#24335;&#35838;&#31243;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.06422</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#30340;&#25935;&#24863;&#24615;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21644;&#23485;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Mixed-Precision Quantization and Width Optimization of Deep Neural Networks Through Cluster-Based Tree-Structured Parzen Estimation. (arXiv:2308.06422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#30340;&#20301;&#23485;&#21644;&#23618;&#23485;&#26469;&#25552;&#39640;&#32593;&#32476;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#21098;&#26525;&#21644;&#32858;&#31867;&#25216;&#26415;&#65292;&#20248;&#21270;&#20102;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25552;&#39640;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#26377;&#25928;&#20248;&#21270;&#26041;&#27861;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25628;&#32034;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#36873;&#25321;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26368;&#20339;&#20301;&#23485;&#21644;&#23618;&#23485;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25928;&#29575;&#30340;&#26126;&#26174;&#25552;&#39640;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;Hessian&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26377;&#36873;&#25321;&#22320;&#20943;&#23569;&#25628;&#32034;&#22495;&#65292;&#30830;&#20445;&#31227;&#38500;&#38750;&#20851;&#38190;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#24320;&#21457;&#26377;&#21033;&#21644;&#19981;&#21033;&#32467;&#26524;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#20801;&#35768;&#23545;&#26550;&#26500;&#21487;&#33021;&#24615;&#36827;&#34892;&#31616;&#21270;&#30340;&#25506;&#32034;&#65292;&#24182;&#36805;&#36895;&#30830;&#23450;&#34920;&#29616;&#26368;&#22909;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#23545;&#30693;&#21517;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#26126;&#26174;&#20248;&#21183;&#12290;&#19982;&#39046;&#20808;&#30340;&#21387;&#32553;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the complexity and computational demands of deep learning models rise, the need for effective optimization methods for neural network designs becomes paramount. This work introduces an innovative search mechanism for automatically selecting the best bit-width and layer-width for individual neural network layers. This leads to a marked enhancement in deep neural network efficiency. The search domain is strategically reduced by leveraging Hessian-based pruning, ensuring the removal of non-crucial parameters. Subsequently, we detail the development of surrogate models for favorable and unfavorable outcomes by employing a cluster-based tree-structured Parzen estimator. This strategy allows for a streamlined exploration of architectural possibilities and swift pinpointing of top-performing designs. Through rigorous testing on well-known datasets, our method proves its distinct advantage over existing methods. Compared to leading compression strategies, our approach records an impressive 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#25512;&#26029;&#33410;&#28857;&#30340;&#21160;&#24577;&#25490;&#24207;&#65292;&#36890;&#36807;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23454;&#29616;&#65292;&#20165;&#38656;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.13544</link><description>&lt;p&gt;
&#22312;&#32593;&#32476;&#20013;&#39640;&#25928;&#21160;&#24577;&#25490;&#24207;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A model for efficient dynamical ranking in networks. (arXiv:2307.13544v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#25512;&#26029;&#33410;&#28857;&#30340;&#21160;&#24577;&#25490;&#24207;&#65292;&#36890;&#36807;&#27714;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#23454;&#29616;&#65292;&#20165;&#38656;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23450;&#21521;&#26102;&#24577;&#32593;&#32476;&#20013;&#30340;&#21160;&#24577;&#25490;&#24207; - &#27599;&#20010;&#23450;&#21521;&#19988;&#24102;&#26102;&#38388;&#25139;&#30340;&#36793;&#21453;&#26144;&#20102;&#19968;&#23545;&#20132;&#20114;&#30340;&#32467;&#26524;&#21644;&#26102;&#38388;&#12290;&#27599;&#20010;&#33410;&#28857;&#30340;&#25512;&#26029;&#25490;&#24207;&#26159;&#23454;&#20540;&#19988;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#65292;&#27599;&#27425;&#26032;&#30340;&#36793;&#32536;&#37117;&#20250;&#25552;&#39640;&#25110;&#38477;&#20302;&#33410;&#28857;&#30340;&#20272;&#35745;&#24378;&#24230;&#25110;&#22768;&#26395;&#65292;&#36825;&#22312;&#30495;&#23454;&#24773;&#26223;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#65292;&#21253;&#25324;&#28216;&#25103;&#24207;&#21015;&#65292;&#38182;&#26631;&#36187;&#25110;&#21160;&#29289;&#31561;&#32423;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#27714;&#35299;&#19968;&#32452;&#32447;&#24615;&#26041;&#31243;&#26469;&#24037;&#20316;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#35843;&#25972;&#19968;&#20010;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#23545;&#24212;&#30340;&#31639;&#27861;&#26159;&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#26041;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#39044;&#27979;&#20132;&#20114;&#65288;&#36793;&#32536;&#23384;&#22312;&#65289;&#21450;&#20854;&#32467;&#26524;&#65288;&#36793;&#32536;&#26041;&#21521;&#65289;&#30340;&#33021;&#21147;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#21160;&#24577;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a physics-inspired method for inferring dynamic rankings in directed temporal networks - networks in which each directed and timestamped edge reflects the outcome and timing of a pairwise interaction. The inferred ranking of each node is real-valued and varies in time as each new edge, encoding an outcome like a win or loss, raises or lowers the node's estimated strength or prestige, as is often observed in real scenarios including sequences of games, tournaments, or interactions in animal hierarchies. Our method works by solving a linear system of equations and requires only one parameter to be tuned. As a result, the corresponding algorithm is scalable and efficient. We test our method by evaluating its ability to predict interactions (edges' existence) and their outcomes (edges' directions) in a variety of applications, including both synthetic and real data. Our analysis shows that in many cases our method's performance is better than existing methods for predicting dyna
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.03759</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#25554;&#20540;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks for Time Series: Forecasting, Classification, Imputation, and Anomaly Detection. (arXiv:2307.03759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#24102;&#26469;&#20102;&#26032;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#26159;&#35760;&#24405;&#21160;&#24577;&#31995;&#32479;&#27979;&#37327;&#32467;&#26524;&#30340;&#20027;&#35201;&#25968;&#25454;&#31867;&#22411;&#65292;&#36890;&#36807;&#29289;&#29702;&#20256;&#24863;&#22120;&#21644;&#22312;&#32447;&#36807;&#31243;&#65288;&#34394;&#25311;&#20256;&#24863;&#22120;&#65289;&#29983;&#25104;&#22823;&#37327;&#25968;&#25454;&#12290;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#23545;&#20110;&#25581;&#31034;&#21487;&#29992;&#25968;&#25454;&#20013;&#25152;&#34164;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22522;&#20110;GNN&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#21644;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#20256;&#32479;&#21644;&#20854;&#20182;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;&#65288;GNN4TS&#65289;&#65292;&#21253;&#25324;&#22235;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#39044;&#27979;&#12289;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#25554;&#20540;&#12290;&#25105;&#20204;&#26088;&#22312;&#25351;&#23548;&#35774;&#35745;&#24072;&#21644;&#23454;&#36341;&#32773;&#20102;&#35299;&#12289;&#26500;&#24314;&#24212;&#29992;&#21644;&#25512;&#21160;GNN4TS&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;GNN4TS&#20998;&#31867;&#20307;&#31995;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Time series are the primary data type used to record dynamic system measurements and generated in great volume by both physical sensors and online processes (virtual sensors). Time series analytics is therefore crucial to unlocking the wealth of information implicit in available data. With the recent advancements in graph neural networks (GNNs), there has been a surge in GNN-based approaches for time series analysis. Approaches can explicitly model inter-temporal and inter-variable relationships, which traditional and other deep neural network-based methods struggle to do. In this survey, we provide a comprehensive review of graph neural networks for time series analysis (GNN4TS), encompassing four fundamental dimensions: Forecasting, classification, anomaly detection, and imputation. Our aim is to guide designers and practitioners to understand, build applications, and advance research of GNN4TS. At first, we provide a comprehensive task-oriented taxonomy of GNN4TS. Then, we present a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19476</link><description>&lt;p&gt;
&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#25506;&#32034;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#26159;&#36890;&#36807;&#40723;&#21169;&#23545;&#35775;&#38382;&#29366;&#24577;&#31354;&#38388;&#30340;&#22343;&#21248;&#35206;&#30422;&#26469;&#26368;&#22823;&#21270;&#24050;&#35775;&#38382;&#29366;&#24577;&#20998;&#24067;&#30340;&#29109;&#65292;&#21363;&#29366;&#24577;&#29109;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26377;&#20219;&#21153;&#22870;&#21169;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#65292;&#20854;&#20013;&#20195;&#29702;&#36235;&#21521;&#20110;&#35775;&#38382;&#39640;&#20215;&#20540;&#29366;&#24577;&#20197;&#21033;&#29992;&#20219;&#21153;&#22870;&#21169;&#12290;&#36825;&#20010;&#20559;&#22909;&#20250;&#23548;&#33268;&#39640;&#20215;&#20540;&#29366;&#24577;&#21644;&#20302;&#20215;&#20540;&#29366;&#24577;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24403;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#22343;&#21248;&#26102;&#65292;&#29366;&#24577;&#29109;&#20250;&#22686;&#21152;&#65292;&#20174;&#32780;&#20559;&#21521;&#20110;&#25506;&#32034;&#20302;&#20215;&#20540;&#21306;&#22495;&#12290;&#24403;&#39640;&#20215;&#20540;&#29366;&#24577;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20998;&#24067;&#29421;&#31364;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#65292;&#23427;&#20998;&#21035;&#20272;&#35745;&#27599;&#20010;&#29366;&#24577;&#20215;&#20540;&#20272;&#35745;&#26465;&#20214;&#19979;&#30340;&#29366;&#24577;&#29109;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#21152;&#26435;&#21644;&#12290;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#37327;&#21270;&#20102;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#21306;&#22495;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#20351;&#20854;&#23545;&#19981;&#24179;&#34913;&#38382;&#39064;&#26356;&#21152;&#20581;&#22766;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
&lt;/p&gt;</description></item><item><title>CREMP&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21253;&#21547;&#36229;&#36807;3&#21315;&#19975;&#20010;&#22823;&#29615;&#32957;&#26500;&#35937;&#24418;&#29366;&#65292;&#26088;&#22312;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#12290;</title><link>http://arxiv.org/abs/2305.08057</link><description>&lt;p&gt;
CREMP: &#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#26059;&#36716;&#32452;&#21512;&#38598;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CREMP: Conformer-Rotamer Ensembles of Macrocyclic Peptides for Machine Learning. (arXiv:2305.08057v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08057
&lt;/p&gt;
&lt;p&gt;
CREMP&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21253;&#21547;&#36229;&#36807;3&#21315;&#19975;&#20010;&#22823;&#29615;&#32957;&#26500;&#35937;&#24418;&#29366;&#65292;&#26088;&#22312;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20110;&#27169;&#25311;&#22823;&#29615;&#32957;&#30340;&#26500;&#35937;&#20855;&#26377;&#28508;&#22312;&#30340;&#21551;&#31034;&#24615;&#20316;&#29992;&#65292;&#33021;&#22815;&#23454;&#29616;&#21512;&#29702;&#30340;&#35774;&#35745;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#20280;&#32553;&#30340;&#24314;&#27169;&#22823;&#29615;&#32957;&#20960;&#20309;&#24418;&#29366;&#30340;&#26041;&#27861;&#20173;&#28982;&#24456;&#38590;&#24471;&#21040;&#12290;&#36817;&#26399;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#21152;&#36895;&#20102;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#21644;&#23567;&#20998;&#23376;&#26500;&#35937;&#38598;&#21512;&#30340;&#29983;&#25104;&#65292;&#20294;&#30001;&#20110;&#22823;&#29615;&#32957;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#22312;&#22823;&#29615;&#32957;&#30340;&#24314;&#27169;&#26041;&#38754;&#20173;&#26410;&#21462;&#24471;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CREMP&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#26469;&#24555;&#36895;&#24320;&#21457;&#21644;&#35780;&#20272;&#22823;&#29615;&#32957;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#12290;CREMP&#21253;&#21547;36,198&#20010;&#29420;&#29305;&#30340;&#22823;&#29615;&#32957;&#21450;&#20854;&#21033;&#29992;&#26500;&#35937;-&#26059;&#36716;&#32452;&#21512;&#37319;&#26679;&#24037;&#20855;&#65288;CREST&#65289;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#26500;&#35937;&#38598;&#21512;&#12290;&#24635;&#20043;&#65292;&#36825;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#36817;3130&#19975;&#20010;&#29420;&#29305;&#30340;&#22823;&#29615;&#32957;&#20960;&#20309;&#24418;&#29366;&#65292;&#27599;&#20010;&#24418;&#29366;&#37117;&#29992;&#21322;&#32463;&#39564;&#25193;&#23637;&#32039;&#26463;&#32538;&#65288;xTB&#65289;DF&#23548;&#20986;&#30340;&#33021;&#37327;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational and machine learning approaches to model the conformational landscape of macrocyclic peptides have the potential to enable rational design and optimization. However, accurate, fast, and scalable methods for modeling macrocycle geometries remain elusive. Recent deep learning approaches have significantly accelerated protein structure prediction and the generation of small-molecule conformational ensembles, yet similar progress has not been made for macrocyclic peptides due to their unique properties. Here, we introduce CREMP, a resource generated for the rapid development and evaluation of machine learning models for macrocyclic peptides. CREMP contains 36,198 unique macrocyclic peptides and their high-quality structural ensembles generated using the Conformer-Rotamer Ensemble Sampling Tool (CREST). Altogether, this new dataset contains nearly 31.3 million unique macrocycle geometries, each annotated with energies derived from semi-empirical extended tight-binding (xTB) DF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.17765</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20284;&#30340;&#32447;&#24615;&#34920;&#31034;&#65306;&#36866;&#24212;&#24615;&#12289;&#26497;&#23567;&#21270;&#12289;&#20197;&#21450;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#27424;&#32570;&#12290;&#26412;&#25991;&#26088;&#22312;&#29702;&#35299;&#20174;&#20855;&#26377;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#32447;&#24615;&#34920;&#31034;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#24120;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#20219;&#21153;&#25110;&#20165;&#30446;&#26631;&#23398;&#20064;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.13458</link><description>&lt;p&gt;
&#31561;&#21464;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#31561;&#21464;&#21644;&#22686;&#24378;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#22312;&#33258;&#28982;&#20551;&#35774;&#19979;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#21644;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20294;&#22686;&#24378;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#23545;&#31216;&#25968;&#25454;&#19978;&#20248;&#21270;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#38480;&#21046;&#26550;&#26500;&#31561;&#21464;&#21644;&#20351;&#29992;&#22686;&#24378;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#23545;&#25439;&#22833;&#21644;&#38750;&#32447;&#24615;&#24615;&#36827;&#34892;&#33258;&#28982;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#21464;&#31283;&#23450;&#28857;&#30340;&#38598;&#21512;&#23545;&#20110;&#36825;&#20004;&#31181;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#19988;&#31561;&#21464;&#23618;&#30340;&#38598;&#21512;&#22312;&#22686;&#24378;&#27169;&#22411;&#30340;&#26799;&#24230;&#27969;&#19979;&#26159;&#19981;&#21464;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#31561;&#21464;&#27169;&#22411;&#30340;&#31283;&#23450;&#28857;&#26159;&#31283;&#23450;&#30340;&#65292;&#22686;&#24378;&#35757;&#32451;&#30340;&#31283;&#23450;&#28857;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#21152;&#24555;&#32593;&#32476;&#35780;&#20272;&#36895;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#38477;&#20302;&#32593;&#32476;&#20934;&#30830;&#24230;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2212.12921</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#30340;&#24191;&#20041;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning k-Level Sparse Neural Networks Using a New Generalized Weighted Group Sparse Envelope Regularization. (arXiv:2212.12921v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#23398;&#20064;k&#32423;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#35777;&#32593;&#32476;&#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#21152;&#24555;&#32593;&#32476;&#35780;&#20272;&#36895;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20960;&#20046;&#19981;&#38477;&#20302;&#32593;&#32476;&#20934;&#30830;&#24230;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#26080;&#32467;&#26500;&#21644;&#26377;&#32467;&#26500;&#31232;&#30095;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;"&#21152;&#26435;&#32452;&#31232;&#30095;&#21253;&#32476;&#20989;&#25968;" (WGSEF) &#30340;&#31232;&#30095;&#21253;&#32476;&#20989;&#25968;&#30340;&#26032;&#24191;&#20041;&#12290;WGSEF&#20316;&#20026;&#19968;&#20010;&#31070;&#32463;&#20803;&#32452;&#36873;&#25321;&#22120;&#65292;&#29992;&#20110;&#24341;&#23548;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#20445;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNN) &#30340;&#30828;&#20214;&#21451;&#22909;&#30340;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#20197;&#26377;&#25928;&#21152;&#36895;DNN&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26159;&#21487;&#36866;&#24212;&#30340;&#65292;&#20801;&#35768;&#20219;&#20309;&#30828;&#20214;&#25351;&#23450;&#32452;&#23450;&#20041;&#65292;&#22914;&#28388;&#27874;&#22120;&#12289;&#36890;&#36947;&#12289;&#28388;&#27874;&#22120;&#24418;&#29366;&#12289;&#23618;&#28145;&#24230;&#12289;&#21333;&#20010;&#21442;&#25968; (&#26080;&#32467;&#26500;)&#31561;&#12290;&#30001;&#20110;WGSEF&#30340;&#29305;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#25910;&#25947;&#26102;&#39044;&#23450;&#20041;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#21516;&#26102;&#20445;&#25345;&#32593;&#32476;&#20934;&#30830;&#24230;&#30340;&#26497;&#23567;&#38477;&#20302;&#29978;&#33267;&#25913;&#21892;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#26469;&#35745;&#31639;&#31934;&#30830;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to learn both unstructured and structured sparse neural networks during training, utilizing a novel generalization of the sparse envelope function (SEF) used as a regularizer, termed {\itshape{weighted group sparse envelope function}} (WGSEF). The WGSEF acts as a neuron group selector, which is leveraged to induce structured sparsity. The method ensures a hardware-friendly structured sparsity of a deep neural network (DNN) to efficiently accelerate the DNN's evaluation. Notably, the method is adaptable, letting any hardware specify group definitions, such as filters, channels, filter shapes, layer depths, a single parameter (unstructured), etc. Owing to the WGSEF's properties, the proposed method allows to a pre-define sparsity level that would be achieved at the training convergence, while maintaining negligible network accuracy degradation or even improvement in the case of redundant parameters. We introduce an efficient technique to calculate the exact
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22270;&#24418;&#26631;&#20934;&#21644;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;CIP&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20844;&#24179;&#24615;&#12289;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.09768</link><description>&lt;p&gt;
&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Counterfactually Invariant Predictors. (arXiv:2207.09768v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22270;&#24418;&#26631;&#20934;&#21644;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;CIP&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20844;&#24179;&#24615;&#12289;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#19981;&#21464;&#24615;&#65288;CI&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20844;&#24179;&#12289;&#24378;&#20581;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#39044;&#27979;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24418;&#26631;&#20934;&#65292;&#23427;&#20197;&#35266;&#27979;&#20998;&#24067;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20316;&#20026;&#39044;&#27979;&#22120;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Counterfactually Invariant Prediction&#65288;CIP&#65289;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#22522;&#20110;Hilbert-Schmidt&#26465;&#20214;&#29420;&#31435;&#20934;&#21017;&#65288;HSCIC&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26465;&#20214;&#20381;&#36182;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#21253;&#25324;&#26631;&#37327;&#21644;&#22810;&#21464;&#37327;&#35774;&#32622;&#22312;&#20869;&#30340;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;CIP&#22312;&#24378;&#21046;&#21453;&#20107;&#23454;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Notions of counterfactual invariance (CI) have proven essential for predictors that are fair, robust, and generalizable in the real world. We propose graphical criteria that yield a sufficient condition for a predictor to be counterfactually invariant in terms of a conditional independence in the observational distribution. In order to learn such predictors, we propose a model-agnostic framework, called Counterfactually Invariant Prediction (CIP), building on the Hilbert-Schmidt Conditional Independence Criterion (HSCIC), a kernel-based conditional dependence measure. Our experimental results demonstrate the effectiveness of CIP in enforcing counterfactual invariance across various simulated and real-world datasets including scalar and multi-variate settings.
&lt;/p&gt;</description></item></channel></rss>