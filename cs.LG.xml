<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FairEHR-CLP&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#30340;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#24182;&#21033;&#29992;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#26041;&#27861;&#28040;&#38500;EHR&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00955</link><description>&lt;p&gt;
FairEHR-CLP&#65306;&#20197;&#23545;&#27604;&#23398;&#20064;&#23454;&#29616;&#30340;&#20844;&#24179;&#24863;&#30693;&#22810;&#27169;&#24577;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FairEHR-CLP: Towards Fairness-Aware Clinical Predictions with Contrastive Learning in Multimodal Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00955
&lt;/p&gt;
&lt;p&gt;
FairEHR-CLP&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#30340;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#24182;&#21033;&#29992;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#26041;&#27861;&#28040;&#38500;EHR&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#39044;&#27979;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#21307;&#30103;&#20915;&#31574;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21333;&#27169;&#24577;&#25968;&#25454;&#65292;&#24182;&#26410;&#35299;&#20915;EHR&#20013;&#19982;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#22810;&#26041;&#38754;&#31038;&#20250;&#20559;&#35265;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FairEHR-CLP&#65306;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#20020;&#24202;&#39044;&#27979;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;EHR&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;FairEHR-CLP&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#25805;&#20316;&#65292;&#21033;&#29992;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#12289;&#32437;&#21521;&#25968;&#25454;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#39318;&#20808;&#65292;&#20026;&#27599;&#20010;&#24739;&#32773;&#29983;&#25104;&#21512;&#25104;&#23545;&#24212;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#20154;&#21475;&#32479;&#35745;&#36523;&#20221;&#65292;&#21516;&#26102;&#20445;&#30041;&#24517;&#35201;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#20844;&#24179;&#24863;&#30693;&#39044;&#27979;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#24739;&#32773;&#30340;&#34920;&#31034;&#22312;&#25935;&#24863;&#23646;&#24615;&#19978;&#36827;&#34892;&#23545;&#40784;&#65292;&#19982;&#20855;&#26377;softmax&#23618;&#30340;MLP&#20998;&#31867;&#22120;&#20849;&#21516;&#20248;&#21270;&#29992;&#20110;&#20020;&#24202;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the high-stakes realm of healthcare, ensuring fairness in predictive models is crucial. Electronic Health Records (EHRs) have become integral to medical decision-making, yet existing methods for enhancing model fairness restrict themselves to unimodal data and fail to address the multifaceted social biases intertwined with demographic factors in EHRs. To mitigate these biases, we present FairEHR-CLP: a general framework for Fairness-aware Clinical Predictions with Contrastive Learning in EHRs. FairEHR-CLP operates through a two-stage process, utilizing patient demographics, longitudinal data, and clinical notes. First, synthetic counterparts are generated for each patient, allowing for diverse demographic identities while preserving essential health information. Second, fairness-aware predictions employ contrastive learning to align patient representations across sensitive attributes, jointly optimized with an MLP classifier with a softmax layer for clinical classification tasks. Ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12406</link><description>&lt;p&gt;
&#36890;&#36807;&#32463;&#39564;&#32972;&#26223;&#21644;&#24067;&#26391;&#36816;&#21160;&#36827;&#34892;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Offline Imitation of Badminton Player Behavior via Experiential Contexts and Brownian Motion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RallyNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#36873;&#25163;&#30340;&#20915;&#31574;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#26102;&#21487;&#33021;&#36935;&#21040;&#30340;&#23618;&#27425;&#32467;&#26500;&#21644;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#23548;&#33268;&#30340;&#22797;&#21512;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#21644;&#24555;&#33410;&#22863;&#30340;&#22522;&#20110;&#36718;&#27425;&#30340;&#20307;&#32946;&#36816;&#21160;&#20013;&#65292;&#32701;&#27611;&#29699;&#20316;&#20026;&#19968;&#31181;&#38656;&#35201;&#36873;&#25163;&#20381;&#36182;&#21464;&#21270;&#30340;&#20915;&#31574;&#30340;&#22266;&#26377;&#33539;&#20363;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#34429;&#28982;&#22312;&#39034;&#24207;&#20915;&#31574;&#30340;&#31163;&#32447;&#19987;&#23478;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#25152;&#28041;&#21450;&#65292;&#20294;&#22914;&#20309;&#20174;&#31163;&#32447;&#32701;&#27611;&#29699;&#27604;&#36187;&#20013;&#27169;&#20223;&#20154;&#31867;&#36873;&#25163;&#30340;&#27604;&#36187;&#34892;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22797;&#21046;&#23545;&#25163;&#30340;&#34892;&#20026;&#26377;&#30410;&#20110;&#36873;&#25163;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27604;&#36187;&#21069;&#26377;&#26041;&#21521;&#22320;&#36827;&#34892;&#25112;&#30053;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#26041;&#27861;&#20250;&#21463;&#21040;&#27604;&#36187;&#30340;&#20869;&#22312;&#23618;&#27425;&#32467;&#26500;&#21644;&#30001;&#20110;&#36718;&#27969;&#37319;&#21462;&#34892;&#21160;&#30340;&#36873;&#25163;&#36718;&#27425;&#24615;&#36136;&#32780;&#20135;&#29983;&#30340;&#22797;&#21512;&#25928;&#24212;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RallyNet&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32701;&#27611;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#30340;&#20998;&#23618;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#27169;&#22411;&#65306;&#65288;i&#65289;RallyNet&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#20026;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12406v1 Announce Type: new  Abstract: In the dynamic and rapid tactic involvements of turn-based sports, badminton stands out as an intrinsic paradigm that requires alter-dependent decision-making of players. While the advancement of learning from offline expert data in sequential decision-making has been witnessed in various domains, how to rally-wise imitate the behaviors of human players from offline badminton matches has remained underexplored. Replicating opponents' behavior benefits players by allowing them to undergo strategic development with direction before matches. However, directly applying existing methods suffers from the inherent hierarchy of the match and the compounding effect due to the turn-based nature of players alternatively taking actions. In this paper, we propose RallyNet, a novel hierarchical offline imitation learning model for badminton player behaviors: (i) RallyNet captures players' decision dependencies by modeling decision-making processes as 
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.10967</link><description>&lt;p&gt;
&#26790;&#24819;&#20013;&#30340;&#35768;&#22810;&#19990;&#30028;&#65306;&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#38646;&#26679;&#28857;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Dreaming of Many Worlds: Learning Contextual World Models Aids Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10967
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19978;&#19979;&#25991;&#19990;&#30028;&#27169;&#22411;&#26377;&#21161;&#20110;&#25552;&#39640;&#22312;&#26410;&#30693;&#19978;&#19979;&#25991;&#19979;&#30340;&#38646;&#26679;&#28857;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#28857;&#27867;&#21270;&#65288;Zero-shot generalization&#65292;ZSG&#65289;&#21040;&#26410;&#35265;&#36807;&#30340;&#21160;&#24577;&#23545;&#20110;&#21019;&#24314;&#20855;&#26377;&#26222;&#36941;&#33021;&#21147;&#30340;&#20307;&#31995;&#20195;&#29702;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#65288;contextual reinforcement learning&#65292;cRL&#65289;&#30340;&#31616;&#21333;&#35774;&#32622;&#24320;&#22987;&#65292;&#20551;&#35774;&#21487;&#35266;&#23519;&#21040;&#21442;&#25968;&#21270;&#31995;&#32479;&#21160;&#24577;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20540;&#65292;&#22914;&#26426;&#22120;&#20154;&#30340;&#36136;&#37327;&#25110;&#23610;&#23544;&#65292;&#32780;&#19981;&#23545;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#30340;&#21487;&#35266;&#23519;&#24615;&#20570;&#36827;&#19968;&#27493;&#31616;&#21270;&#20551;&#35774;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;ZSG&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24490;&#29615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;contextual recurrent state-space model&#65292;cRSSM&#65289;&#65292;&#23427;&#23545;Dreamer&#65288;v3&#65289;&#65288;Hafner&#31561;&#20154;&#65292;2023&#24180;&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20351;&#24471;&#19990;&#30028;&#27169;&#22411;&#21487;&#20197;&#34701;&#20837;&#19978;&#19979;&#25991;&#20197;&#20174;&#35266;&#23519;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#24182;&#24314;&#27169;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31995;&#32479;&#24615;&#22320;&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#20013;&#25552;&#39640;&#20102;&#22312;&#8220;&#26790;&#22659;&#8221;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;ZSG&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10967v1 Announce Type: cross  Abstract: Zero-shot generalization (ZSG) to unseen dynamics is a major challenge for creating generally capable embodied agents. To address the broader challenge, we start with the simpler setting of contextual reinforcement learning (cRL), assuming observability of the context values that parameterize the variation in the system's dynamics, such as the mass or dimensions of a robot, without making further simplifying assumptions about the observability of the Markovian state. Toward the goal of ZSG to unseen variation in context, we propose the contextual recurrent state-space model (cRSSM), which introduces changes to the world model of the Dreamer (v3) (Hafner et al., 2023). This allows the world model to incorporate context for inferring latent Markovian states from the observations and modeling the latent dynamics. Our experiments show that such systematic incorporation of the context improves the ZSG of the policies trained on the ``dreams
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05996</link><description>&lt;p&gt;
&#29992;&#39640;&#26356;&#26032;&#27604;&#20363;&#21078;&#26512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#24212;&#23545;&#20215;&#20540;&#39640;&#20272;&#21644;&#21457;&#25955;
&lt;/p&gt;
&lt;p&gt;
Dissecting Deep RL with High Update Ratios: Combatting Value Overestimation and Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39318;&#35201;&#20559;&#24046;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#22823;&#37327;&#26356;&#26032;&#27604;&#20363;&#19979;&#65292;&#20215;&#20540;&#39640;&#20272;&#26159;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#26681;&#26412;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35774;&#32622;&#20013;&#21487;&#20197;&#22312;&#26799;&#24230;&#26356;&#26032;&#27425;&#25968;&#22823;&#22823;&#36229;&#36807;&#29615;&#22659;&#26679;&#26412;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#37325;&#32622;&#32593;&#32476;&#21442;&#25968;&#12290;&#22312;&#36825;&#31181;&#22823;&#37327;&#26356;&#26032;&#19982;&#25968;&#25454;&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23612;&#22522;&#36763;&#31561;&#20154; (2022) &#30340;&#26368;&#36817;&#19968;&#39033;&#30740;&#31350;&#25351;&#20986;&#20102;&#19968;&#20010;&#39318;&#35201;&#20559;&#24046;&#30340;&#20986;&#29616;&#65292;&#21363;&#20195;&#29702;&#22312;&#26089;&#26399;&#20132;&#20114;&#20013;&#36807;&#25311;&#21512;&#24182;&#28129;&#21270;&#21518;&#32493;&#32463;&#39564;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#20854;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#35299;&#26512;&#20102;&#23548;&#33268;&#39318;&#35201;&#20559;&#24046;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#24212;&#35813;&#23548;&#33268;&#23398;&#20064;&#22833;&#36133;&#30340;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#26159;&#38271;&#26399;&#20197;&#26469;&#23384;&#22312;&#30340;&#38382;&#39064;&#65306;&#20215;&#20540;&#39640;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;Q&#20540;&#19981;&#20165;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#34987;&#39640;&#20272;&#65292;&#32780;&#19988;&#22312;&#20998;&#24067;&#20869;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#65292;&#21487;&#20197;&#36861;&#28335;&#21040;&#30001;&#20248;&#21270;&#22120;&#21160;&#37327;&#25512;&#21160;&#30340;&#26410;&#35265;&#30340;&#21160;&#20316;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21333;&#20301;&#29699;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#26356;&#26032;&#27604;&#20363;&#19979;&#23454;&#29616;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05996v1 Announce Type: cross  Abstract: We show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. Under such large update-to-data ratios, a recent study by Nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. In this work, we dissect the phenomena underlying the primacy bias. We inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. Overinflated Q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. We employ a simple unit-ball normalization that enables learning under large update ratios
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.02411</link><description>&lt;p&gt;
NiNformer: &#19968;&#31181;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#30340;&#32593;&#32476;&#20013;&#32593;&#32476;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#31216;&#20026;NiNformer&#65292;&#20855;&#26377;&#20196;&#29260;&#28151;&#21512;&#29983;&#25104;&#38376;&#25511;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#27880;&#24847;&#26426;&#21046;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#21644;&#25968;&#25454;&#38598;&#35201;&#27714;&#22823;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#33258;&#24341;&#20837;&#20197;&#26469;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36328;&#36234;&#20102;&#35768;&#22810;&#39046;&#22495;&#21644;&#22810;&#20010;&#20219;&#21153;&#12290;&#35813;&#26426;&#21046;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#34987;&#24212;&#29992;&#20026;Vision Transformer ViT&#65292;&#24182;&#19988;&#20854;&#29992;&#36884;&#24050;&#25193;&#23637;&#21040;&#35270;&#35273;&#39046;&#22495;&#30340;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#20998;&#31867;&#12289;&#20998;&#21106;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#22270;&#20687;&#29983;&#25104;&#12290;&#23613;&#31649;&#35813;&#26426;&#21046;&#38750;&#24120;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#33021;&#21147;&#65292;&#20294;&#20854;&#32570;&#28857;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#26377;&#25928;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#35774;&#35745;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#21644;&#32531;&#35299;&#25968;&#25454;&#22823;&#23567;&#35201;&#27714;&#12290;&#22312;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20123;&#23581;&#35797;&#30340;&#20363;&#23376;&#21253;&#25324;MLP-Mixer&#12289;Conv-Mixer&#12289;Perciver-IO&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#22359;&#65292;&#20316;&#20026;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#65292;&#26080;&#38656;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.16517</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#21457;&#29616;&#19981;&#36830;&#32493;Galerkin&#36924;&#36817;&#23432;&#24658;&#23450;&#24459;&#30340;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16517
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#65292;&#26080;&#38656;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#39640;&#38454;&#23432;&#24658;&#23450;&#24459;&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#19981;&#36830;&#32493;&#22788;&#38754;&#20020;Gibbs&#29616;&#35937;&#25361;&#25112;&#12290;&#20154;&#24037;&#31896;&#24615;&#26159;&#22522;&#20110;&#29289;&#29702;&#35265;&#35299;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#38750;&#30417;&#30563;&#33539;&#24335;&#19979;&#30340;&#20154;&#24037;&#31896;&#24615;&#27169;&#22411;&#12290;&#35813;&#31639;&#27861;&#21463;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#23450;&#20041;&#20026;&#30456;&#23545;&#21442;&#32771;&#35299;&#30340;&#24046;&#24322;&#30340;&#25439;&#22833;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#21333;&#20803;&#26684;&#36880;&#20010;&#21333;&#20803;&#26684;&#25805;&#20316;&#65288;&#31896;&#24615;&#27169;&#22411;&#65289;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#26080;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#20854;&#25972;&#21512;&#21040;&#26368;&#20808;&#36827;&#30340;Runge-Kutta&#19981;&#36830;&#32493;Galerkin&#27714;&#35299;&#22120;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#22312;&#26631;&#37327;&#21644;&#30690;&#37327;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20960;&#20010;&#25968;&#20540;&#27979;&#35797;&#65292;&#22914;Burgers'&#21644;Euler&#30340;&#26041;&#31243;&#22312;&#19968;&#32500;&#21644;&#20108;&#32500;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16517v1 Announce Type: cross  Abstract: Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon. Artificial viscosity is a popular and effective solution to this problem based on physical insight. In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm. The algorithm is inspired by reinforcement learning and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation. This enables a dataset-free training procedure. We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase several numerical tests on scalar and vectorial problems, such as Burgers' and Euler's equations in one and tw
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.16364</link><description>&lt;p&gt;
&#20174;&#21738;&#37324;&#20986;&#21457;&#65311;&#26469;&#33258;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#20013;&#30340;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16364
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#33258;&#28982;&#31354;&#38388;&#25551;&#36848;&#36827;&#34892;&#22810;&#23610;&#24230;&#31354;&#38388;&#20851;&#31995;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#36890;&#36807;&#33719;&#30693;&#22320;&#22270;&#30693;&#35782;&#24471;&#21040;&#30340;&#25551;&#36848;&#33021;&#22815;&#25552;&#20379;&#29615;&#22659;&#30340;&#25972;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#33258;&#28982;&#35821;&#35328;&#20256;&#36798;&#36335;&#32447;&#26102;&#65292;&#8220;&#33719;&#24471;&#30340;&#31354;&#38388;&#30693;&#35782;&#8221;&#27010;&#24565;&#23545;&#22320;&#29702;&#20449;&#24687;&#26816;&#32034;&#65288;GIR&#65289;&#21644;&#31354;&#38388;&#35748;&#30693;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23548;&#33322;&#30740;&#31350;&#32463;&#24120;&#24573;&#35270;&#36825;&#31181;&#33719;&#24471;&#30693;&#35782;&#23545;&#25991;&#26412;&#25551;&#36848;&#30340;&#24433;&#21709;&#12290;&#24403;&#21069;&#23548;&#33322;&#30740;&#31350;&#38598;&#20013;&#22312;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#26412;&#22320;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#23427;&#23558;&#22312;&#24744;&#30340;&#21491;&#36793;&#8221;&#65289;&#65292;&#36825;&#20123;&#25551;&#36848;&#38656;&#35201;&#23545;&#20195;&#29702;&#20154;&#30340;&#26412;&#22320;&#30693;&#35273;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#22320;&#22270;&#33719;&#24471;&#30340;&#30693;&#35782;&#22522;&#30784;&#19978;&#30340;&#25551;&#36848;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#25972;&#20307;&#35270;&#22270;&#65292;&#24182;&#25429;&#25417;&#20102;&#20854;&#24635;&#20307;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16364v1 Announce Type: new  Abstract: When communicating routes in natural language, the concept of {\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14294</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#20114;&#25442;&#24615;&#23454;&#29616;&#39640;&#21442;&#25968;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
High-arity PAC learning via exchangeability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14294
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#39640;&#21442;&#25968;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#21644;&#20132;&#25442;&#20998;&#24067;&#21462;&#20195;i.i.d.&#25277;&#26679;&#65292;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#32500;PAC&#23398;&#20064;&#29702;&#35770;&#65292;&#21363;&#22312;&#8220;&#32467;&#26500;&#21270;&#30456;&#20851;&#24615;&#8221;&#23384;&#22312;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#12290; &#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#20551;&#35774;&#21487;&#20197;&#26159;&#22270;&#24418;&#12289;&#36229;&#22270;&#65292;&#25110;&#32773;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26159;&#26377;&#38480;&#20851;&#31995;&#35821;&#35328;&#20013;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;i.i.d.&#25277;&#26679;&#34987;&#25277;&#26679;&#20135;&#29983;&#21487;&#20114;&#25442;&#20998;&#24067;&#30340;&#35825;&#23548;&#23376;&#32467;&#26500;&#21462;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32479;&#35745;&#23398;&#20064;&#22522;&#26412;&#23450;&#29702;&#30340;&#39640;&#32500;&#29256;&#26412;&#65292;&#36890;&#36807;&#34920;&#24449;&#39640;&#32500;&#65288;agnostic&#65289;PAC&#21487;&#23398;&#24615;&#65292;&#20197;&#32431;&#32452;&#21512;&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#21450;&#36866;&#24403;&#29256;&#26412;&#30340;&#22343;&#21248;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14294v1 Announce Type: new  Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. We prove a high-arity version of the fundamental theorem of statistical learning by characterizing high-arity (agnostic) PAC learnability in terms of finiteness of a purely combinatorial dimension and in terms of an appropriate version of uniform convergence.
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12668</link><description>&lt;p&gt;
&#38543;&#26426;&#21270;&#26082;&#21487;&#20197;&#20943;&#23569;&#20559;&#24046;&#21448;&#21487;&#20197;&#20943;&#23569;&#26041;&#24046;&#65306;&#38543;&#26426;&#26862;&#26519;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12668
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#30456;&#23545;&#20110;&#35013;&#34955;&#27861;&#20855;&#26377;&#20943;&#23569;&#20559;&#24046;&#30340;&#33021;&#21147;&#65292;&#22312;&#25581;&#31034;&#25968;&#25454;&#27169;&#24335;&#21644;&#39640;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#30340;&#29305;&#28857;&#65292;&#20026;&#38543;&#26426;&#26862;&#26519;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604;&#29615;&#22659;&#19979;&#30340;&#25104;&#21151;&#25552;&#20379;&#20102;&#35299;&#37322;&#21644;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24448;&#24448;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65292;&#39318;&#27425;&#22312;\cite{breiman2001random}&#20013;&#25351;&#20986;&#65292;&#21363;&#38543;&#26426;&#26862;&#26519;&#20284;&#20046;&#27604;&#35013;&#34955;&#27861;&#20943;&#23569;&#20102;&#20559;&#24046;&#12290;&#21463;\cite{mentch2020randomization}&#19968;&#31687;&#26377;&#36259;&#30340;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#20316;&#32773;&#35748;&#20026;&#38543;&#26426;&#26862;&#26519;&#20943;&#23569;&#20102;&#26377;&#25928;&#33258;&#30001;&#24230;&#65292;&#24182;&#19988;&#21482;&#26377;&#22312;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#29615;&#22659;&#19979;&#25165;&#33021;&#32988;&#36807;&#35013;&#34955;&#38598;&#25104;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#22914;&#20309;&#33021;&#22815;&#25581;&#31034;&#34987;&#35013;&#34955;&#27861;&#24573;&#35270;&#30340;&#25968;&#25454;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#65292;&#22312;&#23384;&#22312;&#36825;&#31181;&#27169;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26862;&#26519;&#19981;&#20165;&#21487;&#20197;&#20943;&#23567;&#20559;&#24046;&#36824;&#33021;&#20943;&#23567;&#26041;&#24046;&#65292;&#24182;&#19988;&#24403;&#20449;&#22122;&#27604;&#39640;&#26102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34920;&#29616;&#24840;&#21457;&#22909;&#20110;&#35013;&#34955;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#20026;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#22312;&#21508;&#31181;&#20449;&#22122;&#27604;&#24773;&#20917;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#25104;&#21151;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#24182;&#22686;&#36827;&#20102;&#25105;&#20204;&#23545;&#38543;&#26426;&#26862;&#26519;&#19982;&#35013;&#34955;&#38598;&#25104;&#22312;&#27599;&#27425;&#20998;&#21106;&#27880;&#20837;&#30340;&#38543;&#26426;&#21270;&#26041;&#38754;&#30340;&#24046;&#24322;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#36824;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12668v1 Announce Type: cross  Abstract: We study the often overlooked phenomenon, first noted in \cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12035</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;: &#22522;&#20934;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Class-incremental Learning for Time Series: Benchmark and Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12035
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#38382;&#39064;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29615;&#22659;&#26412;&#36136;&#19978;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#24341;&#20837;&#26032;&#30340;&#31867;&#21035;&#12290;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23588;&#20026;&#24120;&#35265;&#65292;&#27604;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20986;&#29616;&#26032;&#30340;&#30142;&#30149;&#20998;&#31867;&#65292;&#25110;&#32773;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#28155;&#21152;&#26032;&#30340;&#27963;&#21160;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#19968;&#20010;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#21560;&#25910;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#22686;&#37327;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36827;&#23637;&#65292;&#20294;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22686;&#37327;&#24335;&#23398;&#20064;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23454;&#39564;&#35774;&#35745;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23545;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26102;&#38388;&#24207;&#21015;&#22686;&#37327;&#23398;&#20064;&#65288;TSCIL&#65289;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#20854;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#35206;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>TinyCL&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;&#65292;&#22312;CL&#20013;&#25903;&#25345;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#12290;</title><link>https://arxiv.org/abs/2402.09780</link><description>&lt;p&gt;
TinyCL:&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
TinyCL: An Efficient Hardware Architecture for Continual Learning on Autonomous Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09780
&lt;/p&gt;
&lt;p&gt;
TinyCL&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#20027;&#31995;&#32479;&#25345;&#32493;&#23398;&#20064;&#30340;&#39640;&#25928;&#30828;&#20214;&#26550;&#26500;&#65292;&#22312;CL&#20013;&#25903;&#25345;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#26469;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#33539;&#24335;&#21253;&#25324;&#19981;&#26029;&#28436;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20197;&#36880;&#27493;&#23398;&#20064;&#25191;&#34892;&#26032;&#20219;&#21153;&#65292;&#32780;&#19981;&#38477;&#20302;&#20808;&#21069;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#36991;&#20813;&#25152;&#35859;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;CL&#30340;&#33258;&#20027;&#31995;&#32479;&#20013;&#65292;DNN&#21442;&#25968;&#26356;&#26032;&#23545;&#36164;&#28304;&#35201;&#27714;&#26497;&#39640;&#12290;&#29616;&#26377;&#30340;DNN&#21152;&#36895;&#22120;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;CL&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#25903;&#25345;&#21069;&#21521;&#20256;&#25773;&#30340;&#25191;&#34892;&#12290;&#21482;&#26377;&#23569;&#25968;&#20808;&#21069;&#30340;&#26550;&#26500;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;CL&#30340;&#25511;&#21046;&#21644;&#31649;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30828;&#20214;&#26550;&#26500;TinyCL&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#33258;&#20027;&#31995;&#32479;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#25191;&#34892;&#21069;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#30340;&#22788;&#29702;&#21333;&#20803;&#65292;&#20197;&#21450;&#19968;&#20010;&#31649;&#29702;&#22522;&#20110;&#20869;&#23384;&#30340;CL&#24037;&#20316;&#36127;&#36733;&#30340;&#25511;&#21046;&#21333;&#20803;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#20869;&#23384;&#35775;&#38382;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#28369;&#21160;&#31383;&#21475;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09780v1 Announce Type: new  Abstract: The Continuous Learning (CL) paradigm consists of continuously evolving the parameters of the Deep Neural Network (DNN) model to progressively learn to perform new tasks without reducing the performance on previous tasks, i.e., avoiding the so-called catastrophic forgetting. However, the DNN parameter update in CL-based autonomous systems is extremely resource-hungry. The existing DNN accelerators cannot be directly employed in CL because they only support the execution of the forward propagation. Only a few prior architectures execute the backpropagation and weight update, but they lack the control and management for CL. Towards this, we design a hardware architecture, TinyCL, to perform CL on resource-constrained autonomous systems. It consists of a processing unit that executes both forward and backward propagation, and a control unit that manages memory-based CL workload. To minimize the memory accesses, the sliding window of the con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08225</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#37325;&#20889;&#25552;&#39640;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Black-box Robustness with In-Context Rewriting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#36755;&#20837;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#30340;&#25216;&#26415;&#22312;&#27169;&#22411;&#26159;&#40657;&#30418;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#65292;&#20363;&#22914;&#26435;&#37325;&#34987;&#20923;&#32467;&#65292;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#65292;&#25110;&#32773;&#36890;&#36807;API&#20351;&#29992;&#27169;&#22411;&#12290;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20107;&#21518;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#36755;&#20837;&#30340;&#22810;&#20010;&#22686;&#24378;&#36827;&#34892;&#39044;&#27979;&#32858;&#21512;&#26469;&#32469;&#36807;&#40657;&#30418;&#32422;&#26463;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#30001;&#20110;&#29983;&#25104;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22686;&#24378;&#30340;&#25361;&#25112;&#65292;TTA&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-TTA&#65292;&#23427;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;TTA&#30340;&#22686;&#24378;&#20989;&#25968;&#12290;LLM-TTA&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;BERT&#30340;OOD&#40065;&#26834;&#24615;&#25552;&#39640;&#20102;&#24179;&#22343;4.30&#20010;&#30334;&#20998;&#28857;&#32780;&#19981;&#20250;&#20943;&#36864;&#24179;&#22343;ID pe&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID pe
&lt;/p&gt;</description></item><item><title>AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2402.03309</link><description>&lt;p&gt;
AONeuS: &#19968;&#31181;&#29992;&#20110;&#22768;&#20809;&#20256;&#24863;&#22120;&#34701;&#21512;&#30340;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03309
&lt;/p&gt;
&lt;p&gt;
AONeuS&#26159;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#21644;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#65292;&#33021;&#22815;&#22312;&#21463;&#38480;&#22522;&#32447;&#19979;&#23454;&#29616;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#24863;&#30693;&#21644;&#19977;&#32500;&#34920;&#38754;&#37325;&#24314;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#28041;&#21450;&#24314;&#31569;&#12289;&#23433;&#20840;&#12289;&#28023;&#27915;&#32771;&#21476;&#21644;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#12290;&#24694;&#21155;&#30340;&#25805;&#20316;&#26465;&#20214;&#12289;&#33030;&#24369;&#30340;&#29615;&#22659;&#21644;&#26377;&#38480;&#30340;&#23548;&#33322;&#25511;&#21046;&#36890;&#24120;&#23548;&#33268;&#27700;&#19979;&#33322;&#34892;&#22120;&#38480;&#21046;&#20854;&#36816;&#21160;&#33539;&#22260;&#21644;&#27979;&#37327;&#22522;&#32447;&#12290;&#22312;&#19977;&#32500;&#22330;&#26223;&#37325;&#24314;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30693;&#36947;&#36739;&#23567;&#30340;&#22522;&#32447;&#20250;&#22686;&#21152;&#37325;&#24314;&#38590;&#24230;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#22810;&#27169;&#24577;&#22768;&#20809;&#31070;&#32463;&#34920;&#38754;&#37325;&#24314;&#26694;&#26550;&#65288;AONeuS&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#39640;&#20998;&#36776;&#29575;RGB&#27979;&#37327;&#19982;&#20302;&#20998;&#36776;&#29575;&#28145;&#24230;&#25104;&#20687;&#22768;&#32435;&#27979;&#37327;&#36827;&#34892;&#34701;&#21512;&#12290;&#36890;&#36807;&#34701;&#21512;&#36825;&#20123;&#20114;&#34917;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20174;&#22312;&#21463;&#38480;&#22522;&#32447;&#19978;&#25429;&#33719;&#30340;&#27979;&#37327;&#20013;&#37325;&#24314;&#20986;&#20934;&#30830;&#30340;&#39640;&#20998;&#36776;&#29575;&#19977;&#32500;&#34920;&#38754;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#21644;&#23454;&#39564;&#23460;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate tha
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.13185</link><description>&lt;p&gt;
&#31616;&#21270;&#20132;&#21449;&#39564;&#35777;&#65306;&#39640;&#25928;&#22320;&#35745;&#31639;&#19981;&#38656;&#35201;&#20840;&#37327;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#30340;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$
&lt;/p&gt;
&lt;p&gt;
Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered and Scaled Training Set $\mathbf{X}^\mathbf{T}\mathbf{X}$ and $\mathbf{X}^\mathbf{T}\mathbf{Y}$ Without Full Recomputation of Matrix Products or Statistical Moments. (arXiv:2401.13185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#35757;&#32451;&#38598;$\mathbf{X}^\mathbf{T}\mathbf{X}$&#21644;$\mathbf{X}^\mathbf{T}\mathbf{Y}$&#30340;&#31639;&#27861;&#65292;&#30456;&#27604;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#26174;&#33879;&#21152;&#36895;&#20132;&#21449;&#39564;&#35777;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35745;&#31639;&#30697;&#38453;&#20056;&#31215;&#25110;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#34920;&#29616;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#39044;&#27979;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26680;&#30340;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#27169;&#22411;&#65292;&#38656;&#35201;&#20165;&#20351;&#29992;&#36755;&#20837;&#30697;&#38453;$\mathbf{X}$&#21644;&#36755;&#20986;&#30697;&#38453;$\mathbf{Y}$&#20013;&#30340;&#35757;&#32451;&#38598;&#26679;&#26412;&#26469;&#35745;&#31639;$\mathbf{X}^{\mathbf{T}}\mathbf{X}$&#21644;$\mathbf{X}^{\mathbf{T}}\mathbf{Y}$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#39640;&#25928;&#35745;&#31639;&#36825;&#20123;&#30697;&#38453;&#30340;&#31639;&#27861;&#12290;&#31532;&#19968;&#31181;&#31639;&#27861;&#19981;&#38656;&#35201;&#21015;&#21521;&#39044;&#22788;&#29702;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#20026;&#20013;&#24515;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#12290;&#31532;&#19977;&#31181;&#31639;&#27861;&#20801;&#35768;&#20197;&#35757;&#32451;&#38598;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#20026;&#20013;&#24515;&#21270;&#28857;&#21644;&#26631;&#20934;&#21270;&#28857;&#36827;&#34892;&#21015;&#21521;&#20013;&#24515;&#21270;&#21644;&#26631;&#20934;&#21270;&#12290;&#36890;&#36807;&#35777;&#26126;&#27491;&#30830;&#24615;&#21644;&#20248;&#36234;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#30456;&#27604;&#20110;&#30452;&#25509;&#20132;&#21449;&#39564;&#35777;&#21644;&#20197;&#21069;&#30340;&#24555;&#36895;&#20132;&#21449;&#39564;&#35777;&#24037;&#20316;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#20132;&#21449;&#39564;&#35777;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#25968;&#25454;&#27844;&#38706;&#12290;&#23427;&#20204;&#36866;&#21512;&#24182;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-validation is a widely used technique for assessing the performance of predictive models on unseen data. Many predictive models, such as Kernel-Based Partial Least-Squares (PLS) models, require the computation of $\mathbf{X}^{\mathbf{T}}\mathbf{X}$ and $\mathbf{X}^{\mathbf{T}}\mathbf{Y}$ using only training set samples from the input and output matrices, $\mathbf{X}$ and $\mathbf{Y}$, respectively. In this work, we present three algorithms that efficiently compute these matrices. The first one allows no column-wise preprocessing. The second one allows column-wise centering around the training set means. The third one allows column-wise centering and column-wise scaling around the training set means and standard deviations. Demonstrating correctness and superior computational complexity, they offer significant cross-validation speedup compared with straight-forward cross-validation and previous work on fast cross-validation - all without data leakage. Their suitability for paralle
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.07769</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#29992;&#20110;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#20013;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Evolutional Instant Interest Network for CTR Prediction in Trigger-Induced Recommendation. (arXiv:2401.07769v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07769
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#26469;&#35299;&#20915;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#27969;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#22330;&#26223;&#65292;&#31216;&#20026;&#35302;&#21457;&#24341;&#23548;&#25512;&#33616;&#65288;TIR&#65289;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#35302;&#21457;&#39033;&#26126;&#30830;&#34920;&#36798;&#20182;&#20204;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22312;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#65288;&#22914;&#38463;&#37324;&#24052;&#24052;&#21644;&#20122;&#39532;&#36874;&#65289;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20256;&#32479;&#30340;&#25512;&#33616;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#26126;&#30830;&#24314;&#27169;&#29992;&#25143;&#30340;&#21363;&#26102;&#20852;&#36259;&#65292;&#22240;&#27492;&#22312;TIR&#20013;&#33719;&#24471;&#27425;&#20248;&#32467;&#26524;&#12290;&#23613;&#31649;&#26377;&#19968;&#20123;&#21516;&#26102;&#32771;&#34385;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20173;&#26410;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#30340;&#26102;&#38388;&#20449;&#24687;&#12289;&#29992;&#25143;&#21521;&#19979;&#28378;&#21160;&#26102;&#21363;&#26102;&#20852;&#36259;&#30340;&#21160;&#24577;&#21464;&#21270;&#20197;&#21450;&#35302;&#21457;&#39033;&#21644;&#30446;&#26631;&#39033;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;--&#28145;&#24230;&#36827;&#21270;&#30340;&#21363;&#26102;&#20852;&#36259;&#32593;&#32476;&#65288;DEI2N&#65289;&#65292;&#29992;&#20110;TIR&#22330;&#26223;&#20013;&#30340;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation has been playing a key role in many industries, e.g., e-commerce, streaming media, social media, etc. Recently, a new recommendation scenario, called Trigger-Induced Recommendation (TIR), where users are able to explicitly express their instant interests via trigger items, is emerging as an essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon. Without explicitly modeling the user's instant interest, traditional recommendation methods usually obtain sub-optimal results in TIR. Even though there are a few methods considering the trigger and target items simultaneously to solve this problem, they still haven't taken into account temporal information of user behaviors, the dynamic change of user instant interest when the user scrolls down and the interactions between the trigger and target items. To tackle these problems, we propose a novel method -- Deep Evolutional Instant Interest Network (DEI2N), for click-through rate prediction in TIR scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04071</link><description>&lt;p&gt;
&#26071;&#24092;&#28216;&#25103;&#65306;&#36890;&#36807;&#26071;&#24092;&#27969;&#24418;&#26469;&#33719;&#24471;&#40065;&#26834;&#30340;&#20027;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Fun with Flags: Robust Principal Directions via Flag Manifolds. (arXiv:2401.04071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;PCA&#21644;&#20854;&#21464;&#31181;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#30340;&#32771;&#34385;&#12290;&#36890;&#36807;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#36827;&#34892;&#20248;&#21270;&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#32467;&#21512;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#21450;&#20854;&#23545;&#27969;&#24418;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#25193;&#23637;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PCA&#21450;&#20854;&#21464;&#31181;&#30340;&#32479;&#19968;&#24418;&#24335;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#32447;&#24615;&#23376;&#31354;&#38388;&#26071;&#24092;&#30340;&#26694;&#26550;&#65292;&#21363;&#36880;&#28176;&#22686;&#21152;&#32500;&#24230;&#30340;&#23884;&#22871;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#19981;&#20165;&#20801;&#35768;&#20849;&#21516;&#23454;&#29616;&#65292;&#36824;&#20135;&#29983;&#20102;&#26032;&#30340;&#26410;&#26366;&#25506;&#32034;&#30340;&#21464;&#31181;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#21270;&#20256;&#32479;&#30340;PCA&#26041;&#27861;&#24320;&#22987;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26368;&#22823;&#21270;&#26041;&#24046;&#65292;&#35201;&#20040;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#25105;&#20204;&#25193;&#23637;&#36825;&#20123;&#35299;&#37322;&#65292;&#36890;&#36807;&#32771;&#34385;&#24322;&#24120;&#20540;&#21644;&#25968;&#25454;&#27969;&#24418;&#65292;&#24320;&#21457;&#20986;&#20102;&#22823;&#37327;&#26032;&#30340;&#38477;&#32500;&#31639;&#27861;&#12290;&#20026;&#20102;&#35774;&#35745;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#21644;&#23545;&#20598;&#24418;&#24335;&#30340;PCA&#37325;&#26032;&#26500;&#24314;&#20026;&#22312;&#26071;&#24092;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20027;&#27979;&#22320;&#32447;&#36817;&#20284;&#65288;&#20999;&#32447;PCA&#65289;&#25972;&#21512;&#21040;&#36825;&#20010;&#22522;&#20110;&#26071;&#24092;&#30340;&#26694;&#26550;&#20013;&#65292;&#21019;&#36896;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component analysis (PCA), along with its extensions to manifolds and outlier contaminated data, have been indispensable in computer vision and machine learning. In this work, we present a unifying formalism for PCA and its variants, and introduce a framework based on the flags of linear subspaces, \ie a hierarchy of nested linear subspaces of increasing dimension, which not only allows for a common implementation but also yields novel variants, not explored previously. We begin by generalizing traditional PCA methods that either maximize variance or minimize reconstruction error. We expand these interpretations to develop a wide array of new dimensionality reduction algorithms by accounting for outliers and the data manifold. To devise a common computational approach, we recast robust and dual forms of PCA as optimization problems on flag manifolds. We then integrate tangent space approximations of principal geodesic analysis (tangent-PCA) into this flag-based framework, crea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.02283</link><description>&lt;p&gt;
DEM: &#33322;&#31354;&#33322;&#22825;&#20013;&#29992;&#20110;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#35201;&#27714;&#36981;&#24490;&#20005;&#26684;&#12289;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#21830;&#29992;&#36719;&#20214;&#30340;&#30417;&#31649;&#25351;&#21335;&#65288;&#20363;&#22914;ARP-4754&#21644;DO-178&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#21335;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32452;&#20214;&#30340;&#36719;&#20214;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20351;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#29992;&#20110;DNN&#30340;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#33021;&#22815;&#26631;&#35760;DNN&#36755;&#20986;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#19987;&#23478;&#26816;&#26597;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;DNN&#23545;&#20854;&#20182;&#38468;&#36817;&#36755;&#20837;&#30340;&#39044;&#27979;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21453;&#65292;&#21518;&#32773;&#36890;&#24120;&#35797;&#22270;&#23545;&#25972;&#20010;DNN&#36827;&#34892;&#35748;&#35777;&#65292;&#32780;&#38750;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;&#26694;&#26550;&#65288;LDP-GE&#65289;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;LDP&#26426;&#21046;&#26469;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#34935;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11060</link><description>&lt;p&gt;
&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Graph Embedding. (arXiv:2310.11060v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22270;&#23884;&#20837;&#26694;&#26550;&#65288;LDP-GE&#65289;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;LDP&#26426;&#21046;&#26469;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#20351;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25240;&#34935;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23884;&#20837;&#34987;&#35777;&#26126;&#26159;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#28508;&#22312;&#34920;&#31034;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#22312;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#30340;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#24341;&#21457;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#21457;&#33021;&#28385;&#36275;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#22270;&#23884;&#20837;&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#20445;&#25252;&#22270;&#23884;&#20837;&#26694;&#26550;LDP-GE&#65292;&#29992;&#20110;&#20445;&#25252;&#33410;&#28857;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;LDP&#26426;&#21046;&#26469;&#28151;&#28102;&#33410;&#28857;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#20010;&#24615;&#21270;PageRank&#20316;&#20026;&#36817;&#20284;&#24230;&#24230;&#37327;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;LDP-GE&#26694;&#26550;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25928;&#29992;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;LDP-GE&#22312;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph embedding has been demonstrated to be a powerful tool for learning latent representations for nodes in a graph. However, despite its superior performance in various graph-based machine learning tasks, learning over graphs can raise significant privacy concerns when graph data involves sensitive information. To address this, in this paper, we investigate the problem of developing graph embedding algorithms that satisfy local differential privacy (LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework, to protect the privacy of node data. Specifically, we propose an LDP mechanism to obfuscate node data and adopt personalized PageRank as the proximity measure to learn node representations. Then, we theoretically analyze the privacy guarantees and utility of the LDP-GE framework. Extensive experiments conducted over several real-world graph datasets demonstrate that LDP-GE achieves favorable privacy-utility trade-offs and significantly outperforms existing appr
&lt;/p&gt;</description></item><item><title>FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.02903</link><description>&lt;p&gt;
FroSSL: &#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FroSSL: Frobenius Norm Minimization for Self-Supervised Learning. (arXiv:2310.02903v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02903
&lt;/p&gt;
&lt;p&gt;
FroSSL&#26159;&#19968;&#31181;&#22522;&#20110;Frobenius&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#65292;&#21516;&#26102;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;SSL&#26041;&#27861;&#65292;FroSSL&#25910;&#25947;&#26356;&#24555;&#65292;&#24182;&#19988;&#36825;&#31181;&#24555;&#36895;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#24433;&#21709;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25152;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21487;&#20998;&#31867;&#20026;&#26679;&#26412;&#23545;&#27604;&#12289;&#32500;&#24230;&#23545;&#27604;&#25110;&#38750;&#23545;&#31216;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#23478;&#26063;&#37117;&#26377;&#33258;&#24049;&#30340;&#26041;&#27861;&#26469;&#36991;&#20813;&#20449;&#24687;&#23849;&#28291;&#12290;&#34429;&#28982;&#32500;&#24230;&#23545;&#27604;&#26041;&#27861;&#25910;&#25947;&#21040;&#19982;&#26679;&#26412;&#23545;&#27604;&#26041;&#27861;&#30456;&#20284;&#30340;&#35299;&#65292;&#20294;&#21487;&#20197;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#19968;&#20123;&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#20989;&#25968;FroSSL&#65292;&#23427;&#22312;&#23884;&#20837;&#24402;&#19968;&#21270;&#26041;&#38754;&#26082;&#26159;&#26679;&#26412;&#23545;&#27604;&#21448;&#26159;&#32500;&#24230;&#23545;&#27604;&#12290;FroSSL&#36890;&#36807;&#26368;&#23567;&#21270;&#21327;&#26041;&#24046;Frobenius&#33539;&#25968;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#22343;&#26041;&#24046;&#26469;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FroSSL&#27604;&#20854;&#20182;&#21508;&#31181;SSL&#26041;&#27861;&#26356;&#24555;&#22320;&#25910;&#25947;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#25903;&#25345;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26356;&#24555;&#30340;&#25910;&#25947;&#26159;&#30001;&#20110;FroSSL&#23545;&#23884;&#20837;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is an increasingly popular paradigm for representation learning. Recent methods can be classified as sample-contrastive, dimension-contrastive, or asymmetric network-based, with each family having its own approach to avoiding informational collapse. While dimension-contrastive methods converge to similar solutions as sample-contrastive methods, it can be empirically shown that some methods require more epochs of training to converge. Motivated by closing this divide, we present the objective function FroSSL which is both sample- and dimension-contrastive up to embedding normalization. FroSSL works by minimizing covariance Frobenius norms for avoiding collapse and minimizing mean-squared error for augmentation invariance. We show that FroSSL converges more quickly than a variety of other SSL methods and provide theoretical and empirical support that this faster convergence is due to how FroSSL affects the eigenvalues of the embedding covariance matrices. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.02812</link><description>&lt;p&gt;
&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;: &#23545;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Time-Series Classification in Smart Manufacturing Systems: An Experimental Evaluation of State-of-the-Art Machine Learning Algorithms. (arXiv:2310.02812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#35780;&#20272;&#20102;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22635;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20256;&#24863;&#22120;&#25968;&#37327;&#30340;&#22686;&#21152;&#21644;&#24863;&#30693;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21046;&#36896;&#19994;&#27491;&#22312;&#25910;&#38598;&#22823;&#37327;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#12290;&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479; (SMS) &#29615;&#22659;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36215;&#30528;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867; (TSC) &#22312;&#35813;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23545;&#21046;&#36896;&#19994;&#21644;&#24037;&#19994;&#29615;&#22659;&#20013; TSC &#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312; TSC &#21644;&#21046;&#36896;&#19994;&#25991;&#29486;&#20013;&#25506;&#32034;&#21644;&#32534;&#21046;&#20102;&#19968;&#20221;&#21253;&#21547;&#36229;&#36807;92&#20010;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20840;&#38754;&#21015;&#34920;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20174;&#35813;&#21015;&#34920;&#20013;&#36873;&#25321;&#20102;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;36&#20010;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#22312;&#21508;&#31181;&#21046;&#36896;&#19994;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#32452;&#21253;&#21547;22&#20010;&#21046;&#36896;&#19994;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#21046;&#36896;&#38382;&#39064;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22312;&#21046;&#36896;&#19994;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manufacturing is gathering extensive amounts of diverse data, thanks to the growing number of sensors and rapid advances in sensing technologies. Among the various data types available in SMS settings, time-series data plays a pivotal role. Hence, TSC emerges is crucial in this domain. The objective of this study is to fill this gap by providing a rigorous experimental evaluation of the SoTA ML and DL algorithms for TSC tasks in manufacturing and industrial settings. We first explored and compiled a comprehensive list of more than 92 SoTA algorithms from both TSC and manufacturing literature. Following, we selected the 36 most representative algorithms from this list. To evaluate their performance across various manufacturing classification tasks, we curated a set of 22 manufacturing datasets, representative of different characteristics that cover diverse manufacturing problems. Subsequently, we implemented and evaluated the algorithms on the manufacturing benchmark datasets, and analy
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2309.14857</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#27969;&#24418;&#25237;&#24433;&#36827;&#34892;&#32858;&#31867;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Cluster Exploration using Informative Manifold Projections. (arXiv:2309.14857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#20197;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#36890;&#36807;&#32447;&#24615;&#32452;&#21512;&#23545;&#27604;PCA&#21644;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#20004;&#20010;&#30446;&#26631;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25490;&#38500;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#24182;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38477;&#32500;&#26159;&#21487;&#35270;&#21270;&#25506;&#32034;&#39640;&#32500;&#25968;&#25454;&#21644;&#21457;&#29616;&#20854;&#22312;&#20108;&#32500;&#25110;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#32858;&#31867;&#32467;&#26500;&#30340;&#20851;&#38190;&#24037;&#20855;&#20043;&#19968;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#22823;&#37096;&#20998;&#38477;&#32500;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#23454;&#36341;&#32773;&#21487;&#33021;&#23545;&#25152;&#32771;&#34385;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#23884;&#20837;&#65292;&#19981;&#20165;&#25490;&#38500;&#19982;&#20808;&#39564;&#30693;&#35782;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#32780;&#19988;&#26088;&#22312;&#25581;&#31034;&#20219;&#20309;&#21097;&#20313;&#30340;&#28508;&#22312;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#20010;&#30446;&#26631;&#30340;&#32447;&#24615;&#32452;&#21512;&#65306;&#39318;&#20808;&#26159;&#23545;&#27604;PCA&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#20808;&#39564;&#20449;&#24687;&#30456;&#20851;&#30340;&#32467;&#26500;&#65292;&#20854;&#27425;&#26159;&#23792;&#24230;&#25237;&#24433;&#36861;&#36394;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#24471;&#21040;&#30340;&#23884;&#20837;&#20013;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#20998;&#31163;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#23450;&#20041;&#20026;&#27969;&#24418;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#32771;&#34385;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality reduction (DR) is one of the key tools for the visual exploration of high-dimensional data and uncovering its cluster structure in two- or three-dimensional spaces. The vast majority of DR methods in the literature do not take into account any prior knowledge a practitioner may have regarding the dataset under consideration. We propose a novel method to generate informative embeddings which not only factor out the structure associated with different kinds of prior knowledge but also aim to reveal any remaining underlying structure. To achieve this, we employ a linear combination of two objectives: firstly, contrastive PCA that discounts the structure associated with the prior information, and secondly, kurtosis projection pursuit which ensures meaningful data separation in the obtained embeddings. We formulate this task as a manifold optimization problem and validate it empirically across a variety of datasets considering three distinct types of prior knowledge. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20005;&#26684;&#20998;&#26512;&#20102;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#36924;&#36817;&#32467;&#26524;&#21644;&#29699;&#35856;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#39564;&#35777;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09605</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks. (arXiv:2308.09605v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#20998;&#26512;&#20102;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#36924;&#36817;&#32467;&#26524;&#21644;&#29699;&#35856;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#65292;&#24182;&#24314;&#31435;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#39564;&#35777;&#20102;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35299;&#20915;&#21508;&#31181;&#23454;&#39564;&#35282;&#24230;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#38750;&#24120;&#39640;&#25928;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#34920;&#38754;&#65292;&#21253;&#25324;&#29699;&#38754;&#19978;&#30340;PDEs&#30340;PINN&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;PINNs&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#34920;&#38754;&#25110;&#27969;&#24418;&#19978;&#30340;PINNs&#65292;&#20173;&#28982;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29992;&#20110;&#22312;&#29699;&#38754;&#19978;&#35299;&#20915;PDEs&#30340;&#29289;&#29702;&#20449;&#24687;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#21644;&#25913;&#36827;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#29699;&#35856;&#20998;&#26512;&#30340;&#26368;&#26032;&#36924;&#36817;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#36924;&#36817;&#35823;&#24046;&#19982;Sobolev&#33539;&#25968;&#30340;&#19978;&#30028;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#19982;&#21019;&#26032;&#30340;&#23616;&#37096;&#22797;&#26434;&#24230;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;PICNN&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20063;&#24471;&#21040;&#20102;&#23454;&#39564;&#30340;&#39564;&#35777;&#21644;&#34917;&#20805;&#12290;&#37492;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we expl
&lt;/p&gt;</description></item><item><title>FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2307.04684</link><description>&lt;p&gt;
FreeDrag: &#28857;&#36861;&#36394;&#24182;&#19981;&#36866;&#29992;&#20110;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
FreeDrag: Point Tracking is Not What You Need for Interactive Point-based Image Editing. (arXiv:2307.04684v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04684
&lt;/p&gt;
&lt;p&gt;
FreeDrag&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;DragGAN&#22312;&#28857;&#36861;&#36394;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28385;&#36275;&#22270;&#20687;&#32534;&#36753;&#30340;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#38656;&#27714;&#65292;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#31934;&#30830;&#21644;&#28789;&#27963;&#30340;&#25805;&#32437;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#65292;DragGAN&#36890;&#36807;&#22522;&#20110;&#28857;&#30340;&#25805;&#32437;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;DragGAN&#22312;&#28857;&#30340;&#36861;&#36394;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#21253;&#25324;&#38169;&#35823;&#36861;&#36394;&#21644;&#27169;&#31946;&#36861;&#36394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FreeDrag&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;DragGAN&#20013;&#28857;&#36861;&#36394;&#30340;&#36127;&#25285;&#12290;FreeDrag&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27169;&#26495;&#29305;&#24449;&#12289;&#32447;&#24615;&#25628;&#32034;&#21644;&#27169;&#31946;&#23450;&#20301;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;DragGAN&#65292;&#24182;&#33021;&#22312;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22256;&#38590;&#24773;&#26223;&#19979;&#23454;&#29616;&#31283;&#23450;&#30340;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
To serve the intricate and varied demands of image editing, precise and flexible manipulation of image content is indispensable. Recently, DragGAN has achieved impressive editing results through point-based manipulation. However, we have observed that DragGAN struggles with miss tracking, where DragGAN encounters difficulty in effectively tracking the desired handle points, and ambiguous tracking, where the tracked points are situated within other regions that bear resemblance to the handle points. To deal with the above issues, we propose FreeDrag, which adopts a feature-oriented approach to free the burden on point tracking within the point-oriented methodology of DragGAN. The FreeDrag incorporates adaptive template features, line search, and fuzzy localization techniques to perform stable and efficient point-based image editing. Extensive experiments demonstrate that our method is superior to the DragGAN and enables stable point-based editing in challenging scenarios with similar st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2307.01357</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Principal Component Regression with Applications to Panel Data. (arXiv:2307.01357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#20027;&#25104;&#20998;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#20013;&#33719;&#24471;&#20102;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#38754;&#26495;&#25968;&#25454;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#29305;&#21035;&#26159;&#24403;&#24178;&#39044;&#26041;&#26696;&#26159;&#33258;&#36866;&#24212;&#20998;&#37197;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#25104;&#20998;&#22238;&#24402;(PCR)&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22266;&#23450;&#35774;&#35745;&#35823;&#24046;&#21464;&#37327;&#22238;&#24402;&#25216;&#26415;&#65292;&#23427;&#26159;&#32447;&#24615;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#35266;&#27979;&#30340;&#21327;&#21464;&#37327;&#21463;&#21040;&#38543;&#26426;&#22122;&#22768;&#30340;&#27745;&#26579;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#25910;&#38598;&#26102;&#25552;&#20379;&#20102;&#22312;&#32447;&#65288;&#27491;&#21017;&#21270;&#65289;PCR&#30340;&#31532;&#19968;&#27425;&#22343;&#21248;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#30001;&#20110;&#20998;&#26512;&#22266;&#23450;&#35774;&#35745;&#20013;PCR&#30340;&#35777;&#26126;&#25216;&#26415;&#26080;&#27861;&#24456;&#23481;&#26131;&#22320;&#25193;&#23637;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;&#23558;&#29616;&#20195;&#38789;&#27987;&#24230;&#30340;&#24037;&#20855;&#36866;&#24212;&#21040;&#35823;&#24046;&#21464;&#37327;&#35774;&#32622;&#20013;&#12290;&#20316;&#20026;&#25105;&#20204;&#30028;&#38480;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#38754;&#26495;&#25968;&#25454;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#65292;&#24403;&#24178;&#39044;&#34987;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#26102;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#21512;&#25104;&#25511;&#21046;&#21644;&#21512;&#25104;&#24178;&#39044;&#26694;&#26550;&#30340;&#27867;&#21270;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#36890;&#36807;&#33258;&#36866;&#24212;&#24178;&#39044;&#20998;&#37197;&#31574;&#30053;&#25910;&#38598;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principal component regression (PCR) is a popular technique for fixed-design error-in-variables regression, a generalization of the linear regression setting in which the observed covariates are corrupted with random noise. We provide the first time-uniform finite sample guarantees for online (regularized) PCR whenever data is collected adaptively. Since the proof techniques for analyzing PCR in the fixed design setting do not readily extend to the online setting, our results rely on adapting tools from modern martingale concentration to the error-in-variables setting. As an application of our bounds, we provide a framework for experiment design in panel data settings when interventions are assigned adaptively. Our framework may be thought of as a generalization of the synthetic control and synthetic interventions frameworks, where data is collected via an adaptive intervention assignment policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#35775;&#23454;&#36341;&#32773;&#21644;&#36827;&#34892;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;</title><link>http://arxiv.org/abs/2306.15007</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Quality Issues in Machine Learning Software Systems. (arXiv:2306.15007v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#35775;&#23454;&#36341;&#32773;&#21644;&#36827;&#34892;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#65306;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38656;&#27714;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;ML&#27169;&#22411;&#34987;&#23454;&#29616;&#20026;&#36719;&#20214;&#32452;&#20214;&#24182;&#37096;&#32626;&#22312;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#65288;MLSSs&#65289;&#20013;&#12290;&#38382;&#39064;&#65306;&#26377;&#24517;&#35201;&#30830;&#20445;MLSSs&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#38169;&#35823;&#25110;&#19981;&#33391;&#20915;&#31574;&#21487;&#33021;&#23548;&#33268;&#20854;&#20182;&#31995;&#32479;&#30340;&#25925;&#38556;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#36130;&#21153;&#25439;&#22833;&#65292;&#29978;&#33267;&#23545;&#20154;&#31867;&#29983;&#21629;&#26500;&#25104;&#23041;&#32961;&#12290; MLSSs&#30340;&#36136;&#37327;&#20445;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#26088;&#22312;&#20174;&#23454;&#36341;&#32773;&#30340;&#35282;&#24230;&#30740;&#31350;MLSSs&#20013;&#30495;&#23454;&#36136;&#37327;&#38382;&#39064;&#30340;&#29305;&#24449;&#12290;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;MLSSs&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#19982;&#23454;&#36341;&#32773;/&#19987;&#23478;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#37319;&#35775;&#65292;&#20197;&#33719;&#21462;&#20182;&#20204;&#22788;&#29702;&#36136;&#37327;&#38382;&#39064;&#26102;&#30340;&#32463;&#39564;&#21644;&#20570;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;ML&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#39564;&#35777;&#20102;&#25152;&#30830;&#23450;&#30340;&#36136;&#37327;&#38382;&#39064;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threats to human life. The quality assurance of MLSSs is considered a challenging task and currently is a hot research topic. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of quality issues in MLSSs. Method: We conduct a set of interviews with practitioners/experts, to gather insights about their experience and practices when dealing with quality issues. We validate the identified quality issues via a survey with ML practitioners. Result
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.11169</link><description>&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#35821;&#20041;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Evidence of Meaning in Language Models Trained on Programs. (arXiv:2305.11169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#20351;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#65292;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#35757;&#32451;&#21482;&#26159;&#25191;&#34892;&#25991;&#26412;&#19978;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#31243;&#24207;&#35821;&#26009;&#24211;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#21547;&#20041;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#20197;&#65288;&#25991;&#26412;&#65289;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#30340;&#24418;&#24335;&#20316;&#20026;&#35268;&#33539;&#12290;&#19982;&#31243;&#24207;&#19968;&#36215;&#24037;&#20316;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#23450;&#20041;&#19982;&#35821;&#35328;&#20013;&#26377;&#20851;&#21547;&#20041;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#27491;&#30830;&#24615;&#21644;&#35821;&#20041;&#65289;&#65292;&#20351;&#24471;&#31243;&#24207;&#32508;&#21512;&#25104;&#20026;&#19968;&#20010;&#20013;&#38388;&#27979;&#35797;&#24179;&#21488;&#65292;&#29992;&#20110;&#34920;&#24449;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#21547;&#20041;&#30340;&#23384;&#22312;&#65288;&#25110;&#19981;&#23384;&#22312;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#31243;&#24207;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;Transformer&#27169;&#22411;&#65292;&#28982;&#21518;&#25506;&#26597;&#20102;&#24050;&#32463;&#23436;&#25104;&#35268;&#33539;&#30340;&#31243;&#24207;&#26102;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#23398;&#20064;&#35821;&#35328;&#35821;&#20041;&#25552;&#20379;&#24402;&#32435;&#20559;&#24046;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20174;&#27169;&#22411;&#29366;&#24577;&#20013;&#25552;&#21462;&#24403;&#21069;&#21644;&#26410;&#26469;&#31243;&#24207;&#29366;&#24577;&#30340;&#25277;&#35937;&#12290;&#27492;&#22806;&#65292;&#32447;&#24615;&#25506;&#27979;&#22120;&#30340;&#20934;&#30830;&#24615;&#19982;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#31243;&#24207;&#30340;&#33021;&#21147;&#24378;&#26377;&#21147;&#12289;&#32479;&#35745;&#23398;&#26174;&#33879;&#22320;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09958</link><description>&lt;p&gt;
SIMGA&#65306;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19982;&#39640;&#25928;&#30340;&#20840;&#23616;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SIMGA: A Simple and Effective Heterophilous Graph Neural Network with Efficient Global Aggregation. (arXiv:2305.09958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36935;&#21040;&#24322;&#36136;&#24615;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#22240;&#20026;&#23616;&#37096;&#21644;&#32479;&#19968;&#32858;&#21512;&#32780;&#23548;&#33268;&#30340;&#30456;&#37051;&#33410;&#28857;&#19981;&#30456;&#20284;&#12290;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35797;&#22270;&#25972;&#21512;&#20840;&#23616;&#32858;&#21512;&#30340;&#23581;&#35797;&#36890;&#24120;&#38656;&#35201;&#36845;&#20195;&#22320;&#32500;&#25252;&#21644;&#26356;&#26032;&#20840;&#22270;&#20449;&#24687;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377; $n$ &#20010;&#33410;&#28857;&#30340;&#22270;&#65292;&#36825;&#38656;&#35201; $\mathcal{O}(n^2)$ &#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#22823;&#22411;&#22270;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SIMGA&#65292;&#19968;&#31181;&#23558; SimRank &#32467;&#26500;&#30456;&#20284;&#24230;&#27979;&#37327;&#20316;&#20026;&#20840;&#23616;&#32858;&#21512;&#30340; GNN &#32467;&#26500;&#12290; SIMGA &#30340;&#35774;&#35745;&#31616;&#21333;&#65292;&#19988;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#37117;&#26377;&#30528;&#26377; promising &#30340;&#32467;&#26524;&#12290;SIMGA &#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#25104;&#20026;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340; $n$ &#20256;&#25773;&#25928;&#29575;&#30340;&#24322;&#36136;&#24615; GNN &#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558; SimRank &#35270;&#20026; GNN &#30340;&#19968;&#31181;&#26032;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;&#27719;&#32858;&#33410;&#28857;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts in incoorporating global aggregation for heterophilous GNNs usually require iteratively maintaining and updating full-graph information, which entails $\mathcal{O}(n^2)$ computation efficiency for a graph with $n$ nodes, leading to weak scalability to large graphs. In this paper, we propose SIMGA, a GNN structure integrating SimRank structural similarity measurement as global aggregation. The design of SIMGA is simple, yet it leads to promising results in both efficiency and effectiveness. The simplicity of SIMGA makes it the first heterophilous GNN model that can achieve a propagation efficiency near-linear to $n$. We theoretically demonstrate its effectiveness by treating SimRank as a new interpretation of GNN and prove that the aggregated node representation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2210.15659</link><description>&lt;p&gt;
&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Primal-dual Approach for Solving Variational Inequalities with General-form Constraints. (arXiv:2210.15659v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#20855;&#26377;&#19968;&#33324;&#24418;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Yang&#31561;&#20154;&#26368;&#36817;&#36890;&#36807;&#19968;&#31181;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#35299;&#20915;&#20102;&#20855;&#26377;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#25552;&#20986;&#30340;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#31216;&#20026;ACVI&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#35745;&#31639;&#20854;&#23376;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#24773;&#20917;&#65292;&#22240;&#27492;&#19968;&#33324;&#24773;&#20917;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#28201;&#21551;&#21160;&#25216;&#26415;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36817;&#20284;&#22320;&#35299;&#20915;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22312;&#20808;&#21069;&#36845;&#20195;&#20013;&#25214;&#21040;&#30340;&#36817;&#20284;&#35299;&#21021;&#22987;&#21270;&#21464;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#24403;&#31639;&#23376;&#20026;$L$-Lipschitz&#19988;&#21333;&#35843;&#26102;&#65292;&#36825;&#31181;&#19981;&#31934;&#30830;&#30340;ACVI&#26041;&#27861;&#30340;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#38388;&#38553;&#20989;&#25968;&#19979;&#38477;&#30340;&#36895;&#24230;&#20026;$\mathcal{O}(\frac{1}{\sqrt{K}})$&#65292;&#21069;&#25552;&#26159;&#38169;&#35823;&#20197;&#36866;&#24403;&#30340;&#36895;&#24230;&#19979;&#38477;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#36890;&#24120;&#36825;&#31181;&#25216;&#26415;&#27604;&#20854;&#31934;&#30830;&#23545;&#24212;&#29289;&#25910;&#25947;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Yang et al. (2023) recently addressed the open problem of solving Variational Inequalities (VIs) with equality and inequality constraints through a first-order gradient method. However, the proposed primal-dual method called ACVI is applicable when we can compute analytic solutions of its subproblems; thus, the general case remains an open problem. In this paper, we adopt a warm-starting technique where we solve the subproblems approximately at each iteration and initialize the variables with the approximate solution found at the previous iteration. We prove its convergence and show that the gap function of the last iterate of this inexact-ACVI method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone, provided that the errors decrease at appropriate rates. Interestingly, we show that often in numerical experiments, this technique converges faster than its exact counterpart. Furthermore, for the cases when the inequality constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12494</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#21644;&#19968;&#31867;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#20915;&#23450;&#35266;&#23519;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#30446;&#26631;&#31867;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;GLRT&#65289;&#30340;OCC&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24403;&#30446;&#26631;&#31867;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#26102;&#65292;GLRT&#35299;&#20915;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#12290;GLRT&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#12290;&#23427;&#20204;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#38598;&#35757;&#32451;&#20026;&#20004;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26367;&#20195;&#31867;&#20351;&#29992;&#22312;&#30446;&#26631;&#31867;&#25968;&#25454;&#38598;&#30340;&#23450;&#20041;&#22495;&#19978;&#22343;&#21248;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#21040;&#20102;GLRT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#24403;&#26680;&#20989;&#25968;&#30340;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#65288;OCLSSVM&#65289;&#22312;&#25910;&#25947;&#26102;&#34920;&#29616;&#20026;GLRT&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
&lt;/p&gt;</description></item></channel></rss>