<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01434</link><description>&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#19982;&#26080;&#31351;&#32500;&#25193;&#25955;&#36807;&#31243;&#30340;&#26465;&#20214;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Conditioning non-linear and infinite-dimensional diffusion processes
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#21644;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#38543;&#26426;&#27169;&#22411;&#22312;&#31163;&#25955;&#21270;&#20043;&#21069;&#33258;&#28982;&#22320;&#23384;&#22312;&#20110;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#23558;&#35266;&#27979;&#25968;&#25454;&#32435;&#20837;&#32479;&#35745;&#21644;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#23545;&#35266;&#27979;&#20540;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#22788;&#29702;&#20102;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#23545;&#38750;&#32447;&#24615;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#20808;&#39564;&#31163;&#25955;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;&#20989;&#25968;&#20540;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Girsanov&#23450;&#29702;&#30340;&#26080;&#31351;&#32500;&#29256;&#26412;&#26469;&#23545;&#20989;&#25968;&#20540;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#28041;&#21450;&#24471;&#20998;&#30340;&#26465;&#20214;&#36807;&#31243;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25216;&#26415;&#24212;&#29992;&#20110;&#36827;&#21270;&#29983;&#29289;&#23398;&#20013;&#30340;&#29983;&#29289;&#24418;&#24577;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;Fourier&#22522;&#20989;&#25968;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#26041;&#27861;&#23398;&#20064;&#24471;&#20998;&#20989;&#25968;&#30340;&#31995;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models and many stochastic models in science and engineering naturally live in infinite dimensions before discretisation. To incorporate observed data for statistical and learning tasks, one needs to condition on observations. While recent work has treated conditioning linear processes in infinite dimensions, conditioning non-linear processes in infinite dimensions has not been explored. This paper conditions function valued stochastic processes without prior discretisation. To do so, we use an infinite-dimensional version of Girsanov's theorem to condition a function-valued stochastic process, leading to a stochastic differential equation (SDE) for the conditioned process involving the score. We apply this technique to do time series analysis for shapes of organisms in evolutionary biology, where we discretise via the Fourier basis and then learn the coefficients of the score function with score matching methods.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02175</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#21453;&#24212;&#30340;&#31038;&#20250;&#21160;&#24577;&#65306;&#34701;&#21512;&#32479;&#35745;&#29289;&#29702;&#23398;&#19982;&#33829;&#38144;&#21160;&#24577;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Social Dynamics of Consumer Response: A Unified Framework Integrating Statistical Physics and Marketing Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02175
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#24066;&#22330;&#33829;&#38144;&#21160;&#24577;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#20934;&#30830;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28040;&#36153;&#32773;&#23545;&#24191;&#21578;&#36755;&#20837;&#30340;&#21453;&#24212;&#23545;&#20110;&#26088;&#22312;&#20248;&#21270;&#24191;&#21578;&#31574;&#30053;&#24182;&#25552;&#39640;&#24191;&#21578;&#27963;&#21160;&#26377;&#25928;&#24615;&#30340;&#33829;&#38144;&#20154;&#21592;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#28304;&#33258;&#29289;&#29702;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#30740;&#31350;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26041;&#31243;&#65292;&#25429;&#25417;&#20102;&#24191;&#21578;&#25903;&#20986;&#19982;&#28040;&#36153;&#32773;&#21453;&#24212;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#20102;&#35832;&#22914;&#23545;&#31216;&#24615;&#12289;&#26631;&#24230;&#24459;&#21644;&#30456;&#21464;&#31561;&#27010;&#24565;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#31243;&#39564;&#35777;&#19982;Michaelis-Menten&#21644;Hill&#26041;&#31243;&#31561;&#33879;&#21517;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#22312;&#20934;&#30830;&#34920;&#31034;&#28040;&#36153;&#32773;&#21453;&#24212;&#21160;&#24577;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20998;&#26512;&#24378;&#35843;&#20102;&#20851;&#38190;&#27169;&#22411;&#21442;&#25968;&#65288;&#22914;&#33829;&#38144;&#25928;&#26524;&#12289;&#21453;&#24212;&#25935;&#24863;&#24230;&#21644;&#34892;&#20026;&#25935;&#24863;&#24230;&#65289;&#23545;&#24433;&#21709;&#28040;&#36153;&#32773;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24191;&#21578;&#21830;&#21644;&#33829;&#38144;&#20154;&#21592;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02175v1 Announce Type: cross  Abstract: Comprehending how consumers react to advertising inputs is essential for marketers aiming to optimize advertising strategies and improve campaign effectiveness. This study examines the complex nature of consumer behaviour by applying theoretical frameworks derived from physics and social psychology. We present an innovative equation that captures the relation between spending on advertising and consumer response, using concepts such as symmetries, scaling laws, and phase transitions. By validating our equation against well-known models such as the Michaelis-Menten and Hill equations, we prove its effectiveness in accurately representing the complexity of consumer response dynamics. The analysis emphasizes the importance of key model parameters, such as marketing effectiveness, response sensitivity, and behavioural sensitivity, in influencing consumer behaviour. The work explores the practical implications for advertisers and marketers,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01946</link><description>&lt;p&gt;
&#29992;&#20110;&#40065;&#26834;&#24615;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data for Robust Stroke Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#20013;&#39118;&#20998;&#21106;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#20351;&#29992;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#25193;&#23637;&#20102;SynthSeg&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#23545;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#26080;&#38656;&#29305;&#23450;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#30446;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31070;&#32463;&#24433;&#20687;&#35821;&#20041;&#20998;&#21106;&#38656;&#35201;&#39640;&#20998;&#36776;&#29575;&#25195;&#25551;&#21644;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#36825;&#32473;&#20020;&#24202;&#36866;&#29992;&#24615;&#24102;&#26469;&#20102;&#26174;&#33879;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#30149;&#21464;&#20998;&#21106;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#24050;&#24314;&#31435;&#30340;SynthSeg&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#30149;&#21464;&#29305;&#23450;&#22686;&#24378;&#31574;&#30053;&#30340;&#22823;&#22411;&#24322;&#36136;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20174;&#20581;&#24247;&#21644;&#20013;&#39118;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;&#26631;&#31614;&#26144;&#23556;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#28436;&#31034;&#20102;UNet&#26550;&#26500;&#65292;&#20419;&#36827;&#20102;&#20581;&#24247;&#32452;&#32455;&#21644;&#30149;&#29702;&#30149;&#21464;&#30340;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#20110;&#24207;&#21015;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#38024;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#33021;&#65292;&#19982;&#35757;&#32451;&#39046;&#22495;&#20869;&#30340;&#24403;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#24182;&#22312;OOD&#25968;&#25454;&#19978;&#26174;&#30528;&#20248;&#20110;&#23427;&#20204;&#12290;&#36825;&#19968;&#36129;&#29486;&#26377;&#26395;&#25512;&#21160;&#21307;&#23398;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01946v1 Announce Type: cross  Abstract: Deep learning-based semantic segmentation in neuroimaging currently requires high-resolution scans and extensive annotated datasets, posing significant barriers to clinical applicability. We present a novel synthetic framework for the task of lesion segmentation, extending the capabilities of the established SynthSeg approach to accommodate large heterogeneous pathologies with lesion-specific augmentation strategies. Our method trains deep learning models, demonstrated here with the UNet architecture, using label maps derived from healthy and stroke datasets, facilitating the segmentation of both healthy tissue and pathological lesions without sequence-specific training data. Evaluated against in-domain and out-of-domain (OOD) datasets, our framework demonstrates robust performance, rivaling current methods within the training domain and significantly outperforming them on OOD data. This contribution holds promise for advancing medical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00785</link><description>&lt;p&gt;
&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#20043;&#35868;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30740;&#31350;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#32972;&#26223;&#19979;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#25968;&#25454;&#38598;&#20013;&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#12290;&#20511;&#21161;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21306;&#20998;&#20195;&#34920;&#24180;&#40836;&#21644;&#26159;&#21542;&#24739;&#30149;&#30340;&#20004;&#20010;&#19981;&#21516;&#28508;&#21464;&#37327;&#26469;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;VAE&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22686;&#24378;&#30340;&#35299;&#24320;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#20351;&#29992;&#20102;&#26469;&#33258;DTI&#28023;&#39532;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;3D&#29615;&#24418;&#32593;&#26684;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;3D&#28023;&#39532;&#32593;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#35299;&#24320;&#27169;&#22411;&#22312;&#35299;&#24320;&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#23646;&#24615;&#21644;&#24341;&#23548;VAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#24180;&#40836;&#32452;&#21644;&#30142;&#30149;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#65292;&#24314;&#31435;&#20102;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#65292;&#35777;&#26126;&#20854;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26356;&#32039;&#20945;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.12732</link><description>&lt;p&gt;
&#23545;&#20110;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26356;&#32039;&#20945;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Tighter Confidence Bounds for Sequential Kernel Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12732
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#65292;&#24314;&#31435;&#20102;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#65292;&#35777;&#26126;&#20854;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26356;&#32039;&#20945;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32622;&#20449;&#21306;&#38388;&#26159;&#20005;&#26684;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#25351;&#23548;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#26435;&#34913;&#65292;&#24182;&#26500;&#25104;&#35768;&#22810;&#24207;&#36143;&#23398;&#20064;&#21644;&#20915;&#31574;&#31639;&#27861;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#26356;&#32039;&#20945;&#30340;&#32622;&#20449;&#21306;&#38388;&#24102;&#26469;&#20102;&#20855;&#26377;&#26356;&#22909;&#32463;&#39564;&#24615;&#33021;&#21644;&#26356;&#22909;&#24615;&#33021;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#38789;&#23614;&#24052;&#30028;&#38480;&#21644;&#26080;&#38480;&#32500;&#20984;&#35268;&#21010;&#30340;&#26377;&#38480;&#32500;&#37325;&#26500;&#26469;&#24314;&#31435;&#24207;&#36143;&#26680;&#22238;&#24402;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26032;&#32622;&#20449;&#21306;&#38388;&#22987;&#32456;&#27604;&#29616;&#26377;&#30340;&#26356;&#32039;&#20945;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32622;&#20449;&#21306;&#38388;&#24212;&#29992;&#20110;&#26680;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;&#26410;&#26469;&#30340;&#34892;&#21160;&#21462;&#20915;&#20110;&#20808;&#21069;&#30340;&#21382;&#21490;&#12290;&#24403;&#25105;&#20204;&#30340;&#32622;&#20449;&#21306;&#38388;&#21462;&#20195;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#26102;&#65292;KernelUCB&#65288;GP-UCB&#65289;&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#65292;&#21305;&#37197;&#30340;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#20445;&#35777;&#21644;&#21487;&#27604;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12732v1 Announce Type: cross  Abstract: Confidence bounds are an essential tool for rigorously quantifying the uncertainty of predictions. In this capacity, they can inform the exploration-exploitation trade-off and form a core component in many sequential learning and decision-making algorithms. Tighter confidence bounds give rise to algorithms with better empirical performance and better performance guarantees. In this work, we use martingale tail bounds and finite-dimensional reformulations of infinite-dimensional convex programs to establish new confidence bounds for sequential kernel regression. We prove that our new confidence bounds are always tighter than existing ones in this setting. We apply our confidence bounds to the kernel bandit problem, where future actions depend on the previous history. When our confidence bounds replace existing ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a matching worst-case performance guarantee and compara
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12338</link><description>&lt;p&gt;
&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Halpern iteration in normed spaces and applications to reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;Halpern&#36845;&#20195;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#36817;&#20284;&#26377;&#30028;&#21644;&#25910;&#32553;&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#22312;&#19968;&#20010;&#26377;&#38480;&#32500;&#36171;&#33539;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24213;&#23618;&#30340;&#38543;&#26426;Oracle&#20855;&#26377;&#19968;&#33268;&#26377;&#30028;&#30340;&#26041;&#24046;&#65292;&#21017;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#24635;&#30340;Oracle&#22797;&#26434;&#24230;&#20026;$ \tilde{O} (\varepsilon^{-5})$&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#20026;&#38543;&#26426;Krasnoselskii-Mann&#36845;&#20195;&#24314;&#31435;&#30340;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; $\Omega (\varepsilon^{-3})$&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#25152;&#26377;&#24102;&#26377;&#23567;&#25209;&#22788;&#29702;&#30340;&#24179;&#22343;&#36845;&#20195;&#12290;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#31639;&#23376;&#20026; $\gamma$-&#25910;&#32553;&#30340;&#24773;&#20917;&#19979;&#19968;&#20010; $O(\varepsilon^{-2}(1-\gamma)^{-3})$&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29992;&#20110;&#24179;&#22343;&#22870;&#21169;&#21644;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.11407</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Divide-and-Conquer Posterior Sampling for Denoising Diffusion Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20808;&#39564;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#20998;&#38548;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#20316;&#20026;&#36870;&#36125;&#21494;&#26031;&#38382;&#39064;&#27714;&#35299;&#30340;&#20808;&#39564;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20174;&#32467;&#26524;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#26041;&#27861;&#26469;&#20559;&#32622;&#25193;&#25955;&#30340;&#28418;&#31227;&#39033;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;DDM&#20808;&#39564;&#30340;&#29305;&#23450;&#32467;&#26500;&#26469;&#23450;&#20041;&#19968;&#32452;&#20013;&#38388;&#21644;&#26356;&#31616;&#21333;&#30340;&#21518;&#39564;&#25277;&#26679;&#38382;&#39064;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36924;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#20363;&#23376;&#21644;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#26469;&#23454;&#35777;&#22320;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#23545;&#20110;&#19968;&#33324;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#37325;&#26500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11407v1 Announce Type: cross  Abstract: Interest in the use of Denoising Diffusion Models (DDM) as priors for solving inverse Bayesian problems has recently increased significantly. However, sampling from the resulting posterior distribution poses a challenge. To solve this problem, previous works have proposed approximations to bias the drift term of the diffusion. In this work, we take a different approach and utilize the specific structure of the DDM prior to define a set of intermediate and simpler posterior sampling problems, resulting in a lower approximation error compared to previous methods. We empirically demonstrate the reconstruction capability of our method for general linear inverse problems using synthetic examples and various image restoration tasks.
&lt;/p&gt;</description></item><item><title>MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.10691</link><description>&lt;p&gt;
MYTE&#65306;&#24418;&#24577;&#23398;&#39537;&#21160;&#30340;&#23383;&#33410;&#32534;&#30721;&#65292;&#29992;&#20110;&#26356;&#22909;&#12289;&#26356;&#20844;&#24179;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10691
&lt;/p&gt;
&lt;p&gt;
MYTE&#26159;&#19968;&#31181;&#22522;&#20110;&#24418;&#24577;&#23398;&#30340;&#23383;&#33410;&#32534;&#30721;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#23454;&#29616;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#20026;99&#31181;&#35821;&#35328;&#25552;&#20379;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#32771;&#34385;&#22240;&#32032;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#34920;&#31034;&#20855;&#26377;&#19981;&#21516;&#35789;&#27719;&#21644;&#25991;&#23383;&#30340;&#35821;&#35328;&#12290;&#23613;&#31649;&#24403;&#20195;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#28085;&#30422;&#20102;&#22823;&#22810;&#25968;&#19990;&#30028;&#25991;&#23383;&#31995;&#32479;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20559;&#21521;&#20110;&#20840;&#29699;&#35199;&#26041;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23569;&#25968;&#35821;&#35328;&#30340;&#25991;&#26412;&#24448;&#24448;&#34987;&#20998;&#21106;&#20026;&#19968;&#38271;&#20018;&#22312;&#35821;&#35328;&#23398;&#19978;&#27627;&#26080;&#24847;&#20041;&#30340;&#21333;&#20803;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#19981;&#24179;&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#29992;&#36328;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#19968;&#33268;&#22823;&#23567;&#30340;&#29255;&#27573;&#26469;&#32534;&#30721;&#30456;&#21516;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#32422;&#23450;&#65288;MYTE&#65289;&#22522;&#20110;&#24418;&#24577;&#32032;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24211;&#23384;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#27604;&#23383;&#31526;&#26356;&#24179;&#34913;&#65292;&#32780;&#20197;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#23383;&#31526;&#12290;&#25105;&#20204;&#23637;&#31034;MYTE&#20026;&#25152;&#26377;99&#31181;&#20998;&#26512;&#35821;&#35328;&#20135;&#29983;&#20102;&#26356;&#30701;&#30340;&#32534;&#30721;&#65292;&#20854;&#20013;&#38750;&#27431;&#27954;&#35821;&#35328;&#21644;&#38750;&#25289;&#19969;&#25991;&#23383;&#30340;&#25913;&#36827;&#26368;&#20026;&#26174;&#33879;&#12290;&#36825;&#36827;&#32780;&#25913;&#21892;&#20102;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10691v1 Announce Type: cross  Abstract: A major consideration in multilingual language modeling is how to best represent languages with diverse vocabularies and scripts. Although contemporary text encoding methods cover most of the world's writing systems, they exhibit bias towards the high-resource languages of the Global West. As a result, texts of underrepresented languages tend to be segmented into long sequences of linguistically meaningless units. To address the disparities, we introduce a new paradigm that encodes the same information with segments of consistent size across diverse languages. Our encoding convention (MYTE) is based on morphemes, as their inventories are more balanced across languages than characters, which are used in previous methods. We show that MYTE produces shorter encodings for all 99 analyzed languages, with the most notable improvements for non-European languages and non-Latin scripts. This, in turn, improves multilingual LM performance and di
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.01471</link><description>&lt;p&gt;
&#20445;&#25345;&#30456;&#20851;&#24615;&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preserving correlations: A statistical method for generating synthetic data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01471
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#33298;&#36866;&#30340;&#38544;&#31169;&#32423;&#21035;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#23458;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#29992;&#20110;&#20998;&#26512;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#22411;&#33021;&#28304;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;&#23450;&#24615;&#65288;&#20363;&#22914;&#36890;&#36807;&#21487;&#35270;&#21270;&#30456;&#20851;&#24615;&#22270;&#65289;&#21644;&#23450;&#37327;&#65288;&#20197;&#36866;&#24403;&#30340;$\ell^1$&#31867;&#22411;&#35823;&#24046;&#33539;&#25968;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#65289;&#26041;&#38754;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#26159;&#19968;&#33324;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26399;&#26395;&#23427;&#21487;&#36866;&#29992;&#20110;&#27604;&#27492;&#22788;&#25351;&#31034;&#30340;&#26356;&#24191;&#27867;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01471v1 Announce Type: new  Abstract: We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands.   We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $\ell^1$-type error norms used as evaluation metrics).   The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.
&lt;/p&gt;</description></item><item><title>FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.19197</link><description>&lt;p&gt;
&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19197
&lt;/p&gt;
&lt;p&gt;
FSS&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#26469;&#25913;&#21892;&#32467;&#26524;&#65292;&#21516;&#26102;&#24341;&#20837;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;&#32032;&#23545;&#40784;&#30340;&#38544;&#24335;&#27169;&#22411;&#65292;&#22914;PIFu&#12289;PIFuHD&#21644;ICON&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;&#30528;&#35013;&#20154;&#20307;&#37325;&#24314;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#34180;&#34920;&#38754;&#65288;&#22914;&#32819;&#26421;&#12289;&#25163;&#25351;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#37325;&#24314;&#32593;&#26684;&#20013;&#30340;&#22122;&#22768;&#20266;&#24433;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32454;&#32467;&#26500;&#24863;&#30693;&#37319;&#26679;&#65288;FSS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#21333;&#35270;&#22270;&#20154;&#20307;&#37325;&#24314;&#20013;&#35757;&#32451;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#12290;FSS&#36890;&#36807;&#20027;&#21160;&#36866;&#24212;&#34920;&#38754;&#30340;&#21402;&#24230;&#21644;&#22797;&#26434;&#24615;&#26469;&#35299;&#20915;&#21069;&#36848;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#35757;&#32451;&#26041;&#26696;&#19981;&#21516;&#65292;FSS&#26174;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26679;&#26412;&#28857;&#30340;&#27861;&#32447;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#39640;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#65292;FSS&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20687;&#32032;&#23545;&#40784;&#38544;&#24335;&#27169;&#22411;&#30340;&#32593;&#26684;&#21402;&#24230;&#25439;&#22833;&#20449;&#21495;&#12290;&#36825;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#27861;&#32447;&#21464;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19197v1 Announce Type: cross  Abstract: Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.17106</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#20844;&#24179;&#24615;&#65306;&#22312;&#24744;&#30340;&#25968;&#25454;&#19978;&#23454;&#29616;&#20855;&#26377;&#25928;&#29992;&#20445;&#35777;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Dataset Fairness: Achievable Fairness on Your Data With Utility Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29305;&#24615;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#24182;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#20013;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#23567;&#21270;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#24046;&#24322;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#30340;&#20005;&#37325;&#31243;&#24230;&#22522;&#26412;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#38598;&#30340;&#19981;&#22343;&#34913;&#25110;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#20351;&#29992;&#32479;&#19968;&#30340;&#20844;&#24179;&#24615;&#35201;&#27714;&#20173;&#28982;&#20540;&#24471;&#24576;&#30097;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#25928;&#29992;&#26497;&#20302;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#38598;&#37327;&#36523;&#23450;&#21046;&#30340;&#36817;&#20284;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26354;&#32447;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#20005;&#26684;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;You-Only-Train-Once&#65288;YOTO&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#22312;&#36924;&#36817;&#26435;&#34913;&#26354;&#32447;&#26102;&#38656;&#35201;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#35813;&#26354;&#32447;&#21608;&#22260;&#24341;&#20837;&#32622;&#20449;&#21306;&#38388;&#26469;&#37327;&#21270;&#25105;&#20204;&#36817;&#20284;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17106v1 Announce Type: cross  Abstract: In machine learning fairness, training models which minimize disparity across different sensitive groups often leads to diminished accuracy, a phenomenon known as the fairness-accuracy trade-off. The severity of this trade-off fundamentally depends on dataset characteristics such as dataset imbalances or biases. Therefore using a uniform fairness requirement across datasets remains questionable and can often lead to models with substantially low utility. To address this, we present a computationally efficient approach to approximate the fairness-accuracy trade-off curve tailored to individual datasets, backed by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO) framework, our approach mitigates the computational burden of having to train multiple models when approximating the trade-off curve. Moreover, we quantify the uncertainty in our approximation by introducing confidence intervals around this curve, offe
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.15213</link><description>&lt;p&gt;
&#32479;&#35745;&#26080;&#20559;&#22238;&#24402;&#65306;&#19968;&#31181;&#29992;&#20110;&#39564;&#35777;&#22238;&#24402;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Agnostic Regression: a machine learning method to validate regression models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#20998;&#26512;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#22240;&#21464;&#37327;&#65288;&#36890;&#24120;&#31216;&#20026;&#21709;&#24212;&#21464;&#37327;&#65289;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#33258;&#21464;&#37327;&#65288;&#21363;&#35299;&#37322;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32447;&#24615;&#22238;&#24402;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#39044;&#27979;&#12289;&#39044;&#27979;&#25110;&#22240;&#26524;&#25512;&#26029;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#25191;&#34892;&#27492;&#20219;&#21153;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#12290;&#38500;&#20102;&#35299;&#20915;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#30340;&#21508;&#31181;&#20256;&#32479;&#26041;&#27861;&#22806;&#65292;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#12289;&#23725;&#22238;&#24402;&#25110;&#22871;&#32034;&#22238;&#24402;&#8212;&#8212;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26159;&#26356;&#39640;&#32423;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#30340;&#22522;&#30784;&#8212;&#8212;&#21518;&#32773;&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#20294;&#27809;&#26377;&#23545;&#32479;&#35745;&#26174;&#33879;&#24615;&#36827;&#34892;&#27491;&#24335;&#23450;&#20041;&#12290;&#26368;&#22810;&#65292;&#22522;&#20110;&#32463;&#39564;&#27979;&#37327;&#65288;&#22914;&#27531;&#24046;&#25110;&#20934;&#30830;&#24230;&#65289;&#36827;&#34892;&#32622;&#25442;&#25110;&#22522;&#20110;&#32463;&#20856;&#20998;&#26512;&#65292;&#20197;&#21453;&#26144;ML&#20272;&#35745;&#23545;&#26816;&#27979;&#30340;&#26356;&#39640;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32479;&#35745;&#26080;&#20851;&#22320;&#35780;&#20272;&#20102;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#23545;ML&#20272;&#35745;&#22312;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15213v1 Announce Type: cross  Abstract: Regression analysis is a central topic in statistical modeling, aiming to estimate the relationships between a dependent variable, commonly referred to as the response variable, and one or more independent variables, i.e., explanatory variables. Linear regression is by far the most popular method for performing this task in several fields of research, such as prediction, forecasting, or causal inference. Beyond various classical methods to solve linear regression problems, such as Ordinary Least Squares, Ridge, or Lasso regressions - which are often the foundation for more advanced machine learning (ML) techniques - the latter have been successfully applied in this scenario without a formal definition of statistical significance. At most, permutation or classical analyses based on empirical measures (e.g., residuals or accuracy) have been conducted to reflect the greater ability of ML estimations for detection. In this paper, we introd
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.09821</link><description>&lt;p&gt;
&#38899;&#39057;&#24674;&#22797;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Audio Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#39057;&#25773;&#25918;&#35774;&#22791;&#21644;&#24555;&#36895;&#25968;&#25454;&#20256;&#36755;&#30340;&#21457;&#23637;&#65292;&#23545;&#39640;&#38899;&#36136;&#30340;&#38656;&#27714;&#22312;&#23089;&#20048;&#21644;&#36890;&#20449;&#39046;&#22495;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24405;&#21046;&#36807;&#31243;&#20013;&#30340;&#22833;&#30495;&#21644;&#24178;&#25200;&#65292;&#25110;&#32773;&#30001;&#20110;&#19981;&#23436;&#21892;&#30340;&#20256;&#36755;&#31649;&#36947;&#65292;&#38899;&#39057;&#36136;&#37327;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#38899;&#39057;&#24674;&#22797;&#26041;&#27861;&#26088;&#22312;&#20174;&#25439;&#22351;&#30340;&#36755;&#20837;&#25968;&#25454;&#20013;&#24674;&#22797;&#20986;&#28165;&#26224;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#38899;&#39057;&#24674;&#22797;&#31639;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#35821;&#38899;&#22686;&#24378;&#21644;&#38899;&#20048;&#24674;&#22797;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22522;&#20110;&#25163;&#24037;&#35268;&#21017;&#21644;&#32479;&#35745;&#21551;&#21457;&#27861;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#25105;&#20204;&#23545;&#38899;&#39057;&#20449;&#21495;&#30340;&#35748;&#35782;&#12290;&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#36716;&#21521;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24314;&#27169;&#33021;&#21147;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#25104;&#20026;&#19968;&#31181;&#26032;&#20852;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09821v1 Announce Type: cross  Abstract: With the development of audio playback devices and fast data transmission, the demand for high sound quality is rising, for both entertainment and communications. In this quest for better sound quality, challenges emerge from distortions and interferences originating at the recording side or caused by an imperfect transmission pipeline. To address this problem, audio restoration methods aim to recover clean sound signals from the corrupted input data. We present here audio restoration algorithms based on diffusion models, with a focus on speech enhancement and music restoration tasks. Traditional approaches, often grounded in handcrafted rules and statistical heuristics, have shaped our understanding of audio signals. In the past decades, there has been a notable shift towards data-driven methods that exploit the modeling capabilities of deep neural networks (DNNs). Deep generative models, and among them diffusion models, have emerged 
&lt;/p&gt;</description></item><item><title>NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.08365</link><description>&lt;p&gt;
NeuRes: &#23398;&#20064;&#21629;&#39064;&#21487;&#28385;&#36275;&#24615;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
NeuRes: Learning Proofs of Propositional Satisfiability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08365
&lt;/p&gt;
&lt;p&gt;
NeuRes&#26159;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;&#65292;&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#25214;&#21040;&#21487;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#35777;&#26126;&#20026;&#22522;&#30784;&#30340;SAT&#35299;&#26512;&#22120;NeuRes&#12290;&#19982;&#20854;&#20182;&#31070;&#32463;SAT&#35299;&#31639;&#27861;&#19981;&#21516;&#65292;NeuRes&#33021;&#22815;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#23427;&#12290;NeuRes&#36890;&#36807;&#37319;&#29992;&#21629;&#39064;&#25512;&#29702;&#26469;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#24182;&#21152;&#36895;&#22312;&#19981;&#21487;&#28385;&#36275;&#21644;&#21487;&#28385;&#36275;&#20844;&#24335;&#20013;&#25214;&#21040;&#28385;&#36275;&#30495;&#20540;&#20998;&#37197;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#25351;&#38024;&#32593;&#32476;&#30340;&#20803;&#32032;&#65292;&#20174;&#21160;&#24577;&#22270;&#32467;&#26500;&#20013;&#33258;&#21160;&#36873;&#25321;&#33410;&#28857;&#23545;&#65292;&#36825;&#23545;&#20110;&#29983;&#25104;&#35299;&#26512;&#35777;&#26126;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;NeuroSAT&#30456;&#21516;&#30340;&#38543;&#26426;&#20844;&#24335;&#20998;&#24067;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#25945;&#24072;&#35777;&#26126;&#21644;&#30495;&#20540;&#20998;&#37197;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NeuRes&#22312;&#19981;&#21516;&#20998;&#24067;&#19978;&#27604;NeuroSAT&#35299;&#20915;&#26356;&#22810;&#30340;&#27979;&#35797;&#20844;&#24335;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NeuRes, a neuro-symbolic proof-based SAT solver. Unlike other neural SAT solving methods, NeuRes is capable of proving unsatisfiability as opposed to merely predicting it. By design, NeuRes operates in a certificate-driven fashion by employing propositional resolution to prove unsatisfiability and to accelerate the process of finding satisfying truth assignments in case of unsat and sat formulas, respectively. To realize this, we propose a novel architecture that adapts elements from Graph Neural Networks and Pointer Networks to autoregressively select pairs of nodes from a dynamic graph structure, which is essential to the generation of resolution proofs. Our model is trained and evaluated on a dataset of teacher proofs and truth assignments that we compiled with the same random formula distribution used by NeuroSAT. In our experiments, we show that NeuRes solves more test formulas than NeuroSAT by a rather wide margin on different distributions while being much more data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65288;DDCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#39034;&#24207;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07352</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data Distribution-based Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65288;DDCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#39034;&#24207;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26679;&#26412;&#30340;&#39034;&#24207;&#23545;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#35838;&#31243;&#23398;&#20064;&#26159;&#19968;&#31181;&#23558;&#35757;&#32451;&#26679;&#26412;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;DDCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;DDCL&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#20197;&#21450;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#26469;&#20915;&#23450;&#35757;&#32451;&#26679;&#26412;&#30340;&#39034;&#24207;&#12290;DDCL&#65288;&#23494;&#24230;&#65289;&#21033;&#29992;&#26679;&#26412;&#23494;&#24230;&#20998;&#37197;&#35780;&#20998;&#65292;&#32780;DDCL&#65288;&#28857;&#65289;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;DDCL&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27809;&#26377;&#20219;&#20309;&#35838;&#31243;&#30340;&#26631;&#20934;&#35780;&#20272;&#30456;&#27604;&#65292;&#24212;&#29992;DDCL&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The order of training samples can have a significant impact on the performance of a classifier. Curriculum learning is a method of ordering training samples from easy to hard. This paper proposes the novel idea of a curriculum learning approach called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the data distribution of a dataset to build a curriculum based on the order of samples. Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order. DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier. Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any curriculum. 
&lt;/p&gt;</description></item><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04081</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#25913;&#36827;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Generalization of Weight Space Networks via Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#65288;DWS&#65289;&#20013;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#22312;2D&#21644;3D&#31070;&#32463;&#22330;&#65288;INRs&#65292;NeRFs&#65289;&#20197;&#21450;&#23545;&#20854;&#20182;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#20102;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;DWS&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#34429;&#28982;&#19968;&#20010;&#32473;&#23450;&#30340;&#23545;&#35937;&#21487;&#20197;&#34987;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#37197;&#32622;&#25152;&#34920;&#31034;&#65292;&#20294;&#20856;&#22411;&#30340;INR&#35757;&#32451;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#34920;&#31034;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;INR&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#25193;&#20805;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26435;&#37325;&#31354;&#38388;&#30340;MixUp&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#21319;&#31867;&#20284;&#20110;&#25317;&#26377;&#22810;&#36798;10&#20493;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.03282</link><description>&lt;p&gt;
&#19968;&#20010;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#22312;RLHF&#20013;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Partially Observed Reward-States in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;RLHF&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#32771;&#34385;&#20102;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#32553;&#20943;&#20026;PORRL&#24418;&#24335;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#31639;&#27861;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#30740;&#31350;&#22240;&#20854;&#22312;LLMs&#30340;&#21457;&#23637;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#32780;&#21464;&#24471;&#37325;&#35201;&#12290;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#21050;&#28608;&#30340;&#21453;&#24212;&#24050;&#30693;&#20381;&#36182;&#20110;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#8220;&#20869;&#37096;&#29366;&#24577;&#8221;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;RLHF&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#20013;&#38388;&#21453;&#39304;&#65292;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#23545;&#40784;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#23558;RLHF&#24314;&#27169;&#20026;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#29366;&#24577;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PORRL&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;RLHF&#20013;&#20004;&#31181;&#20027;&#35201;&#24418;&#24335;&#30340;&#20154;&#31867;&#21453;&#39304; - &#22522;&#25968;&#21453;&#39304;&#21644;&#20915;&#26007;&#21453;&#39304;&#21040;PORRL&#30340;&#32553;&#20943;&#12290;&#23545;&#20110;&#22522;&#25968;&#21453;&#39304;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30340;&#32479;&#35745;&#39640;&#25928;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#23454;&#20363;&#21270;&#20026;POR-UCRL&#21644;POR-UCBVI&#12290;&#23545;&#20110;&#20915;&#26007;&#21453;&#39304;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#25968;&#21453;&#39304;&#32553;&#20943;&#19981;&#33021;&#36798;&#21040;&#20122;&#32447;&#24615;&#30340;&#20915;&#26007;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of reinforcement learning from human feedback (RLHF) has gained prominence in recent years due to its role in the development of LLMs. Neuroscience research shows that human responses to stimuli are known to depend on partially-observed "internal states." Unfortunately current models of RLHF do not take take this into consideration. Moreover most RLHF models do not account for intermediate feedback, which is gaining importance in empirical work and can help improve both sample complexity and alignment. To address these limitations, we model RLHF as reinforcement learning with partially observed reward-states (PORRL). We show reductions from the the two dominant forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For cardinal feedback, we develop generic statistically efficient algorithms and instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we show that a naive reduction to cardinal feedback fails to achieve sublinear dueling regr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.14736</link><description>&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Data Diversity Matters for Robust Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14736
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22810;&#26679;&#24615;&#23545;&#40065;&#26834;&#25351;&#20196;&#35843;&#25972;&#38750;&#24120;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;(QDIT)&#65292;&#36890;&#36807;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#31934;&#36873;&#39640;&#36136;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#21160;&#31934;&#36873;&#25110;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;&#33258;&#21160;&#25968;&#25454;&#31934;&#36873;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#20173;&#19981;&#28165;&#26970;&#22914;&#20309;&#20026;&#25351;&#20196;&#35843;&#25972;&#23450;&#20041;&#22810;&#26679;&#24615;&#65292;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#22914;&#20309;&#30456;&#20114;&#20851;&#32852;&#65292;&#20197;&#21450;&#22914;&#20309;&#20248;&#21270;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#36136;&#37327;-&#22810;&#26679;&#24615;&#25351;&#20196;&#35843;&#25972;(QDIT)&#12290;QDIT&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#21516;&#26102;&#25511;&#21046;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#28145;&#20837;&#30740;&#31350;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#23545;&#25351;&#20196;&#35843;&#25972;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#65306;(1)&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;(2)&#22686;&#21152;&#25968;&#25454;&#22810;&#26679;&#24615;&#26174;&#33879;&#25552;&#39640;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25351;&#20196;&#36319;&#38543;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by curating high quality and diverse instruction tuning datasets, we can significantly improve instruction-following capabilities. However, creating such datasets is difficult and most works rely on manual curation or proprietary language models. Automatic data curation is difficult as it is still not clear how we can define diversity for instruction tuning, how diversity and quality depend on one other, and how we can optimize dataset quality and diversity. To resolve these issue, we propose a new algorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple method to simultaneously control dataset diversity and quality, allowing us to conduct an in-depth study on the effect of diversity and quality on instruction tuning performance. From this study we draw two key insights (1) there is a natural tradeoff between data diversity and quality and (2) increasing data diversity significantly improves the worst case instruction following perform
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15043</link><description>&lt;p&gt;
&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#65306;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#21644;&#22686;&#24378;&#23398;&#20064;&#30340;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Health Text Simplification: An Annotated Corpus for Digestive Cancer Education and Novel Strategies for Reinforcement Learning. (arXiv:2401.15043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#30340;&#28040;&#21270;&#30284;&#30151;&#25945;&#32946;&#26448;&#26009;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#24182;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#20581;&#24247;&#25945;&#32946;&#26448;&#26009;&#30340;&#38405;&#35835;&#27700;&#24179;&#26174;&#33879;&#24433;&#21709;&#20449;&#24687;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#25509;&#35302;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#26063;&#35028;&#20154;&#32676;&#12290;&#35768;&#22810;&#24739;&#32773;&#25945;&#32946;&#36164;&#28304;&#36229;&#36807;&#20102;&#24191;&#27867;&#25509;&#21463;&#30340;&#26631;&#20934;&#30340;&#38405;&#35835;&#27700;&#24179;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#20581;&#24247;&#20449;&#24687;&#20013;&#65292;&#24613;&#38656;&#39640;&#24615;&#33021;&#30340;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#20197;&#22686;&#24378;&#20256;&#25773;&#21644;&#35782;&#23383;&#33021;&#21147;&#12290;&#36825;&#31181;&#38656;&#35201;&#22312;&#30284;&#30151;&#25945;&#32946;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#26377;&#25928;&#30340;&#39044;&#38450;&#21644;&#31579;&#26597;&#25945;&#32946;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21270;&#30340;&#28040;&#21270;&#30284;&#30151;&#65288;SimpleDC&#65289;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20581;&#24247;&#25991;&#26412;&#31616;&#21270;&#30740;&#31350;&#12290;&#21033;&#29992;SimpleDC&#21644;&#29616;&#26377;&#30340;Med-EASi&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#21253;&#25324;&#24494;&#35843;&#12289;&#22686;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#12289;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The reading level of health educational materials significantly influences information understandability and accessibility, particularly for minoritized populations. Many patient educational resources surpass the reading level and complexity of widely accepted standards. There is a critical need for high-performing text simplification models in health information to enhance dissemination and literacy. This need is particularly acute in cancer education, where effective prevention and screening education can substantially reduce morbidity and mortality.  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel corpus of cancer education materials tailored for health text simplification research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore Large Language Model (LLM)-based simplification methods, including fine-tuning, reinforcement learning (RL), reinforcement learning with human feedback (RLHF), domain adaptation, and prompt-based app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.12196</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#22810;&#32454;&#32990;&#22270;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamics from Multicellular Graphs with Deep Neural Networks. (arXiv:2401.12196v1 [physics.bio-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#26426;&#26800;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#32773;&#24314;&#35758;&#36890;&#36807;&#21512;&#20316;&#21162;&#21147;&#26469;&#26500;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#32454;&#32990;&#33258;&#32452;&#35013;&#30340;&#25512;&#26029;&#26159;&#29702;&#35299;&#24418;&#24577;&#21457;&#29983;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#21253;&#25324;&#32986;&#32974;&#12289;&#22120;&#23448;&#32467;&#26500;&#12289;&#32959;&#30244;&#31561;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#25214;&#21040;&#33021;&#22815;&#25351;&#31034;&#22810;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#32467;&#26500;&#29305;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#26469;&#21457;&#29616;&#21487;&#20197;&#39044;&#27979;&#21160;&#21147;&#23398;&#30340;&#37325;&#35201;&#22270;&#29305;&#24449;&#12290;&#20026;&#20102;&#35777;&#26126;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#20010;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340; GNN&#65288;piGNN&#65289;&#26469;&#39044;&#27979;&#22810;&#32454;&#32990;&#38598;&#21512;&#20307;&#30340;&#36816;&#21160;&#33021;&#21147;&#65292;&#20174;&#23427;&#20204;&#22312;&#23454;&#39564;&#21644;&#27169;&#25311;&#20013;&#30340;&#20301;&#32622;&#24555;&#29031;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; piGNN &#33021;&#22815;&#22312;&#22810;&#32454;&#32990;&#29983;&#29289;&#31995;&#32479;&#30340;&#22797;&#26434;&#22270;&#29305;&#24449;&#20013;&#23548;&#33322;&#65292;&#36825;&#26159;&#32463;&#20856;&#26426;&#26800;&#27169;&#22411;&#26080;&#27861;&#23454;&#29616;&#30340;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#22810;&#32454;&#32990;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#36827;&#34892;&#21512;&#20316;&#21162;&#21147;&#65292;&#21019;&#24314;&#19968;&#20010;&#22810;&#32454;&#32990;&#25968;&#25454;&#24211;&#65288;MDB&#65289;&#65292;&#20174;&#20013;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#32454;&#32990;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference of multicellular self-assembly is the central quest of understanding morphogenesis, including embryos, organoids, tumors, and many others. However, it has been tremendously difficult to identify structural features that can indicate multicellular dynamics. Here we propose to harness the predictive power of graph-based deep neural networks (GNN) to discover important graph features that can predict dynamics. To demonstrate, we apply a physically informed GNN (piGNN) to predict the motility of multicellular collectives from a snapshot of their positions both in experiments and simulations. We demonstrate that piGNN is capable of navigating through complex graph features of multicellular living systems, which otherwise can not be achieved by classical mechanistic models. With increasing amounts of multicellular data, we propose that collaborative efforts can be made to create a multicellular data bank (MDB) from which it is possible to construct a large multicellular graph m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.10825</link><description>&lt;p&gt;
&#26368;&#26032;&#36827;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on recent advances in named entity recognition. (arXiv:2401.10825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10825
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#31639;&#27861;&#24615;&#33021;&#30340;&#28145;&#24230;&#27604;&#36739;&#65292;&#36824;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#26041;&#27861;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#21629;&#21517;&#30495;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#23376;&#23383;&#31526;&#20018;&#65292;&#24182;&#30830;&#23450;&#20854;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#26159;&#21542;&#25351;&#20154;&#29289;&#25110;&#32452;&#32455;&#65289;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#26368;&#36817;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#20851;&#27880;&#20102;&#22522;&#20110;&#22270;&#21644;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24456;&#23569;&#22312;&#20854;&#20182;&#32508;&#36848;&#20013;&#28041;&#21450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#38024;&#23545;&#31232;&#32570;&#27880;&#37322;&#25968;&#25454;&#38598;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20027;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#23454;&#29616;&#22312;&#21508;&#31181;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#65288;&#39046;&#22495;&#12289;&#35268;&#27169;&#21644;&#31867;&#21035;&#25968;&#65289;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#26410;&#21516;&#26102;&#32771;&#34385;&#30340;&#31639;&#27861;&#30340;&#28145;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#29305;&#24449;&#22914;&#20309;&#24433;&#21709;&#25105;&#20204;&#27604;&#36739;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, but we also look at graph- and transformer- based methods including Large Language Models (LLMs) that have not had much coverage in other surveys. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that are never considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods that we compare.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#19981;&#21487;&#32422;&#34920;&#31034;&#24352;&#37327;&#31215;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31561;&#21464;&#25805;&#20316;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2401.10216</link><description>&lt;p&gt;
&#36890;&#36807;Gaunt&#24352;&#37327;&#31215;&#22312;&#20613;&#37324;&#21494;&#22522;&#30784;&#19978;&#23454;&#29616;&#39640;&#25928;&#30340;&#31561;&#21464;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products. (arXiv:2401.10216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#35745;&#31639;&#19981;&#21487;&#32422;&#34920;&#31034;&#24352;&#37327;&#31215;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31561;&#21464;&#25805;&#20316;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;3D&#25968;&#25454;&#26102;&#65292;&#21457;&#23637;E(3)&#32676;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#29616;&#36825;&#31181;&#31561;&#21464;&#24615;&#20027;&#35201;&#28041;&#21450;&#21040;&#19981;&#21487;&#32422;&#34920;&#31034;&#65288;irreps&#65289;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20351;&#29992;&#39640;&#38454;&#24352;&#37327;&#65292;&#36825;&#20123;&#25805;&#20316;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26174;&#33879;&#22686;&#21152;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#22823;&#22823;&#21152;&#36895;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#24120;&#29992;&#30340;Clebsch-Gordan&#31995;&#25968;&#19982;Gaunt&#31995;&#25968;&#36827;&#34892;&#20102;&#25968;&#23398;&#19978;&#30340;&#36830;&#25509;&#65292;Gaunt&#31995;&#25968;&#26159;&#19977;&#20010;&#29699;&#24418;&#35856;&#27874;&#20056;&#31215;&#30340;&#31215;&#20998;&#12290;&#36890;&#36807;Gaunt&#31995;&#25968;&#65292;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#24352;&#37327;&#31215;&#31561;&#20215;&#20110;&#30001;&#29699;&#24418;&#35856;&#27874;&#34920;&#31034;&#30340;&#29699;&#24418;&#20989;&#25968;&#20043;&#38388;&#30340;&#20056;&#27861;&#12290;&#36825;&#31181;&#35266;&#28857;&#36827;&#19968;&#27493;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#31561;&#21464;&#25805;&#20316;&#30340;&#22522;&#30784;&#20174;&#29699;&#24418;&#35856;&#27874;&#25913;&#21464;&#20026;2D&#20613;&#31435;&#21494;&#22522;&#30784;&#12290;&#22240;&#27492;&#65292;&#29699;&#24418;&#20989;&#25968;&#20043;&#38388;&#30340;&#20056;&#27861;&#21487;&#20197;&#22312;&#20613;&#31435;&#21494;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing equivariant neural networks for the E(3) group plays an important role in modeling 3D data across real-world applications. Enforcing this equivariance primarily involves the tensor products of irreducible representations (irreps). However, the computational complexity of such operations increases significantly as higher-order tensors are used. In this work, we propose a systematic approach to substantially accelerate the computation of the tensor products of irreps. We mathematically connect the commonly used Clebsch-Gordan coefficients to the Gaunt coefficients, which are integrals of products of three spherical harmonics. Through Gaunt coefficients, the tensor product of irreps becomes equivalent to the multiplication between spherical functions represented by spherical harmonics. This perspective further allows us to change the basis for the equivariant operations from spherical harmonics to a 2D Fourier basis. Consequently, the multiplication between spherical functions 
&lt;/p&gt;</description></item><item><title>E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07595</link><description>&lt;p&gt;
E3x&#65306;&#31616;&#21270;&#30340;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy. (arXiv:2401.07595v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07595
&lt;/p&gt;
&lt;p&gt;
E3x&#26159;&#19968;&#31181;&#31616;&#21270;&#20102;$\mathrm{E}(3)$&#31561;&#21464;&#28145;&#24230;&#23398;&#20064;&#30340;&#36719;&#20214;&#21253;&#65292;&#36890;&#36807;&#20869;&#32622;&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;E3x&#65292;&#19968;&#31181;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#36719;&#20214;&#21253;&#65292;&#35813;&#32593;&#32476;&#22312;&#19977;&#32500;&#31354;&#38388;&#30340;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#26041;&#38754;&#31561;&#21464;&#12290;&#19982;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;$\mathrm{E}(3)$-&#31561;&#21464;&#27169;&#22411;&#22312;&#36755;&#20837;&#21644;/&#25110;&#36755;&#20986;&#25968;&#25454;&#26159;&#19982;&#19977;&#32500;&#23545;&#35937;&#30456;&#20851;&#30340;&#25968;&#37327;&#26102;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#26159;&#22240;&#20026;&#27492;&#31867;&#25968;&#37327;&#65288;&#20363;&#22914;&#20301;&#32622;&#65289;&#30340;&#25968;&#20540;&#36890;&#24120;&#21462;&#20915;&#20110;&#25152;&#36873;&#25321;&#30340;&#22352;&#26631;&#31995;&#32479;&#12290;&#22312;&#21442;&#32771;&#31995;&#30340;&#21464;&#25442;&#19979;&#65292;&#36825;&#20123;&#20540;&#20250;&#21487;&#39044;&#27979;&#22320;&#21457;&#29983;&#21464;&#21270;&#65292;&#20294;&#23545;&#20110;&#26222;&#36890;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#35268;&#21017;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#20351;&#29992;&#20869;&#32622;&#30340;$\mathrm{E}(3)$-&#31561;&#21464;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20445;&#35777;&#23436;&#20840;&#28385;&#36275;&#30456;&#20851;&#30340;&#21464;&#25442;&#35268;&#21017;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;E3x&#30340;&#20195;&#30721;&#21487;&#20174;https://github.com/google-research/e3x&#33719;&#24471;&#65292;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#25991;&#26723;&#21644;&#20351;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces E3x, a software package for building neural networks that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$, consisting of translations, rotations, and reflections of three-dimensional space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models promise benefits whenever input and/or output data are quantities associated with three-dimensional objects. This is because the numeric values of such quantities (e.g. positions) typically depend on the chosen coordinate system. Under transformations of the reference frame, the values change predictably, but the underlying rules can be difficult to learn for ordinary machine learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks are guaranteed to satisfy the relevant transformation rules exactly, resulting in superior data efficiency and accuracy. The code for E3x is available from https://github.com/google-research/e3x, detailed documentation and usage examples ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.01762</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Solving Kernel Ridge Regression with Gradient Descent for a Non-Constant Kernel. (arXiv:2311.01762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#27861;&#35299;&#20915;&#38750;&#24120;&#25968;&#26680;&#30340;&#26680;&#23725;&#22238;&#24402;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#65292;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#26159;&#32447;&#24615;&#23725;&#22238;&#24402;&#30340;&#25512;&#24191;&#65292;&#23427;&#22312;&#25968;&#25454;&#20013;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#20294;&#22312;&#21442;&#25968;&#20013;&#26159;&#32447;&#24615;&#30340;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#38381;&#24335;&#35299;&#33719;&#24471;&#65292;&#20854;&#20013;&#21253;&#25324;&#30697;&#38453;&#27714;&#36870;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25913;&#21464;&#26680;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#20102;&#36825;&#23545;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24179;&#31227;&#19981;&#21464;&#26680;&#30340;&#24102;&#23485;&#26356;&#26032;&#26041;&#26696;&#65292;&#20854;&#20013;&#24102;&#23485;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#33267;&#38646;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#24102;&#23485;&#30340;&#20248;&#20110;&#20351;&#29992;&#24120;&#25968;&#24102;&#23485;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#24102;&#23485;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#20351;&#29992;&#36880;&#28176;&#20943;&#23567;&#30340;&#24102;&#23485;&#26102;&#65292;&#25105;&#20204;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Kernel ridge regression, KRR, is a generalization of linear ridge regression that is non-linear in the data, but linear in the parameters. The solution can be obtained either as a closed-form solution, which includes a matrix inversion, or iteratively through gradient descent. Using the iterative approach opens up for changing the kernel during training, something that is investigated in this paper. We theoretically address the effects this has on model complexity and generalization. Based on our findings, we propose an update scheme for the bandwidth of translational-invariant kernels, where we let the bandwidth decrease to zero during training, thus circumventing the need for hyper-parameter selection. We demonstrate on real and synthetic data how decreasing the bandwidth during training outperforms using a constant bandwidth, selected by cross-validation and marginal likelihood maximization. We also show theoretically and empirically that using a decreasing bandwidth, we are able to
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16221</link><description>&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Randomized Smoothing. (arXiv:2310.16221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16221
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#22312;&#22797;&#26434;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21482;&#22312;&#19968;&#20010;&#23545;&#35937;&#30340;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#65292;&#20197;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#36890;&#24120;&#30001;&#21487;&#20998;&#35299;&#20026;&#22810;&#20010;&#23454;&#20307;&#30340;&#23545;&#35937;&#32452;&#25104;&#65288;&#20363;&#22914;&#65292;&#23558;&#22270;&#20687;&#20998;&#35299;&#20026;&#20687;&#32032;&#65292;&#23558;&#22270;&#24418;&#20998;&#35299;&#20026;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#65289;&#12290;&#38543;&#26426;&#24179;&#28369;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#20854;&#36755;&#20837;&#30340;&#24494;&#23567;&#21464;&#21270;&#19978;&#20855;&#26377;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;-&#36890;&#36807;&#22312;&#20998;&#31867;&#20043;&#21069;&#38543;&#26426;&#28155;&#21152;&#22122;&#22768;&#26469;&#20445;&#35777;&#22810;&#25968;&#25237;&#31080;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23545;&#25163;&#19981;&#26159;&#20219;&#24847;&#24178;&#25200;&#25972;&#20010;&#23545;&#35937;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#32780;&#26159;&#23545;&#35937;&#30340;&#26576;&#20010;&#23454;&#20307;&#30340;&#23376;&#38598;&#65288;&#20363;&#22914;&#20687;&#32032;&#65289;&#26102;&#65292;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#23545;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#24615;&#35748;&#35777;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#38543;&#26426;&#24179;&#28369;&#65306;&#25105;&#20204;&#36890;&#36807;&#20165;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23454;&#20307;&#23376;&#38598;&#19978;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#26469;&#37096;&#20998;&#24179;&#28369;&#23545;&#35937;&#12290;&#36890;&#36807;&#20197;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#28155;&#21152;&#22122;&#22768;&#65292;&#25105;&#20204;&#33719;&#24471;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#22122;&#22768;&#20998;&#24067;&#21021;&#22987;&#21270;&#20998;&#23618;&#24179;&#28369;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data is complex and often consists of objects that can be decomposed into multiple entities (e.g. images into pixels, graphs into interconnected nodes). Randomized smoothing is a powerful framework for making models provably robust against small changes to their inputs - by guaranteeing robustness of the majority vote when randomly adding noise before classification. Yet, certifying robustness on such complex data via randomized smoothing is challenging when adversaries do not arbitrarily perturb entire objects (e.g. images) but only a subset of their entities (e.g. pixels). As a solution, we introduce hierarchical randomized smoothing: We partially smooth objects by adding random noise only on a randomly selected subset of their entities. By adding noise in a more targeted manner than existing methods we obtain stronger robustness guarantees while maintaining high accuracy. We initialize hierarchical smoothing using different noising distributions, yielding novel robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.13397</link><description>&lt;p&gt;
&#31561;&#21464;&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Equivariant Deep Weight Space Alignment. (arXiv:2310.13397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#65292;&#20197;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#30340;&#25490;&#21015;&#23545;&#31216;&#24615;&#20351;&#24471;&#31616;&#21333;&#25805;&#20316;&#22914;&#27169;&#22411;&#24179;&#22343;&#21644;&#30456;&#20284;&#24230;&#20272;&#35745;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23545;&#40784;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#21363;&#25214;&#21040;&#23427;&#20204;&#20043;&#38388;&#26368;&#20248;&#25490;&#21015;&#65292;&#26159;&#24517;&#35201;&#30340;&#12290;&#26356;&#19968;&#33324;&#22320;&#35828;&#65292;&#26435;&#37325;&#23545;&#40784;&#23545;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#27169;&#22411;&#21512;&#24182;&#65292;&#36890;&#36807;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#31354;&#38388;&#65292;&#21040;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26435;&#37325;&#23545;&#40784;&#26159;&#19968;&#20010;NP-hard&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#23548;&#33268;&#26041;&#27861;&#32791;&#26102;&#25110;&#32773;&#27425;&#20248;&#35299;&#12290;&#20026;&#20102;&#21152;&#36895;&#23545;&#40784;&#36807;&#31243;&#24182;&#25552;&#39640;&#20854;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Deep-Align&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#35299;&#20915;&#26435;&#37325;&#23545;&#40784;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26435;&#37325;&#23545;&#40784;&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Permutation symmetries of deep networks make simple operations like model averaging and similarity estimation challenging. In many cases, aligning the weights of the networks, i.e., finding optimal permutations between their weights, is necessary. More generally, weight alignment is essential for a wide range of applications, from model merging, through exploring the optimization landscape of deep neural networks, to defining meaningful distance functions between neural networks. Unfortunately, weight alignment is an NP-hard problem. Prior research has mainly focused on solving relaxed versions of the alignment problem, leading to either time-consuming methods or sub-optimal solutions. To accelerate the alignment process and improve its quality, we propose a novel framework aimed at learning to solve the weight alignment problem, which we name Deep-Align. To that end, we first demonstrate that weight alignment adheres to two fundamental symmetries and then, propose a deep architecture 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.04006</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerating optimization over the space of probability measures. (arXiv:2310.04006v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#21152;&#36895;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#30340;&#21152;&#36895;&#26159;&#19968;&#20010;&#38750;&#24120;&#23454;&#29992;&#21644;&#29702;&#35770;&#19978;&#26377;&#24847;&#20041;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#20248;&#21270;&#19978;&#65292;&#20294;&#32771;&#34385;&#21040;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#38656;&#35201;&#22312;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#26159;&#24456;&#26377;&#24847;&#20041;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#30697;&#26041;&#27861;&#30340;&#21704;&#23494;&#39039;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#20219;&#24847;&#39640;&#38454;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25968;&#20540;&#23454;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of gradient-based optimization methods is an issue of significant practical and theoretical interest, particularly in machine learning applications. Most research has focused on optimization over Euclidean spaces, but given the need to optimize over spaces of probability measures in many machine learning problems, it is of interest to investigate accelerated gradient methods in this context too. To this end, we introduce a Hamiltonian-flow approach that is analogous to moment-based approaches in Euclidean space. We demonstrate that algorithms based on this approach can achieve convergence rates of arbitrarily high order. Numerical examples illustrate our claim.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.05153</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood. (arXiv:2309.05153v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#37319;&#26679;&#19968;&#31995;&#21015;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#65292;&#36890;&#36807;&#22312;&#19981;&#26029;&#22024;&#26434;&#21270;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#23450;&#20041;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;EBMs&#65292;&#24182;&#19982;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#21327;&#21516;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20851;&#38381;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#20043;&#38388;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#35757;&#32451;&#33021;&#37327;&#22522;&#20934;&#27169;&#22411;&#65288;EBMs&#65289;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#36739;&#38271;&#12290;&#22240;&#27492;&#65292;EBMs&#21644;&#20854;&#20182;&#29983;&#25104;&#26694;&#26550;&#65288;&#22914;GANs&#21644;&#25193;&#25955;&#27169;&#22411;&#65289;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#26679;&#26412;&#36136;&#37327;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#21463;&#26368;&#36817;&#36890;&#36807;&#26368;&#22823;&#21270;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;DRL&#65289;&#26469;&#23398;&#20064;EBMs&#30340;&#21162;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#21516;&#25193;&#25955;&#24674;&#22797;&#20284;&#28982;&#65288;CDRL&#65289;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#21487;&#34892;&#22320;&#23398;&#20064;&#21644;&#20174;&#19968;&#31995;&#21015;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#20123;EBMs&#23450;&#20041;&#22312;&#36234;&#26469;&#36234;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#29256;&#26412;&#19978;&#65292;&#24182;&#19982;&#27599;&#20010;EBM&#30340;&#21021;&#22987;&#21270;&#27169;&#22411;&#37197;&#23545;&#12290;&#22312;&#27599;&#20010;&#22122;&#22768;&#27700;&#24179;&#19978;&#65292;&#21021;&#22987;&#21270;&#27169;&#22411;&#23398;&#20064;&#22312;EBM&#30340;&#37319;&#26679;&#36807;&#31243;&#20013;&#20998;&#25674;&#65292;&#32780;&#20004;&#20010;&#27169;&#22411;&#22312;&#21327;&#21516;&#35757;&#32451;&#26694;&#26550;&#20869;&#20849;&#21516;&#20272;&#35745;&#12290;&#21021;&#22987;&#21270;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20316;&#20026;&#36215;&#22987;&#28857;&#65292;&#32463;&#36807;EBM&#30340;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#36827;&#34892;&#25913;&#36827;&#12290;&#36890;&#36807;&#25913;&#36827;&#21518;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#24674;&#22797;&#20284;&#28982;&#26469;&#20248;&#21270;EBM&#12290;
&lt;/p&gt;
&lt;p&gt;
Training energy-based models (EBMs) with maximum likelihood estimation on high-dimensional data can be both challenging and time-consuming. As a result, there a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximimizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versons of a dataset, paired with an initializer model for each EBM. At each noise level, the initializer model learns to amortize the sampling process of the EBM, and the two models are jointly estimated within a cooperative training framework. Samples from the initializer serve as starting points that are refined by a few sampling steps from the EBM. With the refined samples, the EBM is optimized by maximizing recovery likelihood, while t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36924;&#36817;&#27874;&#20989;&#25968;&#24182;&#20248;&#21270;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20869;&#30340;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#33021;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.16468</link><description>&lt;p&gt;
&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;
&lt;/p&gt;
&lt;p&gt;
Computing excited states of molecules using normalizing flows. (arXiv:2308.16468v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16468
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35268;&#33539;&#21270;&#27969;&#35745;&#31639;&#20998;&#23376;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36924;&#36817;&#27874;&#20989;&#25968;&#24182;&#20248;&#21270;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20869;&#30340;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#20934;&#30830;&#21644;&#26377;&#25928;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#33021;&#37327;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#36827;&#34892;&#20102;&#26174;&#33879;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#21464;&#20998;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35745;&#31639;&#37327;&#23376;&#31995;&#32479;&#30340;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36890;&#36807;&#19982;&#35268;&#33539;&#21270;&#27969;&#30340;&#32452;&#21512;&#26469;&#36924;&#36817;&#27874;&#20989;&#25968;&#65292;&#36825;&#20123;&#27874;&#20989;&#25968;&#20301;&#20110;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#31354;&#38388;&#20013;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#19977;&#21407;&#23376;H$_2$S&#20998;&#23376;&#30340;&#22823;&#37327;&#25391;&#21160;&#24577;&#20197;&#21450;&#20856;&#22411;&#30340;&#21333;&#30005;&#23376;&#31995;&#32479;&#65288;&#21253;&#25324;&#27682;&#21407;&#23376;&#12289;&#20998;&#23376;&#27682;&#31163;&#23376;&#21644;&#30899;&#21407;&#23376;&#22312;&#21333;&#28608;&#21457;&#30005;&#23376;&#36817;&#20284;&#19979;&#30340;&#22522;&#24577;&#21644;&#22810;&#20010;&#28608;&#21457;&#24577;&#65289;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#21442;&#25968;&#36739;&#23569;&#30340;&#35268;&#33539;&#21270;&#27969;&#65292;&#33021;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#22522;&#32452;&#25910;&#25947;&#36895;&#24230;&#20063;&#26377;&#26174;&#33879;&#25913;&#21892;&#12290;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23545;&#26368;&#20339;&#25429;&#25417;&#24213;&#23618;&#29289;&#29702;&#30340;&#19968;&#32452;&#20869;&#31104;&#22352;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new nonlinear variational framework for simultaneously computing ground and excited states of quantum systems. Our approach is based on approximating wavefunctions in the linear span of basis functions that are augmented and optimized \emph{via} composition with normalizing flows. The accuracy and efficiency of our approach are demonstrated in the calculations of a large number of vibrational states of the triatomic H$_2$S molecule as well as ground and several excited electronic states of prototypical one-electron systems including the hydrogen atom, the molecular hydrogen ion, and a carbon atom in a single-active-electron approximation. The results demonstrate significant improvements in the accuracy of energy predictions and accelerated basis-set convergence even when using normalizing flows with a small number of parameters. The present approach can be also seen as the optimization of a set of intrinsic coordinates that best capture the underlying physics within the gi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#35299;&#20915;&#20102;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#23545;&#35282;&#32447;&#20027;&#23548;&#24773;&#20917;&#19979;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#20197;&#21450;&#20381;&#36182;&#26410;&#30693;&#21442;&#25968;&#30340;&#25238;&#21160;&#23610;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16059</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#25913;&#36827;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Parameter-Free Two-Bit Covariance Estimator with Improved Operator Norm Error Rate. (arXiv:2308.16059v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16059
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21442;&#25968;&#30340;&#20108;&#20301;&#21327;&#26041;&#24046;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#35299;&#20915;&#20102;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#23545;&#35282;&#32447;&#20027;&#23548;&#24773;&#20917;&#19979;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#20197;&#21450;&#20381;&#36182;&#26410;&#30693;&#21442;&#25968;&#30340;&#25238;&#21160;&#23610;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Dirksen, Maly and Rauhut&#22312;&#12298;Annals of Statistics&#12299;&#19978;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#27599;&#20010;&#26465;&#30446;&#20004;&#20301;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#35813;&#20272;&#35745;&#22120;&#22312;&#19968;&#33324;&#20122;&#39640;&#26031;&#20998;&#24067;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26497;&#23567;&#21270;&#36895;&#29575;&#65292;&#20294;&#20063;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#29702;&#35770;&#19978;&#65292;&#22312;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23545;&#35282;&#32447;&#30001;&#23569;&#25968;&#26465;&#30446;&#20027;&#23548;&#26102;&#65292;&#20854;&#20272;&#35745;&#22120;&#19982;&#26679;&#26412;&#21327;&#26041;&#24046;&#20043;&#38388;&#23384;&#22312;&#26412;&#36136;&#19978;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#24046;&#36317;&#65307;&#23454;&#38469;&#19978;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#38656;&#35201;&#26681;&#25454;&#19968;&#20123;&#26410;&#30693;&#21442;&#25968;&#36827;&#34892;&#35843;&#25972;&#30340;&#25238;&#21160;&#23610;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#26032;&#22411;&#20108;&#20301;&#21327;&#26041;&#24046;&#30697;&#38453;&#20272;&#35745;&#22120;&#12290;&#19982;Dirksen&#31561;&#20154;&#37319;&#29992;&#30340;&#22343;&#21248;&#25238;&#21160;&#30456;&#20851;&#30340;&#31526;&#21495;&#37327;&#21270;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#22810;&#20301;&#22343;&#21248;&#37327;&#21270;&#22120;&#21551;&#21457;&#30340;&#19977;&#35282;&#25238;&#21160;&#22120;&#20043;&#21518;&#20877;&#36827;&#34892;&#20108;&#20301;&#37327;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#20010;&#26465;&#30446;&#20043;&#38388;&#21464;&#21270;&#30340;&#25238;&#21160;&#23610;&#24230;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#33719;&#24471;&#20102;&#25913;&#36827;&#30340;&#31639;&#23376;&#33539;&#25968;&#35823;&#24046;&#29575;&#65292;&#35813;&#35823;&#24046;&#29575;&#21462;&#20915;&#20110;...
&lt;/p&gt;
&lt;p&gt;
A covariance matrix estimator using two bits per entry was recently developed by Dirksen, Maly and Rauhut [Annals of Statistics, 50(6), pp. 3538-3562]. The estimator achieves near minimax rate for general sub-Gaussian distributions, but also suffers from two downsides: theoretically, there is an essential gap on operator norm error between their estimator and sample covariance when the diagonal of the covariance matrix is dominated by only a few entries; practically, its performance heavily relies on the dithering scale, which needs to be tuned according to some unknown parameters. In this work, we propose a new 2-bit covariance matrix estimator that simultaneously addresses both issues. Unlike the sign quantizer associated with uniform dither in Dirksen et al., we adopt a triangular dither prior to a 2-bit quantizer inspired by the multi-bit uniform quantizer. By employing dithering scales varying across entries, our estimator enjoys an improved operator norm error rate that depends o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15193</link><description>&lt;p&gt;
&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Repeated Multi-Unit Pay-As-Bid Auctions. (arXiv:2307.15193v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#20248;&#21270;&#20986;&#20215;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#26041;&#26696;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#30899;&#25490;&#25918;&#20132;&#26131;&#26041;&#26696;&#12289;&#22269;&#20538;&#25293;&#21334;&#21644;&#37319;&#36141;&#25293;&#21334;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#37117;&#28041;&#21450;&#25293;&#21334;&#21516;&#36136;&#30340;&#22810;&#20010;&#21333;&#20301;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22914;&#20309;&#22312;&#37325;&#22797;&#30340;&#22810;&#21333;&#20301;&#20184;&#36153;&#25293;&#21334;&#20013;&#23398;&#20064;&#22914;&#20309;&#20986;&#20215;&#30340;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#25293;&#21334;&#20013;&#65292;&#22823;&#37327;&#65288;&#30456;&#21516;&#30340;&#65289;&#29289;&#21697;&#23558;&#34987;&#20998;&#37197;&#32473;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#27599;&#20010;&#20013;&#26631;&#20215;&#31561;&#20110;&#20986;&#20215;&#26412;&#36523;&#12290;&#30001;&#20110;&#34892;&#21160;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#22914;&#20309;&#22312;&#20184;&#36153;&#25293;&#21334;&#20013;&#20986;&#20215;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20851;&#27880;&#31163;&#32447;&#35774;&#32622;&#65292;&#20854;&#20013;&#25237;&#26631;&#20154;&#36890;&#36807;&#21482;&#33021;&#35775;&#38382;&#20854;&#20182;&#25237;&#26631;&#20154;&#36807;&#21435;&#25552;&#20132;&#30340;&#20986;&#20215;&#26469;&#20248;&#21270;&#20182;&#20204;&#30340;&#20986;&#20215;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31163;&#32447;&#38382;&#39064;&#30340;&#26368;&#20248;&#35299;&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26102;&#38388;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26041;&#26696;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#21033;&#29992;DP&#26041;&#26696;&#30340;&#32467;&#26500;&#65292;&#35774;&#35745;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.12361</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340; sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#19982;&#38750;&#32447;&#24615;&#26410;&#30693;&#36755;&#20837;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems. (arXiv:2306.12361v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#20851;&#20110;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;(UI)&#20272;&#35745;&#30340;&#25991;&#29486;&#37117;&#35201;&#27714;UI&#26159;&#32447;&#24615;&#30340;&#65292;&#36825;&#20010;&#38480;&#21046;&#21487;&#33021;&#22826;&#20005;&#26684;&#20102;&#65292;&#22240;&#20026;&#23427;&#24182;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#26410;&#30693;&#36755;&#20837; Sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;(SPKE-nUI)&#65292;&#20854;&#20013; SPKF &#19982;&#26222;&#36890;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23454;&#29616;&#12290;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#20351;&#29992;&#21518;&#39564;&#29366;&#24577;&#20272;&#35745;&#65292;&#36825;&#23545;&#29366;&#24577;&#39044;&#27979;&#35823;&#24046;&#19981;&#22826;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#65292;&#23558;&#29366;&#24577;&#21644; UI &#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837; SPKF-nUI &#30340;&#20272;&#35745;&#20013;&#12290;&#28145;&#20837;&#30340;&#38543;&#26426;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340; SPKF-nUI &#21487;&#20197;&#20135;&#29983;&#25351;&#25968;&#32423;&#25910;&#25947;&#30340;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#27169;&#25311;&#30340;&#36335;&#38754;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on joint state and unknown input (UI) estimation require the assumption that the UIs are linear; this is potentially restrictive as it does not hold in many intelligent autonomous systems. To overcome this restriction and circumvent the need to linearize the system, we propose a derivative-free Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is interconnected with a general nonlinear UI estimator that can be implemented via nonlinear optimization and data-driven approaches. The nonlinear UI estimator uses the posterior state estimate which is less susceptible to state prediction error. In addition, we introduce a joint sigma-point transformation scheme to incorporate both the state and UI uncertainties in the estimation of SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed SPKF-nUI yields exponentially converging estimation error bounds under reasonable assumptions. Finally, two case studies are carried out on a simulation-based ri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11313</link><description>&lt;p&gt;
&#28145;&#24230;&#22270;&#26680;&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep graph kernel point processes. (arXiv:2306.11313v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#36807;&#31243;&#27169;&#22411;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#22270;&#20013;&#24322;&#27493;&#20107;&#20214;&#65292;&#21453;&#26144;&#19981;&#21516;&#31867;&#22411;&#20107;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#12290;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#26102;&#38388;&#21644;&#31867;&#22411;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#19988;&#22270;&#30340;&#22823;&#23567;&#21644;&#25299;&#25169;&#32467;&#26500;&#22686;&#21152;&#20102;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#25581;&#31034;&#20102;&#25429;&#25417;&#22797;&#26434;&#30340;&#20107;&#20214;&#31867;&#21035;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27599;&#20010;&#30446;&#26631;&#20107;&#20214;&#31867;&#22411;&#30340;&#24378;&#24230;&#35745;&#31639;&#20013;&#20351;&#29992;&#20102;&#21253;&#25324;&#25152;&#26377;&#20107;&#20214;&#31867;&#21035;&#22312;&#20869;&#30340;&#26410;&#32463;&#28388;&#27874;&#30340;&#20107;&#20214;&#35760;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#22270;&#28857;&#36807;&#31243;&#26041;&#27861;&#12290;&#23545;&#24212;&#30340;&#26080;&#21521;&#22270;&#20855;&#26377;&#20195;&#34920;&#20107;&#20214;&#31867;&#21035;&#30340;&#33410;&#28857;&#21644;&#34920;&#31034;&#28508;&#22312;&#36129;&#29486;&#20851;&#31995;&#30340;&#36793;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#26680;&#26469;&#25551;&#36848;&#20107;&#20214;&#20043;&#38388;&#30340;&#35302;&#21457;&#21644;&#25233;&#21046;&#25928;&#24212;&#12290;&#26412;&#36136;&#24433;&#21709;&#32467;&#26500;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;-based&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#32858;&#21512;&#36827;&#34892;&#20102;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26356;&#20855;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point process models are widely used to analyze asynchronous events occurring within a graph that reflect how different types of events influence one another. Predicting future events' times and types is a crucial task, and the size and topology of the graph add to the challenge of the problem. Recent neural point process models unveil the possibility of capturing intricate inter-event-category dependencies. However, such methods utilize an unfiltered history of events, including all event categories in the intensity computation for each target event type. In this work, we propose a graph point process method where event interactions occur based on a latent graph topology. The corresponding undirected graph has nodes representing event categories and edges indicating potential contribution relationships. We then develop a novel deep graph kernel to characterize the triggering and inhibiting effects between events. The intrinsic influence structures are incorporated via the graph neural
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17583</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26080;&#38480;&#26641;&#29366;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;&#27010;&#29575;&#22270;&#27169;&#22411;(PGMs)&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#26366;&#32463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#26680;&#26426;&#22120;&#25110;&#26080;&#38480;&#22823;&#23567;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#12290;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01693</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#23454;&#29616;&#36719;&#26426;&#22120;&#20154;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Transfer Learning and State Inference for Soft Robots via a Semi-supervised Sequential Variational Bayes Framework. (arXiv:2303.01693v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36719;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#36328;&#22495;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#36719;&#26426;&#22120;&#20154;&#24314;&#27169;&#21644;&#29366;&#24577;&#25512;&#26029;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#22320;&#36816;&#34892;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#35814;&#23613;&#21644;&#36136;&#37327;&#33391;&#22909;&#30340;&#25968;&#25454;&#37319;&#38598;&#65292;&#23588;&#20854;&#26159;&#29366;&#24577;&#26631;&#31614;&#30340;&#37319;&#38598;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#36719;&#26426;&#22120;&#20154;&#30340;&#20256;&#24863;&#22120;&#21270;&#22256;&#38590;&#21644;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#30340;&#19981;&#20415;&#31561;&#21407;&#22240;&#65292;&#33719;&#21462;&#26631;&#27880;&#30340;&#36719;&#26426;&#22120;&#20154;&#31995;&#32479;&#29366;&#24577;&#25968;&#25454;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#39034;&#24207;&#21464;&#20998;&#36125;&#21494;&#26031;&#65288;DSVB&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26576;&#20123;&#26426;&#22120;&#20154;&#37197;&#32622;&#20013;&#23384;&#22312;&#32570;&#22833;&#29366;&#24577;&#26631;&#31614;&#30340;&#36719;&#26426;&#22120;&#20154;&#30340;&#36801;&#31227;&#23398;&#20064;&#21644;&#29366;&#24577;&#25512;&#26029;&#12290;&#32771;&#34385;&#21040;&#36719;&#26426;&#22120;&#20154;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#37197;&#32622;&#19979;&#21487;&#33021;&#23637;&#29616;&#20986;&#19981;&#21516;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#36801;&#31227;&#31574;&#30053;&#65292;&#20197;&#20419;&#36827;&#22312;&#22810;&#20010;&#37197;&#32622;&#19979;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, data-driven models such as deep neural networks have shown to be promising tools for modelling and state inference in soft robots. However, voluminous amounts of data are necessary for deep models to perform effectively, which requires exhaustive and quality data collection, particularly of state labels. Consequently, obtaining labelled state data for soft robotic systems is challenged for various reasons, including difficulty in the sensorization of soft robots and the inconvenience of collecting data in unstructured environments. To address this challenge, in this paper, we propose a semi-supervised sequential variational Bayes (DSVB) framework for transfer learning and state inference in soft robots with missing state labels on certain robot configurations. Considering that soft robots may exhibit distinct dynamics under different robot configurations, a feature space transfer strategy is also incorporated to promote the adaptation of latent features across multiple config
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2211.14839</link><description>&lt;p&gt;
Waveflow&#65306;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24179;&#28369;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20197;&#36153;&#31859;&#27874;&#20989;&#25968;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions. (arXiv:2211.14839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#24402;&#19968;&#21270;&#27969;&#19978;&#24378;&#21046;&#26045;&#21152;&#29305;&#23450;&#31867;&#21035;&#36793;&#30028;&#26465;&#20214;&#30340;&#25216;&#26415;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; I-Spline &#21452;&#23556;&#65292;&#23427;&#20687;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#26679;&#21033;&#29992;&#20102;&#26679;&#26465;&#26354;&#32447;&#65292;&#20294;&#19982;&#36825;&#20123;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#21019;&#24314;&#20102; Waveflow&#65292;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#19968;&#32500;&#22810;&#31890;&#23376;&#36153;&#31859;&#27874;&#20989;&#25968;&#30340; Ansatz&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#33945;&#29305;&#21345;&#32599;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#26080;&#38656; MCMC &#25110;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#20026;&#20102;&#24378;&#21046;&#36153;&#31859;&#27874;&#20989;&#25968;&#25152;&#38656;&#30340;&#21453;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20165;&#22312;&#25490;&#21015;&#32676;&#30340;&#22522;&#26412;&#22495;&#19978;&#35757;&#32451;&#24402;&#19968;&#21270;&#27969;&#65292;&#36825;&#26377;&#25928;&#22320;&#23558;&#20854;&#20943;&#23569;&#20026;&#19968;&#20010;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce four main novelties: First, we present a new way of handling the topology problem of normalizing flows. Second, we describe a technique to enforce certain classes of boundary conditions onto normalizing flows. Third, we introduce the I-Spline bijection, which, similar to previous work, leverages splines but, in contrast to those works, can be made arbitrarily often differentiable. And finally, we use these techniques to create Waveflow, an Ansatz for the one-space-dimensional multi-particle fermionic wave functions in real space based on normalizing flows, that can be efficiently trained with Variational Quantum Monte Carlo without the need for MCMC nor estimation of a normalization constant. To enforce the necessary anti-symmetry of fermionic wave functions, we train the normalizing flow only on the fundamental domain of the permutation group, which effectively reduces it to a boundary value problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11456</link><description>&lt;p&gt;
MixMask: &#37325;&#26032;&#23457;&#35270;Siamese ConvNets&#30340;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
MixMask: Revisiting Masking Strategy for Siamese ConvNets. (arXiv:2210.11456v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;MixMask&#65292;&#22312;Siamese ConvNets&#20013;&#23454;&#29616;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36827;&#23637;&#23558;Masked Image Modeling&#65288;MIM&#65289;&#21644;Siamese&#32593;&#32476;&#25972;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;Siamese ConvNets&#20013;&#24212;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#31574;&#30053;&#26102;&#65292;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#65288;I&#65289;&#22312;&#36830;&#32493;&#22788;&#29702;&#25968;&#25454;&#26102;&#19981;&#33021;&#25918;&#24323;&#19981;&#30456;&#20851;&#30340;&#36974;&#30422;&#21306;&#22495;&#65292;&#23548;&#33268;&#35757;&#32451;&#25928;&#29575;&#20302;&#20110;ViT&#27169;&#22411;;&#65288;II&#65289;&#22522;&#20110;&#25830;&#38500;&#30340;&#36974;&#30422;&#19982;Siamese ConvNets&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#19981;&#21305;&#37197;&#65292;&#19982;MIM&#26041;&#27861;&#19981;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MixMask&#30340;&#22635;&#20805;&#24335;&#36974;&#30422;&#31574;&#30053;&#65292;&#20197;&#38450;&#27490;&#39321;&#33609;&#36974;&#30422;&#26041;&#27861;&#20013;&#22270;&#20687;&#20013;&#30340;&#38543;&#26426;&#36974;&#30422;&#21306;&#22495;&#23548;&#33268;&#20449;&#24687;&#19981;&#23436;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#32771;&#34385;&#20004;&#20010;&#19981;&#21516;&#28151;&#21512;&#35270;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#36317;&#31163;&#21464;&#21270;&#65292;&#20197;&#36866;&#24212;&#38598;&#25104;&#26550;&#26500;&#24182;&#38450;&#27490;&#36974;&#30422;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MixMask&#26174;&#30528;&#25552;&#39640;&#20102;Siamese ConvNets&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning have integrated Masked Image Modeling (MIM) and Siamese Networks into a unified framework that leverages the benefits of both techniques. However, several issues remain unaddressed when applying conventional erase-based masking with Siamese ConvNets. These include (I) the inability to drop uninformative masked regions in ConvNets as they process data continuously, resulting in low training efficiency compared to ViT models; and (II) the mismatch between erase-based masking and the contrastive-based objective in Siamese ConvNets, which differs from the MIM approach. In this paper, we propose a filling-based masking strategy called MixMask to prevent information incompleteness caused by the randomly erased regions in an image in the vanilla masking method. Furthermore, we introduce a flexible loss function design that considers the semantic distance change between two different mixed views to adapt the integrated architecture and prevent mismat
&lt;/p&gt;</description></item></channel></rss>