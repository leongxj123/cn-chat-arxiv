<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.12372</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Transferable Time Series Classifier with Cross-Domain Pre-training from Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29305;&#24615;&#24046;&#24322;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#26377;&#25928;&#36716;&#25442;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;SSL&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25512;&#21160;&#20102;&#21487;&#20256;&#36882;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#22312;&#36328;&#39046;&#22495;SSL&#39044;&#35757;&#32451;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#38169;&#36807;&#20102;&#38598;&#25104;&#19981;&#21516;&#39046;&#22495;&#27169;&#24335;&#21644;&#29305;&#24449;&#30340;&#23453;&#36149;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CrossTimeNet&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36328;&#39046;&#22495;SSL&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#21508;&#31181;&#39046;&#22495;&#23398;&#20064;&#21487;&#20256;&#36882;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#24378;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12372v1 Announce Type: new  Abstract: Advancements in self-supervised pre-training (SSL) have significantly advanced the field of learning transferable time series representations, which can be very useful in enhancing the downstream task. Despite being effective, most existing works struggle to achieve cross-domain SSL pre-training, missing valuable opportunities to integrate patterns and features from different domains. The main challenge lies in the significant differences in the characteristics of time-series data across different domains, such as variations in the number of channels and temporal resolution scales. To address this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning framework to learn transferable knowledge from various domains to largely benefit the target downstream task. One of the key characteristics of CrossTimeNet is the newly designed time series tokenization module, which could effectively convert the raw time series into a seque
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09940</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25163;&#30340;&#32852;&#37030;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Guarantees for Federated Policy Gradient Methods with Adversaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#23545;&#25163;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#20855;&#26377;&#23545;&#23545;&#25163;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated Reinforcement Learning (FRL)&#20801;&#35768;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#26500;&#24314;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#21407;&#22987;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#20195;&#29702;&#20013;&#21482;&#26377;&#23569;&#37096;&#20998;&#26159;&#23545;&#25163;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23545;&#23545;&#25163;&#20195;&#29702;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#21521;&#26381;&#21153;&#22120;&#21457;&#36865;&#20219;&#24847;&#20540;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24418;&#25104;&#20102;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#21270;&#30340;&#39318;&#20010;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#23545;&#25163;&#30340;&#24377;&#24615;&#65292;&#21516;&#26102;&#36798;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;$\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$&#65292;&#20854;&#20013;$N$&#26159;&#20195;&#29702;&#30340;&#24635;&#25968;&#65292;$f$&#26159;&#23545;&#25163;&#20195;&#29702;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09940v1 Announce Type: cross  Abstract: Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories. However, if a small fraction of these agents are adversarial, it can lead to catastrophic results. We propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server. Under this setting, our results form the first global convergence guarantees with general parametrization. These results demonstrate resilience with adversaries, while achieving sample complexity of order $\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} + \frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and $f$ is the number of adversarial agents.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.07605</link><description>&lt;p&gt;
&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#20197;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#32654;&#23398;&#21644;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;NegOpt&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#30340;&#29983;&#25104;&#65292;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;&#65292;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#24182;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#65292;&#20351;&#29992;&#25551;&#36848;&#19981;&#33391;&#22270;&#20687;&#29305;&#24449;&#30340;&#36127;&#38754;&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#33391;&#22909;&#30340;&#36127;&#38754;&#25552;&#31034;&#26159;&#19968;&#39033;&#25163;&#24037;&#32780;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NegOpt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#36127;&#38754;&#25552;&#31034;&#29983;&#25104;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#22823;&#24133;&#25552;&#39640;&#20102;25%&#30340;Inception Score&#65292;&#24182;&#36229;&#36234;&#20102;&#26469;&#33258;&#27979;&#35797;&#38598;&#30340;&#26631;&#20934;&#36127;&#38754;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;NegOpt&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20248;&#21270;&#23545;&#25105;&#20204;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36127;&#38754;&#25552;&#31034;&#25968;&#25454;&#38598;Negative Prompts DB&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07605v1 Announce Type: cross  Abstract: In text-to-image generation, using negative prompts, which describe undesirable image characteristics, can significantly boost image quality. However, producing good negative prompts is manual and tedious. To address this, we propose NegOpt, a novel method for optimizing negative prompt generation toward enhanced image generation, using supervised fine-tuning and reinforcement learning. Our combined approach results in a substantial increase of 25% in Inception Score compared to other approaches and surpasses ground-truth negative prompts from the test set. Furthermore, with NegOpt we can preferentially optimize the metrics most important to us. Finally, we construct Negative Prompts DB, a dataset of negative prompts.
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;</title><link>https://arxiv.org/abs/2402.15978</link><description>&lt;p&gt;
&#20351;&#29992;&#22885;&#21345;&#22982;&#21059;&#20992;&#21066;&#20943;&#26435;&#37325;&#65306;&#20351;&#29992;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#20284;&#28982;&#30340;&#36125;&#21494;&#26031;&#31232;&#30095;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#21644;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#65292;&#24182;&#37319;&#29992;&#33258;&#21160;&#22885;&#21345;&#22982;&#21059;&#20992;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26435;&#37325;&#21066;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#21270;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#35768;&#22810;&#25104;&#21151;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21464;&#24471;&#36807;&#22823;&#20197;&#33267;&#26080;&#27861;&#30452;&#25509;&#37096;&#32626;&#22312;&#28040;&#36153;&#31867;&#30828;&#20214;&#30340;&#26102;&#20195;&#12290;&#34429;&#28982;&#24456;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#19981;&#21516;&#30340;&#26435;&#37325;&#21098;&#26525;&#20934;&#21017;&#19978;&#65292;&#20294;&#32593;&#32476;&#30340;&#24635;&#20307;&#31232;&#30095;&#24615;&#65292;&#21363;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#21098;&#26525;&#30340;&#33021;&#21147;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#37327;&#65288;Marginal likelihood&#65289;&#30340;&#31232;&#30095;&#24615;&#65288;SpaM&#65289;&#65292;&#19968;&#20010;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20351;&#29992;&#36125;&#21494;&#26031;&#36793;&#32536;&#20284;&#28982;&#19982;&#31232;&#30095;&#35825;&#23548;&#20808;&#39564;&#30456;&#32467;&#21512;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#31232;&#30095;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#33258;&#21160;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65292;&#36873;&#25321;&#26368;&#24819;&#35201;&#21066;&#20943;&#30340;&#27169;&#22411;&#65292;&#20197;&#20381;&#28982;&#33021;&#22815;&#24456;&#22909;&#22320;&#35299;&#37322;&#25968;&#25454;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#32467;&#26500;&#21270;&#36824;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#31232;&#30095;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#20013;&#20351;&#29992;&#30340;&#39044;&#35745;&#31639;&#21518;&#39564;&#40657;&#22622;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15978v1 Announce Type: new  Abstract: Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;</title><link>https://arxiv.org/abs/2402.13393</link><description>&lt;p&gt;
&#38024;&#23545;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20844;&#24179;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Fairness Risks for Group-conditionally Missing Demographics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#24341;&#36215;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#35201;&#27714;&#23436;&#20840;&#20102;&#35299;&#25935;&#24863;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#38544;&#31169;&#12289;&#27861;&#24459;&#38382;&#39064;&#21644;&#20010;&#20154;&#23545;&#27495;&#35270;&#30340;&#24656;&#24807;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#19981;&#21487;&#29992;&#24615;&#30340;&#32676;&#20307;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#65292;&#26576;&#20123;&#24180;&#40836;&#33539;&#22260;&#30340;&#20154;&#21487;&#33021;&#26356;&#19981;&#24895;&#36879;&#38706;&#20182;&#20204;&#30340;&#24180;&#40836;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23545;&#25935;&#24863;&#29305;&#24449;&#36827;&#34892;&#27010;&#29575;&#22635;&#20805;&#65292;&#21516;&#26102;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#30340;&#27010;&#29575;&#65292;&#23558;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#19982;&#20043;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13393v1 Announce Type: new  Abstract: Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08958</link><description>&lt;p&gt;
&#36808;&#21521;&#36229;&#22823;&#35268;&#27169;Transformer&#30340;&#19979;&#19968;&#32423;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#25104;&#20026;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#30005;&#35270;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#36229;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#26696;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23454;&#38469;&#24773;&#20917;&#20013;&#39057;&#32321;&#27169;&#22411;&#26356;&#26032;&#21644;&#22810;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#29942;&#39048;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;PTQ&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#20123;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;Transformer&#20013;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23618;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#31639;&#27861;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#21483;&#20570;aespa&#65292;&#36890;&#36807;&#22312;&#25928;&#29575;&#19978;&#36827;&#34892;&#36880;&#23618;&#37327;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36328;&#23618;&#20381;&#36182;&#20197;&#20445;&#30041;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#25311;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32500;&#25345;&#23545;&#19981;&#21516;&#36136;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#21482;&#20351;&#29992;&#20102;&#24456;&#23569;&#30340;&#26597;&#35810;</title><link>https://arxiv.org/abs/2402.02774</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#20248;&#21270;&#21152;&#36895;&#25311;&#38453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Matroid Optimization through Fast Imprecise Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#21152;&#36895;&#25311;&#38453;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#32500;&#25345;&#23545;&#19981;&#21516;&#36136;&#37327;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#21482;&#20351;&#29992;&#20102;&#24456;&#23569;&#30340;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#22797;&#26434;&#27169;&#22411;&#20197;&#33719;&#24471;&#20934;&#30830;&#20449;&#24687;&#65288;&#20363;&#22914;&#27969;&#37327;&#27169;&#22411;&#12289;&#25968;&#25454;&#24211;&#31995;&#32479;&#12289;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#21644;&#36739;&#38271;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#26597;&#35810;&#24378;&#27169;&#22411;&#35299;&#20915;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#37027;&#20040;&#20351;&#29992;&#33021;&#22815;&#24555;&#36895;&#32473;&#20986;&#19981;&#20934;&#30830;&#32467;&#26524;&#30340;&#36739;&#24369;&#27169;&#22411;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#22312;&#35745;&#31639;&#19968;&#20010;&#25311;&#38453;&#30340;&#26368;&#22823;&#26435;&#37325;&#22522;&#30784;&#30340;&#22522;&#30784;&#38382;&#39064;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#26159;&#35768;&#22810;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#20010;&#24050;&#30693;&#27867;&#21270;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#24178;&#20928;&#30340;&#26597;&#35810;&#25311;&#38453;&#20449;&#24687;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#39069;&#22806;&#25552;&#20379;&#20102;&#19968;&#20010;&#24555;&#36895;&#20294;&#33039;&#30340;&#39044;&#27979;&#27169;&#22411;&#26469;&#27169;&#25311;&#19968;&#20010;&#26410;&#30693;&#30340;&#12289;&#21487;&#33021;&#19981;&#21516;&#30340;&#25311;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#23454;&#38469;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#21482;&#20351;&#29992;&#24456;&#23569;&#25968;&#37327;&#30340;&#24178;&#20928;&#26597;&#35810;&#30456;&#23545;&#20110;&#33039;&#39044;&#27979;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#20219;&#24847;&#36136;&#37327;&#24046;&#30340;&#33039;&#25311;&#38453;&#30340;&#24378;&#20581;&#24615;&#65292;&#24182;&#25509;&#36817;&#32473;&#23450;&#38382;&#39064;&#30340;&#32463;&#20856;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35768;&#22810;&#26041;&#38754;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#26368;&#20339;&#30340;
&lt;/p&gt;
&lt;p&gt;
Querying complex models for precise information (e.g. traffic models, database systems, large ML models) often entails intense computations and results in long response times. Thus, weaker models which give imprecise results quickly can be advantageous, provided inaccuracies can be resolved using few queries to a stronger model. In the fundamental problem of computing a maximum-weight basis of a matroid, a well-known generalization of many combinatorial optimization problems, algorithms have access to a clean oracle to query matroid information. We additionally equip algorithms with a fast but dirty oracle modelling an unknown, potentially different matroid. We design and analyze practical algorithms which only use few clean queries w.r.t. the quality of the dirty oracle, while maintaining robustness against arbitrarily poor dirty matroids, approaching the performance of classic algorithms for the given problem. Notably, we prove that our algorithms are, in many respects, best-possible
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17789</link><description>&lt;p&gt;
&#24377;&#24615;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#20013;&#30340;&#40065;&#26834;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Robustly overfitting latents for flexible neural image compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17789
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#36807;&#25311;&#21512;&#28508;&#21464;&#37327;&#26041;&#27861;&#26469;&#25913;&#36827;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;SGA+&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#20943;&#23569;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#27169;&#22411;&#12290;&#31070;&#32463;&#21387;&#32553;&#27169;&#22411;&#23398;&#20250;&#23558;&#22270;&#20687;&#32534;&#30721;&#20026;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39640;&#25928;&#22320;&#21457;&#36865;&#32473;&#35299;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#20877;&#23558;&#37327;&#21270;&#30340;&#28508;&#21464;&#37327;&#35299;&#30721;&#20026;&#37325;&#24314;&#22270;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20248;&#21270;&#19981;&#23436;&#32654;&#20197;&#21450;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23481;&#37327;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#23548;&#33268;&#20102;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;Gumbel&#36864;&#28779;&#65288;SGA&#65289;&#26469;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#27169;&#22411;&#30340;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SGA+&#25193;&#23637;&#20102;&#36825;&#20010;&#24819;&#27861;&#65292;SGA+&#21253;&#21547;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#24314;&#31435;&#22312;SGA&#30340;&#22522;&#30784;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#25913;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#27599;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#19977;&#20010;&#32780;&#19981;&#26159;&#20004;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural image compression has made a great deal of progress. State-of-the-art models are based on variational autoencoders and are outperforming classical models. Neural compression models learn to encode an image into a quantized latent representation that can be efficiently sent to the decoder, which decodes the quantized latent into a reconstructed image. While these models have proven successful in practice, they lead to sub-optimal results due to imperfect optimization and limitations in the encoder and decoder capacity. Recent work shows how to use stochastic Gumbel annealing (SGA) to refine the latents of pre-trained neural image compression models. We extend this idea by introducing SGA+, which contains three different methods that build upon SGA. Further, we give a detailed analysis of our proposed methods, show how they improve performance, and show that they are less sensitive to hyperparameter choices. Besides, we show how each method can be extended to three- instead of two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.15122</link><description>&lt;p&gt;
&#25193;&#23637;&#23601;&#26159;&#19968;&#20999;&#65306;&#20351;&#29992;JAX&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#35270;&#39057;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#26368;&#20248;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36816;&#34892;&#24517;&#35201;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#38750;&#24120;&#22256;&#38590;&#12290;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#36827;&#34892;&#20998;&#24067;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#19978;&#25910;&#38598;&#32463;&#39564;&#20174;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#19988;&#30495;&#23454;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#30495;&#23454;&#39550;&#39542;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33021;&#21147;&#38598;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.11517</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#39118;&#38505;&#22240;&#32032;&#20998;&#31867;&#19982;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Natural Language Processing-Based Classification and Mode-Based Ranking of Musculoskeletal Disorder Risk Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#26041;&#27861;&#23545;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#30340;&#39118;&#38505;&#22240;&#32032;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25490;&#21517;&#65292;&#20197;&#25552;&#39640;&#23545;&#20854;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#39044;&#38450;&#21644;&#27835;&#30103;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32908;&#32905;&#39592;&#39612;&#30142;&#30149;&#65288;MSD&#65289;&#39118;&#38505;&#22240;&#32032;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30456;&#32467;&#21512;&#12290;&#26088;&#22312;&#31934;&#32454;&#21270;&#29702;&#35299;&#12289;&#20998;&#31867;&#21644;&#20248;&#20808;&#32771;&#34385;&#38024;&#23545;&#24615;&#39044;&#38450;&#21644;&#27835;&#30103;&#12290;&#35780;&#20272;&#20102;&#20843;&#20010;NLP&#27169;&#22411;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#12289;&#20313;&#24358;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#24230;&#37327;&#23558;&#22240;&#32032;&#20998;&#31867;&#20026;&#20010;&#20154;&#12289;&#29983;&#29289;&#21147;&#23398;&#12289;&#24037;&#20316;&#22330;&#25152;&#12289;&#24515;&#29702;&#21644;&#32452;&#32455;&#31561;&#31867;&#21035;&#12290;BERT&#19982;&#20313;&#24358;&#30456;&#20284;&#24230;&#36798;&#21040;28%&#30340;&#20934;&#30830;&#29575;&#65307;&#21477;&#23376;&#36716;&#25442;&#22120;&#19982;&#27431;&#27663;&#12289;&#24067;&#38647;&#26354;&#33922;&#26031;&#21644;&#38389;&#21487;&#22827;&#26031;&#22522;&#36317;&#31163;&#24471;&#20998;&#20026;100%&#12290;&#36890;&#36807;10&#20493;&#20132;&#21449;&#39564;&#35777;&#65292;&#32479;&#35745;&#26816;&#39564;&#30830;&#20445;&#40065;&#26834;&#32467;&#26524;&#12290;&#35843;&#26597;&#25968;&#25454;&#21644;&#22522;&#20110;&#27169;&#24335;&#30340;&#25490;&#21517;&#30830;&#23450;&#20102;&#20005;&#37325;&#24615;&#31561;&#32423;&#65292;&#19982;&#25991;&#29486;&#30456;&#19968;&#33268;&#12290;"&#24037;&#20316;&#23039;&#21183;"&#26159;&#26368;&#20005;&#37325;&#30340;&#65292;&#20984;&#26174;&#20102;&#23039;&#21183;&#30340;&#20316;&#29992;&#12290;&#35843;&#26597;&#32467;&#26524;&#24378;&#35843;&#20102;"&#24037;&#20316;&#19981;&#31283;&#23450;&#24615;"&#12289;"&#24037;&#20316;&#21162;&#21147;&#21644;&#22238;&#25253;&#19981;&#24179;&#34913;"&#21644;"&#21592;&#24037;&#35774;&#26045;&#24046;"&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11517v3 Announce Type: replace  Abstract: This research delves into Musculoskeletal Disorder (MSD) risk factors, using a blend of Natural Language Processing (NLP) and mode-based ranking. The aim is to refine understanding, classification, and prioritization for focused prevention and treatment. Eight NLP models are evaluated, combining pre-trained transformers, cosine similarity, and distance metrics to categorize factors into personal, biomechanical, workplace, psychological, and organizational classes. BERT with cosine similarity achieves 28% accuracy; sentence transformer with Euclidean, Bray-Curtis, and Minkowski distances scores 100%. With 10-fold cross-validation, statistical tests ensure robust results. Survey data and mode-based ranking determine severity hierarchy, aligning with the literature. "Working posture" is the most severe, highlighting posture's role. Survey insights emphasize "Job insecurity," "Effort reward imbalance," and "Poor employee facility" as sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.13447</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#31526;&#21495;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Symbolic Equation Solving via Reinforcement Learning. (arXiv:2401.13447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#26041;&#31243;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36880;&#28176;&#22312;&#21508;&#31181;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#31185;&#23398;&#29615;&#22659;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#28982;&#32780;&#22312;&#31934;&#30830;&#25968;&#23398;&#19978;&#23427;&#20204;&#24120;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#35745;&#31639;&#26426;&#20195;&#25968;&#65292;&#21253;&#25324;&#31616;&#21270;&#25968;&#23398;&#26415;&#35821;&#12289;&#35745;&#31639;&#24418;&#24335;&#23548;&#25968;&#25110;&#25214;&#21040;&#20195;&#25968;&#26041;&#31243;&#30340;&#31934;&#30830;&#35299;&#31561;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#36719;&#20214;&#21253;&#36890;&#24120;&#22522;&#20110;&#19968;&#20010;&#24040;&#22823;&#30340;&#35268;&#21017;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#25551;&#36848;&#20102;&#19968;&#20010;&#29305;&#23450;&#25805;&#20316;&#65288;&#20363;&#22914;&#23548;&#25968;&#65289;&#22914;&#20309;&#23558;&#19968;&#20010;&#26415;&#35821;&#65288;&#20363;&#22914;&#27491;&#24358;&#20989;&#25968;&#65289;&#36716;&#21270;&#20026;&#21478;&#19968;&#20010;&#65288;&#20363;&#22914;&#20313;&#24358;&#20989;&#25968;&#65289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#35268;&#21017;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#21457;&#29616;&#24182;&#32534;&#31243;&#12290;&#37325;&#28857;&#35752;&#35770;&#35299;&#20915;&#31526;&#21495;&#24418;&#24335;&#30340;&#32447;&#24615;&#26041;&#31243;&#30340;&#33539;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#21270;&#22320;&#25214;&#21040;&#22522;&#26412;&#21464;&#25442;&#35268;&#21017;&#21644;&#36880;&#27493;&#35299;&#20915;&#26041;&#26696;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning methods are gradually being adopted in a great variety of social, economic, and scientific contexts, yet they are notorious for struggling with exact mathematics. A typical example is computer algebra, which includes tasks like simplifying mathematical terms, calculating formal derivatives, or finding exact solutions of algebraic equations. Traditional software packages for these purposes are commonly based on a huge database of rules for how a specific operation (e.g., differentiation) transforms a certain term (e.g., sine function) into another one (e.g., cosine function). Thus far, these rules have usually needed to be discovered and subsequently programmed by humans. Focusing on the paradigmatic example of solving linear equations in symbolic form, we demonstrate how the process of finding elementary transformation rules and step-by-step solutions can be automated using reinforcement learning with deep neural networks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05146</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#25968;&#25454;&#22312;&#26412;&#22320;&#23384;&#20648;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#29992;&#25143;&#21644;&#26426;&#26500;&#30340;&#38544;&#31169;&#12290;&#19982;&#38598;&#20013;&#21270;&#21407;&#22987;&#25968;&#25454;&#19981;&#21516;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#26412;&#22320;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#36880;&#27493;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26356;&#21152;&#31526;&#21512;&#26032;&#20852;&#35268;&#23450;&#65292;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#65292;&#20294;&#22312;&#27492;&#32972;&#26223;&#19979;&#30830;&#20445;&#36951;&#24536;&#26435;&#8212;&#8212;&#20801;&#35768;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#26041;&#20174;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#36129;&#29486;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#33021;&#36890;&#36807;&#26356;&#26032;&#23558;&#21518;&#38376;&#27880;&#20837;&#20840;&#23616;&#27169;&#22411;&#65292;&#20363;&#22914;&#23545;&#29305;&#21046;&#25968;&#25454;&#31034;&#20363;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26426;&#21046;&#26469;&#30830;&#20445;&#20010;&#20154;&#26377;&#21487;&#33021;&#22312;&#32858;&#21512;&#21518;&#31227;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#24182;&#28165;&#38500;&#24694;&#24847;&#36129;&#29486;&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#33719;&#24471;&#30340;"&#20840;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired "g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.16713</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#33258;&#27880;&#24847;&#21147;&#25554;&#34917;&#65288;CSAI&#65289;&#27169;&#22411;&#20197;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32454;&#33410;&#30340;&#26465;&#20214;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25554;&#34917;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;CSAI&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;</description></item><item><title>&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#20248;&#21270;&#21160;&#24577;&#65292;&#23545;&#20110;&#27424;&#21442;&#25968;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#21457;&#25955;&#12290;</title><link>http://arxiv.org/abs/2310.04415</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#38656;&#35201;&#26435;&#37325;&#34928;&#20943;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Do We Need Weight Decay in Modern Deep Learning?. (arXiv:2310.04415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04415
&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#20248;&#21270;&#21160;&#24577;&#65292;&#23545;&#20110;&#27424;&#21442;&#25968;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#21457;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#34928;&#20943;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#20316;&#29992;&#20173;&#19981;&#22826;&#34987;&#20102;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#26435;&#37325;&#34928;&#20943;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19982;&#20854;&#22312;&#32463;&#20856;&#23398;&#20064;&#29702;&#35770;&#20013;&#30740;&#31350;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#36890;&#36807;&#25439;&#22833;&#31283;&#23450;&#26426;&#21046;&#25913;&#36827;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#30340;&#20248;&#21270;&#21160;&#24577;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#29992;&#20960;&#20046;&#22312;&#32447;SGD&#35757;&#32451;&#30340;&#27424;&#21442;&#25968;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#22312;&#38543;&#26426;&#20248;&#21270;&#20013;&#24179;&#34913;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#20302;&#30340;&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26435;&#37325;&#34928;&#20943;&#22914;&#20309;&#38450;&#27490;bfloat16&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#20013;&#31361;&#28982;&#30340;&#25439;&#22833;&#21457;&#25955;&#65292;&#36825;&#26159;LLM&#35757;&#32451;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;ResNet&#23545;&#35270;&#35273;&#20219;&#21153;&#21040;LLM&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65306;&#26435;&#37325;&#34928;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight decay is a broadly used technique for training state-of-the-art deep networks, including large language models. Despite its widespread usage, its role remains poorly understood. In this work, we highlight that the role of weight decay in modern deep learning is different from its regularization effect studied in classical learning theory. For overparameterized deep networks, we show how weight decay modifies the optimization dynamics enhancing the ever-present implicit regularization of SGD via the loss stabilization mechanism. In contrast, for underparameterized large language models trained with nearly online SGD, we describe how weight decay balances the bias-variance tradeoff in stochastic optimization leading to lower training loss. Moreover, we show that weight decay also prevents sudden loss divergences for bfloat16 mixed-precision training which is a crucial tool for LLM training. Overall, we present a unifying perspective from ResNets on vision tasks to LLMs: weight dec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15325</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#35266;&#27979;&#37096;&#20998;&#12289;&#31895;&#31890;&#21270;&#21644;&#31561;&#21464;&#24615;&#22312;Koopman&#31639;&#23376;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems. (arXiv:2307.15325v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25511;&#21046;&#22797;&#26434;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#20174;&#27979;&#37327;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#20989;&#25968;&#31354;&#38388;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#65292;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#37096;&#20998;&#35266;&#27979;&#65288;&#22914;&#23454;&#39564;&#25968;&#25454;&#20013;&#38750;&#24120;&#24120;&#35265;&#30340;&#27979;&#37327;&#65289;&#25110;&#32773;&#20986;&#20110;&#25928;&#29575;&#21407;&#22240;&#21051;&#24847;&#36827;&#34892;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#31181;&#24773;&#20917;&#20013;&#30340;&#22256;&#25200;&#65292;&#21363;&#22914;&#26524;&#25105;&#20204;&#19981;&#20180;&#32454;&#36873;&#25321;&#21487;&#35266;&#27979;&#25968;&#37327;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#33258;&#21160;&#25552;&#20379;&#28508;&#22312;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31995;&#32479;&#21160;&#24577;&#20013;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#19982;&#22495;&#20998;&#35299;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems, the main reason being the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. Until now, the situation where for large-scale systems, we (i) only have access to partial observations (i.e., measurements, as is very common for experimental data) or (ii) deliberately perform coarse graining (for efficiency reasons) has not been treated to its full extent. In this paper, we address the pitfall associated with this situation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. Moreover, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficiency. We also briefly draw a connection to domain decompos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.03927</link><description>&lt;p&gt;
&#24555;&#36895;&#32463;&#39564;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Fast Empirical Scenarios. (arXiv:2307.03927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#24555;&#36895;&#30340;&#32463;&#39564;&#22330;&#26223;&#25552;&#21462;&#31639;&#27861;&#65292;&#19968;&#31181;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#24182;&#25552;&#20379;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#65292;&#21478;&#19968;&#31181;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#19968;&#33268;&#65292;&#36825;&#20123;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#36866;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24076;&#26395;&#20174;&#22823;&#22411;&#21644;&#39640;&#32500;&#38754;&#26495;&#25968;&#25454;&#20013;&#25552;&#21462;&#19968;&#23567;&#37096;&#20998;&#19982;&#26679;&#26412;&#30697;&#19968;&#33268;&#30340;&#20195;&#34920;&#24615;&#22330;&#26223;&#12290;&#22312;&#20004;&#31181;&#26032;&#31639;&#27861;&#20013;&#65292;&#31532;&#19968;&#31181;&#31639;&#27861;&#35782;&#21035;&#20043;&#21069;&#26410;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#34920;&#31034;&#12290;&#31532;&#20108;&#31181;&#31639;&#27861;&#20174;&#24050;&#23454;&#29616;&#30340;&#19990;&#30028;&#29366;&#24577;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#19982;&#39640;&#38454;&#26679;&#26412;&#30697;&#20449;&#24687;&#19968;&#33268;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#21487;&#29992;&#20110;&#19968;&#33268;&#30340;&#22522;&#20110;&#22330;&#26223;&#30340;&#24314;&#27169;&#21644;&#39640;&#32500;&#25968;&#20540;&#31215;&#20998;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#21644;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to extract a small number of representative scenarios from large and high-dimensional panel data that are consistent with sample moments. Among two novel algorithms, the first identifies scenarios that have not been observed before, and comes with a scenario-based representation of covariance matrices. The second proposal picks important data points from states of the world that have already realized, and are consistent with higher-order sample moment information. Both algorithms are efficient to compute, and lend themselves to consistent scenario-based modeling and high-dimensional numerical integration. Extensive numerical benchmarking studies and an application in portfolio optimization favor the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03288</link><description>&lt;p&gt;
&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03288
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#29992;&#20110;&#20122;&#32447;&#24615;&#36229;&#20307;&#31215;&#36951;&#25022;&#24230;&#37327;&#30340;&#26368;&#20248;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#65292;&#23558;&#22810;&#20010;&#30446;&#26631;&#20943;&#23569;&#20026;&#19968;&#20010;&#65292;&#20363;&#22914;&#26368;&#36817;&#22312;RLHF&#20013;&#29992;&#20110;&#35757;&#32451;&#26657;&#20934;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20154;&#23545;&#36825;&#31181;&#32463;&#20856;&#26041;&#27861;&#25345;&#21542;&#23450;&#24577;&#24230;&#65292;&#22240;&#20026;&#24050;&#30693;&#32447;&#24615;&#26631;&#37327;&#21270;&#20250;&#24573;&#30053;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20985;&#21306;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25214;&#21040;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#36890;&#36807;&#34987;&#25903;&#37197;&#30340;&#36229;&#20307;&#31215;&#26469;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#19978;&#30340;&#22810;&#26679;&#21270;&#30446;&#26631;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20855;&#26377;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#20196;&#20154;&#24778;&#35766;&#22320;&#26159;&#20026;&#20102;&#35777;&#26126;&#26368;&#23567;&#21270;&#36229;&#20307;&#31215;&#36951;&#25022;&#32780;&#26368;&#20248;&#30340;&#65292;&#23454;&#29616;&#20102; $O(T^{-1/k})$ &#30340;&#26368;&#20248;&#20122;&#32447;&#24615;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#21305;&#37197;&#30340;&#19979;&#30028;&#34920;&#26126;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#27809;&#26377;&#20219;&#20309;&#31639;&#27861;&#33021;&#20570;&#24471;&#26356;&#22909;&#12290;&#20316;&#20026;&#19968;&#20010;&#29702;&#35770;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#30446;&#26631;&#38543;&#26426;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#36229;&#32447;&#24615;&#36951;&#25022;&#30028;&#30340;&#36229;&#20307;&#31215;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01616</link><description>&lt;p&gt;
SageFormer&#65306;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#37325;&#35201;&#24615;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;SageFormer&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26377;&#25928;&#22320;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#20197;&#21450;&#20943;&#23569;&#24207;&#21015;&#20043;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#35758;&#30340;&#31995;&#21015;&#24863;&#30693;&#26694;&#26550;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SageFormer&#30456;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.11701</link><description>&lt;p&gt;
S-JEA: &#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#29992;&#20110;&#33258;&#30417;&#30563;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
S-JEA: Stacked Joint Embedding Architectures for Self-Supervised Visual Representation Learning. (arXiv:2305.11701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#30340;&#22522;&#26412;&#33539;&#24335;&#65292;&#36817;&#24180;&#26469;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#39640;&#24230;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26410;&#33021;&#23398;&#20064;&#21040;&#25429;&#33719;&#20998;&#23618;&#35821;&#20041;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#27010;&#24565;&#26159;&#21487;&#20998;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#22534;&#21472;&#32852;&#21512;&#23884;&#20837;&#32467;&#26500;&#65288;JEA&#65289;&#26469;&#23398;&#20064;&#39640;&#24230;&#21487;&#20998;&#31163;&#30340;&#20998;&#23618;&#35821;&#20041;&#34920;&#31034;&#65292;&#20854;&#20013;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20351;&#29992;&#36739;&#20302;&#32423;&#21035;JEA&#30340;&#34920;&#31034;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#12290;&#36825;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#34920;&#29616;&#20986;&#26356;&#26126;&#26174;&#30340;&#35821;&#20041;&#27010;&#24565;&#23376;&#31867;&#65288;&#22914;&#36710;&#36742;&#30340;&#22411;&#21495;&#21644;&#39068;&#33394;&#65289;&#22312;&#36739;&#39640;&#32423;&#21035;&#30340;JEA&#20013;&#12290;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22534;&#21472;JEA&#30340;&#34920;&#31034;&#19982;&#20256;&#32479;JEA&#30456;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#34920;&#31034;&#31354;&#38388;&#20197;&#39564;&#35777;&#35821;&#20041;&#20998;&#23618;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of Self-Supervised Learning (SSL) as a fundamental paradigm for learning image representations has, and continues to, demonstrate high empirical success in a variety of tasks. However, most SSL approaches fail to learn embeddings that capture hierarchical semantic concepts that are separable and interpretable. In this work, we aim to learn highly separable semantic hierarchical representations by stacking Joint Embedding Architectures (JEA) where higher-level JEAs are input with representations of lower-level JEA. This results in a representation space that exhibits distinct sub-categories of semantic concepts (e.g., model and colour of vehicles) in higher-level JEAs. We empirically show that representations from stacked JEA perform on a similar level as traditional JEA with comparative parameter counts and visualise the representation spaces to validate the semantic hierarchies.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11616</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#28145;&#24230;&#38598;&#25104;&#65306;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11616
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26174;&#33879;&#24615;&#22270;&#26469;&#20419;&#36827;&#28145;&#24230;&#38598;&#25104;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;OOD&#26816;&#27979;&#12289;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#24182;&#22312;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#22312;&#20998;&#31867;&#21644; OOD &#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#25104;&#20013;&#23398;&#20064;&#30340;&#27169;&#24335;&#30340;&#21516;&#36136;&#24615;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20419;&#36827;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#22810;&#26679;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26174;&#33879;&#24615;&#22270;&#12290;&#36890;&#36807;&#25972;&#21512;&#26174;&#33879;&#24615;&#22270;&#22810;&#26679;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#21644;OOD&#26816;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#26657;&#20934;&#24615;&#12290;&#22312;&#24050;&#24314;&#31435;&#30340;OpenOOD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10932</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#19981;&#30830;&#23450;&#32467;&#26524;&#21644;&#30830;&#20445;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#21487;&#38752;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#19982;&#39118;&#38505;&#20013;&#24615;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#21160;&#24577;&#26102;&#38388;&#19968;&#33268;&#39118;&#38505;&#24230;&#37327;&#65292;&#31216;&#20026;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#65288;ECRM&#65289;&#65292;&#24182;&#20026;&#22522;&#20110;ECRM&#30340;&#30446;&#26631;&#20989;&#25968;&#25512;&#23548;&#20986;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#12290;&#22312;&#32422;&#26463;&#30452;&#25509;&#21442;&#25968;&#21270;&#21644;&#26080;&#32422;&#26463;softmax&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13512</link><description>&lt;p&gt;
&#29992;&#20013;&#28857; Mixup &#22312;&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#21487;&#35777;&#26126;&#23398;&#20064;&#22810;&#20803;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Provably Learning Diverse Features in Multi-View Data with Midpoint Mixup. (arXiv:2210.13512v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#28857; Mixup &#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#23398;&#20064;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup &#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#20351;&#29992;&#25968;&#25454;&#28857;&#21644;&#26631;&#31614;&#30340;&#38543;&#26426;&#20984;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#12290;&#36817;&#24180;&#26469;&#65292;Mixup &#24050;&#25104;&#20026;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#26631;&#20934;&#22522;&#20803;&#65292;&#22240;&#20026;&#23427;&#22312;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#27604;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#35299;&#37322;&#19968;&#20123;&#36825;&#31181;&#25104;&#21151;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#20998;&#31867;&#38382;&#39064;&#26159;&#65292;&#27599;&#20010;&#31867;&#21035;&#21487;&#33021;&#20855;&#26377;&#22810;&#20010;&#30456;&#20851;&#29305;&#24449;&#65288;&#25110;&#35270;&#22270;&#65289;&#65292;&#21487;&#29992;&#20110;&#27491;&#30830;&#39044;&#27979;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#27599;&#31867;&#20004;&#20010;&#29305;&#24449;&#30340;&#19968;&#31867;&#38750;&#24179;&#20961;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#20351;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#35757;&#32451; 2 &#23618;&#21367;&#31215;&#32593;&#32476;&#21487;&#33021;&#20250;&#23548;&#33268;&#20960;&#20046;&#25152;&#26377;&#31867;&#21035;&#21482;&#23398;&#20064;&#19968;&#20010;&#29305;&#24449;&#65292;&#32780;&#20351;&#29992; Mixup &#30340;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#27599;&#20010;&#31867;&#21035;&#30340;&#20004;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is a data augmentation technique that relies on training using random convex combinations of data points and their labels. In recent years, Mixup has become a standard primitive used in the training of state-of-the-art image classification models due to its demonstrated benefits over empirical risk minimization with regards to generalization and robustness. In this work, we try to explain some of this success from a feature learning perspective. We focus our attention on classification problems in which each class may have multiple associated features (or views) that can be used to predict the class correctly. Our main theoretical results demonstrate that, for a non-trivial class of data distributions with two features per class, training a 2-layer convolutional network using empirical risk minimization can lead to learning only one feature for almost all classes while training with a specific instantiation of Mixup succeeds in learning both features for every class. We also show
&lt;/p&gt;</description></item></channel></rss>