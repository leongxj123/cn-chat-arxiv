<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#28145;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.12636</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#32479;&#35745;&#36317;&#31163;&#30340;&#23454;&#29992;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Practical Guide to Statistical Distances for Evaluating Generative Models in Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#25351;&#21335;&#65292;&#20171;&#32461;&#20102;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#28145;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#26159;&#38750;&#24120;&#23453;&#36149;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#39640;&#32500;&#21644;&#22797;&#26434;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#36830;&#25509;&#32452;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#29702;&#35299;&#27969;&#34892;&#30340;&#32479;&#35745;&#36317;&#31163;&#27010;&#24565;&#25552;&#20379;&#19968;&#20010;&#26131;&#20110;&#29702;&#35299;&#30340;&#20837;&#21475;&#28857;&#65292;&#21482;&#38656;&#35201;&#25968;&#23398;&#21644;&#32479;&#35745;&#23398;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20195;&#34920;&#19981;&#21516;&#26041;&#27861;&#35770;&#30340;&#22235;&#31181;&#24120;&#29992;&#32479;&#35745;&#36317;&#31163;&#27010;&#24565;&#65306;&#20351;&#29992;&#20302;&#32500;&#25237;&#24433;&#65288;Sliced-Wasserstein; SW)&#12289;&#20351;&#29992;&#20998;&#31867;&#22120;&#33719;&#21462;&#36317;&#31163;&#65288;Classifier Two-Sample Tests; C2ST)&#12289;&#36890;&#36807;&#26680;&#36827;&#34892;&#23884;&#20837;&#65288;Maximum Mean Discrepancy; MMD) &#25110;&#31070;&#32463;&#32593;&#32476;&#65288;Fr\'echet Inception Distance; FID)&#12290;&#25105;&#20204;&#24378;&#35843;&#27599;&#20010;&#36317;&#31163;&#32972;&#21518;&#30340;&#30452;&#35273;&#65292;&#24182;&#35299;&#37322;&#23427;&#20204;&#30340;&#20248;&#28857;&#12289;&#21487;&#20280;&#32553;&#24615;&#12289;&#22797;&#26434;&#24615;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12636v1 Announce Type: new  Abstract: Generative models are invaluable in many fields of science because of their ability to capture high-dimensional and complicated distributions, such as photo-realistic images, protein structures, and connectomes. How do we evaluate the samples these models generate? This work aims to provide an accessible entry point to understanding popular notions of statistical distances, requiring only foundational knowledge in mathematics and statistics. We focus on four commonly used notions of statistical distances representing different methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW), obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST), using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural networks (Fr\'echet Inception Distance; FID). We highlight the intuition behind each distance and explain their merits, scalability, complexity, and pitfalls. To demonstrate how these distanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#38024;&#23545;&#19977;&#31867;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG-RR&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#21333;&#35843;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#22343;&#21248;&#26367;&#25442;&#37319;&#26679;SEG&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.07148</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65306;&#25913;&#36827;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stochastic Extragradient with Random Reshuffling: Improved Convergence for Variational Inequalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#38024;&#23545;&#19977;&#31867;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG-RR&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#21333;&#35843;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#22343;&#21248;&#26367;&#25442;&#37319;&#26679;SEG&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22806;&#25512;&#27861;&#65288;SEG&#65289;&#26041;&#27861;&#26159;&#35299;&#20915;&#20986;&#29616;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#38480;&#27714;&#21644;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65288;VIPs&#65289;&#30340;&#26368;&#27969;&#34892;&#31639;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SEG&#25910;&#25947;&#20998;&#26512;&#19987;&#27880;&#20110;&#20854;&#24102;&#26367;&#25442;&#21464;&#20307;&#65292;&#32780;&#26041;&#27861;&#30340;&#23454;&#38469;&#23454;&#29616;&#20250;&#38543;&#26426;&#37325;&#26032;&#25490;&#21015;&#20998;&#37327;&#24182;&#25353;&#39034;&#24207;&#20351;&#29992;&#23427;&#20204;&#12290;&#19982;&#24191;&#20026;&#30740;&#31350;&#30340;&#24102;&#26367;&#25442;&#21464;&#20307;&#19981;&#21516;&#65292;&#20855;&#26377;&#38543;&#26426;&#37325;&#25490;&#30340;SEG&#65288;SEG-RR&#65289;&#32570;&#20047;&#24050;&#24314;&#31435;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#19977;&#31867;VIPs&#65288;i&#65289;&#24378;&#21333;&#35843;&#65292;&#65288;ii&#65289;&#20223;&#23556;&#21644;&#65288;iii&#65289;&#21333;&#35843;&#25552;&#20379;&#20102;SEG-RR&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;SEG-RR&#23454;&#29616;&#27604;&#22343;&#21248;&#24102;&#26367;&#25442;&#37319;&#26679;SEG&#20855;&#26377;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;&#26465;&#20214;&#12290;&#22312;&#21333;&#35843;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;SEG-RR&#20998;&#26512;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#20219;&#24847;&#31934;&#24230;&#32780;&#26080;&#38656;&#22823;&#25209;&#37327;&#22823;&#23567;&#65292;&#36825;&#26159;&#23545;&#22823;&#25209;&#37327;&#22823;&#23567;&#32780;&#35328;&#30340;&#24378;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07148v1 Announce Type: cross  Abstract: The Stochastic Extragradient (SEG) method is one of the most popular algorithms for solving finite-sum min-max optimization and variational inequality problems (VIPs) appearing in various machine learning tasks. However, existing convergence analyses of SEG focus on its with-replacement variants, while practical implementations of the method randomly reshuffle components and sequentially use them. Unlike the well-studied with-replacement variants, SEG with Random Reshuffling (SEG-RR) lacks established theoretical guarantees. In this work, we provide a convergence analysis of SEG-RR for three classes of VIPs: (i) strongly monotone, (ii) affine, and (iii) monotone. We derive conditions under which SEG-RR achieves a faster convergence rate than the uniform with-replacement sampling SEG. In the monotone setting, our analysis of SEG-RR guarantees convergence to an arbitrary accuracy without large batch sizes, a strong requirement needed in 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.07095</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#24179;&#28369;&#20811;&#26381;&#35748;&#35777;&#22521;&#35757;&#30340;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Paradox of Certified Training with Gaussian Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07095
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#35748;&#35777;&#20934;&#30830;&#24230;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#23613;&#31649;&#35748;&#35777;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#36827;&#34892;&#30028;&#35745;&#31639;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#36739;&#26494;&#30340;&#26494;&#24347;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#36825;&#26159;&#30001;&#36825;&#20123;&#26356;&#32039;&#30340;&#26494;&#24347;&#23548;&#33268;&#30340;&#25439;&#22833;&#34920;&#38754;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#25200;&#21160;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;PGPE&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#35745;&#31639;&#24179;&#28369;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20984;&#25918;&#23485;&#26469;&#30830;&#35748;&#36825;&#19968;&#28857;&#12290;&#22312;&#20351;&#29992;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#30830;&#23454;&#23548;&#33268;&#26356;&#22909;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#30456;&#21516;&#32593;&#32476;&#19978;&#32988;&#36807;&#21516;&#31867;&#25216;&#26415;&#12290;&#23613;&#31649;&#25193;&#23637;&#22522;&#20110;PGPE&#30340;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07095v1 Announce Type: new  Abstract: Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challengin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.15883</link><description>&lt;p&gt;
&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fusion Encoder Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15883
&lt;/p&gt;
&lt;p&gt;
FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;FENs&#65289;&#30340;&#31639;&#27861;&#31867;&#65306;&#29992;&#20110;&#21019;&#24314;&#23558;&#22266;&#23450;&#38271;&#24230;&#24207;&#21015;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#65288;&#20943;&#36731;&#25968;&#25454;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#26102;&#30340;&#36864;&#21270;&#65289;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65288;&#25110;&#32773;&#22312;&#20855;&#26377;&#32447;&#24615;&#22788;&#29702;&#22120;&#25968;&#37327;&#30340;&#23545;&#25968;&#26102;&#38388;&#20869;&#65289;&#12290;FENs&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#23427;&#20204;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#24120;&#28145;&#24230;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#25928;&#26524;&#33391;&#22909;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;FENs&#30340;&#24615;&#33021;&#20165;&#20165;&#26159;&#25512;&#27979;&#65292;&#22240;&#20026;&#25105;&#20204;&#23578;&#26410;&#23454;&#29616;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15020</link><description>&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#20581;&#22766;&#26463;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistically-sound beam search with masked language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15020
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#27010;&#29575;&#20581;&#22766;&#26041;&#27861;&#65292;&#34920;&#26126;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#30340;&#26463;&#25628;&#32034;&#23384;&#22312;&#25361;&#25112;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24207;&#21015;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#19981;&#20687;&#33258;&#22238;&#24402;&#27169;&#22411;&#37027;&#26679;readily available&#12290;&#28982;&#32780;&#65292;&#20272;&#31639;&#36825;&#26679;&#30340;&#20998;&#24067;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#24212;&#29992;&#65292;&#21253;&#25324;&#34507;&#30333;&#24037;&#31243;&#21644;&#21476;&#20195;&#25991;&#26412;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20351;&#29992;MLMs&#36827;&#34892;&#26463;&#25628;&#32034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#20351;&#29992;&#26631;&#20934;&#26463;&#25628;&#32034;&#23545;MLMs&#25191;&#34892;&#25991;&#26412;&#22635;&#20805;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#24403;&#36825;&#20123;&#26465;&#20214;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#27010;&#29575;&#20581;&#22766;&#24615;&#30340;&#20462;&#25913;&#65292;&#32780;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#39044;&#26399;&#26465;&#20214;&#19979;&#23427;&#20248;&#20110;&#21069;&#36848;&#30340;&#26463;&#25628;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27604;&#36739;&#22810;&#20010;&#39046;&#22495;&#20013;&#20960;&#31181;&#20351;&#29992;MLMs&#36827;&#34892;&#22635;&#20805;&#30340;&#26041;&#27861;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15020v1 Announce Type: cross  Abstract: Beam search with masked language models (MLMs) is challenging in part because joint probability distributions over sequences are not readily available, unlike for autoregressive models. Nevertheless, estimating such distributions has applications in many domains, including protein engineering and ancient text restoration. We present probabilistically-sound methods for beam search with MLMs. First, we clarify the conditions under which it is theoretically sound to perform text infilling with MLMs using standard beam search. When these conditions fail, we provide a probabilistically-sound modification with no additional computational complexity and demonstrate that it is superior to the aforementioned beam search in the expected conditions. We then present empirical results comparing several infilling approaches with MLMs across several domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13984</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#24494;Boltzmann&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#30340;&#31283;&#23450;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#65288;NNIPs&#65289;&#26159;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19981;&#31283;&#23450;&#30340;&#27169;&#25311;&#65292;&#37319;&#26679;&#38750;&#29289;&#29702;&#29366;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#23545;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#29616;&#35937;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#35757;&#32451;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#20197;&#20135;&#29983;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;NNIPs&#12290;StABlE&#35757;&#32451;&#36890;&#36807;&#36845;&#20195;&#36816;&#34892;MD&#27169;&#25311;&#20197;&#23547;&#25214;&#19981;&#31283;&#23450;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;&#21442;&#32771;&#21487;&#35266;&#23519;&#37327;&#30340;&#30417;&#30563;&#26469;&#32416;&#27491;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#35813;&#35757;&#32451;&#36807;&#31243;&#30001;Boltzmann&#20272;&#35745;&#22120;&#25903;&#25345;&#65292;&#35813;&#20272;&#35745;&#22120;&#20801;&#35768;&#23545;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#26799;&#24230;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#33021;&#26816;&#27979;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13984v1 Announce Type: new  Abstract: Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local 
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#25903;&#25345;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;&#33021;&#22815;&#33719;&#24471;&#27604;&#23436;&#20840;&#21487;&#24494;&#21270;&#39044;&#27979;&#35774;&#32622;&#39640;&#20986;4.5&#20493;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.12971</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#26102;&#38388;&#23637;&#24320;&#25903;&#25345;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
How Temporal Unrolling Supports Neural Physics Simulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12971
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#25903;&#25345;&#19979;&#65292;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#31070;&#32463;&#29289;&#29702;&#27169;&#25311;&#22120;&#33021;&#22815;&#33719;&#24471;&#27604;&#23436;&#20840;&#21487;&#24494;&#21270;&#39044;&#27979;&#35774;&#32622;&#39640;&#20986;4.5&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#19978;&#23637;&#24320;&#22521;&#35757;&#36712;&#36857;&#24378;&#28872;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#22411;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#25512;&#29702;&#31934;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#19981;&#21516;&#20110;&#31163;&#25955;GroundTruth&#36712;&#36857;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#20307;&#26469;&#20998;&#26512;&#36825;&#20123;&#24433;&#21709;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#19968;&#27493;&#35774;&#32622;&#21644;&#23436;&#20840;&#21487;&#24494;&#30340;&#23637;&#24320;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#31532;&#19977;&#31181;&#19981;&#22826;&#24120;&#29992;&#30340;&#21464;&#20307;&#65306;&#27809;&#26377;&#26102;&#38388;&#26799;&#24230;&#30340;&#23637;&#24320;&#12290;&#27604;&#36739;&#20351;&#29992;&#36825;&#19977;&#31181;&#27169;&#24335;&#35757;&#32451;&#30340;&#32593;&#32476;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20998;&#35299;&#20986;&#23637;&#24320;&#30340;&#20004;&#20010;&#20027;&#35201;&#24433;&#21709;&#65292;&#21363;&#35757;&#32451;&#20998;&#24067;&#30340;&#36716;&#21464;&#21644;&#38271;&#26399;&#26799;&#24230;&#12290;&#25105;&#20204;&#22312;&#29289;&#29702;&#31995;&#32479;&#12289;&#32593;&#32476;&#22823;&#23567;&#12289;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#35774;&#32622;&#21644;&#27979;&#35797;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#23427;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#25552;&#20379;&#20102;&#32463;&#39564;&#22522;&#30784;&#65306;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#22120;&#25903;&#25345;&#30340;&#19981;&#21487;&#24494;&#20998;&#20294;&#23637;&#24320;&#30340;&#22521;&#35757;&#35774;&#32622;&#65292;&#21487;&#20197;&#20351;&#39044;&#27979;&#35774;&#32622;&#30340;&#23436;&#20840;&#21487;&#24494;&#21270;&#24102;&#26469;4.5&#20493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12971v1 Announce Type: cross  Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does no
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#19981;&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12241</link><description>&lt;p&gt;
&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#25910;&#25947;&#24615;&#65306;&#38750;&#28176;&#36817;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#22312;&#19981;&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19979;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#22312;\emph{&#19981;}&#38656;&#35201;&#28023;&#37327;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#38750;&#28176;&#36817;&#24615;&#20998;&#26512;&#65292;(i)&#21033;&#29992;&#24207;&#21015;&#38271;&#24230;$T$&#12289;&#26679;&#26412;&#22823;&#23567;$n$&#21644;&#29615;&#22659;&#32500;&#24230;$d$&#32473;&#20986;&#20102;&#32593;&#32476;&#22823;&#23567;$m$&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;$\tau$&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;(ii)&#30830;&#23450;&#20102;&#21160;&#24577;&#31995;&#32479;&#20013;&#38271;&#26399;&#20381;&#36182;&#23545;&#25910;&#25947;&#21644;&#32593;&#32476;&#23485;&#24230;&#30028;&#38480;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#36825;&#20123;&#30028;&#38480;&#30001;&#28608;&#27963;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20915;&#23450;&#30340;&#25130;&#27490;&#28857;&#26469;&#34920;&#24449;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#19968;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#22949;&#21892;&#21021;&#22987;&#21270;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;$n$&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#32593;&#32476;&#22823;&#23567;$m$&#20165;&#23545;&#25968;&#22320;&#38543;$n$&#25193;&#23637;&#23601;&#36798;&#21040;&#26368;&#20248;&#24615;&#12290;&#36825;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#21069;&#32773;&#38656;&#35201;&#39640;&#38454;&#22810;&#39033;&#24335;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12241v1 Announce Type: new  Abstract: We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dep
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information.</title><link>https://arxiv.org/abs/2401.18057</link><description>&lt;p&gt;
&#25490;&#21517;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Rank Supervised Contrastive Learning for Time Series Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18057
&lt;/p&gt;
&lt;p&gt;
Rank Supervised Contrastive Learning (RankSCL) proposes a targeted data augmentation method and a novel rank loss to improve time series classification by utilizing fine-grained relative similarity information.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#24182;&#23637;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#33539;&#24335;&#26159;&#21033;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#26500;&#24314;&#21487;&#34892;&#30340;&#27491;&#26679;&#26412;&#65292;&#20351;&#24471;&#32534;&#30721;&#22120;&#33021;&#22815;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23558;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#26144;&#23556;&#24471;&#26356;&#36817;&#65292;&#23558;&#19981;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#26144;&#23556;&#24471;&#26356;&#36828;&#65292;&#20174;&#32780;&#20135;&#29983;&#31283;&#20581;&#32780;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#26631;&#35760;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#27491;&#26679;&#26412;&#30340;&#32454;&#31890;&#24230;&#30456;&#23545;&#30456;&#20284;&#24615;&#20449;&#24687;&#65288;&#20363;&#22914;&#25490;&#21517;&#65289;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank Supervised Contrastive Learning&#65288;RankSCL&#65289;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#19982;&#20256;&#32479;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19981;&#21516;&#65292;RankSCL&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20197;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#37319;&#29992;&#29305;&#23450;&#30340;&#36807;&#28388;&#35268;&#21017;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#23545;&#12290;&#27492;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#21517;&#25439;&#22833;&#20989;&#25968;&#65292;&#26469;&#20026;&#19981;&#21516;&#30340;&#26679;&#26412;&#36171;&#20104;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various contrastive learning techniques have been developed to categorize time series data and exhibit promising performance. A general paradigm is to utilize appropriate augmentations and construct feasible positive samples such that the encoder can yield robust and discriminative representations by mapping similar data points closer together in the feature space while pushing dissimilar data points farther apart. Despite its efficacy, the fine-grained relative similarity (e.g., rank) information of positive samples is largely ignored, especially when labeled samples are limited. To this end, we present Rank Supervised Contrastive Learning (RankSCL) to perform time series classification. Different from conventional contrastive learning frameworks, RankSCL augments raw data in a targeted way in the embedding space and adopts certain filtering rules to select more informative positive and negative pairs of samples. Moreover, a novel rank loss is developed to assign different w
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.14922</link><description>&lt;p&gt;
&#20174;&#39640;&#38454;&#32479;&#35745;&#37327;&#20013;&#39640;&#25928;&#23398;&#20064;&#65306;&#20551;&#35774;&#26816;&#39564;&#12289;&#38543;&#26426;&#29305;&#24449;&#21644;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14922
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21457;&#29616;&#32479;&#35745;&#27169;&#24335;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25797;&#38271;&#21457;&#29616;&#39640;&#32500;&#25968;&#25454;&#38598;&#20013;&#30340;&#32479;&#35745;&#27169;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24230;&#37327;&#19977;&#20010;&#25110;&#26356;&#22810;&#21464;&#37327;&#38388;&#30340;&#38750;&#39640;&#26031;&#30456;&#20851;&#24615;&#30340;&#39640;&#38454;&#32047;&#31215;&#37327;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#29305;&#21035;&#37325;&#35201;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#26377;&#22810;&#26377;&#25928;&#22320;&#20174;&#39640;&#38454;&#32047;&#31215;&#37327;&#20013;&#25552;&#21462;&#29305;&#24449;&#65311;&#25105;&#20204;&#22312;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#20013;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#37324;&#32479;&#35745;&#23398;&#23478;&#38656;&#35201;&#20174;$d$&#32500;&#36755;&#20837;&#30340;&#38454;-$p\ge 4$&#32047;&#31215;&#37327;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#29305;&#26435;&#26041;&#21521;&#25110;&#8220;&#23574;&#23792;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#25152;&#38656;&#26679;&#26412;&#25968;$n$&#26469;&#34920;&#24449;&#24674;&#22797;&#23574;&#23792;&#30340;&#22522;&#26412;&#32479;&#35745;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#20197;&#24378;&#28872;&#21306;&#20998;&#26469;&#33258;&#23574;&#23792;&#32047;&#31215;&#37327;&#27169;&#22411;&#21644;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#36755;&#20837;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32479;&#35745;&#19978;&#30340;&#21487;&#21306;&#20998;&#24615;&#38656;&#35201;$n\gtrsim d$&#20010;&#26679;&#26412;&#65292;&#32780;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#21306;&#20998;&#36825;&#20004;&#20010;&#20998;&#24067;&#21017;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14922v2 Announce Type: replace-cross  Abstract: Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. We first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. We find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time require
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#26469;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2205.14839</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adversarial Bandits against Arbitrary Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.14839
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#26469;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;&#20219;&#24847;&#31574;&#30053;&#30340;&#23545;&#25239;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;S&#26159;&#38382;&#39064;&#38590;&#24230;&#30340;&#21442;&#25968;&#65292;&#35813;&#21442;&#25968;&#23545;&#20110;&#20195;&#29702;&#20154;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20351;&#29992;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#65288;OMD&#65289;&#30340;&#20027;&#25511;&#22522;&#30784;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#31616;&#21333;OMD&#30340;&#20027;&#25511;&#22522;&#30784;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;$\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$T^{2/3}$&#26469;&#33258;&#25439;&#22833;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#20026;&#20102;&#20943;&#36731;&#26041;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;OMD&#65292;&#24182;&#23454;&#29616;&#20102;$\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;$\rho_T(h^\dagger)$&#26159;&#25439;&#22833;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the adversarial bandit problem against arbitrary strategies, in which $S$ is the parameter for the hardness of the problem and this parameter is not given to the agent. To handle this problem, we adopt the master-base framework using the online mirror descent method (OMD). We first provide a master-base algorithm with simple OMD, achieving $\tilde{O}(S^{1/2}K^{1/3}T^{2/3})$, in which $T^{2/3}$ comes from the variance of loss estimators. To mitigate the impact of the variance, we propose using adaptive learning rates for OMD and achieve $\tilde{O}(\min\{\mathbb{E}[\sqrt{SKT\rho_T(h^\dagger)}],S\sqrt{KT}\})$, where $\rho_T(h^\dagger)$ is a variance term for loss estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#21033;&#29992;&#24494;&#20998;&#19981;&#21464;&#37327;&#21644;&#23884;&#20837;&#31561;&#21464;&#27969;&#24418;&#20013;&#30340;&#22686;&#24191;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2401.14131</link><description>&lt;p&gt;
&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#19982;&#24494;&#20998;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Equivariant Manifold Neural ODEs and Differential Invariants. (arXiv:2401.14131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#21033;&#29992;&#24494;&#20998;&#19981;&#21464;&#37327;&#21644;&#23884;&#20837;&#31561;&#21464;&#27969;&#24418;&#20013;&#30340;&#22686;&#24191;&#24418;&#24335;&#36827;&#34892;&#24314;&#27169;&#21644;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#31561;&#21464;&#27969;&#24418;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODE&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26126;&#26174;&#20960;&#20309;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20998;&#26512;&#23427;&#20204;&#22312;&#23545;&#31216;&#25968;&#25454;&#24314;&#27169;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;Lie&#32676;G&#22312;&#20809;&#28369;&#27969;&#24418;M&#19978;&#30340;&#20316;&#29992;&#65292;&#24182;&#24314;&#31435;&#20102;&#21521;&#37327;&#22330;&#30340;&#31561;&#21464;&#24615;&#12289;&#30456;&#24212;&#26607;&#35199;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#20197;&#21450;&#20851;&#32852;&#30340;NODE&#30340;&#31561;&#21464;&#24615;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;G&#22312;M&#19978;&#20316;&#29992;&#30340;&#24494;&#20998;&#19981;&#21464;&#37327;&#30340;&#31561;&#21464;NODE&#26032;&#24418;&#24335;&#65292;&#22522;&#20110;Lie&#29702;&#35770;&#26469;&#25551;&#36848;&#24494;&#20998;&#26041;&#31243;&#23545;&#31216;&#24615;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#26082;&#23545;M&#20063;&#23545;G&#37117;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23884;&#20837;&#31561;&#21464;&#27969;&#20013;&#26500;&#36896;&#20102;&#22686;&#24191;&#27969;&#24418;NODE&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#26159;&#20219;&#20309;&#36335;&#24452;&#36830;&#36890;M&#19978;&#31561;&#21464;&#24494;&#20998;&#21516;&#32986;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we develop a manifestly geometric framework for equivariant manifold neural ordinary differential equations (NODEs), and use it to analyse their modelling capabilities for symmetric data. First, we consider the action of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence between equivariance of vector fields, symmetries of the corresponding Cauchy problems, and equivariance of the associated NODEs. We also propose a novel formulation of the equivariant NODEs in terms of the differential invariants of the action of $G$ on $M$, based on Lie theory for symmetries of differential equations, which provides an efficient parameterisation of the space of equivariant vector fields in a way that is agnostic to both the manifold $M$ and the symmetry group $G$. Second, we construct augmented manifold NODEs, through embeddings into equivariant flows, and show that they are universal approximators of equivariant diffeomorphisms on any path-connected $M$. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.05426</link><description>&lt;p&gt;
CoSS&#65306;&#38024;&#23545;&#25968;&#25454;&#39640;&#25928;AI&#30340;&#20256;&#24863;&#22120;&#21644;&#37319;&#26679;&#29575;&#20248;&#21270;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#20351;&#29992;&#22823;&#37327;&#20256;&#24863;&#22120;&#21644;&#39640;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#20302;&#25928;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#24517;&#35201;&#25193;&#23637;&#65292;&#32473;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;HAR&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20998;&#25968;&#8221;&#65292;&#23427;&#20204;&#35780;&#20272;&#35757;&#32451;&#38454;&#27573;&#20013;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20998;&#25968;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;&#20462;&#21098;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#22312;&#35745;&#31639;&#39044;&#31639;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#26681;&#25454;&#36873;&#25321;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#27169;&#22411;&#65292;&#29992;&#20110;BGK&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#30340;&#31934;&#30830;&#38381;&#21512;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.04783</link><description>&lt;p&gt;
BGK&#26041;&#31243;&#30340;&#21452;&#26354;&#32447;&#26426;&#22120;&#23398;&#20064;&#30697;&#38381;&#21253;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Machine Learning Moment Closures for the BGK Equations. (arXiv:2401.04783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#27169;&#22411;&#65292;&#29992;&#20110;BGK&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#30340;&#31934;&#30830;&#38381;&#21512;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;BGK&#21160;&#21147;&#27169;&#22411;&#30340;&#30697;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#23545;Bhatnagar-Gross-Krook&#65288;BGK&#65289;&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#12290;&#36825;&#20010;&#38381;&#21253;&#26159;&#22522;&#20110;&#25105;&#20204;&#22312;&#36755;&#36816;&#23553;&#38381;&#20013;&#23548;&#20986;&#30340;&#33258;&#30001;&#27969;&#26497;&#38480;&#30340;&#31934;&#30830;&#23553;&#38381;&#20851;&#31995;&#32780;&#25552;&#20986;&#30340;&#12290;&#36825;&#20010;&#31934;&#30830;&#23553;&#38381;&#20851;&#31995;&#23558;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#19982;&#22235;&#20010;&#36739;&#20302;&#30697;&#30340;&#26799;&#24230;&#30456;&#20851;&#32852;&#12290;&#19982;&#25105;&#20204;&#36807;&#21435;&#30340;&#24037;&#20316;&#19968;&#26679;&#65292;&#36825;&#37324;&#20171;&#32461;&#30340;&#27169;&#22411;&#36890;&#36807;&#36739;&#20302;&#30697;&#30340;&#26799;&#24230;&#31995;&#25968;&#26469;&#23398;&#20064;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#12290;&#36825;&#24847;&#21619;&#30528;&#24471;&#21040;&#30340;&#21452;&#26354;&#31995;&#32479;&#22312;&#26368;&#39640;&#30697;&#19978;&#24182;&#38750;&#23432;&#24658;&#12290;&#20026;&#20102;&#31283;&#23450;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23618;&#34987;&#35774;&#35745;&#25104;&#24378;&#21046;&#21452;&#26354;&#24615;&#21644;Galileo&#19981;&#21464;&#24615;&#12290;&#36825;&#30830;&#20445;&#27169;&#22411;&#33021;&#22815;&#22312;NN&#30340;&#35757;&#32451;&#31383;&#21475;&#20043;&#22806;&#36816;&#34892;&#12290;&#19982;&#25105;&#20204;&#20197;&#21069;&#22788;&#29702;&#32447;&#24615;&#27169;&#22411;&#30340;&#36752;&#23556;&#36755;&#36816;&#24037;&#20316;&#19981;&#21516;&#65292;BGK&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24615;&#35201;&#27714;&#26356;&#39640;&#32423;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a hyperbolic closure for the Grad moment expansion of the Bhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained on BGK's moment data. This closure is motivated by the exact closure for the free streaming limit that we derived in our paper on closures in transport \cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest moment to the gradient of four lower moments. As with our past work, the model presented here learns the gradient of the highest moment in terms of the coefficients of gradients for all lower ones. By necessity, this means that the resulting hyperbolic system is not conservative in the highest moment. For stability, the output layers of the NN are designed to enforce hyperbolicity and Galilean invariance. This ensures the model can be run outside of the training window of the NN. Unlike our previous work on radiation transport that dealt with linear models, the BGK model's nonlinearity demanded advanced training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#35757;&#32451;&#30340;&#26041;&#24335;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#35757;&#32451;&#20195;&#20215;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02879</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#39640;&#25928;&#21442;&#25968;&#20248;&#21270;&#65306;&#19968;&#31181;&#22312;&#21487;&#21464;&#35757;&#32451;&#20013;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training. (arXiv:2401.02879v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#35757;&#32451;&#30340;&#26041;&#24335;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20197;&#35299;&#20915;&#37327;&#23376;&#26680;&#23545;&#40784;&#30340;&#35757;&#32451;&#20195;&#20215;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#26680;&#20989;&#25968;&#30340;&#37327;&#23376;&#26680;&#23545;&#40784;&#25216;&#26415;&#65292;&#21487;&#20197;&#23545;&#26680;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#19982;&#29305;&#23450;&#25968;&#25454;&#38598;&#23545;&#40784;&#12290;&#23613;&#31649;&#37327;&#23376;&#26680;&#23545;&#40784;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#65292;&#20294;&#30001;&#20110;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#37117;&#24517;&#39035;&#26500;&#24314;&#23436;&#25972;&#30340;&#26680;&#30697;&#38453;&#65292;&#22240;&#27492;&#19968;&#30452;&#21463;&#21040;&#26174;&#33879;&#30340;&#35757;&#32451;&#25104;&#26412;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26088;&#22312;&#24179;&#34913;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23376;&#37319;&#26679;&#35757;&#32451;&#26041;&#27861;&#65292;&#27599;&#27425;&#35757;&#32451;&#27493;&#39588;&#20351;&#29992;&#26680;&#30697;&#38453;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35757;&#32451;&#30340;&#24635;&#20307;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#23376;&#37319;&#26679;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#22312;&#32500;&#25345;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#25152;&#38656;&#35757;&#32451;&#37327;&#23376;&#26680;&#30340;&#30005;&#36335;&#25968;&#37327;&#22823;&#22823;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning with quantum kernels for classification problems is a growing area of research. Recently, quantum kernel alignment techniques that parameterise the kernel have been developed, allowing the kernel to be trained and therefore aligned with a specific dataset. While quantum kernel alignment is a promising technique, it has been hampered by considerable training costs because the full kernel matrix must be constructed at every training iteration. Addressing this challenge, we introduce a novel method that seeks to balance efficiency and performance. We present a sub-sampling training approach that uses a subset of the kernel matrix at each training step, thereby reducing the overall computational cost of the training. In this work, we apply the sub-sampling method to synthetic datasets and a real-world breast cancer dataset and demonstrate considerable reductions in the number of circuits required to train the quantum kernel while maintaining classification accuracy
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01514</link><description>&lt;p&gt;
SelfFed: &#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#22312;&#34892;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#26410;&#26631;&#35760;&#20294;&#23396;&#31435;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;&#26631;&#31614;&#31232;&#32570;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65289;&#26041;&#38754;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#30340;SelfFed&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;SelfFed&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26159;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;Swin Transformer&#30340;&#32534;&#30721;&#22120;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#22686;&#24378;&#24314;&#27169;&#12290;SelfFed&#26694;&#26550;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26377;&#21161;&#20110;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#38454;&#27573;&#26159;&#24494;&#35843;&#33539;&#24335;&#65292;&#24341;&#20837;&#23545;&#27604;&#32593;&#32476;&#21644;&#19968;&#31181;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#22411;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20998;&#25955;&#35757;&#32451;&#12290;&#36825;&#20010;&#24494;&#35843;&#38454;&#27573;&#20811;&#26381;&#20102;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.02766</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#32593;&#32476;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Networked Communication for Decentralised Agents in Mean-Field Games. (arXiv:2306.02766v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#24341;&#20837;&#32593;&#32476;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#26696;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#38469;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#32593;&#32476;&#36890;&#20449;&#24341;&#20837;&#22343;&#22330;&#21338;&#24328;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;oracle&#30340;&#24773;&#20917;&#19979;&#65292;N&#20010;&#20998;&#24067;&#24335;&#26234;&#33021;&#20307;&#27839;&#30528;&#32463;&#36807;&#30340;&#32463;&#39564;&#31995;&#32479;&#30340;&#21333;&#19968;&#38750;&#21608;&#26399;&#28436;&#21270;&#36335;&#24452;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#22312;&#21482;&#26377;&#19968;&#20123;&#20851;&#20110;&#32593;&#32476;&#32467;&#26500;&#30340;&#21512;&#29702;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26679;&#26412;&#20445;&#35777;&#65292;&#22312;&#38598;&#20013;&#23398;&#20064;&#21644;&#29420;&#31435;&#23398;&#20064;&#24773;&#20917;&#20043;&#38388;&#26377;&#30028;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#30340;&#26679;&#26412;&#20445;&#35777;&#23454;&#38469;&#19978;&#24182;&#19981;&#20250;&#23548;&#33268;&#23454;&#38469;&#25910;&#25947;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#24403;&#29702;&#35770;&#21442;&#25968;&#26410;&#34987;&#35266;&#23519;&#21040;&#65288;&#23548;&#33268;Q&#20989;&#25968;&#30340;&#20272;&#35745;&#19981;&#20934;&#30830;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#36890;&#20449;&#26041;&#26696;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21487;&#21462;&#30340;&#38598;&#20013;&#24335;&#25511;&#21046;&#22120;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#29702;&#35770;&#31639;&#27861;&#36827;&#34892;&#20102;&#20960;&#31181;&#23454;&#38469;&#30340;&#25913;&#36827;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#23427;&#20204;&#30340;&#31532;&#19968;&#20010;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic evolution path of the empirical system. We prove that our architecture, with only a few reasonable assumptions about network structure, has sample guarantees bounded between those of the centralised- and independent-learning cases. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. Accordingly, we show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case, without relying on the undesirable assumption of a centralised controller. We contribute several further practical enhancements to all three theoretical algorithms, allowing us to showcase their first empirical demonstrations. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01460</link><description>&lt;p&gt;
ReLU&#25327;&#25937;&#65306;&#29992;&#27491;&#25968;&#20248;&#21183;&#25913;&#36827;&#24744;&#30340;On-Policy Actor-Critic&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReLU to the Rescue: Improve Your On-Policy Actor-Critic with Positive Advantages. (arXiv:2306.01460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22312;&#20445;&#23432;&#20540;&#20272;&#35745;&#21644;&#35880;&#24910;&#25506;&#32034;&#26041;&#38754;&#30340;&#26126;&#30830;&#25972;&#21512;&#26469;&#35299;&#20915;&#20102;&#24403;&#21069;&#31639;&#27861;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;On-Policy&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#26126;&#30830;&#22320;&#25972;&#21512;&#35880;&#24910;&#30340;&#29615;&#22659;&#20132;&#20114;&#26469;&#35299;&#20915;&#24403;&#21069;On-Policy&#31639;&#27861;&#65288;&#22914;Proximal Policy Optimization&#21644;Asynchronous Advantage Actor-Critic&#65289;&#19981;&#33021;&#20805;&#20998;&#32771;&#34385;&#35880;&#24910;&#20132;&#20114;&#30340;&#38382;&#39064;&#65306;&#36890;&#36807;&#26368;&#22823;&#21270;&#30495;&#23454;&#20215;&#20540;&#20989;&#25968;&#21152;&#19978;&#24120;&#37327;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#20419;&#36827;&#8220;&#20445;&#23432;&#20540;&#20272;&#35745;&#8221;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;Thompson&#37319;&#26679;&#26469;&#36827;&#34892;&#35880;&#24910;&#25506;&#32034;&#12290;&#36825;&#20123;&#29305;&#28857;&#36890;&#36807;&#23545;A3C&#31639;&#27861;&#36827;&#34892;&#19977;&#20010;&#24778;&#20154;&#31616;&#21333;&#30340;&#20462;&#25913;&#23454;&#29616;&#65306;&#36890;&#36807;ReLU&#20989;&#25968;&#22788;&#29702;&#20248;&#21183;&#20272;&#35745;&#65292;&#36827;&#34892;&#35889;&#24402;&#19968;&#21270;&#21644;&#38543;&#26426;&#22833;&#27963;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26368;&#22823;&#21270;&#20102;&#19979;&#30028;&#65292;&#36825;&#20063;&#26159;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;Regret Matching Policy Gradients&#65288;RMPG&#65289;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel method for enhancing the effectiveness of on-policy Deep Reinforcement Learning (DRL) algorithms. Current on-policy algorithms, such as Proximal Policy Optimization (PPO) and Asynchronous Advantage Actor-Critic (A3C), do not sufficiently account for cautious interaction with the environment. Our method addresses this gap by explicitly integrating cautious interaction in two critical ways: by maximizing a lower-bound on the true value function plus a constant, thereby promoting a \textit{conservative value estimation}, and by incorporating Thompson sampling for cautious exploration. These features are realized through three surprisingly simple modifications to the A3C algorithm: processing advantage estimates through a ReLU function, spectral normalization, and dropout. We provide theoretical proof that our algorithm maximizes the lower bound, which also grounds Regret Matching Policy Gradients (RMPG), a discrete-action on-policy method for multi-agen
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.17026</link><description>&lt;p&gt;
&#35770;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Computational Power of Decoder-Only Transformer Language Models. (arXiv:2305.17026v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#65292;&#34920;&#26126;&#21363;&#20351;&#21482;&#26377;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#65292;&#20173;&#28982;&#20855;&#26377;&#22270;&#28789;&#23436;&#22791;&#24615;&#65292;&#20854;&#20013;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#23545;&#35299;&#30721;&#22120;Transformer&#27169;&#22411;&#30340;&#35745;&#31639;&#26222;&#36866;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Transformer&#27169;&#22411;&#30340;&#29702;&#35770;&#25991;&#29486;&#65292;&#24182;&#34920;&#26126;&#20165;&#20351;&#29992;&#21333;&#23618;&#21644;&#21333;&#27880;&#24847;&#21147;&#22836;&#30340;&#35299;&#30721;&#22120;Transformer&#32467;&#26500;&#65292;&#22312;&#21512;&#29702;&#20551;&#35774;&#19979;&#20855;&#22791;&#22270;&#28789;&#23436;&#22791;&#24615;&#12290;&#20174;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35789;&#23884;&#20837;&#30340;&#31232;&#30095;&#24615;/&#21487;&#21387;&#32553;&#24615;&#26159;&#22270;&#28789;&#23436;&#22791;&#24615;&#25104;&#31435;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a theoretical evaluation of the computational universality of decoder-only transformer models. We extend the theoretical literature on transformer models and show that decoder-only transformer architectures (even with only a single layer and single attention head) are Turing complete under reasonable assumptions. From the theoretical analysis, we show sparsity/compressibility of the word embedding to be a necessary condition for Turing completeness to hold.
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04613</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Descriptive Complexity of Graph Neural Networks. (arXiv:2303.04613v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04613
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#37027;&#20123;&#29992;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#23450;&#20041;&#30340;&#12290;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GNN&#23478;&#26063;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#23454;&#25968;&#26435;&#20540;&#21644;&#21253;&#25324;&#26631;&#20934;ReLU&#12289;Logistic&#8220;sigmod&#8221;&#21644;&#21452;&#26354;&#27491;&#20999;&#20989;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#31867;&#12290;&#22914;&#26524;GNN&#34987;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#20840;&#23616;&#35835;&#21462;&#65288;&#36825;&#20123;&#37117;&#26159;GNN&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21151;&#33021;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#35745;&#31639;&#19982;&#38408;&#38376;&#30340;&#26377;&#30028;&#28145;&#24230;&#24067;&#23572;&#30005;&#36335;&#23436;&#20840;&#30456;&#21516;&#30340;&#26597;&#35810;&#65292;&#21363;&#22312;TC^0&#20013;&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#21644;&#26377;&#29702;&#26435;&#37325;&#30340;&#21333;&#20010;GNN&#21487;&#20197;&#22312;&#19981;&#24314;&#36896;&#20869;&#37096;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#30001;GFO+C&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse the power of graph neural networks (GNNs) in terms of Boolean circuit complexity and descriptive complexity.  We prove that the graph queries that can be computed by a polynomial-size bounded-depth family of GNNs are exactly those definable in the guarded fragment GFO+C of first-order logic with counting and with built-in relations. This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN families may use arbitrary real weights and a wide class of activation functions that includes the standard ReLU, logistic "sigmod", and hyperbolic tangent functions. If the GNNs are allowed to use random initialisation and global readout (both standard features of GNNs widely used in practice), they can compute exactly the same queries as bounded depth Boolean circuits with threshold gates, that is, exactly the queries in TC^0.  Moreover, we show that queries computable by a single GNN with piecewise linear activations and rational weights are definable in GFO+C without bui
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;&#65292;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2302.00860</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interventional and Counterfactual Inference with Diffusion Models. (arXiv:2302.00860v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#23427;&#21487;&#20197;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#26029;&#65292;&#20854;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#26512;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#21644;&#22240;&#26524;&#22270;&#21487;&#29992;&#30340;&#22240;&#26524;&#20805;&#20998;&#35774;&#32622;&#20013;&#22238;&#31572;&#35266;&#27979;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#30340;&#38382;&#39064;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#22240;&#26524;&#27169;&#22411; (DCM)&#65292;&#26469;&#23398;&#20064;&#29983;&#25104;&#29420;&#29305;&#30340;&#28508;&#22312;&#32534;&#30721;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#36825;&#20123;&#32534;&#30721;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24178;&#39044;&#19979;&#30452;&#25509;&#37319;&#26679;&#21644;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#26029;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#37324;&#26159;&#19968;&#20010;&#33258;&#28982;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#33410;&#28857;&#32534;&#30721;&#20026;&#19968;&#20010;&#20195;&#34920;&#22806;&#29983;&#22122;&#22768;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22238;&#31572;&#22240;&#26524;&#26597;&#35810;&#26041;&#38754;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#20026;&#20998;&#26512;&#19968;&#33324;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#25552;&#20379;&#19968;&#31181;&#26041;&#27861;&#65292;&#36825;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#22806;&#30340;&#35774;&#32622;&#21487;&#33021;&#20063;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings. These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#29992;&#20110;&#21452;&#23618;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming. (arXiv:2211.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#35268;&#21010;&#36817;&#26399;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#21333;&#26426;&#25110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#24182;&#23384;&#22312;&#36890;&#20449;&#25104;&#26412;&#39640;&#21644;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#20989;&#25968;&#30340;&#20998;&#25955;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel programming has recently received attention in the literature, due to a wide range of applications, including reinforcement learning and hyper-parameter optimization. However, it is widely assumed that the underlying bilevel optimization problem is solved either by a single machine or in the case of multiple machines connected in a star-shaped network, i.e., federated learning setting. The latter approach suffers from a high communication cost on the central node (e.g., parameter server) and exhibits privacy vulnerabilities. Hence, it is of interest to develop methods that solve bilevel optimization problems in a communication-efficient decentralized manner. To that end, this paper introduces a penalty function based decentralized algorithm with theoretical guarantees for this class of optimization problems. Specifically, a distributed alternating gradient-type algorithm for solving consensus bilevel programming over a decentralized network is developed. A key feature of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;</title><link>http://arxiv.org/abs/1703.01347</link><description>&lt;p&gt;
&#24102;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#65306;&#26397;&#21521;&#36125;&#21494;&#26031;&#31070;&#35861;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Contextual Linear Bandits under Noisy Features: Towards Bayesian Oracles. (arXiv:1703.01347v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1703.01347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22122;&#22768;&#29305;&#24449;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#31070;&#35861;&#24182;&#24471;&#21040;&#20102;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24102;&#26377;&#22122;&#22768;&#21644;&#32570;&#22833;&#39033;&#30340;&#19978;&#19979;&#25991;&#32447;&#24615;Bandit&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#32473;&#20986;&#30340;&#36125;&#21494;&#26031;&#31070;&#35861;&#12290;&#25105;&#20204;&#30340;&#36125;&#21494;&#26031;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#20248;&#20551;&#35774;&#21487;&#33021;&#20250;&#36828;&#31163;&#28508;&#22312;&#30340;&#21487;&#23454;&#29616;&#20989;&#25968;&#65292;&#36825;&#21462;&#20915;&#20110;&#22122;&#22768;&#29305;&#24449;&#65292;&#36825;&#26159;&#39640;&#24230;&#38750;&#30452;&#35266;&#30340;&#65292;&#24182;&#19988;&#22312;&#32463;&#20856;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#19981;&#20250;&#21457;&#29983;&#12290;&#36825;&#24847;&#21619;&#30528;&#32463;&#20856;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#38750;&#24179;&#20961;&#30340;&#36951;&#25022;&#30028;&#65288;regret bound&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#36825;&#20010;&#27169;&#22411;&#19979;&#30340;&#35266;&#23519;&#20449;&#24687;&#20013;&#23454;&#29616;&#36125;&#21494;&#26031;&#31070;&#35861;&#65292;&#24403;&#26377;&#22823;&#37327;&#25163;&#33218;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;$\tilde{O}(d\sqrt{T})$&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual linear bandit problems under feature uncertainty; they are noisy with missing entries. To address the challenges of the noise, we analyze Bayesian oracles given observed noisy features. Our Bayesian analysis finds that the optimal hypothesis can be far from the underlying realizability function, depending on the noise characteristics, which are highly non-intuitive and do not occur for classical noiseless setups. This implies that classical approaches cannot guarantee a non-trivial regret bound. Therefore, we propose an algorithm that aims at the Bayesian oracle from observed information under this model, achieving $\tilde{O}(d\sqrt{T})$ regret bound when there is a large number of arms. We demonstrate the proposed algorithm using synthetic and real-world datasets.
&lt;/p&gt;</description></item></channel></rss>