<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.16907</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#30340;&#25193;&#25955;&#21518;&#39564;&#36817;&#20284;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Diffusion Posterior Proximal Sampling for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#20174;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#23454;&#29616;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#21151;&#25928;&#12290;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#31639;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#21033;&#29992;&#25968;&#25454;&#20808;&#39564;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#32487;&#25215;&#33258;&#26080;&#26465;&#20214;&#29983;&#25104;&#33539;&#24335;&#30340;&#20803;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#33539;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36873;&#25321;&#22312;&#27599;&#20010;&#29983;&#25104;&#27493;&#39588;&#20013;&#19982;&#27979;&#37327;&#26631;&#35782;&#19968;&#33268;&#30340;&#26679;&#26412;&#65292;&#21033;&#29992;&#37319;&#26679;&#36873;&#25321;&#20316;&#20026;&#36755;&#20986;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#30340;&#36884;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#19982;&#27979;&#37327;&#20449;&#21495;&#30456;&#32467;&#21512;&#30340;&#21021;&#22987;&#21270;&#24320;&#22987;&#24674;&#22797;&#36807;&#31243;&#65292;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16907v1 Announce Type: cross  Abstract: Diffusion models have demonstrated remarkable efficacy in generating high-quality samples. Existing diffusion-based image restoration algorithms exploit pre-trained diffusion models to leverage data priors, yet they still preserve elements inherited from the unconditional generation paradigm. These strategies initiate the denoising process with pure white noise and incorporate random noise at each generative step, leading to over-smoothed results. In this paper, we introduce a refined paradigm for diffusion-based image restoration. Specifically, we opt for a sample consistent with the measurement identity at each generative step, exploiting the sampling selection as an avenue for output stability and enhancement. Besides, we start the restoration process with an initialization combined with the measurement signal, providing supplementary information to better align the generative process. Extensive experimental results and analyses val
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12022</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Text-Attributed Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#26159;&#36830;&#25509;&#30340;&#25991;&#26412;&#25991;&#26723;&#22270;&#12290;&#22270;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;TAGs&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36825;&#20123;&#26631;&#31614;&#24456;&#23569;&#25110;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;TAG&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#20256;&#25480;&#32473;TAG&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#65292;&#20174;&#32780;&#21327;&#21516;LLMs&#21644;&#22270;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09146</link><description>&lt;p&gt;
ResQuNNs: &#23454;&#29616;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;ResQuNNs&#65292;&#22312;quanvolutional&#23618;&#20013;&#24341;&#20837;&#21487;&#35757;&#32451;&#24615;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#35299;&#20915;&#20102;&#36328;&#23618;&#26799;&#24230;&#35775;&#38382;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QuNNs&#65289;&#24615;&#33021;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#24182;&#35299;&#20915;&#19982;&#20854;&#30456;&#20851;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;quanvolutional&#23618;&#34429;&#28982;&#26377;&#21161;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20294;&#24448;&#24448;&#26159;&#38745;&#24577;&#30340;&#65292;&#36866;&#24212;&#24615;&#26377;&#38480;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#22312;&#36825;&#20123;&#23618;&#20869;&#37096;&#36827;&#34892;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;QuNNs&#30340;&#28789;&#27963;&#24615;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22810;&#20010;&#21487;&#35757;&#32451;&#30340;quanvolutional&#23618;&#30340;&#24341;&#20837;&#32473;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#24102;&#26469;&#20102;&#22797;&#26434;&#24615;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#38590;&#20197;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#35775;&#38382;&#26799;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Residual Quanvolutional Neural Networks (ResQuNNs)&#65292;&#21033;&#29992;&#27531;&#24046;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#22312;&#36825;&#20123;&#23618;&#20043;&#38388;&#28155;&#21152;&#36339;&#36807;&#36830;&#25509;&#20197;&#20419;&#36827;&#26799;&#24230;&#30340;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09146v1 Announce Type: new Abstract: In this paper, we present a novel framework for enhancing the performance of Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional layers and addressing the critical challenges associated with them. Traditional quanvolutional layers, although beneficial for feature extraction, have largely been static, offering limited adaptability. Unlike state-of-the-art, our research overcomes this limitation by enabling training within these layers, significantly increasing the flexibility and potential of QuNNs. However, the introduction of multiple trainable quanvolutional layers induces complexities in gradient-based optimization, primarily due to the difficulty in accessing gradients across these layers. To resolve this, we propose a novel architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging the concept of residual learning, which facilitates the flow of gradients by adding skip connections between 
&lt;/p&gt;</description></item><item><title>&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.08871</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Challenges and Opportunities in Topological Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08871
&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20316;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#34917;&#20805;&#65292;&#32473;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#25552;&#20379;&#20102;&#33258;&#28982;&#36873;&#25321;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#26469;&#29702;&#35299;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#36890;&#36807;&#34701;&#20837;&#25299;&#25169;&#27010;&#24565;&#65292;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#34917;&#20805;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#25104;&#20026;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23454;&#29992;&#30410;&#22788;&#21040;&#29702;&#35770;&#22522;&#30784;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#65292;&#23427;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#20063;&#26159;&#23545;&#31185;&#23398;&#30028;&#30340;&#36992;&#35831;&#65292;&#24076;&#26395;&#31215;&#26497;&#21442;&#19982;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24320;&#21457;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08871v1 Announce Type: new Abstract: Topological deep learning (TDL) is a rapidly evolving field that uses topological features to understand and design deep learning models. This paper posits that TDL may complement graph representation learning and geometric deep learning by incorporating topological concepts, and can thus provide a natural choice for various machine learning settings. To this end, this paper discusses open problems in TDL, ranging from practical benefits to theoretical foundations. For each problem, it outlines potential solutions and future research opportunities. At the same time, this paper serves as an invitation to the scientific community to actively participate in TDL research to unlock the potential of this emerging field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02338</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Adaptation for Networking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36866;&#24212;&#32593;&#32476;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#30340;&#30446;&#26631;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#35768;&#22810;&#32593;&#32476;&#20219;&#21153;&#37117;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#39044;&#27979;&#21644;&#31995;&#32479;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;DL&#30340;&#31639;&#27861;&#30340;&#35774;&#35745;&#21746;&#23398;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#24037;&#31243;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#32593;&#32476;&#20219;&#21153;&#25163;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#27492;&#22806;&#65292;DNN&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#20998;&#24067;/&#29615;&#22659;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#25104;&#21151;&#30340;&#25512;&#21160;&#19979;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLM&#29992;&#20110;&#32593;&#32476;&#30340;&#36866;&#24212;&#24615;&#65292;&#20197;&#25506;&#32034;&#26356;&#21487;&#25345;&#32493;&#30340;&#35774;&#35745;&#21746;&#23398;&#12290;&#20973;&#20511;&#28023;&#37327;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;LLM&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#19988;&#26377;&#26395;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#8220;&#19968;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#8221;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NetLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23558;LLM&#24212;&#29992;&#20110;&#35299;&#20915;&#32593;&#32476;&#38382;&#39064;&#30340;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;NetLLM&#35299;&#20915;&#20102;&#35768;&#22810;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many networking tasks now employ deep learning (DL) to solve complex prediction and system optimization problems. However, current design philosophy of DL-based algorithms entails intensive engineering overhead due to the manual design of deep neural networks (DNNs) for different networking tasks. Besides, DNNs tend to achieve poor generalization performance on unseen data distributions/environments.   Motivated by the recent success of large language models (LLMs), for the first time, this work studies the LLM adaptation for networking to explore a more sustainable design philosophy. With the massive pre-trained knowledge and powerful inference ability, LLM can serve as the foundation model, and is expected to achieve "one model for all" with even better performance and stronger generalization for various tasks. In this paper, we present NetLLM, the first LLM adaptation framework that efficiently adapts LLMs to solve networking problems. NetLLM addresses many practical challenges in L
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00035</link><description>&lt;p&gt;
&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#33322;&#29677;&#28369;&#34892;&#23433;&#20840;&#30340;&#36305;&#36947;&#29289;&#20307;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;&#65292;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#35780;&#20272;&#20102;&#35813;&#20998;&#31867;&#22120;&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#35768;&#22810;&#35745;&#31639;&#38382;&#39064;&#19978;&#25104;&#20026;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#33322;&#31354;&#19994;&#24076;&#26395;&#25506;&#32034;&#23427;&#20204;&#22312;&#20943;&#36731;&#39134;&#34892;&#21592;&#36127;&#25285;&#21644;&#25913;&#21892;&#36816;&#33829;&#23433;&#20840;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20351;&#29992;DNNs&#38656;&#35201;&#36827;&#34892;&#24443;&#24213;&#30340;&#35748;&#35777;&#36807;&#31243;&#12290;&#36825;&#19968;&#38656;&#27714;&#21487;&#20197;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26469;&#35299;&#20915;&#65292;&#24418;&#24335;&#39564;&#35777;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#35777;&#26126;&#26576;&#20123;&#35823;&#21028;&#30340;&#19981;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Airbus&#24403;&#21069;&#27491;&#22312;&#24320;&#21457;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;DNN&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#22312;&#39134;&#26426;&#28369;&#34892;&#38454;&#27573;&#20351;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35780;&#20272;&#36825;&#20010;DNN&#23545;&#19977;&#31181;&#24120;&#35265;&#22270;&#20687;&#25200;&#21160;&#31867;&#22411;&#30340;&#40065;&#26834;&#24615;&#65306;&#22122;&#22768;&#12289;&#20142;&#24230;&#21644;&#23545;&#27604;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#37096;&#20998;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#22810;&#27425;&#35843;&#29992;&#24213;&#23618;&#39564;&#35777;&#22120;&#65292;&#36825;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.16803</link><description>&lt;p&gt;
PBSCSR&#65306;&#38050;&#29748;&#40657;&#24066;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset. (arXiv:2401.16803v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#30740;&#31350;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26082;&#20687;MNIST&#19968;&#26679;&#26131;&#20110;&#33719;&#21462;&#65292;&#21448;&#20687;ImageNet&#19968;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;IMSLP&#30340;&#38050;&#29748;&#20048;&#35889;&#22270;&#20687;&#20013;&#37319;&#26679;&#22266;&#23450;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#29255;&#27573;&#12290;&#25968;&#25454;&#38598;&#26412;&#36523;&#21253;&#21547;40,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;9&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;100,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;100&#20998;&#31867;&#20219;&#21153;&#65292;&#36824;&#26377;29,310&#20010;&#26080;&#26631;&#31614;&#30340;&#21487;&#21464;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#26631;&#35760;&#25968;&#25454;&#20197;&#19982;MNIST&#22270;&#20687;&#31867;&#20284;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#26497;&#20854;&#26041;&#20415;&#22320;&#21487;&#35270;&#21270;&#12289;&#25805;&#20316;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#65292;&#20197;&#20801;&#35768;&#35775;&#38382;IMSLP&#19978;&#30340;&#21407;&#22987;&#20048;&#35889;&#22270;&#20687;&#21644;&#20854;&#20182;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#20010;&#21487;&#20197;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is "as accessible as MNIST and as challenging as ImageNet." To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11044</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#22312;&#20851;&#38190;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Significance of Data Abstraction Methods in Machine Learning Classification Processes for Critical Decision-Making. (arXiv:2401.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#36807;&#31243;&#20013;&#25968;&#25454;&#25277;&#35937;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#37319;&#29992;ROC&#26354;&#32447;&#26041;&#27861;&#24320;&#21457;&#30340;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#35813;&#26041;&#27861;&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26041;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#34892;&#20026;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#65292;&#20854;&#20013;&#36131;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;Small and Incomplete Dataset Analyser (SaNDA)&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;ROC&#26354;&#32447;&#30340;&#26041;&#27861;&#24320;&#21457;&#25968;&#25454;&#25277;&#35937;&#21327;&#35758;&#65292;&#20197;&#22686;&#24378;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#25191;&#34892;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21015;&#38388;&#25968;&#25454;&#36716;&#25442;&#65292;&#21363;&#25277;&#35937;&#65292;&#36825;&#23545;SaNDA&#30340;&#20998;&#31867;&#36807;&#31243;&#38750;&#24120;&#20851;&#38190;&#65292;&#24182;&#25506;&#35752;&#20102;&#26367;&#20195;&#30340;&#25277;&#35937;&#21327;&#35758;&#65292;&#22914;&#24120;&#37327;&#20998;&#31665;&#21644;&#20998;&#20301;&#25968;&#12290;&#23558;&#26368;&#20339;&#30340;&#26041;&#27861;&#19982;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#27169;&#22411;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#25968;&#25454;&#19981;&#23436;&#25972;&#65292;SaNDA&#22312;&#32570;&#23569;&#20540;&#24456;&#23569;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21487;&#20197;&#25104;&#20026;&#38543;&#26426;&#26862;&#26519;&#30340;&#21487;&#34892;&#26367;&#20195;&#21697;&#65292;&#24182;&#19988;&#22987;&#32456;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The applicability of widely adopted machine learning (ML) methods to classification is circumscribed by the imperatives of explicability and uncertainty, particularly evident in domains such as healthcare, behavioural sciences, and finances, wherein accountability assumes priority. Recently, Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the ability to perform classification in such domains, by developing a data abstraction protocol using a ROC curve-based method. This paper focuses on column-wise data transformations called abstractions, which are crucial for SaNDA's classification process and explores alternative abstractions protocols, such as constant binning and quantiles. The best-performing methods have been compared against Random Forest as a baseline for explainable methods. The results suggests that SaNDA can be a viable substitute for Random Forest when data is incomplete, even with minimal missing values. It consistently maintains high accuracy e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.10107</link><description>&lt;p&gt;
&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#19982;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30446;&#30340;&#65306;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#30446;&#21069;&#34987;&#29992;&#20316;&#35780;&#20272;&#30561;&#30496;&#38556;&#30861;&#30340;&#22522;&#20934;&#12290;&#20854;&#19981;&#33298;&#36866;&#12289;&#19981;&#36866;&#21512;&#23478;&#24237;&#20351;&#29992;&#20197;&#21450;&#22312;&#30561;&#30496;&#36136;&#37327;&#35780;&#20272;&#20013;&#24341;&#20837;&#20559;&#24046;&#30340;&#38382;&#39064;&#38656;&#35201;&#25506;&#32034;&#26356;&#23569;&#20405;&#20837;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#26159;&#32819;&#20869;&#33041;&#30005;&#20256;&#24863;&#22120;&#65292;&#23427;&#22312;&#33298;&#36866;&#24615;&#12289;&#22266;&#23450;&#30005;&#26497;&#20301;&#32622;&#12289;&#25239;&#30005;&#30913;&#24178;&#25200;&#24615;&#21644;&#26131;&#20110;&#20351;&#29992;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#35780;&#20272;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#19982;&#26631;&#20934;PSG&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35780;&#20272;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#25512;&#23548;&#30340;&#30561;&#30496;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;30&#31186;&#26102;&#22495;&#21644;&#39057;&#22495;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#21482;&#32771;&#34385;&#22312;PSG&#35780;&#20998;&#21592;&#21644;&#32819;&#20869;&#33041;&#30005;&#35780;&#20998;&#21592;&#36798;&#25104;&#19968;&#33268;&#26102;&#30340;&#26102;&#27573;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;PSG&#25512;&#23548;&#21644;&#21333;&#36890;&#36947;&#32819;&#20869;&#33041;&#30005;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.08847</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#12289;&#40065;&#26834;&#21644;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Over-Memorization During Natural, Robust and Catastrophic Overfitting. (arXiv:2310.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#22312;&#33258;&#28982;&#35757;&#32451;&#36824;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38590;&#20197;&#19968;&#33268;&#22320;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#36890;&#24120;&#35774;&#35745;&#20102;&#38024;&#23545;&#33258;&#28982;&#27169;&#24335;&#25110;&#23545;&#25239;&#27169;&#24335;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#20165;&#20851;&#27880;&#33258;&#28982;&#27169;&#24335;&#65292;&#21435;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DNN&#20013;&#30340;&#35760;&#24518;&#25928;&#24212;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#36807;&#24230;&#35760;&#24518;&#30340;&#20849;&#21516;&#34892;&#20026;&#65292;&#36825;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20026;DNN&#31361;&#28982;&#23545;&#26576;&#20123;&#35757;&#32451;&#27169;&#24335;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#65292;&#24182;&#23545;&#20854;&#20445;&#25345;&#25345;&#20037;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#24403;DNN&#36807;&#24230;&#35760;&#24518;&#19968;&#31181;&#23545;&#25239;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#21516;&#26102;&#23637;&#29616;&#20986;&#23545;&#24212;&#33258;&#28982;&#27169;&#24335;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#36825;&#20123;&#21457;&#29616;&#28608;&#21169;&#25105;&#20204;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#38459;&#30861;&#36807;&#24230;&#35760;&#24518;&#34892;&#20026;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. In this work, we adopt a unified perspective by solely focusing on natural patterns to explore different types of overfitting. Specifically, we examine the memorization effect in DNNs and reveal a shared behaviour termed over-memorization, which impairs their generalization capacity. This behaviour manifests as DNNs suddenly becoming high-confidence in predicting certain training patterns and retaining a persistent memory for them. Furthermore, when DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit high-confidence prediction for the corresponding natural pattern. These findings motivate us to holistically mitigate different types of overfitting by hinder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#36807;&#31243;&#23545;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#36827;&#34892;&#20928;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#20223;&#23398;&#20064;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07143</link><description>&lt;p&gt;
&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Purified Demonstration. (arXiv:2310.07143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#36807;&#31243;&#23545;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#36827;&#34892;&#20928;&#21270;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#20223;&#23398;&#20064;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20551;&#35774;&#19987;&#23478;&#28436;&#31034;&#26159;&#26368;&#20248;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19987;&#23478;&#28436;&#31034;&#36890;&#24120;&#26159;&#19981;&#23436;&#32654;&#30340;&#65292;&#23548;&#33268;&#22312;&#26377;&#25928;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#30528;&#30524;&#20110;&#20248;&#21270;&#19981;&#23436;&#32654;&#28436;&#31034;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19968;&#23450;&#27604;&#20363;&#30340;&#26368;&#20248;&#28436;&#31034;&#26469;&#20445;&#35777;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#20928;&#21270;&#28508;&#22312;&#25200;&#21160;&#24182;&#38543;&#21518;&#20174;&#32431;&#20928;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#30340;&#20004;&#27493;&#20928;&#21270;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#27491;&#21521;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#22122;&#22768;&#26469;&#26377;&#25928;&#22320;&#24179;&#28369;&#19981;&#23436;&#32654;&#28436;&#31034;&#20013;&#30340;&#28508;&#22312;&#25200;&#21160;&#12290;&#38543;&#21518;&#65292;&#36827;&#34892;&#36870;&#21521;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning has emerged as a promising approach for addressing sequential decision-making problems, with the assumption that expert demonstrations are optimal. However, in real-world scenarios, expert demonstrations are often imperfect, leading to challenges in effectively applying imitation learning. While existing research has focused on optimizing with imperfect demonstrations, the training typically requires a certain proportion of optimal demonstrations to guarantee performance. To tackle these problems, we propose to purify the potential perturbations in imperfect demonstrations and subsequently conduct imitation learning from purified demonstrations. Motivated by the success of diffusion models, we introduce a two-step purification via the diffusion process. In the first step, we apply a forward diffusion process to effectively smooth out the potential perturbations in imperfect demonstrations by introducing additional noise. Subsequently, a reverse generative process is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03720</link><description>&lt;p&gt;
HeaP: &#20351;&#29992;LLMs&#36827;&#34892;&#23618;&#27425;&#21270;Web&#21160;&#20316;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#37327;&#25968;&#25454;&#21644;&#38646;-shot&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25945;&#25480;LLMs&#22312;Web&#19978;&#25191;&#34892;&#20219;&#21153;&#38754;&#20020;&#30528;&#22522;&#26412;&#25361;&#25112; - &#32452;&#21512;&#24615;&#22823;&#30340;&#24320;&#25918;&#19990;&#30028;&#20219;&#21153;&#21644;Web&#25509;&#21475;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;LLMs&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#32423;&#30340;&#38381;&#29615;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31574;&#30053;&#26500;&#25104;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#27861;&#65292;&#21363;&#26032;&#30340;Web&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#31574;&#30053;&#30340;&#32452;&#21512;&#26469;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;LLMs&#30340;Hierarchical Policies for Web Actions&#65288;HeaP&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#19968;&#32452;&#23618;&#27425;&#21270;&#30340;LLM&#25552;&#31034;&#26469;&#35268;&#21010;&#39640;&#32423;&#20219;&#21153;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#31574;&#30053;&#25191;&#34892;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#22871;Web&#20219;&#21153;&#65292;&#21253;&#25324;MiniWoB++&#65292;WebArena&#65292;&#27169;&#25311;&#33322;&#31354;CRM&#20197;&#21450;&#23454;&#38469;&#32593;&#31449;&#26469;&#35780;&#20272;HeaP&#19982;&#19968;&#31995;&#21015;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#22312;&#33410;&#28857;&#32423;&#21035;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21270;&#25200;&#21160;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#39057;&#29575;&#20272;&#35745;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25968;&#25454;&#20013;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08569</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65306;&#19968;&#31181;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Differential Privacy in Graph Neural Networks: a Reconstruction Approach. (arXiv:2309.08569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#36890;&#36807;&#22312;&#33410;&#28857;&#32423;&#21035;&#23545;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#21270;&#25200;&#21160;&#26469;&#23454;&#29616;&#12290;&#36890;&#36807;&#39057;&#29575;&#20272;&#35745;&#21644;&#37325;&#26500;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25968;&#25454;&#20013;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23545;&#24314;&#27169;&#22797;&#26434;&#22270;&#25968;&#25454;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;GNN&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#30740;&#31350;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#20007;&#22833;&#22826;&#22810;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#20026;&#29992;&#25143;&#25552;&#20379;&#33410;&#28857;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#24046;&#20998;&#38544;&#31169;&#27010;&#24565;&#65292;&#21363;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#22312;&#25968;&#25454;&#34987;&#38598;&#20013;&#26381;&#21153;&#22120;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#65292;&#23545;&#33410;&#28857;&#32423;&#21035;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#25968;&#25454;&#24212;&#29992;&#38543;&#26426;&#21270;&#26426;&#21046;&#36827;&#34892;&#25200;&#21160;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#29305;&#24449;&#35774;&#32622;&#20013;&#24212;&#29992;&#38543;&#26426;&#21270;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;LDP&#21327;&#35758;&#12290;&#22522;&#20110;&#38543;&#26426;&#21270;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#30340;&#39057;&#29575;&#20272;&#35745;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#37325;&#26500;&#26041;&#27861;&#26469;&#36817;&#20284;&#20174;&#25200;&#21160;&#25968;&#25454;&#20013;&#24674;&#22797;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#22270;&#32858;&#31867;&#20013;&#30340;&#39057;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks have achieved tremendous success in modeling complex graph data in a variety of applications. However, there are limited studies investigating privacy protection in GNNs. In this work, we propose a learning framework that can provide node privacy at the user level, while incurring low utility loss. We focus on a decentralized notion of Differential Privacy, namely Local Differential Privacy, and apply randomization mechanisms to perturb both feature and label data at the node level before the data is collected by a central server for model training. Specifically, we investigate the application of randomization mechanisms in high-dimensional feature settings and propose an LDP protocol with strict privacy guarantees. Based on frequency estimation in statistical analysis of randomized data, we develop reconstruction methods to approximate features and labels from perturbed data. We also formulate this learning framework to utilize frequency estimates of graph cluste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.15325</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#35266;&#27979;&#37096;&#20998;&#12289;&#31895;&#31890;&#21270;&#21644;&#31561;&#21464;&#24615;&#22312;Koopman&#31639;&#23376;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale dynamical systems. (arXiv:2307.15325v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#20013;&#37096;&#20998;&#35266;&#27979;&#25110;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#20934;&#30830;&#25552;&#20379;Koopman&#31639;&#23376;&#36817;&#20284;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#31995;&#32479;&#21160;&#24577;&#30340;&#23545;&#31216;&#24615;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#25968;&#25454;&#39537;&#21160;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25511;&#21046;&#22797;&#26434;&#31995;&#32479;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#20174;&#27979;&#37327;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#20989;&#25968;&#31354;&#38388;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#31995;&#32479;&#65292;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#37096;&#20998;&#35266;&#27979;&#65288;&#22914;&#23454;&#39564;&#25968;&#25454;&#20013;&#38750;&#24120;&#24120;&#35265;&#30340;&#27979;&#37327;&#65289;&#25110;&#32773;&#20986;&#20110;&#25928;&#29575;&#21407;&#22240;&#21051;&#24847;&#36827;&#34892;&#31895;&#31890;&#21270;&#30340;&#24773;&#20917;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#31181;&#24773;&#20917;&#20013;&#30340;&#22256;&#25200;&#65292;&#21363;&#22914;&#26524;&#25105;&#20204;&#19981;&#20180;&#32454;&#36873;&#25321;&#21487;&#35266;&#27979;&#25968;&#37327;&#65292;&#32463;&#20856;&#30340;EDMD&#31639;&#27861;&#19981;&#33021;&#33258;&#21160;&#25552;&#20379;&#28508;&#22312;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31995;&#32479;&#21160;&#24577;&#20013;&#30340;&#23545;&#31216;&#24615;&#21487;&#20197;&#36716;&#31227;&#21040;Koopman&#31639;&#23376;&#20013;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#31616;&#35201;&#35752;&#35770;&#20102;&#19982;&#22495;&#20998;&#35299;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator has become an essential tool for data-driven analysis, prediction and control of complex systems, the main reason being the enormous potential of identifying linear function space representations of nonlinear dynamics from measurements. Until now, the situation where for large-scale systems, we (i) only have access to partial observations (i.e., measurements, as is very common for experimental data) or (ii) deliberately perform coarse graining (for efficiency reasons) has not been treated to its full extent. In this paper, we address the pitfall associated with this situation, that the classical EDMD algorithm does not automatically provide a Koopman operator approximation for the underlying system if we do not carefully select the number of observables. Moreover, we show that symmetries in the system dynamics can be carried over to the Koopman operator, which allows us to massively increase the model efficiency. We also briefly draw a connection to domain decompos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09958</link><description>&lt;p&gt;
SIMGA&#65306;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19982;&#39640;&#25928;&#30340;&#20840;&#23616;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SIMGA: A Simple and Effective Heterophilous Graph Neural Network with Efficient Global Aggregation. (arXiv:2305.09958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36935;&#21040;&#24322;&#36136;&#24615;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#22240;&#20026;&#23616;&#37096;&#21644;&#32479;&#19968;&#32858;&#21512;&#32780;&#23548;&#33268;&#30340;&#30456;&#37051;&#33410;&#28857;&#19981;&#30456;&#20284;&#12290;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35797;&#22270;&#25972;&#21512;&#20840;&#23616;&#32858;&#21512;&#30340;&#23581;&#35797;&#36890;&#24120;&#38656;&#35201;&#36845;&#20195;&#22320;&#32500;&#25252;&#21644;&#26356;&#26032;&#20840;&#22270;&#20449;&#24687;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377; $n$ &#20010;&#33410;&#28857;&#30340;&#22270;&#65292;&#36825;&#38656;&#35201; $\mathcal{O}(n^2)$ &#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#22823;&#22411;&#22270;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SIMGA&#65292;&#19968;&#31181;&#23558; SimRank &#32467;&#26500;&#30456;&#20284;&#24230;&#27979;&#37327;&#20316;&#20026;&#20840;&#23616;&#32858;&#21512;&#30340; GNN &#32467;&#26500;&#12290; SIMGA &#30340;&#35774;&#35745;&#31616;&#21333;&#65292;&#19988;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#37117;&#26377;&#30528;&#26377; promising &#30340;&#32467;&#26524;&#12290;SIMGA &#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#25104;&#20026;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340; $n$ &#20256;&#25773;&#25928;&#29575;&#30340;&#24322;&#36136;&#24615; GNN &#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558; SimRank &#35270;&#20026; GNN &#30340;&#19968;&#31181;&#26032;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;&#27719;&#32858;&#33410;&#28857;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts in incoorporating global aggregation for heterophilous GNNs usually require iteratively maintaining and updating full-graph information, which entails $\mathcal{O}(n^2)$ computation efficiency for a graph with $n$ nodes, leading to weak scalability to large graphs. In this paper, we propose SIMGA, a GNN structure integrating SimRank structural similarity measurement as global aggregation. The design of SIMGA is simple, yet it leads to promising results in both efficiency and effectiveness. The simplicity of SIMGA makes it the first heterophilous GNN model that can achieve a propagation efficiency near-linear to $n$. We theoretically demonstrate its effectiveness by treating SimRank as a new interpretation of GNN and prove that the aggregated node representation
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05339</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#26816;&#27979;&#19982;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Deep-learning assisted detection and quantification of (oo)cysts of Giardia and Cryptosporidium on smartphone microscopy images. (arXiv:2304.05339v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;RetinaNet&#27169;&#22411;&#38024;&#23545;&#37319;&#29992;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#31995;&#32479;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#36827;&#34892;&#26816;&#27979;&#21644;&#35745;&#25968;&#65292;&#24182;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20339;&#25928;&#26524;&#65292;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29992;&#21463;&#24494;&#29983;&#29289;&#27745;&#26579;&#30340;&#39135;&#29289;&#21644;&#27700;&#27599;&#24180;&#36896;&#25104;&#25968;&#30334;&#19975;&#20154;&#27515;&#20129;&#12290;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#26159;&#19968;&#31181;&#20415;&#25658;&#12289;&#20302;&#25104;&#26412;&#21644;&#27604;&#20256;&#32479;&#30340;&#20142;&#22330;&#26174;&#24494;&#38236;&#26356;&#26131;&#25509;&#36817;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;&#28982;&#32780;&#65292;&#26234;&#33021;&#25163;&#26426;&#26174;&#24494;&#38236;&#30340;&#22270;&#20687;&#26377;&#24456;&#22810;&#22122;&#22768;&#65292;&#38656;&#35201;&#22521;&#35757;&#26377;&#32032;&#30340;&#25216;&#26415;&#20154;&#21592;&#36827;&#34892;&#25163;&#21160;&#22218;&#27873;&#35782;&#21035;&#65292;&#32780;&#36825;&#36890;&#24120;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#23545;&#35937;&#26816;&#27979;&#33258;&#21160;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#21487;&#33021;&#20026;&#27492;&#38480;&#21046;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#30340;&#25928;&#26524;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;&#34092;&#33756;&#26679;&#21697;&#20013;&#33719;&#21462;&#30340;&#26234;&#33021;&#25163;&#26426;&#21644;&#20142;&#22330;&#26174;&#24494;&#38236;&#22270;&#20687;&#12290;Faster RCNN&#12289;RetinaNet&#21644;You Only Look Once&#65288;YOLOv8s&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#29992;&#26469;&#25506;&#32034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#21365;/&#26797;&#29366;&#20307;&#65292;&#20294;RetinaNet&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#20004;&#31181;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#26174;&#24494;&#31995;&#32479;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#33258;&#21160;&#26816;&#27979;&#21644;&#35745;&#25968;&#36158;&#31532;&#34411;&#21644;&#38544;&#23394;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The consumption of microbial-contaminated food and water is responsible for the deaths of millions of people annually. Smartphone-based microscopy systems are portable, low-cost, and more accessible alternatives for the detection of Giardia and Cryptosporidium than traditional brightfield microscopes. However, the images from smartphone microscopes are noisier and require manual cyst identification by trained technicians, usually unavailable in resource-limited settings. Automatic detection of (oo)cysts using deep-learning-based object detection could offer a solution for this limitation. We evaluate the performance of three state-of-the-art object detectors to detect (oo)cysts of Giardia and Cryptosporidium on a custom dataset that includes both smartphone and brightfield microscopic images from vegetable samples. Faster RCNN, RetinaNet, and you only look once (YOLOv8s) deep-learning models were employed to explore their efficacy and limitations. Our results show that while the deep-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17963</link><description>&lt;p&gt;
&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States. (arXiv:2303.17963v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26410;&#30693;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25511;&#21046;&#24037;&#31243;&#26041;&#27861;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#25104;&#20026;&#29289;&#29702;&#24314;&#27169;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#29366;&#24577;&#27979;&#37327;&#30340;&#21487;&#29992;&#24615;&#65292;&#32780;&#22797;&#26434;&#31995;&#32479;&#30340;&#29366;&#24577;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#27979;&#37327;&#30340;&#12290;&#22240;&#27492;&#65292;&#21487;&#33021;&#38656;&#35201;&#21516;&#26102;&#20272;&#35745;&#21160;&#21147;&#23398;&#21644;&#28508;&#22312;&#29366;&#24577;&#65292;&#20174;&#32780;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#22320;&#35774;&#35745;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#28508;&#22312;&#29366;&#24577;&#30340;&#26410;&#30693;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#36755;&#20837;&#36712;&#36857;&#12290;&#23545;&#32467;&#26524;&#36755;&#20837;&#36712;&#36857;&#36827;&#34892;&#20102;&#27010;&#29575;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39564;&#35777;&#20219;&#24847;&#25511;&#21046;&#24459;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While many of these approaches rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and a latent state, making it considerably more challenging to design controllers with performance guarantees. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20026;&#28145;&#24230;&#22270;&#32858;&#31867;&#25552;&#20379;&#39640;&#36136;&#37327;&#21644;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#30697;&#38453;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03559</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Attribute Graph Clustering via Learnable Augmentation. (arXiv:2212.03559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20026;&#28145;&#24230;&#22270;&#32858;&#31867;&#25552;&#20379;&#39640;&#36136;&#37327;&#21644;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#30697;&#38453;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#22270;&#32858;&#31867;&#65288;CDGC&#65289;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#33410;&#28857;&#20998;&#32452;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#12290;&#26356;&#22909;&#30340;&#22686;&#24378;&#25216;&#26415;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#27604;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#25104;&#20026;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22686;&#24378;&#26679;&#26412;&#22987;&#32456;&#30001;&#20154;&#31867;&#32463;&#39564;&#39044;&#23450;&#20041;&#65292;&#24182;&#19988;&#19982;&#19979;&#28216;&#20219;&#21153;&#32858;&#31867;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#20154;&#21147;&#36164;&#28304;&#25104;&#26412;&#39640;&#21644;&#24615;&#33021;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65288;AGCLA&#65289;&#65292;&#20026;CDGC&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#19988;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20998;&#21035;&#29992;&#20110;&#23646;&#24615;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#20102;&#20004;&#20010;&#25913;&#36827;&#30697;&#38453;&#65292;&#21253;&#25324;&#39640;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30697;&#38453;&#21644;&#36328;&#35270;&#22270;&#26679;&#26412;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#30340;&#20146;&#21644;&#30697;&#38453;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#19981;&#26029;&#35843;&#25972;&#36825;&#20123;&#22686;&#24378;&#22120;&#21644;&#25913;&#36827;&#30697;&#38453;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive deep graph clustering (CDGC) utilizes contrastive learning to group nodes into different clusters. Better augmentation techniques benefit the quality of the contrastive samples, thus being one of key factors to improve performance. However, the augmentation samples in existing methods are always predefined by human experiences, and agnostic from the downstream task clustering, thus leading to high human resource costs and poor performance. To this end, we propose an Attribute Graph Clustering method via Learnable Augmentation (\textbf{AGCLA}), which introduces learnable augmentors for high-quality and suitable augmented samples for CDGC. Specifically, we design two learnable augmentors for attribute and structure information, respectively. Besides, two refinement matrices, including the high-confidence pseudo-label matrix and the cross-view sample similarity matrix, are generated to improve the reliability of the learned affinity matrix. During the training procedure, we no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2211.16237</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#20998;&#21106;&#26041;&#27861;&#32553;&#23567;SVRG&#19982;TD-SVRG&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;TD&#23398;&#20064;&#35270;&#20026;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#20998;&#21106;&#65292;&#23558;TD&#21644;SVRG&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD&#65288;&#26102;&#24207;&#24046;&#20998;&#65289;&#23398;&#20064;&#26159;&#19968;&#31181;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#12290;&#26368;&#36817;&#65292;&#22810;&#20010;&#24037;&#20316;&#23581;&#35797;&#23558;TD&#23398;&#20064;&#19982;SVRG&#30456;&#32467;&#21512;&#65292;&#20197;&#33719;&#24471;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#25910;&#25947;&#36895;&#24230;&#30340;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#20984;&#20248;&#21270;&#35774;&#32622;&#19979;&#65292;&#25152;&#24471;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#26126;&#26174;&#19981;&#21450;SVRG&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#26368;&#36817;&#23545;TD&#23398;&#20064;&#30340;&#35299;&#37322;&#65292;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#36866;&#24403;&#36873;&#25321;&#20989;&#25968;&#30340;&#26799;&#24230;&#30340;&#20998;&#21106;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#31639;&#27861;&#65292;&#24182;&#23558;TD&#19982;SVRG&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#20855;&#26377;&#39044;&#23450;&#23398;&#20064;&#36895;&#29575;&#20026;1/8&#30340;&#20960;&#20309;&#25910;&#25947;&#30028;&#38480;&#65292;&#19982;&#20984;&#35774;&#32622;&#19979;SVRG&#30340;&#25910;&#25947;&#30028;&#38480;&#30456;&#21516;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference (TD) learning is a policy evaluation in reinforcement learning whose performance can be enhanced by variance reduction techniques. Recently, multiple works have sought to fuse TD learning with SVRG to obtain a policy evaluation method with a geometric rate of convergence. However, the resulting convergence rate is significantly weaker than what is achieved by SVRG in the setting of convex optimization. In this work we utilize a recent interpretation of TD-learning as the splitting of the gradient of an appropriately chosen function, thus simplifying the algorithm and fusing TD with SVRG. Our main result is a geometric convergence bound with predetermined learning rate of $1/8$, which is identical to the convergence bound available for SVRG in the convex setting. Our theoretical findings are supported by a set of experiments.
&lt;/p&gt;</description></item></channel></rss>