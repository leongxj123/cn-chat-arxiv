<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.17561</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21450;&#20854;&#26368;&#26032;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning and State-of-the-arts Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17561
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#30340;&#26368;&#26032;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;, &#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#23618;&#20114;&#36830;&#21333;&#20803;&#65288;&#31070;&#32463;&#20803;&#65289;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#30452;&#25509;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#34920;&#31034;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#21463;&#21040;&#36825;&#31181;&#23398;&#20064;&#33021;&#21147;&#30340;&#36171;&#33021;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26159;&#35768;&#22810;&#31361;&#30772;&#24615;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#39537;&#21160;&#21147;&#12290;&#26500;&#24314;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#29616;&#23454;&#38382;&#39064;&#30340;&#21160;&#24577;&#24615;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#31867;&#22411;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#26032;&#21457;&#23637;&#30340;&#35206;&#30422;&#38754;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16871</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Prediction for Multi-Agent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16871
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;OPP&#65289;&#65292;&#21363;&#20165;&#20351;&#29992;&#22312;&#19968;&#20010;&#27491;&#24120;&#65288;&#34892;&#20026;&#65289;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#30446;&#26631;&#31574;&#30053;&#30340;&#32467;&#26524;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20998;&#26512;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;&#37096;&#32626;&#26032;&#31574;&#30053;&#21487;&#33021;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#20449;&#30340;&#31163;&#31574;&#30053;&#39044;&#27979;&#65292;&#26368;&#36817;&#20851;&#20110;&#19968;&#33268;&#31163;&#31574;&#30053;&#39044;&#27979;&#65288;COPP&#65289;&#30340;&#24037;&#20316;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26694;&#26550;&#26469;&#22312;&#30446;&#26631;&#36807;&#31243;&#19979;&#25512;&#23548;&#24102;&#26377;&#27010;&#29575;&#20445;&#35777;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;COPP&#26041;&#27861;&#21487;&#20197;&#32771;&#34385;&#30001;&#31574;&#30053;&#20999;&#25442;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20294;&#20165;&#38480;&#20110;&#21333;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#26631;&#37327;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;&#22870;&#21169;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MA-COPP&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#35299;&#20915;&#28041;&#21450;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;OPP&#38382;&#39064;&#30340;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#8220;&#33258;&#25105;&#8221;&#26234;&#33021;&#20307;&#25913;&#21464;&#31574;&#30053;&#26102;&#20026;&#25152;&#26377;&#26234;&#33021;&#20307;&#36712;&#36857;&#25512;&#23548;&#32852;&#21512;&#39044;&#27979;&#21306;&#22495;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#22330;&#26223;&#19981;&#21516;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16871v1 Announce Type: cross  Abstract: Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy using only data collected under a nominal (behavioural) policy, is a paramount problem in data-driven analysis of safety-critical systems where the deployment of a new policy may be unsafe. To achieve dependable off-policy predictions, recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal prediction framework to derive prediction regions with probabilistic guarantees under the target process. Existing COPP methods can account for the distribution shifts induced by policy switching, but are limited to single-agent systems and scalar outcomes (e.g., rewards). In this work, we introduce MA-COPP, the first conformal prediction method to solve OPP problems involving multi-agent systems, deriving joint prediction regions for all agents' trajectories when one or more "ego" agents change their policies. Unlike the single-agent scenario, this se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.13704</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer through an Implicit-Explicit (IMEX) time-stepping approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13704
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#26174;&#24335;(IMEX)&#26102;&#38388;&#27493;&#36827;&#26041;&#27861;&#25913;&#36827;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;ADAM&#65289;&#38543;&#26426;&#20248;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#27604;&#32463;&#20856;Adam&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Adam&#20248;&#21270;&#22120;&#36890;&#24120;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#65292;&#23545;&#24212;&#20110;&#22312;&#38750;&#24120;&#23567;&#30340;&#23398;&#20064;&#36895;&#29575;&#38480;&#21046;&#19979;&#30340;&#22522;&#26412;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#32463;&#20856;Adam&#31639;&#27861;&#26159;&#24213;&#23618;ODE&#30340;&#19968;&#38454;&#38544;&#24335;&#26174;&#24335;(IMEX) Euler&#31163;&#25955;&#21270;&#12290;&#20174;&#26102;&#38388;&#31163;&#25955;&#21270;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#38454;IMEX&#26041;&#27861;&#26469;&#35299;&#20915;ODE&#30340;Adam&#26041;&#26696;&#30340;&#26032;&#25193;&#23637;&#12290;&#22522;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#20960;&#20010;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#19978;&#27604;&#32463;&#20856;Adam&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13704v1 Announce Type: cross  Abstract: The Adam optimizer, often used in Machine Learning for neural network training, corresponds to an underlying ordinary differential equation (ODE) in the limit of very small learning rates. This work shows that the classical Adam algorithm is a first order implicit-explicit (IMEX) Euler discretization of the underlying ODE. Employing the time discretization point of view, we propose new extensions of the Adam scheme obtained by using higher order IMEX methods to solve the ODE. Based on this approach, we derive a new optimization algorithm for neural network training that performs better than classical Adam on several regression and classification problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.10996</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#21487;&#25345;&#32493;Sim2Real&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20998;&#25955;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#35299;&#20915;&#20132;&#36890;&#36335;&#21475;&#31359;&#36234;&#21644;&#33258;&#20027;&#36187;&#36710;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25345;&#32493;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#25353;&#38656;&#25193;&#23637;&#24182;&#34892;&#21270;&#35757;&#32451;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#21033;&#29992;&#26368;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AutoDRIVE&#29983;&#24577;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#21551;&#21160;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#36716;&#31227;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#29616;&#23454;&#19990;&#30028;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#31350;&#20102;4&#21488;&#21512;&#20316;&#36710;&#36742;(Nigel)&#22312;&#21333;&#26234;&#33021;&#20307;&#21644;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#29615;&#22659;&#20013;&#20849;&#20139;&#26377;&#38480;&#29366;&#24577;&#20449;&#24687;&#30340;&#20132;&#21449;&#36941;&#21382;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20010;&#20307;&#31574;&#30053;&#26041;&#27861;&#30740;&#31350;&#20102;2&#36742;&#36710;(F1TENTH)&#30340;&#23545;&#25239;&#24615;&#33258;&#20027;&#36187;&#36710;&#38382;&#39064;&#12290;&#22312;&#20219;&#20309;&#19968;&#32452;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#26550;&#26500;&#65292;&#36825;&#20801;&#35768;&#23545;&#31574;&#30053;&#36827;&#34892;&#26377;&#21147;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10996v1 Announce Type: cross  Abstract: This work presents a sustainable multi-agent deep reinforcement learning framework capable of selectively scaling parallelized training workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. We introduce AutoDRIVE Ecosystem as an enabling digital twin framework to train, deploy, and transfer cooperative as well as competitive multi-agent reinforcement learning policies from simulation to reality. Particularly, we first investigate an intersection traversal problem of 4 cooperative vehicles (Nigel) that share limited state information in single as well as multi-agent learning settings using a common policy approach. We then investigate an adversarial autonomous racing problem of 2 vehicles (F1TENTH) using an individual policy approach. In either set of experiments, a decentralized learning architecture was adopted, which allowed robust training and testing of the policies 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.10586</link><description>&lt;p&gt;
&#20174;&#31639;&#27861;&#21040;&#32467;&#26524;&#65306;&#23457;&#35270;&#20154;&#24037;&#26234;&#33021;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
From Algorithms to Outcomes: Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10586
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#22797;&#21457;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#20316;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27835;&#30103;&#25104;&#26412;&#65292;&#24182;&#26377;&#25928;&#35268;&#21010;&#27835;&#30103;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33152;&#33009;&#30284;&#26159;&#33521;&#22269;&#27599;&#22825;&#36896;&#25104;15&#20154;&#27515;&#20129;&#30340;&#39046;&#20808;&#27852;&#23615;&#36947;&#30284;&#30151;&#12290;&#36825;&#31181;&#30284;&#30151;&#20027;&#35201;&#34920;&#29616;&#20026;&#38750;&#32908;&#23618;&#20405;&#34989;&#24615;&#33152;&#33009;&#30284;&#65288;NMIBC&#65289;&#65292;&#20854;&#29305;&#28857;&#26159;&#32959;&#30244;&#36824;&#26410;&#28183;&#36879;&#21040;&#33152;&#33009;&#22721;&#30340;&#32908;&#32905;&#23618;&#12290; NMIBC&#30340;&#22797;&#21457;&#29575;&#38750;&#24120;&#39640;&#65292;&#36798;&#21040;70-80&#65285;&#65292;&#22240;&#27492;&#27835;&#30103;&#25104;&#26412;&#26368;&#39640;&#12290;&#30446;&#21069;&#29992;&#20110;&#39044;&#27979;&#22797;&#21457;&#30340;&#24037;&#20855;&#20351;&#29992;&#35780;&#20998;&#31995;&#32479;&#26469;&#39640;&#20272;&#39118;&#38505;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#23545;&#22797;&#21457;&#30340;&#19981;&#20934;&#30830;&#21644;&#24310;&#36831;&#39044;&#27979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#39044;&#27979;&#22797;&#21457;&#23545;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#31649;&#29702;&#21644;&#27835;&#30103;&#35745;&#21010;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#20986;&#29616;&#30340;&#22320;&#26041;&#65292;&#36890;&#36807;&#21033;&#29992;&#20998;&#23376;&#21644;&#20020;&#24202;&#25968;&#25454;&#39044;&#27979;NMIBC&#22797;&#21457;&#65292;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#26412;&#27425;&#23457;&#26597;&#23545;&#39044;&#27979;NMIBC&#22797;&#21457;&#30340;ML&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#35780;&#20272;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10586v1 Announce Type: cross  Abstract: Bladder cancer, the leading urinary tract cancer, is responsible for 15 deaths daily in the UK. This cancer predominantly manifests as non-muscle-invasive bladder cancer (NMIBC), characterised by tumours not yet penetrating the muscle layer of the bladder wall. NMIBC is plagued by a very high recurrence rate of 70-80% and hence the costliest treatments. Current tools for predicting recurrence use scoring systems that overestimate risk and have poor accuracy. Inaccurate and delayed prediction of recurrence significantly elevates the likelihood of mortality. Accurate prediction of recurrence is hence vital for cost-effective management and treatment planning. This is where Machine learning (ML) techniques have emerged as a promising approach for predicting NMIBC recurrence by leveraging molecular and clinical data. This review provides a comprehensive analysis of ML approaches for predicting NMIBC recurrence. Our systematic evaluation de
&lt;/p&gt;</description></item><item><title>CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.10164</link><description>&lt;p&gt;
CoReEcho: 2D+&#26102;&#38388;&#36229;&#22768;&#24515;&#21160;&#22270;&#20998;&#26512;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CoReEcho: Continuous Representation Learning for 2D+time Echocardiography Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10164
&lt;/p&gt;
&lt;p&gt;
CoReEcho&#25552;&#20986;&#20102;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19968;&#30452;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#21253;&#25324;&#36229;&#22768;&#24515;&#21160;&#22270;&#65292;&#22312;&#25552;&#20379;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#21516;&#26102;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#27700;&#32447;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#26080;&#27861;&#25429;&#33719;&#36229;&#22768;&#24515;&#21160;&#22270;&#29255;&#27573;&#20043;&#38388;&#30340;&#36830;&#32493;&#20851;&#31995;&#65292;&#23548;&#33268;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23545;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoReEcho&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#35843;&#38024;&#23545;&#30452;&#25509;EF&#22238;&#24402;&#30340;&#36830;&#32493;&#34920;&#31034;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;CoReEcho&#65306;1&#65289;&#22312;&#26368;&#22823;&#30340;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#38598;&#65288;EchoNet-Dynamic&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;3.90&#21644;R2 o
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10164v1 Announce Type: cross  Abstract: Deep learning (DL) models have been advancing automatic medical image analysis on various modalities, including echocardiography, by offering a comprehensive end-to-end training pipeline. This approach enables DL models to regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting in superior performance. However, the end-to-end training pipeline makes the learned representations less explainable. The representations may also fail to capture the continuous relation among echocardiogram clips, indicating the existence of spurious correlations, which can negatively affect the generalization. To mitigate this issue, we propose CoReEcho, a novel training framework emphasizing continuous representations tailored for direct EF regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms the current state-of-the-art (SOTA) on the largest echocardiography dataset (EchoNet-Dynamic) with MAE of 3.90 &amp; R2 o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#32500;&#25252;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.11628</link><description>&lt;p&gt;
&#31163;&#25955;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Discrete Neural Algorithmic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11628
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#32500;&#25252;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#65292;&#24182;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#27169;&#20223;&#32463;&#20856;&#31639;&#27861;&#30340;&#25191;&#34892;&#26469;&#25429;&#25417;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35745;&#31639;&#12290;&#23613;&#31649;&#24120;&#35265;&#30340;&#26550;&#26500;&#36275;&#22815;&#34920;&#36798;&#27491;&#30830;&#30340;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#65292;&#20294;&#24403;&#21069;&#30340;&#31070;&#32463;&#25512;&#29702;&#22120;&#22312;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#26102;&#38754;&#20020;&#27867;&#21270;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#35745;&#31639;&#19981;&#21463;&#20998;&#24067;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25551;&#36848;&#20026;&#31163;&#25955;&#35745;&#31639;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24378;&#21046;&#31070;&#32463;&#25512;&#29702;&#22120;&#23558;&#25191;&#34892;&#36712;&#36857;&#20316;&#20026;&#26377;&#38480;&#39044;&#23450;&#20041;&#29366;&#24577;&#30340;&#32452;&#21512;&#36827;&#34892;&#32500;&#25252;&#12290;&#36890;&#36807;&#23545;&#31639;&#27861;&#29366;&#24577;&#36716;&#25442;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#19982;&#21407;&#22987;&#31639;&#27861;&#23436;&#32654;&#23545;&#40784;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;SALSA-CLRS&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#20026;&#25152;&#26377;&#20219;&#21153;&#33719;&#24471;&#20102;&#23436;&#32654;&#30340;&#27979;&#35797;&#25104;&#32489;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11628v1 Announce Type: new  Abstract: Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05956</link><description>&lt;p&gt;
Pathformer: &#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pathformer: Multi-scale transformers with Adaptive Pathways for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pathformer&#30340;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20248;&#21270;&#24314;&#27169;&#36807;&#31243;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20174;&#26377;&#38480;&#25110;&#22266;&#23450;&#23610;&#24230;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#36328;&#22810;&#20010;&#23610;&#24230;&#30340;&#19981;&#21516;&#29305;&#24449;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#33258;&#36866;&#24212;&#36335;&#24452;&#65288;Pathformer&#65289;&#30340;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21516;&#26102;&#25972;&#21512;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#21644;&#26102;&#38388;&#36317;&#31163;&#36827;&#34892;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#22810;&#23610;&#24230;&#21010;&#20998;&#36816;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#22359;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#25104;&#19981;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22522;&#20110;&#27599;&#20010;&#23610;&#24230;&#30340;&#21010;&#20998;&#65292;&#23545;&#36825;&#20123;&#25968;&#25454;&#22359;&#36827;&#34892;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#24615;&#21644;&#23616;&#37096;&#32454;&#33410;&#20316;&#20026;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#33258;&#36866;&#24212;&#36335;&#24452;&#26469;&#20016;&#23500;&#22810;&#23610;&#24230;Transformer&#65292;&#35813;&#36335;&#24452;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#26102;&#38388;&#21160;&#24577;&#35843;&#25972;&#22810;&#23610;&#24230;&#24314;&#27169;&#36807;&#31243;&#65292;&#25552;&#39640;Pathformer&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;11&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved some success in time series forecasting. Existing methods mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. In this paper, we propose multi-scale transformers with adaptive pathways (Pathformer). The proposed Transformer integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics in the input time series, improving the prediction accuracy and generalization of Pathformer. Extensive experiments on eleven rea
&lt;/p&gt;</description></item><item><title>CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04620</link><description>&lt;p&gt;
CataractBot&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04620
&lt;/p&gt;
&lt;p&gt;
CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#34892;&#19994;&#30340;&#21457;&#23637;&#65292;&#24739;&#32773;&#36234;&#26469;&#36234;&#36861;&#27714;&#26356;&#21487;&#38752;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20449;&#24687;&#26469;&#28304;&#65292;&#20294;&#25968;&#23383;&#26102;&#20195;&#21364;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#36807;&#22810;&#19988;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#24739;&#32773;&#20027;&#35201;&#20449;&#20219;&#21307;&#29983;&#21644;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#65292;&#31361;&#26174;&#20102;&#19987;&#23478;&#35748;&#21487;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19987;&#23478;&#38754;&#20020;&#30340;&#21387;&#21147;&#23548;&#33268;&#20102;&#27807;&#36890;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CataractBot&#65292;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#19982;&#21360;&#24230;&#19968;&#23478;&#19977;&#32423;&#30524;&#31185;&#21307;&#38498;&#21512;&#20316;&#24320;&#21457;&#30340;CataractBot&#36890;&#36807;&#26597;&#35810;&#31574;&#21010;&#30340;&#30693;&#35782;&#24211;&#65292;&#21363;&#26102;&#22238;&#31572;&#30333;&#20869;&#38556;&#25163;&#26415;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#24322;&#27493;&#25552;&#20379;&#19987;&#23478;&#39564;&#35777;&#30340;&#31572;&#22797;&#12290;CataractBot&#20855;&#22791;&#22810;&#27169;&#24335;&#25903;&#25345;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#19982;49&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#65292;CataractBot&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.01930</link><description>&lt;p&gt;
&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;
&lt;/p&gt;
&lt;p&gt;
Reducing Optimism Bias in Incomplete Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#21327;&#21516;&#20915;&#31574;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25351;&#23450;&#19968;&#20010;&#21512;&#20316;&#21338;&#24328;&#38656;&#35201;&#20026;&#25351;&#25968;&#22810;&#20010;&#32852;&#30431;&#20998;&#37197;&#20215;&#20540;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#19968;&#20010;&#32852;&#30431;&#20215;&#20540;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#19981;&#20844;&#24320;&#26576;&#20123;&#32852;&#30431;&#30340;&#20215;&#20540;&#20250;&#24341;&#20837;&#20851;&#20110;&#20010;&#20307;&#23545;&#38598;&#20307;&#22823;&#32852;&#30431;&#30340;&#36129;&#29486;&#30340;&#27169;&#31946;&#24615;&#12290;&#36825;&#31181;&#27169;&#31946;&#24615;&#32463;&#24120;&#23548;&#33268;&#29609;&#23478;&#25345;&#26377;&#36807;&#20110;&#20048;&#35266;&#30340;&#26399;&#26395;&#65292;&#20854;&#28304;&#20110;&#20869;&#22312;&#20559;&#35265;&#25110;&#25112;&#30053;&#32771;&#34385;&#65292;&#36827;&#32780;&#24120;&#24120;&#23548;&#33268;&#38598;&#20307;&#35201;&#27714;&#36229;&#36807;&#23454;&#38469;&#30340;&#22823;&#32852;&#30431;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#32553;&#23567;&#21512;&#20316;&#21338;&#24328;&#20013;&#29609;&#23478;&#26399;&#26395;&#19982;&#21487;&#23454;&#29616;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2311.14220</link><description>&lt;p&gt;
&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Assumption-lean and Data-adaptive Post-Prediction Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#19988;&#26377;&#21147;&#22320;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31185;&#23398;&#30740;&#31350;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#26082;&#32791;&#36153;&#26102;&#38388;&#21448;&#36153;&#21147;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#31185;&#23398;&#23478;&#20204;&#20381;&#36182;&#20110;ML&#31639;&#27861;&#20351;&#29992;&#26131;&#24471;&#30340;&#21327;&#21464;&#37327;&#26469;&#39044;&#27979;&#36825;&#20123;&#40644;&#37329;&#26631;&#20934;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39044;&#27979;&#32467;&#26524;&#24120;&#24120;&#30452;&#25509;&#29992;&#20110;&#21518;&#32493;&#30340;&#32479;&#35745;&#20998;&#26512;&#20013;&#65292;&#24573;&#30053;&#20102;&#39044;&#27979;&#36807;&#31243;&#24341;&#20837;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#24322;&#36136;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#34394;&#20551;&#30340;&#27491;&#38754;&#32467;&#26524;&#21644;&#26080;&#25928;&#30340;&#31185;&#23398;&#32467;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20551;&#35774;&#31616;&#21270;&#21644;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#21518;&#39044;&#27979;&#25512;&#26029;&#65288;POP-Inf&#65289;&#36807;&#31243;&#65292;&#23427;&#20801;&#35768;&#22522;&#20110;ML&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#21644;&#26377;&#21147;&#30340;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#20551;&#35774;&#31616;&#21270;&#8221;&#23646;&#24615;&#20445;&#35777;&#22312;&#24191;&#27867;&#30340;&#32479;&#35745;&#37327;&#19978;&#19981;&#22522;&#20110;ML&#39044;&#27979;&#20570;&#20986;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#23427;&#30340;&#8220;&#25968;&#25454;&#33258;&#36866;&#24212;&#8221;&#29305;&#24615;&#20445;&#35777;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary challenge facing modern scientific research is the limited availability of gold-standard data which can be both costly and labor-intensive to obtain. With the rapid development of machine learning (ML), scientists have relied on ML algorithms to predict these gold-standard outcomes with easily obtained covariates. However, these predicted outcomes are often used directly in subsequent statistical analyses, ignoring imprecision and heterogeneity introduced by the prediction procedure. This will likely result in false positive findings and invalid scientific conclusions. In this work, we introduce an assumption-lean and data-adaptive Post-Prediction Inference (POP-Inf) procedure that allows valid and powerful inference based on ML-predicted outcomes. Its "assumption-lean" property guarantees reliable statistical inference without assumptions on the ML-prediction, for a wide range of statistical quantities. Its "data-adaptive'" feature guarantees an efficiency gain over existing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;</title><link>https://arxiv.org/abs/2310.10461</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#19979;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#30340;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Model Selection of Zero-shot Anomaly Detectors in the Absence of Labeled Validation Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#26631;&#31614;&#39564;&#35777;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#39564;&#35777;&#38598;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#39640;&#30340;AUROC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#38656;&#35201;&#22312;&#22823;&#22411;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#24322;&#24120;&#26679;&#26412;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#24120;&#24120;&#21463;&#21040;&#26631;&#31614;&#25968;&#25454;&#30340;&#32570;&#20047;&#30340;&#38480;&#21046; - &#22312;&#27809;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35780;&#20272;&#20854;&#26816;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;SWSA&#65288;Selection With Synthetic Anomalies&#65289;&#26469;&#36873;&#25321;&#22522;&#20110;&#22270;&#20687;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24322;&#24120;&#29983;&#25104;&#26041;&#27861;&#20551;&#35774;&#21482;&#26377;&#23569;&#37327;&#30340;&#27491;&#24120;&#22270;&#20687;&#25903;&#25345;&#38598;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#29983;&#25104;&#21518;&#65292;&#25105;&#20204;&#30340;&#21512;&#25104;&#39564;&#35777;&#38598;&#34987;&#29992;&#20110;&#21019;&#24314;&#27169;&#22411;&#36873;&#25321;&#30340;&#39564;&#35777;&#26694;&#26550;&#20013;&#30340;&#26816;&#27979;&#20219;&#21153;&#12290;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;SWSA&#24120;&#24120;&#36873;&#25321;&#19982;&#30495;&#23454;&#39564;&#35777;&#38598;&#36873;&#25321;&#30456;&#21305;&#37197;&#30340;&#27169;&#22411;&#65292;&#32467;&#26524;&#27604;&#22522;&#32447;&#26041;&#27861;&#30340;AUROC&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection requires detecting abnormal samples in large unlabeled datasets. While progress in deep learning and the advent of foundation models has produced powerful zero-shot anomaly detection methods, their deployment in practice is often hindered by the lack of labeled data -- without it, their detection performance cannot be evaluated reliably. In this work, we propose SWSA (Selection With Synthetic Anomalies): a general-purpose framework to select image-based anomaly detectors with a generated synthetic validation set. Our proposed anomaly generation method assumes access to only a small support set of normal images and requires no training or fine-tuning. Once generated, our synthetic validation set is used to create detection tasks that compose a validation framework for model selection. In an empirical study, we find that SWSA often selects models that match selections made with a ground-truth validation set, resulting in higher AUROCs than baseline methods. We also find
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2401.12830</link><description>&lt;p&gt;
&#25552;&#21319;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#30495;&#23454;&#33322;&#31354;&#25968;&#25454;&#30340;&#26032;&#39062;LSTM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12830
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#25552;&#21319;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#20248;&#21270;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#20132;&#36890;&#34892;&#19994;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#26053;&#34892;&#32773;&#30340;&#19979;&#19968;&#20010;&#30446;&#30340;&#22320;&#20026;&#20844;&#21496;&#24102;&#26469;&#24456;&#22810;&#22909;&#22788;&#65292;&#20363;&#22914;&#25552;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#21644;&#23450;&#21521;&#33829;&#38144;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20934;&#30830;&#25429;&#25417;&#26053;&#34892;&#25968;&#25454;&#20013;&#30340;&#24207;&#21015;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#20010;&#20154;&#26053;&#34892;&#32773;&#26410;&#26469;&#30446;&#30340;&#22320;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#30340;&#26032;&#39062;&#27169;&#22411;&#26550;&#26500;&#65292;&#29992;&#20110;&#20132;&#36890;&#19994;&#20013;&#30340;&#30446;&#30340;&#22320;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#21644;&#24615;&#33021;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#21644;&#39640;&#20998;&#25968;&#12290;&#26412;&#30740;&#31350;&#22312;&#25512;&#36827;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20351;&#20844;&#21496;&#33021;&#22815;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#24182;&#20248;&#21270;&#21160;&#24577;&#26053;&#34892;&#29615;&#22659;&#20013;&#30340;&#23458;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern transportation industry, accurate prediction of travelers' next destinations brings multiple benefits to companies, such as customer satisfaction and targeted marketing. This study focuses on developing a precise model that captures the sequential patterns and dependencies in travel data, enabling accurate predictions of individual travelers' future destinations. To achieve this, a novel model architecture with a sliding window approach based on Long Short-Term Memory (LSTM) is proposed for destination prediction in the transportation industry. The experimental results highlight satisfactory performance and high scores achieved by the proposed model across different data sizes and performance metrics. This research contributes to advancing destination prediction methods, empowering companies to deliver personalized recommendations and optimize customer experiences in the dynamic travel landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.07187</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#32508;&#36848;&#65306;&#36817;&#20284;&#65292;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models. (arXiv:2401.07187v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#32479;&#35745;&#29702;&#35770;&#65292;&#21253;&#25324;&#36817;&#20284;&#26041;&#27861;&#12289;&#35757;&#32451;&#21160;&#24577;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#38750;&#21442;&#25968;&#26694;&#26550;&#20013;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#32593;&#32476;&#20197;&#25214;&#21040;&#33391;&#22909;&#30340;&#27867;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#35282;&#24230;&#22238;&#39038;&#20102;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#32479;&#35745;&#29702;&#35770;&#30340;&#25991;&#29486;&#12290;&#31532;&#19968;&#37096;&#20998;&#22238;&#39038;&#20102;&#22312;&#22238;&#24402;&#25110;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#19979;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#36807;&#24230;&#39118;&#38505;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#32467;&#26524;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#24335;&#26500;&#36896;&#65292;&#20197;&#21450;&#37319;&#29992;&#20102;&#36817;&#20284;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#23548;&#33268;&#36807;&#24230;&#39118;&#38505;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#29575;&#12290;&#36890;&#36807;&#36825;&#20123;&#26500;&#36896;&#65292;&#21487;&#20197;&#29992;&#26679;&#26412;&#22823;&#23567;&#12289;&#25968;&#25454;&#32500;&#24230;&#21644;&#20989;&#25968;&#24179;&#28369;&#24615;&#26469;&#34920;&#36798;&#32593;&#32476;&#30340;&#23485;&#24230;&#21644;&#28145;&#24230;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#22522;&#26412;&#20998;&#26512;&#20165;&#36866;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#38750;&#20984;&#30340;&#20840;&#23616;&#26497;&#23567;&#20540;&#28857;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#31532;&#20108;&#37096;&#20998;&#22238;&#39038;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#37027;&#20123;&#35797;&#22270;&#22238;&#31572;&#8220;&#22522;&#20110;&#26799;&#24230;&#26041;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#25214;&#21040;&#33021;&#22815;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#35299;&#8221;&#30340;&#35770;&#25991;&#12290;&#23588;&#20854;&#26159;&#20004;&#20010;&#30693;&#21517;&#30340;
&lt;/p&gt;
&lt;p&gt;
In this article, we review the literature on statistical theories of neural networks from three perspectives. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression or classification. These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks, in that tools from the approximation theory are adopted. Through these constructions, the width and depth of the networks can be expressed in terms of sample size, data dimension, and function smoothness. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-know
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26222;&#36866;&#19988;&#26080;&#38656;&#20551;&#35774;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#20851;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.04778</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Generative neural networks for characteristic functions. (arXiv:2401.04778v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26222;&#36866;&#19988;&#26080;&#38656;&#20551;&#35774;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#20851;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#31639;&#27861;&#26469;&#20174;&#19968;&#20010;&#65288;&#22810;&#20803;&#65289;&#29305;&#24449;&#20989;&#25968;&#20013;&#27169;&#25311;&#65292;&#35813;&#29305;&#24449;&#20989;&#25968;&#20165;&#20197;&#40657;&#30418;&#26684;&#24335;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#30340;&#29305;&#23450;&#34920;&#31034;&#65292;&#30452;&#25509;&#32467;&#21512;&#30446;&#26631;&#29305;&#24449;&#20989;&#25968;&#12290;&#36825;&#31181;&#26500;&#36896;&#20855;&#26377;&#26222;&#36941;&#24615;&#65292;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23545;&#32473;&#23450;&#29305;&#24449;&#20989;&#25968;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#36824;&#24471;&#20986;&#20102;&#20851;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#30340;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30701;&#26399;&#27169;&#25311;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a simulation algorithm to simulate from a (multivariate) characteristic function, which is only accessible in a black-box format. We construct a generative neural network, whose loss function exploits a specific representation of the Maximum-Mean-Discrepancy metric to directly incorporate the targeted characteristic function. The construction is universal in the sense that it is independent of the dimension and that it does not require any assumptions on the given characteristic function. Furthermore, finite sample guarantees on the approximation quality in terms of the Maximum-Mean Discrepancy metric are derived. The method is illustrated in a short simulation study.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02080</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#29992;&#20110;&#39640;&#25928;&#37319;&#26679;Boltzmann&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Energy based diffusion generator for efficient sampling of Boltzmann distributions. (arXiv:2401.02080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02080
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#33021;&#37327;&#30340;&#25193;&#25955;&#29983;&#25104;&#22120;&#30340;&#26032;&#22411;&#37319;&#26679;&#22120;&#65292;&#29992;&#20110;&#20174;&#20219;&#24847;&#30446;&#26631;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#37319;&#26679;&#27169;&#22411;&#37319;&#29992;&#31867;&#20284;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#21033;&#29992;&#35299;&#30721;&#22120;&#23558;&#26469;&#33258;&#31616;&#21333;&#20998;&#24067;&#30340;&#28508;&#22312;&#21464;&#37327;&#36716;&#25442;&#20026;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#12290;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#22797;&#26434;&#20998;&#24067;&#30340;&#24378;&#22823;&#24314;&#27169;&#33021;&#21147;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#29983;&#25104;&#26679;&#26412;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#20934;&#30830;&#21464;&#20998;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24191;&#20041;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#35299;&#30721;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22797;&#26434;&#20998;&#24067;&#20989;&#25968;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel sampler called the energy based diffusion generator for generating samples from arbitrary target distributions. The sampling model employs a structure similar to a variational autoencoder, utilizing a decoder to transform latent variables from a simple distribution into random variables approximating the target distribution, and we design an encoder based on the diffusion model. Leveraging the powerful modeling capacity of the diffusion model for complex distributions, we can obtain an accurate variational estimate of the Kullback-Leibler divergence between the distributions of the generated samples and the target. Moreover, we propose a decoder based on generalized Hamiltonian dynamics to further enhance sampling performance. Through empirical evaluation, we demonstrate the effectiveness of our method across various complex distribution functions, showcasing its superiority compared to existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.08847</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#12289;&#40065;&#26834;&#21644;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Over-Memorization During Natural, Robust and Catastrophic Overfitting. (arXiv:2310.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#22312;&#33258;&#28982;&#35757;&#32451;&#36824;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38590;&#20197;&#19968;&#33268;&#22320;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#36890;&#24120;&#35774;&#35745;&#20102;&#38024;&#23545;&#33258;&#28982;&#27169;&#24335;&#25110;&#23545;&#25239;&#27169;&#24335;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#20165;&#20851;&#27880;&#33258;&#28982;&#27169;&#24335;&#65292;&#21435;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DNN&#20013;&#30340;&#35760;&#24518;&#25928;&#24212;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#36807;&#24230;&#35760;&#24518;&#30340;&#20849;&#21516;&#34892;&#20026;&#65292;&#36825;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20026;DNN&#31361;&#28982;&#23545;&#26576;&#20123;&#35757;&#32451;&#27169;&#24335;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#65292;&#24182;&#23545;&#20854;&#20445;&#25345;&#25345;&#20037;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#24403;DNN&#36807;&#24230;&#35760;&#24518;&#19968;&#31181;&#23545;&#25239;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#21516;&#26102;&#23637;&#29616;&#20986;&#23545;&#24212;&#33258;&#28982;&#27169;&#24335;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#36825;&#20123;&#21457;&#29616;&#28608;&#21169;&#25105;&#20204;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#38459;&#30861;&#36807;&#24230;&#35760;&#24518;&#34892;&#20026;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. In this work, we adopt a unified perspective by solely focusing on natural patterns to explore different types of overfitting. Specifically, we examine the memorization effect in DNNs and reveal a shared behaviour termed over-memorization, which impairs their generalization capacity. This behaviour manifests as DNNs suddenly becoming high-confidence in predicting certain training patterns and retaining a persistent memory for them. Furthermore, when DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit high-confidence prediction for the corresponding natural pattern. These findings motivate us to holistically mitigate different types of overfitting by hinder
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07745</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey. (arXiv:2310.07745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07745
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#24212;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#32593;&#32476;&#25915;&#20987;&#25968;&#37327;&#30340;&#24555;&#36895;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;&#38024;&#23545;&#24694;&#24847;&#34892;&#20026;&#30340;&#32593;&#32476;&#38450;&#24481;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;DRL&#22312;&#32593;&#32476;&#38450;&#24481;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23558;DRL&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#34892;&#21160;&#65288;ACO&#65289;&#20043;&#21069;&#65292;&#36824;&#38656;&#35201;&#20811;&#26381;&#35768;&#22810;&#25361;&#25112;&#12290;&#38656;&#35201;&#20026;&#19982;&#23398;&#20064;&#32773;&#38754;&#23545;&#38750;&#24120;&#39640;&#32500;&#24230;&#29366;&#24577;&#31354;&#38388;&#12289;&#22823;&#35268;&#27169;&#22810;&#31163;&#25955;&#25805;&#20316;&#31354;&#38388;&#21644;&#23545;&#25239;&#23398;&#20064;&#30456;&#36935;&#30340;&#29615;&#22659;&#25552;&#20379;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#38024;&#23545;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#20063;&#36827;&#34892;&#20102;&#21360;&#35937;&#28145;&#21051;&#30340;&#24037;&#31243;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#23436;&#25972;&#30340;ACO&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;DRL&#25991;&#29486;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26500;&#24819;&#20102;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;ACO-DRL&#20195;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#65306;i.) &#39046;&#22495;&#29305;&#24615;&#30340;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber-defence, numerous challenges must be overcome before DRL can be applied to autonomous cyber-operations (ACO) at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACO problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACO-DRL agent. We provide: i.) A summary of the domain properties t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.11295</link><description>&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#25299;&#25169;&#20998;&#26512;&#26469;&#20272;&#31639;Transformer&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices. (arXiv:2308.11295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#30830;&#23450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#23545;&#20110;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#24182;&#19981;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20219;&#21153;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#28857;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#25903;&#25345;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20196;&#29260;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#21033;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#25506;&#32034;&#20869;&#37096;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25299;&#25169;&#24615;&#36136;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#36767;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#38656;&#35201;...
&lt;/p&gt;
&lt;p&gt;
Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#21644;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#65292;&#21152;&#24555;&#20102;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17267</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#40065;&#26834;&#30340;&#20998;&#23618;&#23398;&#20064;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Fast and Robust State Estimation and Tracking via Hierarchical Learning. (arXiv:2306.17267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#21644;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#65292;&#21152;&#24555;&#20102;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#20195;&#29702;&#32593;&#32476;&#30340;&#23436;&#20840;&#20998;&#24067;&#24335;&#20272;&#35745;&#21644;&#36319;&#36394;&#35299;&#20915;&#26041;&#26696;&#25910;&#25947;&#36895;&#24230;&#24930;&#19988;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25925;&#38556;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#20998;&#23618;&#31995;&#32479;&#26550;&#26500;&#26469;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#35813;&#26550;&#26500;&#20013;&#65292;&#20195;&#29702;&#34987;&#20998;&#25104;&#36739;&#23567;&#30340;&#32593;&#32476;&#65292;&#19968;&#20010;&#21442;&#25968;&#26381;&#21153;&#22120;&#29992;&#20110;&#24110;&#21161;&#32593;&#32476;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#12290;&#32593;&#32476;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#20195;&#20215;&#39640;&#19988;&#36739;&#23569;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#29366;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#38382;&#39064;&#30340;&#20849;&#35782;+&#21019;&#26032;&#31639;&#27861;&#12290;&#22312;&#36825;&#20004;&#20010;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#25512;&#36865;&#21644;&#27714;&#21644;&#20849;&#35782;&#32452;&#20214;&#12290;&#23545;&#20110;&#29366;&#24577;&#20272;&#35745;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21452;&#24179;&#22343;&#20316;&#20026;&#23616;&#37096;&#21019;&#26032;&#32452;&#20214;&#12290;&#22312;&#23384;&#22312;&#26029;&#38142;&#25925;&#38556;&#30340;&#24773;&#20917;&#19979;&#65292;&#29366;&#24577;&#36319;&#36394;&#26356;&#21152;&#22256;&#38590;&#65292;&#32780;&#26631;&#20934;&#30340;&#20849;&#35782;&#21644;&#21019;&#26032;&#26041;&#27861;&#30340;&#38598;&#25104;&#19981;&#20877;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully distributed estimation and tracking solutions to large-scale multi-agent networks suffer slow convergence and are vulnerable to network failures. In this paper, we aim to speed up the convergence and enhance the resilience of state estimation and tracking using a simple hierarchical system architecture wherein agents are clusters into smaller networks, and a parameter server exists to aid the information exchanges among networks. The information exchange among networks is expensive and occurs only once in a while.  We propose two consensus + innovation algorithms for the state estimation and tracking problems, respectively. In both algorithms, we use a novel hierarchical push-sum consensus component. For the state estimation, we use dual averaging as the local innovation component. State tracking is much harder to tackle in the presence of dropping-link failures and the standard integration of the consensus and innovation approaches are no longer applicable. Moreover, dual averag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.05649</link><description>&lt;p&gt;
&#20351;&#29992;CVXPY&#35268;&#23450;&#21644;&#35299;&#20915;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Specifying and Solving Robust Empirical Risk Minimization Problems Using CVXPY. (arXiv:2306.05649v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#40065;&#26834;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#22320;&#35299;&#20915;&#21508;&#31181;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#40065;&#26834;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#21442;&#25968;&#34987;&#36873;&#20026;&#20351;&#24471;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#32473;&#23450;&#30340;&#20984;&#19981;&#30830;&#23450;&#24615;&#38598;&#20869;&#21464;&#21270;&#26102;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#32463;&#39564;&#25439;&#22833;&#12290;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34920;&#36798;&#20026;&#35299;&#26512;&#24418;&#24335;&#12290;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#20598;&#21270;&#20351;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#65292;&#36825;&#23558;&#19968;&#20010;min-max&#38382;&#39064;&#36716;&#25442;&#20026;&#19968;&#20010;min-min&#38382;&#39064;&#12290;&#23545;&#20598;&#21270;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#24456;&#28902;&#29712;&#20063;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CVXPY&#20197;&#29992;&#25143;&#21451;&#22909;&#30340;&#26041;&#24335;&#33258;&#21160;&#21270;&#36825;&#20010;&#23545;&#20598;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20174;&#19968;&#20010;&#19968;&#33324;&#30340;&#20984;&#25439;&#22833;&#31867;&#20013;&#25429;&#25417;&#35768;&#22810;&#26631;&#20934;&#30340;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#19988;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#25351;&#23450;&#20219;&#20309;&#21487;&#20197;&#29992;&#32426;&#24459;&#21270;&#20984;&#35268;&#21010;&#65288;DCP&#65289;&#32422;&#26463;&#34920;&#31034;&#30340;&#22797;&#26434;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider robust empirical risk minimization (ERM), where model parameters are chosen to minimize the worst-case empirical loss when each data point varies over a given convex uncertainty set. In some simple cases, such problems can be expressed in an analytical form. In general the problem can be made tractable via dualization, which turns a min-max problem into a min-min problem. Dualization requires expertise and is tedious and error-prone. We demonstrate how CVXPY can be used to automate this dualization procedure in a user-friendly manner. Our framework allows practitioners to specify and solve robust ERM problems with a general class of convex losses, capturing many standard regression and classification problems. Users can easily specify any complex uncertainty set that is representable via disciplined convex programming (DCP) constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#35299;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#33719;&#24471;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#21487;&#20197;&#33719;&#24471;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.07316</link><description>&lt;p&gt;
&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#25239;&#24178;&#25200;&#32858;&#31867;&#38382;&#39064;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Parameterized Approximation for Robust Clustering in Discrete Geometric Spaces. (arXiv:2305.07316v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#31163;&#25955;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#35299;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#33719;&#24471;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#21487;&#20197;&#33719;&#24471;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Robust $(k, z)$-Clustering&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#40065;&#26834;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#39046;&#22495;&#20013;&#12290;&#24050;&#30693;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35813;&#38382;&#39064;&#20855;&#26377;$O(\log m/\log\log m)$&#30340;&#36817;&#20284;&#22240;&#23376;&#65292;&#22312;FPT&#26102;&#38388;&#20869;&#20855;&#26377;$(3^z+\epsilon)$&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the well-studied Robust $(k, z)$-Clustering problem, which generalizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a constant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$ weighted points in a metric space $(M,\delta)$ and a positive integer $k$. Further, each point belongs to one (or more) of the $m$ many different groups $S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that $\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized.  This problem arises in the domains of robust optimization [Anthony, Goyal, Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For polynomial time computation, an approximation factor of $O(\log m/\log\log m)$ is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible complexity assumption even in the line metrics. For FPT time, there is a $(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal, Jaiswal, In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03890</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#32593;&#32476;&#36924;&#36817;&#29992;&#20110;&#36328;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20247;&#22810;&#36807;&#31243;&#65288;&#22914;&#65306;&#27973;&#23618;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#21644;&#21508;&#31181;&#20869;&#26680;&#26041;&#27861;&#65289;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#34920;&#36798;&#33021;&#21147;&#65289;&#30740;&#31350;&#20013;&#20419;&#36827;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21464;&#23398;&#20064;&#12289;&#20256;&#36882;&#23398;&#20064;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#31561;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#26469;&#30740;&#31350;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#32452;&#20869;&#26680;&#30340;&#26356;&#19968;&#33324;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#24179;&#31227;&#32593;&#32476;&#65288;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#31227;&#19981;&#21464;&#26680;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65289;&#21644;&#26059;&#36716;&#21306;&#20989;&#25968;&#26680;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#35201;&#27714;&#20869;&#26680;&#26159;&#27491;&#23450;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#21487;&#33021;&#22312;&#20998;&#24067;&#19978;&#19981;&#21516;&#30340;&#36328;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;FW&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#31639;&#27861;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;&#20984;&#20989;&#25968;&#21644;&#38750;&#20984;&#20989;&#25968;&#65292;&#19988;&#20855;&#26377;&#26368;&#20339;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#20004;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#27704;&#20037;&#25910;&#38598;&#22823;&#25209;&#25968;&#25454;&#21644;&#20840;&#30830;&#23450;&#24615;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11737</link><description>&lt;p&gt;
Sarah Frank-Wolfe&#65306;&#20855;&#26377;&#26368;&#20339;&#36895;&#29575;&#21644;&#23454;&#29992;&#29305;&#28857;&#30340;&#32422;&#26463;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates and Practical Features. (arXiv:2304.11737v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#38543;&#26426;FW&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#31639;&#27861;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;&#20984;&#20989;&#25968;&#21644;&#38750;&#20984;&#20989;&#25968;&#65292;&#19988;&#20855;&#26377;&#26368;&#20339;&#25910;&#25947;&#20445;&#35777;&#12290;&#21516;&#26102;&#20004;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#27704;&#20037;&#25910;&#38598;&#22823;&#25209;&#25968;&#25454;&#21644;&#20840;&#30830;&#23450;&#24615;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frank-Wolfe&#65288;FW&#65289;&#26041;&#27861;&#26159;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20986;&#29616;&#30340;&#32467;&#26500;&#21270;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#21040;&#22823;&#25968;&#25454;&#38598;&#30340;&#21551;&#21457;&#65292;FW&#30340;&#38543;&#26426;&#29256;&#26412;&#21464;&#24471;&#26356;&#21152;&#27969;&#34892;&#65292;&#22240;&#20026;&#35745;&#31639;&#20840;&#26799;&#24230;&#20195;&#20215;&#36807;&#39640;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;FW&#38543;&#26426;&#26377;&#38480;&#21644;&#26368;&#23567;&#21270;&#31639;&#27861;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26082;&#36866;&#29992;&#20110;&#20984;&#20989;&#25968;&#21448;&#36866;&#29992;&#20110;&#38750;&#20984;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#23384;&#22312;&#27704;&#20037;&#25910;&#38598;&#22823;&#25209;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#35768;&#22810;&#25237;&#24433;&#26080;&#32422;&#26463;&#38543;&#26426;&#26041;&#27861;&#30340;&#20849;&#21516;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31532;&#20108;&#31181;&#26041;&#27861;&#26082;&#19981;&#38656;&#35201;&#22823;&#25209;&#37327;&#30340;&#25968;&#25454;&#20063;&#19981;&#38656;&#35201;&#20840;&#30830;&#23450;&#24615;&#26799;&#24230;&#65292;&#36825;&#26159;&#35768;&#22810;&#26377;&#38480;&#21644;&#38382;&#39064;&#25216;&#26415;&#30340;&#20856;&#22411;&#24369;&#28857;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26356;&#24555;&#25910;&#25947;&#36895;&#24230;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Frank-Wolfe (FW) method is a popular approach for solving optimization problems with structured constraints that arise in machine learning applications. In recent years, stochastic versions of FW have gained popularity, motivated by large datasets for which the computation of the full gradient is prohibitively expensive. In this paper, we present two new variants of the FW algorithms for stochastic finite-sum minimization. Our algorithms have the best convergence guarantees of existing stochastic FW approaches for both convex and non-convex objective functions. Our methods do not have the issue of permanently collecting large batches, which is common to many stochastic projection-free approaches. Moreover, our second approach does not require either large batches or full deterministic gradients, which is a typical weakness of many techniques for finite-sum problems. The faster theoretical rates of our approaches are confirmed experimentally.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.11777</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#20301;&#24212;&#29992;&#30340;&#36335;&#24452;&#25439;&#32791;&#21644;&#21040;&#36798;&#26102;&#38388;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dataset of Pathloss and ToA Radio Maps With Localization Application. (arXiv:2212.11777v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#29983;&#25104;&#24182;&#20844;&#24320;&#25552;&#20379;&#30340;&#19968;&#32452;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;/&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#21644;&#21040;&#36798;&#26102;&#38388;&#65288;ToA&#65289;&#26080;&#32447;&#22320;&#22270;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#30340;&#31264;&#23494;&#22478;&#24066;&#35774;&#32622;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#26159;1&#65289;&#20174;&#36755;&#20837;&#30340;&#22478;&#24066;&#22320;&#22270;&#39044;&#27979;&#36335;&#24452;&#25439;&#32791;&#30340;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#25311;&#65289;&#65292;&#20197;&#21450;2&#65289;&#26080;&#32447;&#23450;&#20301;&#12290;RSS&#21644;ToA&#22320;&#22270;&#36890;&#36807;&#30456;&#21516;&#30340;&#27169;&#25311;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#20986;&#65292;&#21487;&#20197;&#23545;&#22522;&#20110;RSS&#21644;ToA&#30340;&#23450;&#20301;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a collection of radio map datasets in dense urban setting, which we generated and made publicly available. The datasets include simulated pathloss/received signal strength (RSS) and time of arrival (ToA) radio maps over a large collection of realistic dense urban setting in real city maps. The two main applications of the presented dataset are 1) learning methods that predict the pathloss from input city maps (namely, deep learning-based simulations), and, 2) wireless localization. The fact that the RSS and ToA maps are computed by the same simulations over the same city maps allows for a fair comparison of the RSS and ToA-based localization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2211.15856</link><description>&lt;p&gt;
&#36229;&#36234;&#38598;&#21512;&#24179;&#22343;&#20540;&#65306;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Beyond Ensemble Averages: Leveraging Climate Model Ensembles for Subseasonal Forecasting. (arXiv:2211.15856v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24179;&#22343;&#26041;&#27861;&#65292;&#21033;&#29992;&#38598;&#21512;&#39044;&#27979;&#20013;&#30340;&#20449;&#24687;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20851;&#27880;&#20102;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#24615;&#39044;&#27979;&#20013;&#65292;&#23545;&#20110;&#20851;&#38190;&#27668;&#20505;&#21464;&#37327;&#65288;&#22914;&#28201;&#24230;&#21644;&#38477;&#27700;&#65289;&#36827;&#34892;&#39640;&#36136;&#37327;&#30340;&#22320;&#34920;&#23395;&#33410;&#24615;&#26102;&#38388;&#23610;&#24230;&#39044;&#27979;&#19968;&#30452;&#23384;&#22312;&#30528;&#24046;&#36317;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#25512;&#36827;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#65288;SSF&#65289;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#20102;&#29289;&#29702;&#22522;&#20110;&#27169;&#22411;&#38598;&#21512;&#30340;&#24179;&#22343;&#20316;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#28982;&#32780;&#38598;&#21512;&#39044;&#27979;&#20013;&#21253;&#21547;&#20102;&#21487;&#20197;&#24110;&#21161;&#39044;&#27979;&#30340;&#20449;&#24687;&#65292;&#19981;&#20165;&#20165;&#26159;&#38598;&#21512;&#22343;&#20540;&#12290;&#20854;&#27425;&#65292;&#36807;&#21435;&#30340;&#26041;&#27861;&#20851;&#27880;&#24179;&#22343;&#24615;&#33021;&#65292;&#28982;&#32780;&#23545;&#20110;&#35745;&#21010;&#21644;&#20943;&#28798;&#30446;&#30340;&#26469;&#35828;&#65292;&#26497;&#31471;&#20107;&#20214;&#30340;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#31532;&#19977;&#65292;&#27668;&#20505;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#20010;&#31354;&#38388;&#21464;&#21270;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#32780;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#32771;&#34385;&#20102;&#21709;&#24212;&#30340;&#31354;&#38388;&#21487;&#21464;&#24615;&#12290;&#27169;&#22411;&#22534;&#21472;&#21487;&#20197;&#32531;&#35299;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#38598;&#21512;&#36827;&#34892;&#22320;&#34920;&#23395;&#33410;&#24615;&#39044;&#27979;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Producing high-quality forecasts of key climate variables such as temperature and precipitation on subseasonal time scales has long been a gap in operational forecasting. Recent studies have shown promising results using machine learning (ML) models to advance subseasonal forecasting (SSF), but several open questions remain. First, several past approaches use the average of an ensemble of physics-based forecasts as an input feature of these models. However, ensemble forecasts contain information that can aid prediction beyond only the ensemble mean. Second, past methods have focused on average performance, whereas forecasts of extreme events are far more important for planning and mitigation purposes. Third, climate forecasts correspond to a spatially-varying collection of forecasts, and different methods account for spatial variability in the response differently. Trade-offs between different approaches may be mitigated with model stacking. This paper describes the application of a va
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;</title><link>http://arxiv.org/abs/2210.01672</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#23558;&#26426;&#22120;&#20154;&#20998;&#31867;&#24102;&#20837;&#36830;&#32493;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Bringing robotics taxonomies to continuous domains via GPLVM on hyperbolic manifolds. (arXiv:2210.01672v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#21452;&#26354;&#27969;&#24418;&#19978;&#20351;&#29992;GPLVM&#26469;&#22312;&#36830;&#32493;&#39046;&#22495;&#20013;&#24212;&#29992;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#26469;&#23454;&#29616;&#20998;&#31867;&#27861;&#32467;&#26500;&#30340;&#32435;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20998;&#31867;&#34987;&#29992;&#20316;&#23558;&#20154;&#31867;&#30340;&#31227;&#21160;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#39640;&#23618;&#27425;&#30340;&#20998;&#23618;&#25277;&#35937;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20998;&#26512;&#25235;&#21462;&#12289;&#25805;&#32437;&#25216;&#33021;&#21644;&#20840;&#36523;&#25903;&#25745;&#23039;&#21183;&#38750;&#24120;&#26377;&#29992;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#35774;&#35745;&#23618;&#27425;&#32467;&#26500;&#21644;&#22522;&#30784;&#31867;&#21035;&#26041;&#38754;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#20351;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#32570;&#20047;&#22635;&#34917;&#20998;&#31867;&#23618;&#32423;&#32467;&#26500;&#21644;&#19982;&#20854;&#31867;&#21035;&#30456;&#20851;&#32852;&#30340;&#39640;&#32500;&#24322;&#26500;&#25968;&#25454;&#20043;&#38388;&#24046;&#36317;&#30340;&#35745;&#31639;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25429;&#25417;&#30456;&#20851;&#23618;&#27425;&#32467;&#26500;&#30340;&#21452;&#26354;&#23884;&#20837;&#26469;&#24314;&#27169;&#20998;&#31867;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#39640;&#26031;&#36807;&#31243;&#21452;&#26354;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#22270;&#24418;&#20808;&#39564;&#21644;&#20445;&#25345;&#36317;&#31163;&#30340;&#21518;&#21521;&#32422;&#26463;&#23558;&#20998;&#31867;&#27861;&#32467;&#26500;&#32435;&#20837;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#20154;&#20998;&#31867;&#27861;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic taxonomies serve as high-level hierarchical abstractions that classify how humans move and interact with their environment. They have proven useful to analyse grasps, manipulation skills, and whole-body support poses. Despite substantial efforts devoted to design their hierarchy and underlying categories, their use in application fields remains limited. This may be attributed to the lack of computational models that fill the gap between the discrete hierarchical structure of the taxonomy and the high-dimensional heterogeneous data associated to its categories. To overcome this problem, we propose to model taxonomy data via hyperbolic embeddings that capture the associated hierarchical structure. We achieve this by formulating a novel Gaussian process hyperbolic latent variable model that incorporates the taxonomy structure through graph-based priors on the latent space and distance-preserving back constraints. We validate our model on three different robotics taxonomies to lear
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2205.12706</link><description>&lt;p&gt;
&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy on Exponential Windows for Online Change Detection. (arXiv:2205.12706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Maximum Mean Discrepancy on Exponential Windows (MMDEW) algorithm for online change detection, which efficiently detects changes in data streams.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25968;&#25454;&#27969;&#26102;&#65292;&#26816;&#27979;&#21464;&#21270;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#27450;&#35784;&#26816;&#27979;&#25110;&#21307;&#23398;&#12290;&#19968;&#31181;&#26816;&#27979;&#21464;&#21270;&#30340;&#21407;&#21017;&#26041;&#27861;&#26159;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#23558;&#27969;&#20013;&#35266;&#27979;&#20540;&#30340;&#20998;&#24067;&#30456;&#20114;&#27604;&#36739;&#12290;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65307;&#20063;&#31216;&#20026;&#33021;&#37327;&#36317;&#31163;&#65289;&#26159;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#20247;&#25152;&#21608;&#30693;&#30340;&#65288;&#21322;&#65289;&#24230;&#37327;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;MMD&#22312;&#26680;&#23500;&#38598;&#22495;&#19978;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#38750;&#21442;&#25968;&#20004;&#26679;&#26412;&#26816;&#39564;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#21464;&#21270;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#21487;&#21462;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;MMD&#20272;&#35745;&#22120;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#36825;&#31105;&#27490;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDEW&#65289;&#65292;&#23427;&#21033;&#29992;MMD&#20004;&#26679;&#26412;&#26816;&#39564;&#65292;&#22312;&#20219;&#20309;&#26680;&#23500;&#38598;&#22495;&#19978;&#20419;&#36827;&#20854;&#26377;&#25928;&#30340;&#22312;&#32447;&#35745;&#31639;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting changes is of fundamental importance when analyzing data streams and has many applications, e.g., predictive maintenance, fraud detection, or medicine. A principled approach to detect changes is to compare the distributions of observations within the stream to each other via hypothesis testing. Maximum mean discrepancy (MMD; also called energy distance) is a well-known (semi-)metric on the space of probability distributions. MMD gives rise to powerful non-parametric two-sample tests on kernel-enriched domains under mild conditions, which makes its deployment for change detection desirable. However, the classic MMD estimators suffer quadratic complexity, which prohibits their application in the online change detection setting. We propose a general-purpose change detection algorithm, Maximum Mean Discrepancy on Exponential Windows (MMDEW), which leverages the MMD two-sample test, facilitates its efficient online computation on any kernel-enriched domain, and is able to detect a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;&#21457;&#29616;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#36890;&#24120;&#27604;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#27979;&#35797;&#22312;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/1910.08883</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and universally consistent k-sample tests. (arXiv:1910.08883v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;&#21457;&#29616;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#36890;&#24120;&#27604;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#27979;&#35797;&#22312;&#39640;&#26031;&#20998;&#24067;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
k&#26679;&#26412;&#26816;&#39564;&#38382;&#39064;&#28041;&#21450;&#30830;&#23450;$k$&#32452;&#25968;&#25454;&#28857;&#26159;&#21542;&#37117;&#26469;&#33258;&#21516;&#19968;&#20010;&#20998;&#24067;&#12290;&#23613;&#31649;&#22810;&#20803;&#26041;&#24046;&#20998;&#26512;(MANOVA)&#26159;&#29983;&#29289;&#21307;&#23398;&#20013;&#24120;&#29992;&#30340;k&#26679;&#26412;&#26816;&#39564;&#26041;&#27861;&#65292;&#20294;&#23427;&#20381;&#36182;&#20110;&#24378;&#22823;&#19988;&#36890;&#24120;&#19981;&#21512;&#36866;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#21644;k&#26679;&#26412;&#27979;&#35797;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20123;&#26222;&#36941;&#19968;&#33268;&#30340;&#39640;&#32500;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#22914;&#36317;&#31163;&#30456;&#20851;(Discrepancy)&#21644;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;(Hsic)&#65292;&#20855;&#26377;&#22362;&#23454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29420;&#31435;&#24615;&#27979;&#35797;&#23454;&#29616;&#20102;&#26222;&#36941;&#19968;&#33268;&#30340;k&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#19988;k&#26679;&#26412;&#32479;&#35745;&#37327;&#65292;&#22914;Energy&#21644;Maximum Mean Discrepancy(MMD)&#65292;&#19982;Discrepancy&#23436;&#20840;&#31561;&#20215;&#12290;&#23545;&#38750;&#21442;&#25968;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#27969;&#34892;&#30340;MANOVA&#27979;&#35797;&#34920;&#29616;&#26356;&#22909;&#65292;&#21363;&#20351;&#22312;&#39640;&#26031;&#20998;&#24067;&#30340;&#22330;&#26223;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
The k-sample testing problem involves determining whether $k$ groups of data points are each drawn from the same distribution. The standard method for k-sample testing in biomedicine is Multivariate analysis of variance (MANOVA), despite that it depends on strong, and often unsuitable, parametric assumptions. Moreover, independence testing and k-sample testing are closely related, and several universally consistent high-dimensional independence tests such as distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic) enjoy solid theoretical and empirical properties. In this paper, we prove that independence tests achieve universally consistent k-sample testing and that k-sample statistics such as Energy and Maximum Mean Discrepancy (MMD) are precisely equivalent to Dcorr. An empirical evaluation of nonparametric independence tests showed that they generally perform better than the popular MANOVA test, even in Gaussian distributed scenarios. The evaluation included se
&lt;/p&gt;</description></item></channel></rss>