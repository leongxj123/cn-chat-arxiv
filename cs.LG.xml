<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.13612</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#33021;&#23548;&#33268;&#21512;&#25104;&#21457;&#29616;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Differentially Private Synthetic Data Lead to Synthetic Discoveries?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13612
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#30340;Mann-Whitney U&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;&#20849;&#20139;&#25935;&#24863;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#21311;&#21517;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#24212;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#32467;&#26500;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#20027;&#20307;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30446;&#21069;&#34987;&#35748;&#20026;&#26159;&#24179;&#34913;&#36825;&#31181;&#26435;&#34913;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#35780;&#20272;&#22312;&#24046;&#20998;&#38544;&#31169;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;Mann-Whitney U&#26816;&#39564;&#22312;I&#22411;&#21644;II&#22411;&#38169;&#35823;&#26041;&#38754;&#65292;&#20197;&#30830;&#23450;&#22312;&#38544;&#31169;&#20445;&#25252;&#21512;&#25104;&#25968;&#25454;&#19978;&#25191;&#34892;&#30340;&#32479;&#35745;&#20551;&#35774;&#26816;&#39564;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#27979;&#35797;&#26377;&#25928;&#24615;&#30340;&#20007;&#22833;&#25110;&#21151;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13612v1 Announce Type: new  Abstract: Background: Synthetic data has been proposed as a solution for sharing anonymized versions of sensitive biomedical datasets. Ideally, synthetic data should preserve the structure and statistical properties of the original data, while protecting the privacy of the individual subjects. Differential privacy (DP) is currently considered the gold standard approach for balancing this trade-off.   Objectives: The aim of this study is to evaluate the Mann-Whitney U test on DP-synthetic biomedical data in terms of Type I and Type II errors, in order to establish whether statistical hypothesis testing performed on privacy preserving synthetic data is likely to lead to loss of test's validity or decreased power.   Methods: We evaluate the Mann-Whitney U test on DP-synthetic data generated from real-world data, including a prostate cancer dataset (n=500) and a cardiovascular dataset (n=70 000), as well as on data drawn from two Gaussian distribution
&lt;/p&gt;</description></item><item><title>TimeMachine&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36890;&#36807;&#20840;&#38754;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09898</link><description>&lt;p&gt;
TimeMachine: &#19968;&#31181;&#29992;&#20110;&#38271;&#26399;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20215;&#20540;&#30456;&#24403;&#20110;4&#26465;&#30524;&#38236;&#34503;
&lt;/p&gt;
&lt;p&gt;
TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09898
&lt;/p&gt;
&lt;p&gt;
TimeMachine&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36890;&#36807;&#20840;&#38754;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30001;&#20110;&#25429;&#25417;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12289;&#23454;&#29616;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#22256;&#38590;&#32780;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TimeMachine&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#65292;&#21033;&#29992;Mamba&#65292;&#19968;&#31181;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#21644;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;TimeMachine&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#26174;&#33879;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#24182;&#21033;&#29992;&#21019;&#26032;&#30340;&#25972;&#21512;&#22235;&#37325;Mamba&#26550;&#26500;&#26469;&#32479;&#19968;&#22788;&#29702;&#36890;&#36947;&#28151;&#21512;&#21644;&#36890;&#36947;&#29420;&#31435;&#24773;&#20917;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20840;&#23616;&#21644;&#26412;&#22320;&#24773;&#22659;&#22312;&#19981;&#21516;&#23610;&#24230;&#19979;&#36827;&#34892;&#26377;&#25928;&#20869;&#23481;&#36873;&#25321;&#36827;&#34892;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TimeMachine&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09898v1 Announce Type: new  Abstract: Long-term time-series forecasting remains challenging due to the difficulty in capturing long-term dependencies, achieving linear scalability, and maintaining computational efficiency. We introduce TimeMachine, an innovative model that leverages Mamba, a state-space model, to capture long-term dependencies in multivariate time series data while maintaining linear scalability and small memory footprints. TimeMachine exploits the unique properties of time series data to produce salient contextual cues at multi-scales and leverage an innovative integrated quadruple-Mamba architecture to unify the handling of channel-mixing and channel-independence situations, thus enabling effective selection of contents for prediction against global and local contexts at different scales. Experimentally, TimeMachine achieves superior performance in prediction accuracy, scalability, and memory efficiency, as extensively validated using benchmark datasets. C
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14874</link><description>&lt;p&gt;
&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65306;&#21033;&#29992;&#23545;&#27604;&#35299;&#30721;&#21644;&#33976;&#39311;&#25552;&#21319;LLM&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#25216;&#26415;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#23545;&#27604;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33976;&#39311;&#23545;&#27604;&#35299;&#30721;&#65288;DCD&#65289;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#19994;&#20313;&#27169;&#22411;&#25110;&#38544;&#34255;&#29366;&#24577;&#24046;&#24322;&#20998;&#26512;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DCD&#37319;&#29992;&#20102;&#23545;&#27604;&#24335;&#24605;&#32500;&#24341;&#23548;&#21644;&#20808;&#36827;&#30340;&#33976;&#39311;&#25216;&#26415;&#65292;&#21253;&#25324;Dropout&#21644;&#37327;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#23545;&#27604;&#35299;&#30721;&#65288;CD&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#21644;&#19994;&#20313;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#23545;&#27604;&#25552;&#31034;&#19982;&#33976;&#39311;&#30456;&#32467;&#21512;&#65292;DCD&#28040;&#38500;&#20102;&#23545;&#19994;&#20313;&#27169;&#22411;&#30340;&#38656;&#27714;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DCD&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;GSM8K&#21644;StrategyQA&#25968;&#25454;&#38598;&#20013;&#22343;&#36229;&#36807;&#20102;CD&#21644;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14874v1 Announce Type: cross  Abstract: We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.14202</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#27604;&#36739;&#22270;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Comparing Graph Transformers via Positional Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#22312;&#26368;&#22823;&#21270;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#25442;&#22120;&#30340;&#21306;&#20998;&#33021;&#21147;&#19982;&#20301;&#32622;&#32534;&#30721;&#30340;&#36873;&#25321;&#32039;&#23494;&#30456;&#20851;&#65306;&#29992;&#20110;&#22686;&#24378;&#22522;&#26412;&#21464;&#25442;&#22120;&#19982;&#22270;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26377;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#20301;&#32622;&#32534;&#30721;&#65306;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;APEs&#65289;&#21644;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#65288;RPEs&#65289;&#12290;APEs&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#29305;&#24449;&#65292;&#24182;&#20316;&#20026;&#21464;&#25442;&#22120;&#30340;&#36755;&#20837;&#12290;&#32780;RPEs&#21017;&#20026;&#27599;&#23545;&#33410;&#28857;&#65288;&#20363;&#22914;&#65292;&#22270;&#36317;&#31163;&#65289;&#20998;&#37197;&#19968;&#20010;&#29305;&#24449;&#65292;&#24182;&#29992;&#20110;&#22686;&#24378;&#27880;&#24847;&#21147;&#22359;&#12290;&#20808;&#39564;&#19978;&#65292;&#30446;&#21069;&#19981;&#28165;&#26970;&#21738;&#31181;&#26041;&#27861;&#26356;&#26377;&#21033;&#20110;&#26368;&#22823;&#21270;&#29983;&#25104;&#30340;&#22270;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#36825;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#20301;&#32622;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;APEs&#21644;RPEs&#30340;&#22270;&#21464;&#25442;&#22120;&#22312;&#21306;&#20998;&#33021;&#21147;&#26041;&#38754;&#26159;&#31561;&#25928;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20854;&#21306;&#20998;&#33021;&#21147;&#30340;&#21516;&#26102;&#20132;&#25442;APEs&#21644;RPEs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14202v1 Announce Type: new  Abstract: The distinguishing power of graph transformers is closely tied to the choice of positional encoding: features used to augment the base transformer with information about the graph. There are two primary types of positional encoding: absolute positional encodings (APEs) and relative positional encodings (RPEs). APEs assign features to each node and are given as input to the transformer. RPEs instead assign a feature to each pair of nodes, e.g., graph distance, and are used to augment the attention block. A priori, it is unclear which method is better for maximizing the power of the resulting graph transformer. In this paper, we aim to understand the relationship between these different types of positional encodings. Interestingly, we show that graph transformers using APEs and RPEs are equivalent in terms of distinguishing power. In particular, we demonstrate how to interchange APEs and RPEs while maintaining their distinguishing power in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#31934;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#65292;&#20351;&#24471;&#33021;&#22815;&#29992;&#20110;&#35782;&#21035;DNA&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#21151;&#33021;&#21644;&#21464;&#24322;&#12290;</title><link>https://arxiv.org/abs/2311.02333</link><description>&lt;p&gt;
&#20351;&#29992;&#23383;&#33410;&#32423;&#31934;&#30830;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22522;&#20110;Transformer&#27169;&#22411;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#31934;&#24230;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;DNA&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#65292;&#20351;&#24471;&#33021;&#22815;&#29992;&#20110;&#35782;&#21035;DNA&#24207;&#21015;&#20013;&#30340;&#21508;&#31181;&#21151;&#33021;&#21644;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23383;&#33410;&#32423;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#30340;&#38598;&#21512;&#26680;&#33527;&#37240;&#23383;&#33410;&#32423;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;(ENBED)&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23383;&#33410;&#32423;&#31934;&#24230;&#19978;&#20998;&#26512;DNA&#24207;&#21015;&#12290;ENBED&#20351;&#29992;&#27425;&#20108;&#27425;&#30340;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#36827;&#34892;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#36716;&#25442;&#65292;&#27867;&#21270;&#20808;&#21069;&#22522;&#22240;&#32452;&#27169;&#22411;&#21482;&#37319;&#29992;&#32534;&#30721;&#22120;&#25110;&#32773;&#35299;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26469;&#39044;&#35757;&#32451;&#36825;&#20010;&#22522;&#30784;&#27169;&#22411;&#65292;&#20351;&#29992;&#21442;&#32771;&#22522;&#22240;&#32452;&#24207;&#21015;&#24182;&#23558;&#20854;&#24212;&#29992;&#21040;&#20197;&#19979;&#19979;&#28216;&#20219;&#21153;&#19978;&#65306;(1)&#35782;&#21035;&#22686;&#24378;&#23376;&#12289;&#21551;&#21160;&#23376;&#21644;&#21098;&#20999;&#20301;&#28857;&#65292;(2)&#35782;&#21035;&#21253;&#21547;&#30897;&#22522;&#35843;&#29992;&#19981;&#21305;&#37197;&#21644;&#25554;&#20837;/&#32570;&#22833;&#38169;&#35823;&#30340;&#24207;&#21015;&#65292;&#36825;&#26159;&#23545;&#22810;&#20010;&#30897;&#22522;&#23545;&#36827;&#34892;&#26631;&#35760;&#21270;&#30340;&#26041;&#26696;&#30340;&#20248;&#21183;&#65292;&#20002;&#22833;&#20102;&#23383;&#33410;&#32423;&#31934;&#24230;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;(3)&#35782;&#21035;&#22522;&#22240;&#32452;&#24207;&#21015;&#30340;&#29983;&#29289;&#21151;&#33021;&#27880;&#37322;&#65292;&#20197;&#21450;(4)&#29983;&#25104;&#31361;&#21464;&#22522;&#22240;&#32452;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02333v2 Announce Type: replace Abstract: This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutatio
&lt;/p&gt;</description></item><item><title>FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.09789</link><description>&lt;p&gt;
FLrce: &#20855;&#26377;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09789
&lt;/p&gt;
&lt;p&gt;
FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#30701;&#32570;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;FLrce&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#19981;&#27844;&#38706;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03069</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#36719;&#20214;&#31995;&#32479;&#19968;&#26679;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20063;&#23384;&#22312;Bug&#65292;&#36825;&#21487;&#33021;&#23545;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;Bug&#30340;&#35299;&#20915;&#12290;&#29616;&#26377;&#25991;&#29486;&#25351;&#20986;&#65292;&#20165;&#26377;3%&#30340;&#28145;&#24230;&#23398;&#20064;Bug&#26159;&#21487;&#22797;&#29616;&#30340;&#65292;&#36825;&#20984;&#26174;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#32771;&#23519;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#35782;&#21035;&#21487;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Stack Overflow&#21644;Defects4ML&#30340;3&#20010;&#26694;&#26550;&#21644;22&#20010;&#26550;&#26500;&#30340;668&#20010;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#36873;&#25321;&#20102;102&#20010;Bug&#65292;&#24182;&#23581;&#35797;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#22312;&#22797;&#29616;&#36825;&#20123;Bug&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12808</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#21512;&#24182;&#65292;&#20294;&#20026;&#20160;&#20040;&#20250;&#36215;&#20316;&#29992;&#65292;&#20160;&#20040;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#24179;&#22343;&#30340;&#19981;&#20934;&#30830;&#24615;&#19982;&#26799;&#24230;&#19981;&#21305;&#37197;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#32852;&#31995;&#36824;&#25581;&#31034;&#20102;&#20854;&#20182;&#26041;&#26696;&#65288;&#22914;&#24179;&#22343;&#20540;&#12289;&#20219;&#21153;&#31639;&#26415;&#21644;Fisher&#21152;&#26435;&#24179;&#22343;&#65289;&#20013;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#38754;&#37117;&#22312;&#24615;&#33021;&#21644;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SDGym&#65292;&#19968;&#20010;&#22522;&#20110;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#26469;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#26500;&#24314;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2310.12494</link><description>&lt;p&gt;
SDGym: &#20351;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models. (arXiv:2310.12494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SDGym&#65292;&#19968;&#20010;&#22522;&#20110;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#26469;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#26500;&#24314;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#24178;&#39044;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;&#23545;&#20110;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#36890;&#24120;&#38590;&#20197;&#24212;&#23545;&#31038;&#20250;&#30340;&#22797;&#26434;&#12289;&#36866;&#24212;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#26159;&#20248;&#21270;&#21160;&#24577;&#29615;&#22659;&#19979;&#20915;&#31574;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#20173;&#28982;&#26159;&#26500;&#24314;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#65288;SD&#65289;&#39046;&#22495;&#20316;&#20026;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#32435;&#20837;&#21327;&#20316;&#20223;&#30495;&#27169;&#22411;&#35268;&#33539;&#23454;&#36341;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SDGym&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Gym&#26694;&#26550;&#26500;&#24314;&#30340;&#20302;&#20195;&#30721;&#24211;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;SD&#27169;&#25311;&#27169;&#22411;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#12290;&#36890;&#36807;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#21487;&#20197;&#20174;&#29616;&#26377;SD&#27169;&#22411;&#21644;&#23569;&#37327;&#37197;&#32622;&#20195;&#30721;&#20013;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#33391;&#22909;&#12289;&#20016;&#23500;&#30340;RL&#29615;&#22659;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SDGym&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08750</link><description>&lt;p&gt;
Search-Adaptor: &#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#25991;&#26412;&#23884;&#20837;&#20010;&#24615;&#21270;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#26412;&#23884;&#20837;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;&#38500;&#20102;&#19968;&#30452;&#20197;&#26469;&#24120;&#35268;&#20351;&#29992;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#22806;&#65292;&#21033;&#29992;&#30456;&#20851;&#26597;&#35810;-&#35821;&#26009;&#24211;&#37197;&#23545;&#25968;&#25454;&#30340;&#20449;&#24687;&#33021;&#21147;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Search-Adaptor&#65292;&#20197;&#20415;&#20197;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#26041;&#24335;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#12290;Search-Adaptor&#21487;&#20197;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21407;&#22987;&#25991;&#26412;&#23884;&#20837;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#21253;&#25324;&#21482;&#33021;&#36890;&#36807;API&#35775;&#38382;&#30340;&#27169;&#22411;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33521;&#25991;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Search-Adaptor&#30340;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;--&#20363;&#22914;&#65292;&#22312;13&#20010;BEIR&#25968;&#25454;&#38598;&#19978;&#65292;nDCG@10&#30456;&#23545;&#20110;Google Embedding APIs&#24179;&#22343;&#25552;&#39640;&#20102;5.2%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#21644;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.06935</link><description>&lt;p&gt;
&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#37327;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantum Shadow Gradient Descent for Quantum Learning. (arXiv:2310.06935v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#21644;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#38452;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;QSGD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#27425;&#24615;&#25805;&#20316;&#30340;&#20248;&#28857;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26679;&#26412;&#22797;&#21046;&#65292;&#21516;&#26102;&#20855;&#26377;&#19982;&#20351;&#29992;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#30340;&#29702;&#24819;&#26356;&#26032;&#35268;&#21017;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#37327;&#23376;&#38452;&#24433;&#26679;&#26412;&#65288;QSS&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29983;&#25104;&#37327;&#23376;&#38452;&#24433;&#32780;&#19981;&#26159;&#29616;&#26377;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#38452;&#24433;&#12290;&#36890;&#36807;&#27979;&#37327;&#37327;&#23376;&#38452;&#24433;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#35745;&#31639;&#26102;&#22240;&#32500;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;&#20316;&#20026;&#31532;&#20108;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26356;&#19968;&#33324;&#30340;&#38750;&#20056;&#31215;&#24418;&#24335;&#30340;&#27169;&#25311;&#21464;&#20998;&#21704;&#23494;&#39039;&#37327;&#65292;&#24418;&#24335;&#20026;$\exp\{i\sum_j \theta_j A_j\}$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#21487;&#20197;&#29992;&#26131;&#20110;&#27979;&#37327;&#30340;&#21333;&#21442;&#25968;&#27169;&#25311;&#24577;&#30340;&#26799;&#24230;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new procedure called quantum shadow gradient descent (QSGD) that addresses these key challenges. Our method has the benefits of a one-shot approach, in not requiring any sample duplication while having a convergence rate comparable to the ideal update rule using exact gradient computation. We propose a new technique for generating quantum shadow samples (QSS), which generates quantum shadows as opposed to classical shadows used in existing works. With classical shadows, the computations are typically performed on classical computers and, hence, are prohibitive since the dimension grows exponentially. Our approach resolves this issue by measurements of quantum shadows. As the second main contribution, we study more general non-product ansatz of the form $\exp\{i\sum_j \theta_j A_j\}$ that model variational Hamiltonians. We prove that the gradient can be written in terms of the gradient of single-parameter ansatzes that can be easily measured. Our proof is based on 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2309.13080</link><description>&lt;p&gt;
SPICED: &#20855;&#26377;&#22810;&#20010;&#20027;&#39064;&#21644;&#22797;&#26434;&#31243;&#24230;&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#26234;&#33021;&#31995;&#32479;&#26469;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#26032;&#38395;&#23186;&#20307;&#30340;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34394;&#20551;&#21457;&#29616;&#65306;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27604;&#22914;&#19968;&#23545;&#26032;&#38395;&#26159;&#21542;&#37117;&#28041;&#21450;&#25919;&#27835;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23558;&#26032;&#38395;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#20998;&#21106;&#25104;&#20027;&#39064;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#22312;&#26356;&#29421;&#31364;&#30340;&#39046;&#22495;&#20013;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23384;&#22312;&#30446;&#21069;&#32570;&#20047;&#30340;&#19987;&#39064;&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30456;&#20284;&#26032;&#38395;&#25968;&#25454;&#38598;SPICED&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65306;&#29359;&#32618;&#19982;&#27861;&#24459;&#12289;&#25991;&#21270;&#19982;&#23089;&#20048;&#12289;&#28798;&#38590;&#19982;&#20107;&#25925;&#12289;&#32463;&#27982;&#19982;&#21830;&#19994;&#12289;&#25919;&#27835;&#19982;&#20914;&#31361;&#12289;&#31185;&#23398;&#19982;&#25216;&#26415;&#20197;&#21450;&#20307;&#32946;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;&#21644;&#21521;&#37327;&#37117;&#23384;&#22312;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#22312;&#35299;&#20915;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#21463;&#21040;&#120591;&#30340;&#22823;&#23567;&#24433;&#21709;&#65292;&#20854;&#20013;&#120591;&#34920;&#31034;&#24102;&#26377;&#22122;&#22768;&#30340;&#31995;&#25968;&#30697;&#38453;A&#30340;&#20056;&#23376;&#33539;&#25968;&#30340;&#24179;&#26041;&#19982;Frobenius&#33539;&#25968;&#30340;&#24179;&#26041;&#30340;&#20056;&#31215;&#12290;</title><link>http://arxiv.org/abs/2308.16904</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#20915;&#21452;&#21521;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#30340;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#30340;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note on Randomized Kaczmarz Algorithm for Solving Doubly-Noisy Linear Systems. (arXiv:2308.16904v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;&#21644;&#21521;&#37327;&#37117;&#23384;&#22312;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#26102;&#65292;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#22312;&#35299;&#20915;&#22122;&#22768;&#32447;&#24615;&#31995;&#32479;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#21463;&#21040;&#120591;&#30340;&#22823;&#23567;&#24433;&#21709;&#65292;&#20854;&#20013;&#120591;&#34920;&#31034;&#24102;&#26377;&#22122;&#22768;&#30340;&#31995;&#25968;&#30697;&#38453;A&#30340;&#20056;&#23376;&#33539;&#25968;&#30340;&#24179;&#26041;&#19982;Frobenius&#33539;&#25968;&#30340;&#24179;&#26041;&#30340;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#32447;&#24615;&#31995;&#32479;Ax=b&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#36845;&#20195;&#27714;&#35299;&#22120;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#25805;&#20316;&#35823;&#24046;&#25110;&#38169;&#35823;&#30340;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#65292;&#36825;&#20123;&#31995;&#32479;&#20250;&#20986;&#29616;&#22122;&#22768;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;Kaczmarz&#65288;RK&#65289;&#31639;&#27861;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#39640;&#25928;&#36845;&#20195;&#27714;&#35299;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23545;RK&#22312;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;&#26377;&#38480;&#65292;&#21482;&#32771;&#34385;&#21491;&#20391;&#21521;&#37327;b&#20013;&#30340;&#27979;&#37327;&#22122;&#22768;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#24182;&#19981;&#24635;&#26159;&#36825;&#26679;&#65307;&#31995;&#25968;&#30697;&#38453;A&#20063;&#21487;&#33021;&#26159;&#26377;&#22122;&#22768;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#31995;&#25968;&#30697;&#38453;A&#20197;&#21450;&#21521;&#37327;b&#37117;&#21463;&#26377;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#24433;&#21709;&#26102;&#65292;RK&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#21464;&#37327; &#120591;=&#8741; &#120591; &#119860; &#8727; &#8741;2^2 &#8741; &#119836;&#119863;&#119867;&#8727;&#119888; &#8722; &#119835; &#8741;_&#119865;^2&#127542; &#119900; &#119877; &#30340;&#22823;&#23567;&#20250;&#24433;&#21709;RK&#30340;&#25910;&#25947;&#24615;&#65292;&#20854;&#20013; &#120591;&#119860; &#34920;&#31034;A&#30340;&#24102;&#26377;&#22122;&#22768;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#20998;&#26512;&#26159;&#20581;&#22766;&#19988;&#36924;&#36817;&#23454;&#38469;&#30340;.
&lt;/p&gt;
&lt;p&gt;
Large-scale linear systems, $Ax=b$, frequently arise in practice and demand effective iterative solvers. Often, these systems are noisy due to operational errors or faulty data-collection processes. In the past decade, the randomized Kaczmarz (RK) algorithm has been studied extensively as an efficient iterative solver for such systems. However, the convergence study of RK in the noisy regime is limited and considers measurement noise in the right-hand side vector, $b$. Unfortunately, in practice, that is not always the case; the coefficient matrix $A$ can also be noisy. In this paper, we analyze the convergence of RK for noisy linear systems when the coefficient matrix, $A$, is corrupted with both additive and multiplicative noise, along with the noisy vector, $b$. In our analyses, the quantity $\tilde R=\| \tilde A^{\dagger} \|_2^2 \|\tilde A \|_F^2$ influences the convergence of RK, where $\tilde A$ represents a noisy version of $A$. We claim that our analysis is robust and realistic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23558;&#22522;&#20110;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23450;&#20041;&#35299;&#37322;&#24615;&#27010;&#24565;&#21644;&#36873;&#25321;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.12745</link><description>&lt;p&gt;
EEG&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Concept-based explainability for an EEG transformer model. (arXiv:2307.12745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23558;&#22522;&#20110;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23450;&#20041;&#35299;&#37322;&#24615;&#27010;&#24565;&#21644;&#36873;&#25321;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30001;&#20110;&#20854;&#35268;&#27169;&#12289;&#32467;&#26500;&#20197;&#21450;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#22312;&#38543;&#26426;&#24615;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#36873;&#25321;&#25968;&#25454;&#38598;&#21644;&#24402;&#32435;&#20559;&#35265;&#20063;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#25361;&#25112;&#65292;Kim&#31561;&#20154;&#65288;2018&#65289;&#24341;&#20837;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#65292;&#26088;&#22312;&#20174;&#20154;&#31867;&#23545;&#40784;&#30340;&#27010;&#24565;&#35282;&#24230;&#29702;&#35299;&#28145;&#24230;&#27169;&#22411;&#30340;&#20869;&#37096;&#29366;&#24577;&#12290;&#36825;&#20123;&#27010;&#24565;&#23545;&#24212;&#20110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#20351;&#29992;&#32447;&#24615;&#21028;&#21035;&#27861;&#36827;&#34892;&#35782;&#21035;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#39318;&#20808;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#20294;&#21518;&#26469;&#34987;&#36866;&#24212;&#21040;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20869;&#30340;&#20854;&#20182;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;Kostas&#31561;&#20154;&#30340;BENDR&#65288;2021&#65289;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#39033;&#21162;&#21147;&#30340;&#20851;&#38190;&#37096;&#20998;&#21253;&#25324;&#23450;&#20041;&#35299;&#37322;&#24615;&#27010;&#24565;&#21644;&#36873;&#25321;&#30456;&#20851;&#25968;&#25454;&#38598;&#20197;&#23558;&#27010;&#24565;&#19982;&#28508;&#22312;&#31354;&#38388;&#30456;&#23545;&#24212;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;EEG&#27010;&#24565;&#24418;&#25104;&#30340;&#20004;&#20010;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models are complex due to their size, structure, and inherent randomness in training procedures. Additional complexity arises from the selection of datasets and inductive biases. Addressing these challenges for explainability, Kim et al. (2018) introduced Concept Activation Vectors (CAVs), which aim to understand deep models' internal states in terms of human-aligned concepts. These concepts correspond to directions in latent space, identified using linear discriminants. Although this method was first applied to image classification, it was later adapted to other domains, including natural language processing. In this work, we attempt to apply the method to electroencephalogram (EEG) data for explainability in Kostas et al.'s BENDR (2021), a large-scale transformer model. A crucial part of this endeavor involves defining the explanatory concepts and selecting relevant datasets to ground concepts in the latent space. Our focus is on two mechanisms for EEG concept formation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11892</link><description>&lt;p&gt;
&#20851;&#20110;&#21463;&#24694;&#24847;&#22122;&#22768;&#24433;&#21709;&#30340;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#20351;&#29992;&#38543;&#26426;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#31934;&#24230;&#19978;&#21482;&#25439;&#22833;$\Theta(\alpha)$&#21644;$O(\sqrt{\alpha})$&#65292;&#23545;&#24212;&#19981;&#21516;&#30340;&#20844;&#27491;&#32422;&#26463;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20844;&#27491;&#32422;&#26463;&#23398;&#20064;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#24494;&#23567;&#24694;&#24847;&#22122;&#22768;&#30340;&#33030;&#24369;&#24615;&#12290;Konstantinov&#21644;Lampert (2021)&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#36127;&#38754;&#32467;&#26524;&#65292;&#34920;&#26126;&#22312;&#19981;&#24179;&#34913;&#30340;&#32676;&#32452;&#22823;&#23567;&#19979;&#23384;&#22312;&#19968;&#20123;&#25968;&#25454;&#20998;&#24067;&#65292;&#20219;&#20309;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#37117;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26356;&#20048;&#35266;&#30340;&#35266;&#28857;&#65292;&#22914;&#26524;&#20801;&#35768;&#38543;&#26426;&#20998;&#31867;&#22120;&#65292;&#21017;&#24773;&#20917;&#26356;&#21152;&#32454;&#33268;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$\Theta(\alpha)$&#30340;&#31934;&#24230;&#25439;&#22833;&#65292;&#20854;&#20013;$\alpha$&#26159;&#24694;&#24847;&#22122;&#22768;&#29575;&#65292;&#29978;&#33267;&#21487;&#20197;&#19982;&#27809;&#26377;&#20844;&#27491;&#32422;&#26463;&#30340;&#24773;&#20917;&#23436;&#20840;&#21305;&#37197;&#12290;&#23545;&#20110;&#26426;&#20250;&#22343;&#31561;&#24615;&#65292;&#25105;&#20204;&#26174;&#31034;&#21482;&#20250;&#20135;&#29983;$O(\sqrt{\alpha})$&#30340;&#25439;&#22833;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21305;&#37197;&#30340;$\Omega(\sqrt{\alpha})$&#30340;&#19979;&#30028;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Konstantinov&#21644;Lampert (2021)&#31034;&#33539;&#20102;&#23545;&#20110;&#36866;&#24403;&#30340;&#23398;&#20064;&#22120;&#65292;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#31934;&#24230;&#25439;&#22833;&#37117;&#26159;$\Omega(1)$&#12290;&#20851;&#38190;&#30340;&#25216;&#26415;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. Konstantinov and Lampert (2021) initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert (2021) showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#25237;&#24433;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.11672</link><description>&lt;p&gt;
&#24555;&#36895;&#33258;&#36866;&#24212;&#27979;&#35797;&#38450;&#24481;&#19982;&#31283;&#20581;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Fast Adaptive Test-Time Defense with Robust Features. (arXiv:2307.11672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#36890;&#36807;&#25237;&#24433;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#38477;&#20302;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#27979;&#35797;&#38450;&#24481;&#34987;&#29992;&#26469;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#23545;&#27169;&#22411;&#21442;&#25968;&#25110;&#36755;&#20837;&#36827;&#34892;&#39069;&#22806;&#30340;&#20248;&#21270;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#22823;&#24133;&#22686;&#21152;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27979;&#35797;&#38450;&#24481;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#65288;&#31283;&#20581;&#30340;&#65289;&#35757;&#32451;&#36807;&#31243;&#36731;&#26494;&#38598;&#25104;&#65292;&#24182;&#19988;&#26080;&#38656;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#25237;&#24433;&#21040;&#26368;&#31283;&#20581;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#38477;&#20302;&#23545;&#38750;&#31283;&#20581;&#26041;&#21521;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#24191;&#20041;&#21487;&#21152;&#24615;&#27169;&#22411;&#21644;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#20989;&#25968;&#65288;NTK&#65289;&#31561;&#20215;&#27861;&#35777;&#26126;&#20102;&#29305;&#24449;&#30697;&#38453;&#30340;&#39030;&#23618;&#29305;&#24449;&#31354;&#38388;&#26356;&#21152;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#29992;&#20110;&#20960;&#20010;&#31283;&#20581;&#24615;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive test-time defenses are used to improve the robustness of deep neural networks to adversarial examples. However, existing methods significantly increase the inference time due to additional optimization on the model parameters or the input at test time. In this work, we propose a novel adaptive test-time defense strategy that is easy to integrate with any existing (robust) training procedure without additional test-time computation. Based on the notion of robustness of features that we present, the key idea is to project the trained models to the most robust feature space, thereby reducing the vulnerability to adversarial attacks in non-robust directions. We theoretically show that the top eigenspace of the feature matrix are more robust for a generalized additive model and support our argument for a large width neural network with the Neural Tangent Kernel (NTK) equivalence. We conduct extensive experiments on CIFAR-10 and CIFAR-100 datasets for several robustness benchmarks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14861</link><description>&lt;p&gt;
&#21033;&#29992;&#20219;&#21153;&#32467;&#26500;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#32467;&#26500;&#30340;&#21487;&#35782;&#21035;&#24615;&#29702;&#35770;&#65292;&#25299;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#30340;&#24037;&#20316;&#12290;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#12290;&#22312;&#20551;&#35774;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#21487;&#36776;&#21035;&#24615;&#30340;&#29702;&#35770;&#65292;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#20219;&#21153;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#21518;&#26524;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#23454;&#29616;&#21487;&#35782;&#21035;&#24615;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#20165;&#38480;&#20110;&#21333;&#20219;&#21153;&#20998;&#31867;&#24773;&#20917;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#21153;&#20998;&#24067;&#30340;&#23384;&#22312;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#26465;&#20214;&#20808;&#39564;&#65292;&#23558;&#21487;&#35782;&#21035;&#24615;&#30340;&#31561;&#20215;&#31867;&#38477;&#20302;&#21040;&#25490;&#21015;&#21644;&#32553;&#25918;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#24378;&#22823;&#21644;&#26356;&#26377;&#29992;&#30340;&#32467;&#26524;&#12290;&#24403;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#26368;&#22823;&#36793;&#38469;&#20284;&#28982;&#20248;&#21270;&#65292;&#24182;&#22312;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#19979;&#28216;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24674;&#22797;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#35268;&#33539;&#34920;&#31034;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.05567</link><description>&lt;p&gt;
&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#29289;&#32852;&#32593;&#26694;&#26550;&#19979;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#65306;&#22797;&#26434;&#32593;&#32476;&#21644;&#31995;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems. (arXiv:2306.05567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26080;&#32447;&#20256;&#24863;&#31995;&#32479;&#26469;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#20010;&#29289;&#32852;&#32593;&#26550;&#26500;&#21644;&#26694;&#26550;&#30340;&#32452;&#20214;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#20998;&#26512;&#65292;&#19981;&#20165;&#25910;&#38598;&#21644;&#23384;&#20648;&#20449;&#24687;&#65292;&#32780;&#19988;&#36824;&#26159;&#20854;&#20182;&#20225;&#19994;&#24320;&#21457;&#24212;&#29992;&#30340;&#24179;&#21488;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;&#33021;&#28304;&#36164;&#28304;&#30340;&#28040;&#32791;&#21644;&#38656;&#27714;&#22686;&#21152;&#23548;&#33268;&#20102;&#33410;&#33021;&#19982;&#20248;&#21270;&#31649;&#29702;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21457;&#29616;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.19947</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Geometric Perspective on Diffusion Models. (arXiv:2305.19947v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21457;&#29616;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#20102;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#21644;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#26159;&#20351;&#29992;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25551;&#36848;&#25968;&#25454;&#25200;&#21160;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#20197;&#23454;&#29616;&#32479;&#19968;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#20960;&#20010;&#26377;&#36259;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20026;&#20854;&#37319;&#26679;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#20180;&#32454;&#26816;&#26597;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#24046;&#29190;&#28856;SDE&#21450;&#20854;&#20445;&#25345;&#36793;&#38469;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#29992;&#20110;&#37319;&#26679;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20998;&#24067;&#21644;&#22122;&#22768;&#20998;&#24067;&#36890;&#36807;&#19968;&#20010;&#26126;&#30830;&#30340;&#20934;&#32447;&#24615;&#37319;&#26679;&#36712;&#36857;&#21644;&#21478;&#19968;&#20010;&#38544;&#24335;&#30340;&#21435;&#22122;&#36712;&#36857;&#24179;&#28369;&#36830;&#25509;&#65292;&#21363;&#20351;&#22312;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#20063;&#25910;&#25947;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#36215;&#22522;&#20110;ODE&#30340;&#26368;&#20248;&#37319;&#26679;&#21644;&#32463;&#20856;&#30340;&#22343;&#20540;&#28418;&#31227;&#65288;&#23547;&#25214;&#27169;&#24335;&#65289;&#31639;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant progress in developing efficient training and fast sampling approaches for diffusion models. A recent remarkable advancement is the use of stochastic differential equations (SDEs) to describe data perturbation and generative modeling in a unified mathematical framework. In this paper, we reveal several intriguing geometric structures of diffusion models and contribute a simple yet powerful interpretation to their sampling dynamics. Through carefully inspecting a popular variance-exploding SDE and its marginal-preserving ordinary differential equation (ODE) for sampling, we discover that the data distribution and the noise distribution are smoothly connected with an explicit, quasi-linear sampling trajectory, and another implicit denoising trajectory, which even converges faster in terms of visual quality. We also establish a theoretical relationship between the optimal ODE-based sampling and the classic mean-shift (mode-seeking) algorithm, with w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;</title><link>http://arxiv.org/abs/2305.05448</link><description>&lt;p&gt;
&#20511;&#21161;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#40065;&#26834;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#26377;&#35768;&#22810;&#25554;&#20540;&#35299;; &#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#25351;&#29305;&#23450;&#20248;&#21270;&#26041;&#27861;&#23545;&#20247;&#22810;&#25554;&#20540;&#35299;&#20043;&#19968;&#30340;&#38544;&#21547;&#21916;&#22909;&#12290;&#24050;&#32463;&#24314;&#31435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#22312;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#26102;&#20542;&#21521;&#20110;&#20855;&#26377;&#20302;&#31209;&#21644;/&#25110;&#31232;&#30095;&#35299;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24179;&#26041;&#25439;&#22833;&#30446;&#26631;&#29702;&#35770;&#36890;&#24120;&#38656;&#35201;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#38750;&#24120;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#20013;&#20026;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#32780;&#21021;&#22987;&#21270;&#30340;&#26356;&#22823;&#35268;&#27169;&#30340;&#26435;&#37325;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32435;&#20837;&#24182;&#20998;&#26512;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#26435;&#37325;&#21521;&#37327;&#20197;&#26497;&#22352;&#26631;&#21442;&#25968;&#21270;&#65292;&#23548;&#33268;&#33258;&#28982;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#26435;&#37325;&#21521;&#37327;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;&#20559;&#24046;&#19982;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32463;&#39564;&#33539;&#25968;&#27491;&#21017;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item></channel></rss>