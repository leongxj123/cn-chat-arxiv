<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;</title><link>https://arxiv.org/abs/2404.01099</link><description>&lt;p&gt;
&#20320;&#30340;&#8220;&#23433;&#20840;&#8221;&#25968;&#25454;&#20013;&#26377;&#20160;&#20040;&#65311;&#65306;&#35782;&#21035;&#30772;&#22351;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01099
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35782;&#21035;&#37027;&#20123;&#22312;&#24494;&#35843;&#21518;&#26356;&#21487;&#33021;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#26377;&#23475;&#35831;&#27714;&#30340;&#21709;&#24212;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21363;&#20351;&#32463;&#36807;&#35843;&#25972;&#20197;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23545;&#40784;&#24615;&#65292;&#20063;&#23481;&#26131;&#34987;&#36234;&#29425;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#26159;&#36827;&#19968;&#27493;&#20351;&#29992;&#33391;&#24615;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#25454;&#65289;&#23545;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#33391;&#24615;&#24494;&#35843;&#19981;&#32463;&#24847;&#38388;&#23548;&#33268;&#36234;&#29425;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#35270;&#35282;&#34920;&#24449;&#24494;&#35843;&#25968;&#25454;&#65306;&#34920;&#31034;&#21644;&#26799;&#24230;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#38170;&#23450;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#38752;&#36817;&#26377;&#23475;&#31034;&#20363;&#24182;&#36828;&#31163;&#33391;&#24615;&#31034;&#20363;&#30340;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26356;&#26377;&#21487;&#33021;&#22312;&#24494;&#35843;&#21518;&#38477;&#20302;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#33391;&#24615;&#25968;&#25454;&#23376;&#38598;&#12290;&#20165;&#20165;&#35757;&#32451;100&#20010;&#36825;&#20123;&#30475;&#20284;&#33391;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#23601;&#21487;&#20197;&#20351;&#24494;&#35843;&#27169;&#22411;&#32943;&#23450;&#22320;&#22238;&#24212;&#36229;&#36807;70&#65285;&#30340;&#34987;&#27979;&#35797;&#30340;&#26377;&#23475;&#35831;&#27714;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01099v1 Announce Type: cross  Abstract: Current Large Language Models (LLMs), even those tuned for safety and alignment, are susceptible to jailbreaking. Some have found that just further fine-tuning an aligned model with benign data (i.e., data without harmful content) surprisingly leads to substantial degradation in safety. We delve into the data-centric aspects of why benign fine-tuning inadvertently contributes to jailbreaking. First, we represent fine-tuning data through two lenses: representation and gradient spaces. Furthermore, we propose a bi-directional anchoring method that prioritizes data points that are close to harmful examples and distant from benign ones. By doing so, our approach effectively identifies subsets of benign data that are more likely to degrade the model's safety after fine-tuning. Training on just 100 of these seemingly benign datapoints can lead to the fine-tuned model affirmatively responding to &gt; 70% of tested harmful requests, compared to &lt;
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20026;IPF&#30340;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#32467;&#26500;&#30456;&#20851;&#30340;&#35823;&#24046;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.18697</link><description>&lt;p&gt;
&#20174;&#36793;&#38469;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#30340;&#26041;&#27861;&#65306;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18697
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#21487;&#20197;&#20026;IPF&#30340;&#21442;&#25968;&#20272;&#35745;&#25552;&#20379;&#32467;&#26500;&#30456;&#20851;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#29616;&#23454;&#25968;&#25454;&#32422;&#26463;&#30340;&#24120;&#35265;&#32593;&#32476;&#25512;&#26029;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#26102;&#38388;&#32858;&#21512;&#30340;&#37051;&#25509;&#30697;&#38453;&#21644;&#26102;&#38388;&#21464;&#21270;&#36793;&#38469;&#65288;&#21363;&#34892;&#21521;&#37327;&#21644;&#21015;&#21521;&#37327;&#20043;&#21644;&#65289;&#25512;&#26029;&#21160;&#24577;&#32593;&#32476;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#37325;&#26032;&#21033;&#29992;&#20102;&#32463;&#20856;&#30340;&#36845;&#20195;&#27604;&#20363;&#25311;&#21512;&#65288;IPF&#65289;&#36807;&#31243;&#65292;&#20063;&#31216;&#20026;Sinkhorn&#31639;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;IPF&#30340;&#32479;&#35745;&#22522;&#30784;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65306;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#65292;IPF&#25552;&#20379;&#20102;&#20174;&#36793;&#38469;&#20934;&#30830;&#20272;&#35745;&#21160;&#24577;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20272;&#35745;&#20102;&#32593;&#32476;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#20010;&#35774;&#32622;&#65292;&#36890;&#36807;&#35782;&#21035;&#19968;&#20010;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#65292;IPF&#21487;&#20197;&#24674;&#22797;&#20854;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25581;&#31034;&#20102;&#20851;&#20110;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#20351;&#29992;IPF&#30340;&#38544;&#21547;&#20551;&#35774;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26032;&#30340;&#20998;&#26512;&#65292;&#22914;&#26377;&#20851;IPF&#21442;&#25968;&#20272;&#35745;&#30340;&#32467;&#26500;&#30456;&#20851;&#35823;&#24046;&#30028;&#12290;&#24403;IPF&#22833;&#36133;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18697v1 Announce Type: cross  Abstract: A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#31616;&#21333;&#28789;&#27963;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#21644;&#26377;&#26102;&#26159;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11173</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#38544;&#31169;&#26465;&#20214;&#19979;&#20351;&#26799;&#24230;&#21464;&#24471;&#26356;&#23567;&#65306;&#25913;&#36827;&#30340;&#24046;&#20998;&#38544;&#31169;&#38750;&#20984;&#20248;&#21270;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#31616;&#21333;&#28789;&#27963;&#26694;&#26550;&#65292;&#29992;&#20110;&#23547;&#25214;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#25913;&#36827;&#21644;&#26377;&#26102;&#26159;&#26368;&#20248;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20351;&#29992;&#31169;&#26377;&#30340;&#36817;&#20284;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#26469;&#8220;&#28909;&#21551;&#21160;&#8221;&#21478;&#19968;&#20010;&#29992;&#20110;&#23547;&#25214;&#31283;&#23450;&#28857;&#30340;&#31169;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#33719;&#24471;&#23545;&#20960;&#31867;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#20248;&#36895;&#29575;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#23547;&#25214;&#24179;&#28369;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#31283;&#23450;&#28857;&#30340;&#36895;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#19987;&#38376;&#38024;&#23545;&#22840;&#33832;-&#20984;&#20989;&#25968;&#65292;&#36825;&#31181;&#20989;&#25968;&#27010;&#25324;&#20102;&#26143;-&#20984;&#20989;&#25968;&#65292;&#24182;&#22312;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#21644;&#35757;&#32451;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#26102;&#20986;&#29616;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#31867;&#21035;&#23454;&#29616;&#20102;&#26368;&#20248;&#36895;&#29575;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#28385;&#36275;Kurdyka-Lojasiewicz&#65288;KL&#65289;&#26465;&#20214;&#30340;&#20989;&#25968;&#23547;&#25214;&#31283;&#23450;&#28857;&#30340;&#26368;&#20248;&#31639;&#27861;&#12290;&#20363;&#22914;&#65292;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#28385;&#36275;&#36825;&#20010;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11173v1 Announce Type: new  Abstract: We provide a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions. Our framework is based on using a private approximate risk minimizer to "warm start" another private algorithm for finding stationary points. We use this framework to obtain improved, and sometimes optimal, rates for several classes of non-convex loss functions. First, we obtain improved rates for finding stationary points of smooth non-convex empirical loss functions. Second, we specialize to quasar-convex functions, which generalize star-convex functions and arise in learning dynamical systems and training some neural nets. We achieve the optimal rate for this class. Third, we give an optimal algorithm for finding stationary points of functions satisfying the Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural networks often satisfy this condition. Fourth, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03629</link><description>&lt;p&gt;
&#31169;&#26377;&#25512;&#26029;&#30340;&#32447;&#24615;&#21270;&#23545;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#19981;&#23545;&#31216;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Disparate Impact on Group Accuracy of Linearization for Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#20855;&#26377;&#23494;&#30721;&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#25512;&#26029;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20013;&#26114;&#36149;&#30340;&#21152;&#23494;&#35745;&#31639;&#30340;&#29942;&#39048;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#24314;&#35758;&#22312;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#32447;&#24615;&#21270;&#30446;&#26631;&#37096;&#20998;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#35745;&#31639;&#20248;&#21183;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#25104;&#26412;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#23545;&#20915;&#31574;&#36793;&#30028;&#24615;&#36136;&#36827;&#34892;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#31243;&#24207;&#25913;&#21464;&#32447;&#24615;&#27169;&#22411;&#30340;&#24494;&#35843;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01975</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#65292;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#22411;&#30340;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#30001;&#20854;&#21407;&#23376;&#12289;&#21407;&#23376;&#23646;&#24615;&#21644;&#20998;&#23376;&#30340;&#20849;&#20215;&#38190;&#32452;&#25104;&#12290;&#20998;&#23376;&#30340;3D&#65288;&#20960;&#20309;&#65289;&#34920;&#31034;&#31216;&#20026;&#26500;&#22411;&#65292;&#30001;&#20854;&#21407;&#23376;&#31867;&#22411;&#21644;&#31515;&#21345;&#23572;&#22352;&#26631;&#32452;&#25104;&#12290;&#27599;&#20010;&#26500;&#22411;&#37117;&#20855;&#26377;&#28508;&#22312;&#33021;&#37327;&#65292;&#33021;&#37327;&#36234;&#20302;&#65292;&#20854;&#22312;&#33258;&#28982;&#30028;&#20013;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#36234;&#22823;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35201;&#20040;&#21482;&#32771;&#34385;2D&#20998;&#23376;&#22270;&#65292;&#35201;&#20040;&#21482;&#32771;&#34385;3D&#26500;&#22411;&#32467;&#26500;&#34920;&#31034;&#12290;&#21463;&#21040;&#26368;&#36817;&#20851;&#20110;&#22312;2D&#22270;&#34920;&#31034;&#21644;&#26500;&#22411;&#38598;&#21512;&#20013;&#20351;&#29992;&#38598;&#25104;&#30340;&#30740;&#31350;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;E(3) &#19981;&#21464;&#24615;&#20998;&#23376;&#26500;&#22411;&#32858;&#21512;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#23558;&#20998;&#23376;&#30340;2D&#34920;&#31034;&#19982;&#20854;&#22810;&#20010;&#26500;&#22411;&#30340;&#34920;&#31034;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#19982;&#20197;&#24448;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27714;&#35299;&#22120;&#30340;&#26032;&#22411;2D-3D&#32858;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#34701;&#21512;Gromov-Wasserstein&#21464;&#37327;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#30340;&#22312;&#32447;&#26500;&#22411;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A molecule's 2D representation consists of its atoms, their attributes, and the molecule's covalent bonds. A 3D (geometric) representation of a molecule is called a conformer and consists of its atom types and Cartesian coordinates. Every conformer has a potential energy, and the lower this energy, the more likely it occurs in nature. Most existing machine learning methods for molecular property prediction consider either 2D molecular graphs or 3D conformer structure representations in isolation. Inspired by recent work on using ensembles of conformers in conjunction with 2D graph representations, we propose E(3)-invariant molecular conformer aggregation networks. The method integrates a molecule's 2D representation with that of multiple of its conformers. Contrary to prior work, we propose a novel 2D--3D aggregation mechanism based on a differentiable solver for the \emph{Fused Gromov-Wasserstein Barycenter} problem and the use of an efficient online conformer generation method based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26041;&#24046;&#20943;&#23567;&#30340;&#25216;&#26415;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2401.12508</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#30340;&#38543;&#26426;&#65288;&#26041;&#24046;&#20943;&#23567;&#65289;&#36817;&#31471;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26041;&#24046;&#20943;&#23567;&#30340;&#25216;&#26415;&#20197;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#38750;&#26126;&#26174;&#35774;&#32622;&#20013;&#30340;&#27491;&#21017;&#21270;&#39044;&#26399;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28085;&#30422;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#29616;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#26679;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#21644;&#20998;&#26512;&#20102;&#32463;&#20856;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#22312;&#26631;&#20934;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;$O(\epsilon^{-4})$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26469;&#23547;&#25214;$\epsilon$-&#31283;&#23450;&#28857;&#12290;&#30001;&#20110;&#32463;&#20856;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#36890;&#24120;&#36739;&#22823;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#26041;&#24046;&#20943;&#23567;&#36817;&#31471;&#26799;&#24230;&#27861;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#27010;&#29575;&#26799;&#24230;&#20272;&#35745;&#22120; (PAGE)&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#20195;&#34920;&#20102;&#22312;&#35299;&#20915;&#19968;&#33324;&#30340;&#27491;&#21017;&#21270;&#22870;&#21169;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#39069;&#22806;&#26465;&#20214;&#19979;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#20174;$O(\epsilon^{-4})$&#25552;&#39640;&#21040;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a regularized expected reward optimization problem in the non-oblivious setting that covers many existing problems in reinforcement learning (RL). In order to solve such an optimization problem, we apply and analyze the classical stochastic proximal gradient method. In particular, the method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an $\epsilon$-stationary point, under standard conditions. Since the variance of the classical stochastic gradient estimator is typically large which slows down the convergence, we also apply an efficient stochastic variance-reduce proximal gradient method with an importance sampling based ProbAbilistic Gradient Estimator (PAGE). To the best of our knowledge, the application of this method represents a novel approach in addressing the general regularized reward optimization problem. Our analysis shows that the sample complexity can be improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional conditions. Our resu
&lt;/p&gt;</description></item><item><title>PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>http://arxiv.org/abs/2312.07910</link><description>&lt;p&gt;
PromptBench&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
PromptBench: A Unified Library for Evaluation of Large Language Models. (arXiv:2312.07910v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07910
&lt;/p&gt;
&lt;p&gt;
PromptBench&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#24211;&#65292;&#21253;&#25324;&#20102;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#31561;&#32452;&#20214;&#65292;&#26088;&#22312;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#21644;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#20197;&#21450;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#23545;&#20110;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptBench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#32479;&#19968;&#24211;&#12290;&#23427;&#30001;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#21644;&#25193;&#23637;&#65306;&#25552;&#31034;&#35821;&#26500;&#24314;&#12289;&#25552;&#31034;&#35821;&#24037;&#31243;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21152;&#36733;&#12289;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#12289;&#21160;&#24577;&#35780;&#20272;&#21327;&#35758;&#21644;&#20998;&#26512;&#24037;&#20855;&#12290;PromptBench&#26088;&#22312;&#25104;&#20026;&#19968;&#20010;&#24320;&#25918;&#12289;&#36890;&#29992;&#21644;&#28789;&#27963;&#30340;&#20195;&#30721;&#24211;&#65292;&#20197;&#20419;&#36827;&#21407;&#21019;&#30740;&#31350;&#65292;&#21019;&#24314;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12289;&#37096;&#32626;&#19979;&#28216;&#24212;&#29992;&#21644;&#35774;&#35745;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/microsoft/promptbench&#19978;&#25214;&#21040;&#65292;&#24182;&#23558;&#25345;&#32493;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of large language models (LLMs) is crucial to assess their performance and mitigate potential security risks. In this paper, we introduce PromptBench, a unified library to evaluate LLMs. It consists of several key components that are easily used and extended by researchers: prompt construction, prompt engineering, dataset and model loading, adversarial prompt attack, dynamic evaluation protocols, and analysis tools. PromptBench is designed to be an open, general, and flexible codebase for research purposes that can facilitate original study in creating new benchmarks, deploying downstream applications, and designing new evaluation protocols. The code is available at: https://github.com/microsoft/promptbench and will be continuously supported.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.15290</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#21487;&#38752;&#22320;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#20016;&#23500;&#30340;&#24739;&#32773;&#32423;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#26816;&#39564;&#12289;&#33647;&#29289;&#21644;&#35786;&#26029;&#65292;&#20026;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#24120;&#24120;&#38480;&#21046;&#20102;&#23545;EHR&#30340;&#35775;&#38382;&#65292;&#38459;&#30861;&#20102;&#19979;&#28216;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;EHR&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#19971;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#26469;&#22686;&#24378;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2310.13479</link><description>&lt;p&gt;
&#20998;&#27573;&#12289;&#36873;&#25321;&#12289;&#32416;&#27491;&#65306;&#19968;&#31181;&#24369;&#30417;&#30563;&#25351;&#20195;&#20998;&#21106;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation. (arXiv:2310.13479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13479
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20998;&#35299;&#20026;&#33719;&#21462;&#23454;&#20363;&#25513;&#27169;&#12289;&#36873;&#25321;&#27491;&#30830;&#25513;&#27169;&#21644;&#32416;&#27491;&#38169;&#35823;&#25513;&#27169;&#30340;&#19977;&#20010;&#27493;&#39588;&#65292;&#22635;&#34917;&#20102;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#22270;&#20687;&#20998;&#21106;&#65288;RIS&#65289;&#26159;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#23545;&#35937;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#20027;&#35201;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#25351;&#20195;&#26631;&#27880;&#25513;&#27169;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#65292;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#19981;&#21450;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;RIS&#20998;&#35299;&#25104;&#19977;&#20010;&#27493;&#39588;&#36827;&#34892;&#22788;&#29702;&#65306;&#33719;&#21462;&#34987;&#25552;&#21450;&#25351;&#20196;&#20013;&#30340;&#23545;&#35937;&#30340;&#23454;&#20363;&#25513;&#27169;&#65288;&#20998;&#27573;&#65289;&#65292;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#26469;&#36873;&#25321;&#32473;&#23450;&#25351;&#20196;&#30340;&#28508;&#22312;&#27491;&#30830;&#25513;&#27169;&#65288;&#36873;&#25321;&#65289;&#65292;&#24182;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#26469;&#20462;&#27491;&#38646;&#26679;&#26412;&#36873;&#25321;&#30340;&#38169;&#35823;&#65288;&#32416;&#27491;&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#20165;&#20351;&#29992;&#21069;&#20004;&#20010;&#27493;&#39588;&#65288;&#38646;&#26679;&#26412;&#20998;&#27573;&#21644;&#36873;&#25321;&#65289;&#27604;&#20854;&#20182;&#38646;&#26679;&#26412;&#22522;&#32447;&#25552;&#39640;&#20102;&#22810;&#36798;19%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referring Image Segmentation (RIS) - the problem of identifying objects in images through natural language sentences - is a challenging task currently mostly solved through supervised learning. However, while collecting referred annotation masks is a time-consuming process, the few existing weakly-supervised and zero-shot approaches fall significantly short in performance compared to fully-supervised learning ones. To bridge the performance gap without mask annotations, we propose a novel weakly-supervised framework that tackles RIS by decomposing it into three steps: obtaining instance masks for the object mentioned in the referencing instruction (segment), using zero-shot learning to select a potentially correct mask for the given instruction (select), and bootstrapping a model which allows for fixing the mistakes of zero-shot selection (correct). In our experiments, using only the first two steps (zero-shot segment and select) outperforms other zero-shot baselines by as much as 19%,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09462</link><description>&lt;p&gt;
&#19968;&#20010;&#36171;&#20104;&#22240;&#26524;&#20998;&#26512;&#33021;&#21147;&#30340;&#22686;&#24378;&#23398;&#20064;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65306;&#22686;&#24378;&#33258;&#21160;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
A Framework for Empowering Reinforcement Learning Agents with Causal Analysis: Enhancing Automated Cryptocurrency Trading. (arXiv:2310.09462v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26694;&#26550;CausalReinforceNet&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#20132;&#26131;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#20132;&#26131;&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#20013;&#24320;&#21457;&#30408;&#21033;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20132;&#26131;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#38024;&#23545;&#20116;&#31181;&#28909;&#38376;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65288;&#21363;&#27604;&#29305;&#24065;&#20197;&#22806;&#30340;&#21152;&#23494;&#36135;&#24065;&#65289;&#65306;&#24065;&#23433;&#24065;&#12289;&#20197;&#22826;&#22346;&#12289;&#33713;&#29305;&#24065;&#12289;&#29790;&#27874;&#24065;&#21644;&#27888;&#36798;&#24065;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CausalReinforceNet&#65292;&#19968;&#20010;&#34987;&#26500;&#24314;&#20026;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;&#20316;&#20026;&#20132;&#26131;&#31995;&#32479;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;CausalReinforceNet&#26694;&#26550;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#22686;&#24378;&#20102;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#22312;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#35782;&#21035;&#24433;&#21709;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#21464;&#21160;&#30340;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#23558;&#27010;&#29575;&#24615;&#20215;&#26684;&#26041;&#21521;&#20449;&#21495;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#20197;&#22686;&#24378;&#20132;&#26131;&#31995;&#32479;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in artificial intelligence-enhanced trading methods, developing a profitable automated trading system remains challenging in the rapidly evolving cryptocurrency market. This study aims to address these challenges by developing a reinforcement learning-based automated trading system for five popular altcoins~(cryptocurrencies other than Bitcoin): Binance Coin, Ethereum, Litecoin, Ripple, and Tether. To this end, we present CausalReinforceNet, a framework framed as a decision support system. Designed as the foundational architecture of the trading system, the CausalReinforceNet framework enhances the capabilities of the reinforcement learning agent through causal analysis. Within this framework, we use Bayesian networks in the feature engineering process to identify the most relevant features with causal relationships that influence cryptocurrency price movements. Additionally, we incorporate probabilistic price direction signals from dynamic Bayesian networks to enhance
&lt;/p&gt;</description></item><item><title>FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.07048</link><description>&lt;p&gt;
FedMFS: &#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#30340;&#32852;&#37030;&#22810;&#27169;&#24577;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMFS: Federated Multimodal Fusion Learning with Selective Modality Communication. (arXiv:2310.07048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07048
&lt;/p&gt;
&lt;p&gt;
FedMFS&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#27169;&#24577;&#36890;&#20449;&#35299;&#20915;&#20102;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#20197;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#20165;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#35775;&#38382;&#12289;&#20405;&#29359;&#25110;&#27844;&#38706;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20351;&#23458;&#25143;&#33021;&#22815;&#21512;&#20316;&#12290;&#22312;&#29289;&#32852;&#32593;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#32452;&#21512;&#21644;&#34701;&#21512;&#33539;&#24335;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#30001;&#20110;&#32570;&#20047;&#29305;&#23450;&#27169;&#24577;&#30340;&#24322;&#26500;&#23458;&#25143;&#24341;&#36215;&#30340;&#38382;&#39064;&#65307;&#65288;&#20108;&#65289;&#35774;&#35745;&#19968;&#31181;&#26368;&#20248;&#30340;&#27169;&#24577;&#19978;&#20256;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#21516;&#26102;&#26368;&#22823;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;FedMFS&#65292;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;Shapley&#20540;&#26469;&#37327;&#21270;&#27599;&#20010;&#27169;&#24577;&#30340;&#36129;&#29486;&#21644;&#27169;&#24577;&#27169;&#22411;&#22823;&#23567;&#26469;&#34913;&#37327;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#20415;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning (ML) paradigm that enables clients to collaborate without accessing, infringing upon, or leaking original user data by sharing only model parameters. In the Internet of Things (IoT), edge devices are increasingly leveraging multimodal data compositions and fusion paradigms to enhance model performance. However, in FL applications, two main challenges remain open: (i) addressing the issues caused by heterogeneous clients lacking specific modalities and (ii) devising an optimal modality upload strategy to minimize communication overhead while maximizing learning performance. In this paper, we propose Federated Multimodal Fusion learning with Selective modality communication (FedMFS), a new multimodal fusion FL methodology that can tackle the above mentioned challenges. The key idea is to utilize Shapley values to quantify each modality's contribution and modality model size to gauge communication overhead, so that each client can 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#28040;&#24687;&#20132;&#25442;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;D-ADMM&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.14353</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#20998;&#24067;&#24335;ADMM&#23454;&#29616;&#26377;&#38480;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Limited Communications Distributed Optimization via Deep Unfolded Distributed ADMM. (arXiv:2309.14353v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#31639;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#36890;&#20449;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#28040;&#24687;&#20132;&#25442;&#30340;&#25968;&#37327;&#24182;&#20445;&#25345;&#20102;D-ADMM&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20248;&#21270;&#26159;&#22312;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#36827;&#34892;&#21327;&#20316;&#25512;&#29702;&#21644;&#20915;&#31574;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#25805;&#20316;&#34987;&#24314;&#27169;&#20026;&#20849;&#21516;&#26368;&#23567;&#21270;&#19968;&#20010;&#20849;&#20139;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#27599;&#20010;&#26234;&#33021;&#20307;&#26412;&#22320;&#25910;&#38598;&#30340;&#35266;&#27979;&#12290;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#24120;&#35265;&#30340;D-ADMM&#65289;&#36890;&#36807;&#36845;&#20195;&#22320;&#32467;&#21512;&#26412;&#22320;&#35745;&#31639;&#21644;&#28040;&#24687;&#20132;&#25442;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#19982;&#20998;&#24067;&#24335;&#20248;&#21270;&#65288;&#29305;&#21035;&#26159;D-ADMM&#65289;&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340;&#36890;&#20449;&#65292;&#21363;&#20195;&#29702;&#20043;&#38388;&#20132;&#25442;&#30340;&#28040;&#24687;&#65292;&#20197;&#36798;&#25104;&#20849;&#35782;&#12290;&#36825;&#20351;&#24471;D-ADMM&#22312;&#21151;&#32791;&#12289;&#24310;&#36831;&#21644;&#36890;&#36947;&#36164;&#28304;&#26041;&#38754;&#21464;&#24471;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23637;&#24320;&#30340;D-ADMM&#65292;&#23427;&#37319;&#29992;&#26032;&#20852;&#30340;&#28145;&#24230;&#23637;&#24320;&#26041;&#27861;&#65292;&#20351;D-ADMM&#33021;&#22815;&#21487;&#38752;&#22320;&#36890;&#36807;&#27599;&#20010;&#26234;&#33021;&#20307;&#20107;&#20808;&#23450;&#20041;&#30340;&#23569;&#37327;&#28040;&#24687;&#36827;&#34892;&#25805;&#20316;&#12290;&#23637;&#24320;&#30340;D-ADMM&#23436;&#20840;&#20445;&#30041;D-ADMM&#30340;&#25805;&#20316;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#20943;&#23569;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization is a fundamental framework for collaborative inference and decision making in decentralized multi-agent systems. The operation is modeled as the joint minimization of a shared objective which typically depends on observations gathered locally by each agent. Distributed optimization algorithms, such as the common D-ADMM, tackle this task by iteratively combining local computations and message exchanges. One of the main challenges associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications, i.e., messages exchanged between the agents, to reach consensus. This can make D-ADMM costly in power, latency, and channel resources. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging dat
&lt;/p&gt;</description></item><item><title>SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.08698</link><description>&lt;p&gt;
&#26080;&#38656;&#25554;&#20540;&#30340;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08698
&lt;/p&gt;
&lt;p&gt;
SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65288;ISTS&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36716;&#25442;&#20026;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#26469;&#22788;&#29702;ISTS&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#32570;&#22833;&#26426;&#21046;&#65292;&#23548;&#33268;&#20102;&#19981;&#24076;&#26395;&#30340;&#20559;&#24046;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SLAN&#65288;Switch LSTM Aggregate Network&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#32452;LSTM&#23545;ISTS&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#26080;&#38656;&#25554;&#20540;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#28508;&#22312;&#36807;&#31243;&#30340;&#20551;&#35774;&#12290;&#23427;&#26681;&#25454;&#27979;&#37327;&#20256;&#24863;&#22120;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20854;&#26550;&#26500;&#12290;SLAN&#21033;&#29992;&#19981;&#35268;&#21017;&#24615;&#20449;&#24687;&#26126;&#30830;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SLAN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MIMIC-III&#12289;Physionet 2012&#21644;Physionet 2019&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Rohit102497/SLAN&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.06642</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25193;&#25955;&#65306;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models. (arXiv:2309.06642v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26679;&#26412;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#27714;&#35299;&#22120;&#22312;&#36866;&#24212;&#37325;&#24314;&#20219;&#21153;&#22256;&#38590;&#31243;&#24230;&#12289;&#25512;&#29702;&#26102;&#38388;&#21644;&#36164;&#28304;&#20998;&#37197;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#39064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#20854;&#30446;&#26631;&#26159;&#20174;&#22024;&#26434;&#21644;&#21487;&#33021;&#26159;&#65288;&#38750;&#65289;&#32447;&#24615;&#30340;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#19968;&#20010;&#24178;&#20928;&#30340;&#20449;&#21495;&#12290;&#37325;&#24314;&#38382;&#39064;&#30340;&#22256;&#38590;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#21407;&#22987;&#20449;&#21495;&#30340;&#32467;&#26500;&#65292;&#36864;&#21270;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#37325;&#24314;&#27169;&#22411;&#30340;&#38544;&#24335;&#20559;&#24046;&#20197;&#21450;&#19978;&#36848;&#22240;&#32032;&#20043;&#38388;&#22797;&#26434;&#30340;&#20132;&#20114;&#12290;&#36825;&#23548;&#33268;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#22312;&#26679;&#26412;&#38388;&#23384;&#22312;&#33258;&#28982;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;&#29616;&#20195;&#25216;&#26415;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#38382;&#39064;&#27714;&#35299;&#22120;&#22312;&#21508;&#31181;&#37325;&#24314;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#32570;&#28857;&#26159;&#35745;&#31639;&#22797;&#26434;&#65292;&#38590;&#20197;&#23454;&#26045;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#27714;&#35299;&#22120;&#32570;&#20047;&#26681;&#25454;&#37325;&#24314;&#20219;&#21153;&#30340;&#22256;&#38590;&#31243;&#24230;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#29702;&#26102;&#38388;&#38271;&#65292;&#24615;&#33021;&#19981;&#20339;&#19988;&#36164;&#28304;&#20998;&#37197;&#28010;&#36153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#36866;&#24212;&#25193;&#25955;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems arise in a multitude of applications, where the goal is to recover a clean signal from noisy and possibly (non)linear observations. The difficulty of a reconstruction problem depends on multiple factors, such as the structure of the ground truth signal, the severity of the degradation, the implicit bias of the reconstruction model and the complex interactions between the above factors. This results in natural sample-by-sample variation in the difficulty of a reconstruction task, which is often overlooked by contemporary techniques. Recently, diffusion-based inverse problem solvers have established new state-of-the-art in various reconstruction tasks. However, they have the drawback of being computationally prohibitive. Our key observation in this paper is that most existing solvers lack the ability to adapt their compute power to the difficulty of the reconstruction task, resulting in long inference times, subpar performance and wasteful resource allocation. We propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05680</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#26816;&#32034;&#25214;&#21040;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#65306;&#23454;&#29616;&#36328;&#35821;&#35328;&#12289;&#36328;&#25968;&#25454;&#38598;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Finding Already Debunked Narratives via Multistage Retrieval: Enabling Cross-Lingual, Cross-Dataset and Zero-Shot Learning. (arXiv:2308.05680v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#20197;&#21450;&#25552;&#20986;&#22810;&#38454;&#27573;&#26694;&#26550;&#26469;&#35299;&#20915;&#20102;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24050;&#32463;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#30340;&#20219;&#21153;&#26088;&#22312;&#26816;&#27979;&#24050;&#32463;&#32463;&#36807;&#20107;&#23454;&#26680;&#26597;&#30340;&#25925;&#20107;&#12290;&#25104;&#21151;&#26816;&#27979;&#21040;&#24050;&#34987;&#28548;&#28165;&#30340;&#22768;&#26126;&#19981;&#20165;&#20943;&#23569;&#20102;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#25163;&#21160;&#21162;&#21147;&#65292;&#36824;&#21487;&#20197;&#26377;&#21161;&#20110;&#20943;&#32531;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32771;&#34385;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#65292;&#21363;&#22312;&#26816;&#26597;&#30340;&#22312;&#32447;&#24086;&#23376;&#30340;&#35821;&#35328;&#19982;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#30340;&#35821;&#35328;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65306;&#65288;i&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#20801;&#35768;&#23545;&#24050;&#34987;&#28548;&#28165;&#30340;&#21465;&#36848;&#36827;&#34892;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#25512;&#25991;&#20316;&#20026;&#23545;&#20107;&#23454;&#26680;&#26597;&#25991;&#31456;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65307;&#65288;ii&#65289;&#23637;&#31034;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#32463;&#36807;&#24494;&#35843;&#21644;&#29616;&#25104;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#26694;&#26550;&#65292;&#23558;&#36825;&#20010;&#36328;&#35821;&#35328;&#28548;&#28165;&#26816;&#32034;&#38382;&#39064;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of retrieving already debunked narratives aims to detect stories that have already been fact-checked. The successful detection of claims that have already been debunked not only reduces the manual efforts of professional fact-checkers but can also contribute to slowing the spread of misinformation. Mainly due to the lack of readily available data, this is an understudied problem, particularly when considering the cross-lingual task, i.e. the retrieval of fact-checking articles in a language different from the language of the online post being checked. This paper fills this gap by (i) creating a novel dataset to enable research on cross-lingual retrieval of already debunked narratives, using tweets as queries to a database of fact-checking articles; (ii) presenting an extensive experiment to benchmark fine-tuned and off-the-shelf multilingual pre-trained Transformer models for this task; and (iii) proposing a novel multistage framework that divides this cross-lingual debunk ret
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#26102;&#38388;&#20381;&#36182;&#30340;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#25345;&#32493;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#39281;&#21644;&#12290;&#36825;&#19982;&#27809;&#26377;VEEF&#30340;&#24773;&#20917;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;&#39281;&#21644;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2307.11609</link><description>&lt;p&gt;
&#25345;&#32493;&#30340;&#24377;&#36947;&#32416;&#32544;&#20256;&#25773;&#19982;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Persistent Ballistic Entanglement Spreading with Optimal Control in Quantum Spin Chains. (arXiv:2307.11609v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#26102;&#38388;&#20381;&#36182;&#30340;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#25345;&#32493;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#65292;&#30452;&#21040;&#36798;&#21040;&#39281;&#21644;&#12290;&#36825;&#19982;&#27809;&#26377;VEEF&#30340;&#24773;&#20917;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;&#39281;&#21644;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20256;&#25773;&#26159;&#29702;&#35299;&#37327;&#23376;&#22810;&#20307;&#31995;&#32479;&#22312;&#24179;&#34913;&#21644;&#38750;&#24179;&#34913;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#8220;&#21464;&#20998;&#32416;&#32544;&#22686;&#24378;&#8221;&#22330;&#65288;VEEF&#65289;&#21487;&#22312;&#37327;&#23376;&#33258;&#26059;&#38142;&#20013;&#25345;&#32493;&#22320;&#35825;&#23548;&#32416;&#32544;&#30340;&#24377;&#36947;&#20256;&#25773;&#12290;VEEF&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#36890;&#36807;&#26368;&#20248;&#25511;&#21046;&#20197;&#26368;&#22823;&#21270;&#26368;&#32456;&#24577;&#30340;&#20108;&#20998;&#32416;&#32544;&#29109;&#65288;EE&#65289;&#12290;&#36825;&#31181;&#32447;&#24615;&#22686;&#38271;&#25345;&#32493;&#21040;EE&#36798;&#21040;&#30495;&#23454;&#39281;&#21644;&#24230;$\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$&#65292;&#20854;&#20013;$N$&#26159;&#33258;&#26059;&#24635;&#25968;&#12290;&#22312;&#26102;&#38388;$t \leq \frac{N}{2v}$&#26102;&#65292;EE&#28385;&#36275;$S(t) = v t$&#65292;&#20854;&#20013;$v$&#26159;&#36895;&#24230;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#27809;&#26377;VEEF&#30340;&#34892;&#20026;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#27809;&#26377;VEEF&#26102;&#65292;EE&#36890;&#24120;&#22312;&#38271;&#26102;&#38388;&#26497;&#38480;&#19979;&#36235;&#20110;&#19968;&#20010;&#27425;&#39281;&#21644;&#20540;&#65292;&#21363;Page&#20540;$\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$&#65292;&#19988;&#32416;&#32544;&#22686;&#38271;&#22312;&#36798;&#21040;Page&#20540;&#20043;&#21069;&#23601;&#20559;&#31163;&#20102;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement propagation provides a key routine to understand quantum many-body dynamics in and out of equilibrium. In this work, we uncover that the ``variational entanglement-enhancing'' field (VEEF) robustly induces a persistent ballistic spreading of entanglement in quantum spin chains. The VEEF is time dependent, and is optimally controlled to maximize the bipartite entanglement entropy (EE) of the final state. Such a linear growth persists till the EE reaches the genuine saturation $\tilde{S} = - \log_{2} 2^{-\frac{N}{2}}=\frac{N}{2}$ with $N$ the total number of spins. The EE satisfies $S(t) = v t$ for the time $t \leq \frac{N}{2v}$, with $v$ the velocity. These results are in sharp contrast with the behaviors without VEEF, where the EE generally approaches a sub-saturation known as the Page value $\tilde{S}_{P} =\tilde{S} - \frac{1}{2\ln{2}}$ in the long-time limit, and the entanglement growth deviates from being linear before the Page value is reached. The dependence between t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02969</link><description>&lt;p&gt;
DPM: &#36890;&#36807;&#20998;&#31163;&#32858;&#31867;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DPM: Clustering Sensitive Data through Separation. (arXiv:2307.02969v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#36890;&#36807;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#26469;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#32858;&#31867;&#12290;&#20851;&#38190;&#36129;&#29486;&#26159;&#35782;&#21035;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#24182;&#21512;&#29702;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#20998;&#32452;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#20449;&#24687;&#24471;&#20197;&#20445;&#25252;&#12290;&#20808;&#21069;&#30340;&#38544;&#31169;&#20445;&#25252;&#32858;&#31867;&#20851;&#27880;&#28857;&#22312;&#20110;&#35782;&#21035;&#28857;&#20113;&#30340;&#32858;&#38598;&#12290;&#26412;&#25991;&#21017;&#37319;&#21462;&#21478;&#19968;&#31181;&#26041;&#27861;&#65292;&#20851;&#27880;&#20110;&#35782;&#21035;&#36866;&#24403;&#30340;&#20998;&#31163;&#22120;&#20197;&#20998;&#31163;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;&#31639;&#27861;DPM&#65292;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#25628;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#28857;&#20998;&#31163;&#22120;&#12290;DPM&#35299;&#20915;&#20102;&#23547;&#25214;&#20934;&#30830;&#20998;&#31163;&#22120;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35782;&#21035;&#32858;&#31867;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31163;&#22120;&#32780;&#19981;&#26159;&#32858;&#31867;&#20869;&#30340;&#23567;&#38388;&#38548;&#20998;&#31163;&#22120;&#65292;&#20197;&#21450;&#22312;&#24320;&#38144;&#38544;&#31169;&#39044;&#31639;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#36739;&#22823;&#23376;&#37096;&#20998;&#30340;&#20998;&#31163;&#22120;&#12290;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#25351;&#25968;&#26426;&#21046;&#65292;DPM&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#20855;&#26377;&#39640;&#25928;&#29992;&#24615;&#30340;&#32858;&#31867;&#20998;&#31163;&#22120;&#65306;&#23545;&#20110;&#25968;&#25454;&#38598;D&#65292;&#22914;&#26524;&#20013;&#24515;&#30340;60%&#20998;&#20301;&#25968;&#20013;&#23384;&#22312;&#23485;&#30340;&#20302;&#23494;&#24230;&#20998;&#31163;&#22120;&#65292;DPM&#20250;&#21457;&#29616;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving clustering groups data points in an unsupervised manner whilst ensuring that sensitive information remains protected. Previous privacy-preserving clustering focused on identifying concentration of point clouds. In this paper, we take another path and focus on identifying appropriate separators that split a data set. We introduce the novel differentially private clustering algorithm DPM that searches for accurate data point separators in a differentially private manner. DPM addresses two key challenges for finding accurate separators: identifying separators that are large gaps between clusters instead of small gaps within a cluster and, to efficiently spend the privacy budget, prioritising separators that split the data into large subparts. Using the differentially private Exponential Mechanism, DPM randomly chooses cluster separators with provably high utility: For a data set $D$, if there is a wide low-density separator in the central $60\%$ quantile, DPM finds that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.07215</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#19982;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#21644;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#26469;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22686;&#21152;&#20102;&#23545;&#26377;&#25928;&#27169;&#22411;&#37096;&#32626;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26159;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#26435;&#37325;&#21644;&#28608;&#27963;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;QAT&#26041;&#27861;&#38656;&#35201;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20250;&#23548;&#33268;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#39640;&#33021;&#32791;&#12290;&#26680;&#24515;&#38598;&#36873;&#25321;&#26159;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#20887;&#20313;&#24615;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#22312;&#39640;&#25928;&#35757;&#32451;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#25552;&#39640;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22522;&#20110;QAT&#30340;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25351;&#26631;&#65306;&#35823;&#24046;&#21521;&#37327;&#20998;&#25968;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#29992;&#20110;&#37327;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#27599;&#20010;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#37325;&#35201;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#30340;&#33258;&#36866;&#24212;&#26680;&#24515;&#38598;&#36873;&#25321;&#65288;ACS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
&lt;/p&gt;</description></item><item><title>NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14405</link><description>&lt;p&gt;
NeuralMatrix: &#23558;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#31227;&#21160;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix: Moving Entire Neural Networks to General Matrix Multiplication for Efficient Inference. (arXiv:2305.14405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14405
&lt;/p&gt;
&lt;p&gt;
NeuralMatrix&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#65292;&#24182;&#21487;&#22312;&#20445;&#25345;&#25512;&#29702;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#36798;113&#20493;&#33267;19.44&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NeuralMatrix&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23427;&#20351;&#24471;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#65288;GEMM&#65289;&#21152;&#36895;&#22120;&#19978;&#35745;&#31639;&#22810;&#21151;&#33021;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#22522;&#20110;ASIC&#30340;&#21152;&#36895;&#22120;&#30340;&#19987;&#29992;&#24615;&#38480;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;CPU&#21644;GPU&#31561;&#36890;&#29992;&#22788;&#29702;&#22120;&#30456;&#27604;&#30340;&#24212;&#29992;&#29305;&#23450;&#21152;&#36895;&#27700;&#24179;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;DNN&#35745;&#31639;&#20013;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36816;&#31639;&#26144;&#23556;&#21040;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#20197;&#21450;&#20351;&#29992;GEMM&#21152;&#36895;&#22120;&#23545;DNN&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19977;&#31181;&#27969;&#34892;&#31867;&#21035;&#30340;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65288;&#21363;CNN&#65292;Transformers&#21644;GNN&#65289;&#20316;&#20026;&#31034;&#20363;&#30340;&#25903;&#25745;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;DNN&#36716;&#25442;&#20026;&#36890;&#29992;&#30697;&#38453;&#20056;&#27861;&#21518;&#20165;&#20250;&#20986;&#29616;&#39640;&#36798;2.02&#65285;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#21516;&#26102;&#23558;&#21534;&#21520;&#37327;&#19982;&#21151;&#29575;&#30340;&#27604;&#20540;&#19982;CPU&#21644;GPU&#30456;&#27604;&#25552;&#39640;&#20102;113&#20493;&#21040;19.44&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce NeuralMatrix, a novel framework that enables the computation of versatile deep neural networks (DNNs) on a single general matrix multiplication (GEMM) accelerator. The proposed approach overcomes the specificity limitations of ASIC-based accelerators while achieving application-specific acceleration levels compared to general-purpose processors such as CPUs and GPUs. We address the challenges of mapping both linear and nonlinear operations in DNN computation to general matrix multiplications and the impact of using a GEMM accelerator on DNN inference accuracy. Extensive experiments are conducted on various DNN models from three popular categories (i.e., CNN, Transformers, and GNN) as illustrative backbone models. Our results demonstrate that DNNs suffer only up to a 2.02% accuracy loss after being converted to general matrix multiplication, while achieving 113x to 19.44x improvements in throughput per power compared to CPUs and GPUs.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14353</link><description>&lt;p&gt;
DiracDiffusion: &#30830;&#20445;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency. (arXiv:2303.14353v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14353
&lt;/p&gt;
&lt;p&gt;
DiracDiffusion&#26159;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#65292;&#24182;&#20445;&#35777;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#24674;&#22797;&#65289;&#24050;&#32463;&#24314;&#31435;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#22522;&#20110;&#25193;&#25955;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#22120;&#20174;&#20005;&#37325;&#25439;&#22351;&#30340;&#27979;&#37327;&#25968;&#25454;&#20013;&#29983;&#25104;&#20986;&#20855;&#26377;&#20986;&#33394;&#35270;&#35273;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#25152;&#35859;&#30340;&#24863;&#30693;-&#22833;&#30495;&#26435;&#34913;&#20013;&#65292;&#24863;&#30693;&#25928;&#26524;&#20248;&#31168;&#30340;&#37325;&#24314;&#32467;&#26524;&#36890;&#24120;&#26159;&#20197;&#36864;&#21270;&#30340;&#22833;&#30495;&#24230;&#37327;&#65288;&#22914;PSNR&#65289;&#20026;&#20195;&#20215;&#30340;&#12290;&#22833;&#30495;&#24230;&#37327;&#34913;&#37327;&#23545;&#35266;&#23519;&#30340;&#24544;&#23454;&#24230;&#65292;&#36825;&#22312;&#36870;&#38382;&#39064;&#20013;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#21363;&#25105;&#20204;&#20551;&#35774;&#35266;&#23519;&#20540;&#26469;&#33258;&#19968;&#20010;&#38543;&#26426;&#21155;&#21270;&#36807;&#31243;&#65292;&#36880;&#28176;&#38477;&#20302;&#21644;&#22122;&#22768;&#21270;&#21407;&#22987;&#24178;&#20928;&#22270;&#20687;&#65292;&#28982;&#21518;&#23398;&#20064;&#36870;&#36716;&#21155;&#21270;&#36807;&#31243;&#20197;&#24674;&#22797;&#24178;&#20928;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#25972;&#20010;&#36870;&#36716;&#36807;&#31243;&#20013;&#20445;&#25345;&#19982;&#21407;&#22987;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20801;&#35768;&#22312;&#24863;&#30693;&#36136;&#37327;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#20043;&#38388;&#21462;&#24471;&#24040;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DiracDiffusion&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#30001;Dirac&#33021;&#37327;&#20989;&#25968;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#21435;&#22122;&#21644;&#22686;&#37327;&#37325;&#24314;&#22312;&#20869;&#30340;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have established new state of the art in a multitude of computer vision tasks, including image restoration. Diffusion-based inverse problem solvers generate reconstructions of exceptional visual quality from heavily corrupted measurements. However, in what is widely known as the perception-distortion trade-off, the price of perceptually appealing reconstructions is often paid in declined distortion metrics, such as PSNR. Distortion metrics measure faithfulness to the observation, a crucial requirement in inverse problems. In this work, we propose a novel framework for inverse problem solving, namely we assume that the observation comes from a stochastic degradation process that gradually degrades and noises the original clean image. We learn to reverse the degradation process in order to recover the clean image. Our technique maintains consistency with the original measurement throughout the reverse process, and allows for great flexibility in trading off perceptual qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#24037;&#20855;&#20197;&#23454;&#29616;&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#23548;&#33268;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.05949</link><description>&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Valid Inference after Causal Discovery. (arXiv:2208.05949v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#24037;&#20855;&#20197;&#23454;&#29616;&#22240;&#26524;&#21457;&#29616;&#21518;&#30340;&#26377;&#25928;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#23548;&#33268;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#21644;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#20219;&#21153;&#12290;&#34429;&#28982;&#24050;&#32463;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#21333;&#29420;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#26159;&#21516;&#26102;&#24212;&#29992;&#36825;&#20123;&#26041;&#27861;&#26102;&#20250;&#20986;&#29616;&#32479;&#35745;&#19978;&#30340;&#25361;&#25112;&#65306;&#22312;&#23545;&#30456;&#21516;&#25968;&#25454;&#36816;&#34892;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21518;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#20250;&#23548;&#33268;"&#21452;&#37325;&#25361;&#36873;"&#65292;&#20174;&#32780;&#20351;&#32463;&#20856;&#32622;&#20449;&#21306;&#38388;&#30340;&#35206;&#30422;&#20445;&#35777;&#26080;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#22240;&#26524;&#21457;&#29616;&#21518;&#26377;&#25928;&#30340;&#25512;&#26029;&#24037;&#20855;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22825;&#30495;&#32452;&#21512;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21644;&#38543;&#21518;&#25512;&#26029;&#31639;&#27861;&#20250;&#23548;&#33268;&#39640;&#24230;&#33192;&#32960;&#30340;&#35823;&#35206;&#30422;&#29575;&#65292;&#32780;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#21017;&#25552;&#20379;&#21487;&#38752;&#30340;&#35206;&#30422;&#24182;&#23454;&#29616;&#27604;&#25968;&#25454;&#20998;&#21106;&#26356;&#20934;&#30830;&#30340;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery and causal effect estimation are two fundamental tasks in causal inference. While many methods have been developed for each task individually, statistical challenges arise when applying these methods jointly: estimating causal effects after running causal discovery algorithms on the same data leads to "double dipping," invalidating the coverage guarantees of classical confidence intervals. To this end, we develop tools for valid post-causal-discovery inference. Across empirical studies, we show that a naive combination of causal discovery and subsequent inference algorithms leads to highly inflated miscoverage rates; on the other hand, applying our method provides reliable coverage while achieving more accurate causal discovery than data splitting.
&lt;/p&gt;</description></item></channel></rss>