<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00859</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#25552;&#21069;&#20026;&#26410;&#26469;&#26631;&#35760;&#36827;&#34892;&#35268;&#21010;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models plan ahead for future tokens?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00859
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#25552;&#21069;&#20934;&#22791;&#26410;&#26469;&#26631;&#35760;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#26159;&#36890;&#36807;&#39044;&#32531;&#23384;&#25110;&#38754;&#21253;&#23633;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#32473;&#23450;&#20301;&#32622;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#21464;&#21387;&#22120;&#26159;&#21542;&#20250;&#8220;&#25552;&#21069;&#24605;&#32771;&#8221;&#65311;&#24050;&#30693;&#21464;&#21387;&#22120;&#22312;$t$&#30340;&#21069;&#21521;&#20256;&#36882;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20934;&#22791;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;&#26410;&#26469;&#30340;&#21069;&#21521;&#20256;&#36882;$t+\tau$&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#37322;&#36825;&#31181;&#29616;&#35937;&#30340;&#21487;&#33021;&#24615;&#65306;&#39044;&#32531;&#23384;&#65292;&#21363;&#35757;&#32451;&#20013;&#23384;&#22312;&#30340;&#38750;&#23545;&#35282;&#26799;&#24230;&#39033;&#23548;&#33268;&#27169;&#22411;&#22312;$t$&#35745;&#31639;&#19982;&#24403;&#21069;&#25512;&#29702;&#20219;&#21153;&#26080;&#20851;&#20294;&#23545;&#26410;&#26469;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#38754;&#21253;&#23633;&#65292;&#21363;&#19982;&#26102;&#38388;&#27493;&#38271;$t$&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#24050;&#32463;&#19982;&#37027;&#20123;&#23558;&#26368;&#26377;&#21033;&#20110;&#26102;&#38388;&#27493;&#38271;$t+\tau$&#30340;&#29305;&#24449;&#30456;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19981;&#23558;&#26799;&#24230;&#20256;&#25773;&#21040;&#36807;&#21435;&#26102;&#38388;&#27493;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27979;&#35797;&#36825;&#20123;&#20551;&#35774;&#65292;&#36825;&#31181;&#26041;&#26696;&#25105;&#20204;&#27491;&#24335;&#31216;&#20026;&#30701;&#35270;&#35757;&#32451;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#39044;&#32531;&#23384;&#30340;&#26126;&#30830;&#35777;&#25454;&#12290;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#24314;&#27169;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#26356;&#22810;&#22320;&#25903;&#25345;&#20102;&#38754;&#21253;&#23633;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00859v1 Announce Type: cross  Abstract: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present in training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#20302;&#23618;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#39640;&#23618;&#34892;&#20026;&#20811;&#38534;&#35268;&#21010;&#22120;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.20328</link><description>&lt;p&gt;
&#20174;&#31034;&#33539;&#23398;&#20064;&#35270;&#35273;&#22235;&#36275;&#36208;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning Visual Quadrupedal Loco-Manipulation from Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#20302;&#23618;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#21644;&#39640;&#23618;&#34892;&#20026;&#20811;&#38534;&#35268;&#21010;&#22120;&#65292;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#26426;&#22120;&#20154;&#36880;&#28176;&#34987;&#25972;&#21512;&#36827;&#20154;&#31867;&#29615;&#22659;&#12290;&#23613;&#31649;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#34892;&#36208;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#20294;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#19982;&#29289;&#20307;&#30340;&#20114;&#21160;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35753;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#30495;&#23454;&#19990;&#30028;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#36208;&#34892;&#25805;&#32437;&#36807;&#31243;&#20998;&#35299;&#20026;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20302;&#23618;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#30340;&#39640;&#23618;&#35268;&#21010;&#22120;&#12290;&#36890;&#36807;&#21442;&#25968;&#21270;&#25805;&#32437;&#36712;&#36857;&#65292;&#25105;&#20204;&#21516;&#27493;&#19978;&#23618;&#21644;&#19979;&#23618;&#30340;&#21162;&#21147;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;RL&#21644;BC&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#21644;&#29616;&#23454;&#19990;&#30028;&#23454;&#39564;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20328v1 Announce Type: cross  Abstract: Quadruped robots are progressively being integrated into human environments. Despite the growing locomotion capabilities of quadrupedal robots, their interaction with objects in realistic scenes is still limited. While additional robotic arms on quadrupedal robots enable manipulating objects, they are sometimes redundant given that a quadruped robot is essentially a mobile unit equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence, we aim to empower a quadruped robot to execute real-world manipulation tasks using only its legs. We decompose the loco-manipulation process into a low-level reinforcement learning (RL)-based controller and a high-level Behavior Cloning (BC)-based planner. By parameterizing the manipulation trajectory, we synchronize the efforts of the upper and lower layers, thereby leveraging the advantages of both RL and BC. Our approach is validated through simulations and real-world experiments, d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.13940</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#22120;&#38598;&#21512;&#20013;&#36873;&#25321;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#22810;&#26631;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#33719;&#21462;&#26356;&#29702;&#24819;&#39044;&#27979;&#30340;&#26367;&#20195;&#22330;&#26223;&#12290;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#31181;&#26041;&#27861;&#29983;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#19981;&#21516;&#12289;&#26377;&#26102;&#26159;&#20914;&#31361;&#30340;&#36136;&#37327;&#24230;&#37327;&#65292;&#24182;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20043;&#19968;&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#22522;&#20110;&#22810;&#26631;&#20934;&#20998;&#26512;&#26469;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#32780;&#19981;&#26159;&#24378;&#36843;&#29992;&#25143;&#27979;&#35797;&#35768;&#22810;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#24182;&#20998;&#26512;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22949;&#21327;&#26041;&#26696;&#65292;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#36739;&#39640;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25903;&#37197;&#20851;&#31995;&#21644;&#29702;&#24819;&#28857;&#20915;&#31574;&#36741;&#21161;&#26041;&#27861;&#65292;&#20174;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#36873;&#25321;&#19968;&#20010;&#21453;&#20107;&#23454;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13940v1 Announce Type: cross  Abstract: Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that the propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#27169;&#22411;&#21516;&#26102;&#35299;&#37322;&#20102;&#20219;&#21153;&#23398;&#20064;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20998;&#26512;&#20102;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.18819</link><description>&lt;p&gt;
In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dual Operating Modes of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;In-Context Learning&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#27169;&#22411;&#21516;&#26102;&#35299;&#37322;&#20102;&#20219;&#21153;&#23398;&#20064;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#23545;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#20998;&#26512;&#20102;&#20248;&#21270;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL)&#23637;&#31034;&#20102;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#65306;&#20219;&#21153;&#23398;&#20064;&#65292;&#21363;&#20174;&#19978;&#19979;&#25991;&#26679;&#26412;&#20013;&#33719;&#21462;&#26032;&#25216;&#33021;&#65292;&#21644;&#20219;&#21153;&#26816;&#32034;&#65292;&#21363;&#26597;&#25214;&#24182;&#28608;&#27963;&#30456;&#20851;&#30340;&#39044;&#35757;&#32451;&#25216;&#33021;&#12290;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25506;&#35752;&#20102;&#21508;&#31181;&#25968;&#23398;&#27169;&#22411;&#26469;&#20998;&#26512;ICL&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#19968;&#27425;&#21482;&#33021;&#35299;&#37322;&#19968;&#31181;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#37322;ICL&#30340;&#21452;&#37325;&#36816;&#34892;&#27169;&#24335;&#12290;&#19987;&#27880;&#20110;&#32447;&#24615;&#20989;&#25968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#20219;&#21153;&#32452;&#21644;&#20219;&#21153;&#30456;&#20851;&#30340;&#36755;&#20837;&#20998;&#24067;&#25193;&#23637;&#29616;&#26377;&#27169;&#22411;&#29992;&#20110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#21518;&#20998;&#26512;&#22312;&#24179;&#26041;&#25439;&#22833;&#19979;&#34920;&#29616;&#26368;&#20248;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21363;&#32473;&#23450;&#19978;&#19979;&#25991;&#26679;&#26412;&#26631;&#31614;&#30340;&#26368;&#23567;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#22120;&#12290;&#23558;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#35270;&#20026;&#20808;&#39564;&#65292;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#35270;&#20026;&#35266;&#27979;&#20540;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20219;&#21153;&#21518;&#39564;&#20998;&#24067;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18819v1 Announce Type: new  Abstract: In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16827</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Data Selection for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16827
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20294;&#22914;&#20309;&#20248;&#21270;&#36873;&#25321;&#25968;&#25454;&#20197;&#38477;&#20302;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#20173;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#19968;&#20010;&#20027;&#35201;&#22240;&#32032;&#26159;&#21033;&#29992;&#24040;&#22823;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65288;&#25110;&#19981;&#21487;&#34892;&#65289;&#65292;&#22240;&#20026;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36136;&#37327;&#21487;&#33021;&#26377;&#25152;&#19981;&#21516;&#12290;&#25968;&#25454;&#36807;&#28388;&#20063;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#37327;&#26469;&#38477;&#20302;&#35757;&#32451;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#21644;&#36130;&#21153;&#25104;&#26412;&#12290;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#35201;&#21253;&#25324;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#21738;&#20123;&#20505;&#36873;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#22914;&#20309;&#20174;&#25152;&#36873;&#25968;&#25454;&#28857;&#20013;&#36866;&#24403;&#37319;&#26679;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#21069;&#26223;&#24050;&#32463;&#23548;&#33268;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#36805;&#36895;&#25193;&#22823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#20027;&#35201;&#21463;&#23454;&#35777;&#35777;&#25454;&#39537;&#21160;&#65292;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#25104;&#26412;&#26114;&#36149;&#65292;&#24456;&#23569;&#26377;&#32452;&#32455;&#25317;&#26377;&#36164;&#28304;&#36827;&#34892;&#24191;&#27867;&#30340;&#25968;&#25454;&#36873;&#25321;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#25968;&#25454;&#36873;&#25321;&#30340;&#30693;&#35782;&#21487;&#33021;&#22823;&#22810;&#23616;&#38480;&#20110;&#22823;&#22411;&#25216;&#26415;&#20844;&#21496;&#25110;&#30740;&#31350;&#26426;&#26500;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16827v1 Announce Type: new  Abstract: A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required.   Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15132</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Sentence Embeddings with an Automatically Generated NLI Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15132
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;STS&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#36825;&#22312;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#21516;&#26679;&#25104;&#31435;&#65292;&#20854;&#20013;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;PromptEOL &#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;PromptEOL &#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21033;&#29992;&#20102;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25968;&#25454;&#38598;&#30340;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;LLM&#33258;&#21160;&#29983;&#25104;&#30340;NLI&#25968;&#25454;&#38598;&#26469;&#25913;&#36827;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;PromptEOL&#12290;&#22312;STS&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#36798;&#21040;&#20102;82.21&#30340;&#24179;&#22343;Spearman&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;&#65292;&#20174;&#32780;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#26080;&#38656;&#20351;&#29992;&#22823;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15132v1 Announce Type: new  Abstract: Decoder-based large language models (LLMs) have shown high performance on many tasks in natural language processing. This is also true for sentence embedding learning, where a decoder-based model, PromptEOL, has achieved the best performance on semantic textual similarity (STS) tasks. However, PromptEOL makes great use of fine-tuning with a manually annotated natural language inference (NLI) dataset. We aim to improve sentence embeddings learned in an unsupervised setting by automatically generating an NLI dataset with an LLM and using it to fine-tune PromptEOL. In experiments on STS tasks, the proposed method achieved an average Spearman's rank correlation coefficient of 82.21 with respect to human evaluation, thus outperforming existing methods without using large, manually annotated datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2302.05737</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Reparameterized Discrete Diffusion Model for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#28789;&#27963;&#24615;&#12289;&#35757;&#32451;&#25216;&#26415;&#21644;&#29983;&#25104;&#25928;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36739;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20174;&#31163;&#25955;&#25193;&#25955;&#36807;&#31243;&#20013;&#37319;&#26679;&#30340;&#21478;&#19968;&#31181;&#31561;&#20215;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#24320;&#21457;&#20102;&#19968;&#26063;&#37325;&#26032;&#21442;&#25968;&#21270;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#12290;&#36825;&#20010;&#27966;&#29983;&#30340;&#36890;&#29992;&#26694;&#26550;&#38750;&#24120;&#28789;&#27963;&#65292;&#20026;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#36807;&#31243;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20855;&#22791;&#26356;&#26377;&#25928;&#30340;&#35757;&#32451;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#19978;&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;LQR&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#31639;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#30005;&#36335;&#20013;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.01258</link><description>&lt;p&gt;
&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;&#36890;&#36807;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#30340;LQR&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Model-Free LQR Control over Rate-Limited Channels. (arXiv:2401.01258v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01258
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36895;&#29575;&#38480;&#21046;&#36890;&#36947;&#19978;&#23454;&#29616;&#27169;&#22411;&#26080;&#20851;&#30340;LQR&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#31639;&#27861;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#22312;&#22122;&#22768;&#30005;&#36335;&#20013;&#21487;&#20197;&#23454;&#29616;&#25511;&#21046;&#38382;&#39064;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#22312;&#35768;&#22810;&#38382;&#39064;&#35774;&#32622;&#20013;&#30340;&#25511;&#21046;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#20250;&#38382;&#65292;&#22914;&#26524;&#21033;&#29992;&#23454;&#38469;&#30340;&#36890;&#20449;&#36890;&#36947;&#26469;&#20256;&#36755;&#26799;&#24230;&#25110;&#31574;&#30053;&#65292;&#24773;&#20917;&#20250;&#22914;&#20309;&#25913;&#21464;&#12290;&#23613;&#31649;&#30001;&#27492;&#20135;&#29983;&#30340;&#38382;&#39064;&#19982;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#30740;&#31350;&#30340;&#20844;&#24335;&#26377;&#31867;&#20284;&#20043;&#22788;&#65292;&#20294;&#37027;&#20010;&#39046;&#22495;&#30340;&#20016;&#23500;&#25991;&#29486;&#36890;&#24120;&#20551;&#23450;&#31995;&#32479;&#30340;&#27169;&#22411;&#26159;&#24050;&#30693;&#30340;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#35774;&#35745;&#21644;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;\textit{&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36895;&#29575;&#38480;&#21046;&#30340;&#36890;&#36947;&#20197;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#35299;&#20915;&#22522;&#26412;&#30340;&#25511;&#21046;&#38382;&#39064;-&#20363;&#22914;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#65311;}&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#20854;&#20013;&#19968;&#20010;&#24037;&#20316;&#20195;&#29702;&#36890;&#36807;&#19968;&#20010;&#26080;&#22122;&#22768;&#20449;&#36947;&#20197;&#26377;&#38480;&#30340;&#20301;&#36895;&#29575;&#20256;&#36755;&#37327;&#21270;&#31574;&#30053;&#26799;&#24230;&#65288;LQR&#25104;&#26412;&#65289;&#21040;&#19968;&#20010;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#37327;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;AQGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
Given the success of model-free methods for control design in many problem settings, it is natural to ask how things will change if realistic communication channels are utilized for the transmission of gradients or policies. While the resulting problem has analogies with the formulations studied under the rubric of networked control systems, the rich literature in that area has typically assumed that the model of the system is known. As a step towards bridging the fields of model-free control design and networked control systems, we ask: \textit{Is it possible to solve basic control problems - such as the linear quadratic regulator (LQR) problem - in a model-free manner over a rate-limited channel?} Toward answering this question, we study a setting where a worker agent transmits quantized policy gradients (of the LQR cost) to a server over a noiseless channel with a finite bit-rate. We propose a new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and prove that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;6G&#23376;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36739;&#23567;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.00950</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;6G&#23376;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks. (arXiv:2401.00950v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;6G&#23376;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36739;&#23567;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#36827;&#34892;&#39057;&#29575;&#23376;&#24102;&#20998;&#37197;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#24037;&#21378;&#29615;&#22659;&#20013;&#23494;&#38598;&#37096;&#32626;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#23376;&#39057;&#24102;&#65292;&#24517;&#39035;&#34987;&#20248;&#21270;&#22320;&#20998;&#37197;&#20197;&#21327;&#35843;&#23376;&#32593;&#32476;&#38388;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#23558;&#23376;&#32593;&#32476;&#37096;&#32626;&#24314;&#27169;&#20026;&#19968;&#20010;&#20914;&#31361;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#22270;&#30528;&#33394;&#21551;&#21457;&#21644;Potts&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20248;&#21270;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#25968;&#20540;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36739;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#21551;&#21457;&#24335;&#26041;&#27861;&#25509;&#36817;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38656;&#35201;&#25152;&#26377;&#20114;&#30456;&#24178;&#25200;&#30340;&#20449;&#36947;&#20449;&#24687;&#30340;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#26356;&#23569;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#32593;&#32476;&#35774;&#32622;&#20855;&#26377;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an unsupervised approach for frequency sub-band allocation in wireless networks using graph-based learning. We consider a dense deployment of subnetworks in the factory environment with a limited number of sub-bands which must be optimally allocated to coordinate inter-subnetwork interference. We model the subnetwork deployment as a conflict graph and propose an unsupervised learning approach inspired by the graph colouring heuristic and the Potts model to optimize the sub-band allocation using graph neural networks. The numerical evaluation shows that the proposed method achieves close performance to the centralized greedy colouring sub-band allocation heuristic with lower computational time complexity. In addition, it incurs reduced signalling overhead compared to iterative optimization heuristics that require all the mutual interfering channel information. We further demonstrate that the method is robust to different network settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.11891</link><description>&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Hyperparameter Study for Quantum Kernel Methods. (arXiv:2310.11891v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26680;&#26041;&#27861;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#22240;&#20026;&#19982;&#20043;&#30456;&#20851;&#30340;&#20445;&#35777;&#12290;&#23427;&#20204;&#30340;&#21487;&#35775;&#38382;&#24615;&#20063;&#25171;&#24320;&#20102;&#22522;&#20110;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#20808;&#31579;&#36873;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#26089;&#26399;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#20960;&#20309;&#24046;&#24322;&#65292;&#23427;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#20004;&#31181;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#24230;&#37327;&#65292;&#29305;&#21035;&#26159;&#37327;&#23376;&#26680;&#21644;&#32463;&#20856;&#26680;&#20043;&#38388;&#30340;&#25509;&#36817;&#24230;&#12290;&#35813;&#24230;&#37327;&#25351;&#31034;&#20102;&#37327;&#23376;&#21644;&#32463;&#20856;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#23427;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#19982;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#20851;&#31995;&#65292;&#20960;&#20309;&#24046;&#24322;&#26159;&#21542;&#21487;&#20197;&#25104;&#20026;&#38500;&#20102;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#20043;&#22806;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#21644;&#32463;&#20856;&#26680;&#19982;&#37327;&#23376;&#26680;&#20043;&#38388;&#30340;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum kernel methods are a promising method in quantum machine learning thanks to the guarantees connected to them. Their accessibility for analytic considerations also opens up the possibility of prescreening datasets based on their potential for a quantum advantage. To do so, earlier works developed the geometric difference, which can be understood as a closeness measure between two kernel-based machine learning approaches, most importantly between a quantum kernel and classical kernel. This metric links the quantum and classical model complexities. Therefore, it raises the question of whether the geometric difference, based on its relation to model complexity, can be a useful tool in evaluations other than for the potential for quantum advantage. In this work, we investigate the effects of hyperparameter choice on the model performance and the generalization gap between classical and quantum kernels. The importance of hyperparameter optimization is well known also for classical ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2310.01651</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#25490;&#21015;&#27450;&#39575;&#65288;&#35270;&#35273;&#21644;&#65289;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#32780;&#36805;&#36895;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#65292;&#21363;&#20180;&#32454;&#20998;&#26512;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#20013;&#26159;&#21542;&#36275;&#22815;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#27969;&#34892;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65288;MCQA&#65289;&#20013;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#27169;&#22411;&#26131;&#21463;&#22810;&#39033;&#36873;&#25321;&#25552;&#31034;&#20013;&#31572;&#26696;&#38598;&#30340;&#23545;&#25239;&#24615;&#25490;&#21015;&#25915;&#20987;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27169;&#22411;&#29702;&#24819;&#19978;&#24212;&#35813;&#21644;&#20154;&#31867;&#19968;&#26679;&#23545;&#25552;&#31034;&#25490;&#21015;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#19979;&#25345;&#32493;&#23384;&#22312;&#65292;&#24182;&#23384;&#22312;&#20110;&#26368;&#26032;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03452</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Guidance Network For Missing Modality Inference. (arXiv:2309.03452v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03452
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#36807;&#31243;&#20013;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#26631;&#20934;&#22810;&#27169;&#24577;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#38454;&#27573;&#21644;&#25512;&#26029;&#38454;&#27573;&#27169;&#24577;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#35768;&#22810;&#22330;&#26223;&#26080;&#27861;&#28385;&#36275;&#36825;&#26679;&#30340;&#20551;&#35774;&#65292;&#25512;&#26029;&#36807;&#31243;&#20013;&#20250;&#20986;&#29616;&#32570;&#22833;&#27169;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#24314;&#32570;&#22833;&#27169;&#24577;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#22411;&#37096;&#32626;&#31995;&#32479;&#32780;&#35328;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#26041;&#38754;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#32593;&#32476;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#34920;&#31034;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;&#22312;&#26292;&#21147;&#26816;&#27979;&#30340;&#29616;&#23454;&#29983;&#27963;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal models have gained significant success in recent years. Standard multimodal approaches often assume unchanged modalities from training stage to inference stage. In practice, however, many scenarios fail to satisfy such assumptions with missing modalities during inference, leading to limitations on where multimodal models can be applied. While existing methods mitigate the problem through reconstructing the missing modalities, it increases unnecessary computational cost, which could be just as critical, especially for large, deployed systems. To solve the problem from both sides, we propose a novel guidance network that promotes knowledge sharing during training, taking advantage of the multimodal representations to train better single-modality models for inference. Real-life experiment in violence detection shows that our proposed framework trains single-modality models that significantly outperform its traditionally trained counterparts while maintaining the same inference 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.11635</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#22312;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive Learning for Cross-Subject EEG-based Emotion Recognition. (arXiv:2308.11635v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; DS-AGC&#65292;&#29992;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#24182;&#34892;&#27969;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#35299;&#20915;&#20998;&#24067;&#24046;&#24322;&#21644;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270; (EEG) &#26159;&#19968;&#31181;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#30340;&#23458;&#35266;&#24773;&#32490;&#35782;&#21035;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#22522;&#20110;&#33041;&#30005;&#30340;&#24773;&#32490;&#35782;&#21035;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#21452;&#27969;&#33258;&#27880;&#24847;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550; (&#31616;&#31216;&#20026; DS-AGC)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#36328;&#20027;&#20307;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#20013;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;DS-AGC &#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#27969;&#65292;&#29992;&#20110;&#25552;&#21462;&#38750;&#32467;&#26500;&#21270;&#21644;&#32467;&#26500;&#21270;&#33041;&#30005;&#29305;&#24449;&#12290;&#38750;&#32467;&#26500;&#21270;&#27969;&#37319;&#29992;&#21322;&#30417;&#30563;&#22810;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#28304;&#22495;&#12289;&#26410;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#30693;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#32467;&#26500;&#21270;&#27969;&#21017;&#24320;&#21457;&#20102;&#19968;&#31181;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21322;&#30417;&#30563;&#26041;&#24335;&#20174;&#22810;&#20010;&#33041;&#30005;&#36890;&#36947;&#20013;&#25552;&#21462;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#27880;&#24847;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-Attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.07292</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Urban Spatiotemporal Data Synthesis via Neural Disaggregation. (arXiv:2306.07292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#26469;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#30340;&#32454;&#33410;&#32423;&#21035;&#24120;&#24120;&#19982;&#20854;&#25152;&#33021;&#25552;&#20379;&#30340;&#23454;&#38469;&#25928;&#30410;&#21457;&#29983;&#20914;&#31361;&#12290;&#36739;&#19981;&#32454;&#21270;&#30340;&#25968;&#25454;&#21487;&#20197;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#65292;&#20294;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#29306;&#29298;&#20102;&#24320;&#25918;&#25968;&#25454;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#21327;&#21161;&#30740;&#31350;&#30340;&#25215;&#35834;&#12290;&#31867;&#20284;&#20110;&#22478;&#24066;&#29615;&#22659;&#20013;&#65292;&#39640;&#23618;&#27425;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#21487;&#33021;&#20250;&#25513;&#30422;&#22478;&#24066;&#21160;&#24577;&#30340;&#24213;&#23618;&#29305;&#24449;&#65292;&#20302;&#32423;&#21035;&#22320;&#29702;&#21333;&#20803;&#30340;&#21464;&#21270;&#21487;&#33021;&#26356;&#20026;&#26126;&#26174;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#35299;&#31895;&#31961;&#30340;&#20302;&#20998;&#36776;&#29575;&#22320;&#29702;&#21333;&#20803;&#30340;&#32858;&#21512;&#22478;&#24066;&#25968;&#25454;&#65292;&#21512;&#25104;&#32454;&#31890;&#24230;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#22478;&#24066;&#25968;&#25454;&#65292;&#20197;&#22686;&#21152;&#39640;&#24230;&#32858;&#21512;&#30340;&#22478;&#24066;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#23454;&#29616;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#19968;&#20123;&#20256;&#32479;&#20998;&#35299;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#38382;&#39064;-1) &#25105;&#20204;&#23581;&#35797;&#20102;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#31070;&#32463;&#26041;&#27861;&#20063;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The level of granularity of open data often conflicts the benefits it can provide. Less granular data can protect individual privacy, but to certain degrees, sabotage the promise of open data to promote transparency and assist research. Similar in the urban setting, aggregated urban data at high-level geographic units can mask out the underline particularities of city dynamics that may vary at lower areal levels. In this work, we aim to synthesize fine-grained, high resolution urban data, by breaking down aggregated urban data at coarse, low resolution geographic units. The goal is to increase the usability and realize the values as much as possible of highly aggregated urban data. To address the issue of simplicity of some traditional disaggregation methods -- 1) we experimented with numerous neural-based models that are capable of modeling intricate non-linear relationships among features. Neural methods can also leverage both spatial and temporal information concurrently. We showed 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.18493</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Insights from the Design Space Exploration of Flow-Guided Nanoscale Localization. (arXiv:2305.18493v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18493
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#32771;&#34385;&#20102;&#33021;&#28304;&#21644;&#20449;&#21495;&#34928;&#20943;&#31561;&#22240;&#32032;&#65292;&#20026;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22826;&#36203;&#20857;&#26080;&#32447;&#36890;&#20449;&#33021;&#21147;&#30340;&#32435;&#31859;&#35774;&#22791;&#20026;&#22312;&#20154;&#31867;&#34880;&#28082;&#20013;&#36827;&#34892;&#27969;&#23548;&#21521;&#23450;&#20301;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#27492;&#31867;&#23450;&#20301;&#20351;&#24471;&#23558;&#25152;&#24863;&#21463;&#21040;&#30340;&#20107;&#20214;&#30340;&#20301;&#32622;&#19982;&#20107;&#20214;&#26412;&#36523;&#36827;&#34892;&#21305;&#37197;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31934;&#20934;&#21307;&#30103;&#26041;&#38754;&#30340;&#26089;&#26399;&#21644;&#31934;&#20934;&#35786;&#26029;&#12289;&#38477;&#20302;&#25104;&#26412;&#21644;&#20405;&#20837;&#24615;&#12290;&#27969;&#23548;&#21521;&#23450;&#20301;&#20173;&#22788;&#20110;&#21407;&#22987;&#38454;&#27573;&#65292;&#21482;&#26377;&#23569;&#25968;&#35770;&#25991;&#28041;&#21450;&#27492;&#38382;&#39064;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#35780;&#20272;&#20173;&#28982;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#36890;&#24120;&#21482;&#32771;&#34385;&#21333;&#19968;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24573;&#30053;&#20102;&#22312;&#36825;&#31181;&#35268;&#27169;&#65288;&#20363;&#22914;&#65292;&#32435;&#31859;&#22120;&#20214;&#30340;&#33021;&#37327;&#21463;&#38480;&#65289;&#21644;&#23545;&#20110;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#20307;&#20869;&#22826;&#36203;&#20857;&#20256;&#25773;&#30340;&#20005;&#37325;&#34928;&#20943;&#65289;&#19979;&#30456;&#20851;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#35780;&#20272;&#20855;&#26377;&#20302;&#27700;&#24179;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#20197;&#23458;&#35266;&#30340;&#26041;&#24335;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20256;&#36755;&#33021;&#37327;&#28040;&#32791;&#21644;&#20449;&#21495;&#34928;&#20943;&#65292;&#23545;&#27969;&#23548;&#21521;&#32435;&#31859;&#23450;&#20301;&#30340;&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#33021;&#37327;&#28040;&#32791;&#21644;&#23450;&#20301;&#31934;&#24230;&#65289;&#21644;&#25361;&#25112;&#65288;&#20363;&#22914;&#36523;&#20307;&#36816;&#21160;&#21644;&#34880;&#21387;&#65289;&#65292;&#23548;&#33268;&#25105;&#20204;&#21487;&#20197;&#20026;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#25552;&#20379;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nanodevices with Terahertz (THz)-based wireless communication capabilities are providing a primer for flow-guided localization within the human bloodstreams. Such localization is allowing for assigning the locations of sensed events with the events themselves, providing benefits in precision medicine along the lines of early and precise diagnostics, and reduced costs and invasiveness. Flow-guided localization is still in a rudimentary phase, with only a handful of works targeting the problem. Nonetheless, the performance assessments of the proposed solutions are already carried out in a non-standardized way, usually along a single performance metric, and ignoring various aspects that are relevant at such a scale (e.g., nanodevices' limited energy) and for such a challenging environment (e.g., extreme attenuation of in-body THz propagation). As such, these assessments feature low levels of realism and cannot be compared in an objective way. Toward addressing this issue, we account for t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12653</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20581;&#22766;&#20840;&#24687;&#27627;&#31859;&#27874;&#27874;&#26463;&#25104;&#24418;
&lt;/p&gt;
&lt;p&gt;
Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22825;&#32447;&#38453;&#21015;&#30340;&#27874;&#26463;&#25104;&#24418;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;5G&#21644;&#21363;&#23558;&#25512;&#20986;&#30340;6G&#20013;&#65292;&#22240;&#27492;&#21508;&#31181;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#12289;&#39640;&#32423;&#20248;&#21270;&#31639;&#27861;&#31561;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#21069;&#30740;&#31350;&#26041;&#26696;&#20013;&#20854;&#24615;&#33021;&#30456;&#24403;&#21560;&#24341;&#20154;&#65292;&#20294;&#36890;&#24120;&#24403;&#29615;&#22659;&#25110;&#25968;&#25454;&#38598;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#36805;&#36895;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20855;&#26377;&#24378;&#22823;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#27874;&#26463;&#25104;&#24418;&#32593;&#32476;&#26159;&#26234;&#33021;&#26080;&#32447;&#36890;&#20449;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#27874;&#26463;&#25104;&#24418;&#33258;&#30417;&#30563;&#32593;&#32476;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#28151;&#21512;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#32593;&#32476;&#22312;&#32463;&#20856;&#30340;DeepMIMO&#21644;&#26032;&#30340;WAIR-D&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21407;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#32763;&#35793;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beamforming with large-scale antenna arrays has been widely used in recent years, which is acknowledged as an important part in 5G and incoming 6G. Thus, various techniques are leveraged to improve its performance, e.g., deep learning, advanced optimization algorithms, etc. Although its performance in many previous research scenarios with deep learning is quite attractive, usually it drops rapidly when the environment or dataset is changed. Therefore, designing effective beamforming network with strong robustness is an open issue for the intelligent wireless communications. In this paper, we propose a robust beamforming self-supervised network, and verify it in two kinds of different datasets with various scenarios. Simulation results show that the proposed self-supervised network with hybrid learning performs well in both classic DeepMIMO and new WAIR-D dataset with the strong robustness under the various environments. Also, we present the principle to explain the rationality of this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.00736</link><description>&lt;p&gt;
&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#30340;&#24433;&#21709;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25216;&#24039;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#24067;&#36890;&#24120;&#19981;&#21487;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#24314;&#27169;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;Lipschitz&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#32447;&#24615;&#27169;&#22411;&#25110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#27839;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20018;&#34892;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#26032;&#22411;PAC&#36125;&#21494;&#26031;&#30028;&#38480;&#26469;&#30830;&#23450;&#19968;&#20010;&#38543;&#26426;&#20272;&#35745;&#20540;&#12290;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#27169;&#25311;STOU&#36807;&#31243;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
&lt;/p&gt;</description></item></channel></rss>