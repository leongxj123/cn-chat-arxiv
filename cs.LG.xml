<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.17844</link><description>&lt;p&gt;
&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#30340;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Mechanistic Design and Scaling of Hybrid Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17844
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#29702;&#35774;&#35745;&#21644;&#23610;&#24230;&#21464;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#21487;&#20197;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24320;&#21457;&#26159;&#19968;&#20010;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#35774;&#35745;&#31354;&#38388;&#24191;&#38420;&#12289;&#21407;&#22411;&#21046;&#20316;&#26102;&#38388;&#38271;&#20197;&#21450;&#19982;&#35268;&#27169;&#21270;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#20197;&#31471;&#21040;&#31471;&#26426;&#26800;&#24335;&#26550;&#26500;&#35774;&#35745;&#65288;MAD&#65289;&#31649;&#32447;&#20026;&#22522;&#30784;&#26469;&#31616;&#21270;&#36825;&#19968;&#36807;&#31243;&#65292;&#21253;&#25324;&#23567;&#35268;&#27169;&#33021;&#21147;&#21333;&#20803;&#27979;&#35797;&#65292;&#39044;&#27979;&#23610;&#24230;&#35268;&#24459;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#21512;&#25104;&#30340;&#20196;&#29260;&#25805;&#20316;&#20219;&#21153;&#65292;&#22914;&#21387;&#32553;&#21644;&#22238;&#24518;&#65292;&#26088;&#22312;&#25506;&#32034;&#33021;&#21147;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#27979;&#35797;&#30001;&#21508;&#31181;&#35745;&#31639;&#22522;&#20803;&#26500;&#24314;&#30340;&#26032;&#22411;&#28151;&#21512;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35745;&#31639;&#20248;&#21270;&#21644;&#19968;&#39033;&#26032;&#30340;&#29366;&#24577;&#26368;&#20248;&#21270;&#23610;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;70M&#21040;7B&#21442;&#25968;&#20043;&#38388;&#30340;500&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26524;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;MAD&#21512;&#25104;&#19982;&#35745;&#31639;&#26368;&#20248;&#22256;&#24785;&#24230;&#30456;&#20851;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#26032;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17844v1 Announce Type: new  Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectur
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;</title><link>https://arxiv.org/abs/2403.17632</link><description>&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#33021;&#32791;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#30005;&#21160;&#24494;&#31227;&#21160;&#24037;&#20855;&#22312;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#33021;&#32791;&#24314;&#27169;&#30340;&#22256;&#38590;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#25317;&#22581;&#21644;&#29615;&#22659;&#24694;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#26085;&#30410;&#21152;&#21095;&#65292;&#20984;&#26174;&#20102;&#22312;&#22478;&#24066;&#31354;&#38388;&#25512;&#34892;E-Mobility&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;E-&#28369;&#26495;&#36710;&#21644;E-&#33258;&#34892;&#36710;&#31561;&#24494;&#22411;E-Mobility&#24037;&#20855;&#22312;&#36825;&#19968;&#36716;&#21464;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20026;&#22478;&#24066;&#36890;&#21220;&#32773;&#25552;&#20379;&#21487;&#25345;&#32493;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#30340;&#33021;&#32791;&#27169;&#24335;&#26159;&#24433;&#21709;&#20854;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#23545;&#20110;&#20986;&#34892;&#35268;&#21010;&#20197;&#21450;&#22686;&#24378;&#29992;&#25143;&#22312;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#26102;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#38024;&#23545;&#29305;&#23450;&#31227;&#21160;&#24037;&#20855;&#21644;&#26465;&#20214;&#23450;&#21046;&#30340;&#29289;&#29702;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#25928;&#24615;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#24443;&#24213;&#27169;&#22411;&#35780;&#20272;&#21644;&#39564;&#35777;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29233;&#23572;&#20848;&#37117;&#26575;&#26519;&#25910;&#38598;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#33021;&#32791;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.17447</link><description>&lt;p&gt;
&#21387;&#32553;&#38142;&#65306;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#32452;&#21512;&#21387;&#32553;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21387;&#32553;&#38142;&#8221;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#31561;&#24120;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#65292;&#20294;&#23427;&#20204;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#30340;&#23494;&#38598;&#24615;&#32473;&#36164;&#28304;&#26377;&#38480;&#30340;&#35745;&#31639;&#31995;&#32479;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#23454;&#26102;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#20943;&#36731;&#36127;&#25285;&#65292;&#27169;&#22411;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#35768;&#22810;&#26041;&#27861;&#65292;&#22914;&#37327;&#21270;&#12289;&#21098;&#26525;&#12289;&#25552;&#21069;&#36864;&#20986;&#21644;&#30693;&#35782;&#33976;&#39311;&#24050;&#32463;&#35777;&#26126;&#20102;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#20013;&#20887;&#20313;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#21487;&#20197;&#26126;&#26174;&#30475;&#20986;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20102;&#20854;&#29420;&#29305;&#30340;&#29305;&#24615;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#19988;&#24403;&#23427;&#20204;&#32467;&#21512;&#22312;&#19968;&#36215;&#26102;&#20063;&#21487;&#20197;&#23637;&#29616;&#20986;&#20114;&#34917;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20174;&#20114;&#34917;&#29305;&#24615;&#20013;&#33719;&#30410;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21387;&#32553;&#38142;&#65292;&#23427;&#22312;&#32452;&#21512;&#24207;&#21015;&#19978;&#25805;&#20316;&#65292;&#24212;&#29992;&#36825;&#20123;&#24120;&#35265;&#25216;&#26415;&#26469;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17447v1 Announce Type: new  Abstract: Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classi
&lt;/p&gt;</description></item><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14379</link><description>&lt;p&gt;
&#21367;&#31215;&#27169;&#22411;&#30340;&#24352;&#37327;&#32593;&#32476;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tensor network compressibility of convolutional models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14379
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#21270;&#26159;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#32039;&#20945;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#22240;&#23376;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#24352;&#37327;&#21270;&#22914;&#20309;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#21367;&#31215;&#26680;&#26469;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20195;&#34920;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20043;&#19968;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#19968;&#33324;&#24773;&#20917;&#19979;&#26356;&#22823;&#30340;CNNs&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#36890;&#36807;&#8220;&#24352;&#37327;&#21270;&#8221;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23567;&#23427;&#20204;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#24352;&#37327;&#21270;&#21253;&#25324;&#23558;&#21367;&#31215;&#26680;&#26367;&#25442;&#20026;&#22914;Tucker&#12289;Canonical Polyadic&#20998;&#35299;&#25110;&#21463;&#37327;&#23376;&#21551;&#21457;&#30340;&#20998;&#35299;&#65288;&#22914;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65289;&#31561;&#32039;&#20945;&#30340;&#20998;&#35299;&#65292;&#24182;&#30452;&#25509;&#35757;&#32451;&#20998;&#35299;&#20013;&#30340;&#22240;&#23376;&#65292;&#20197;&#20559;&#21521;&#20110;&#20302;&#31209;&#20998;&#35299;&#12290;&#20294;&#20026;&#20160;&#20040;&#24352;&#37327;&#21270;&#20284;&#20046;&#23545;&#20934;&#30830;&#24615;&#27809;&#26377;&#19981;&#21033;&#24433;&#21709;&#65311;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25130;&#26029;&#23494;&#38598;&#65288;&#38750;&#24352;&#37327;&#21270;&#65289;CNNs&#30340;&#21367;&#31215;&#26680;&#23545;&#20854;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#25506;&#35752;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14379v1 Announce Type: cross  Abstract: Convolutional neural networks (CNNs) represent one of the most widely used neural network architectures, showcasing state-of-the-art performance in computer vision tasks. Although larger CNNs generally exhibit higher accuracy, their size can be effectively reduced by "tensorization" while maintaining accuracy. Tensorization consists of replacing the convolution kernels with compact decompositions such as Tucker, Canonical Polyadic decompositions, or quantum-inspired decompositions such as matrix product states, and directly training the factors in the decompositions to bias the learning towards low-rank decompositions. But why doesn't tensorization seem to impact the accuracy adversely? We explore this by assessing how truncating the convolution kernels of dense (untensorized) CNNs impact their accuracy. Specifically, we truncated the kernels of (i) a vanilla four-layer CNN and (ii) ResNet-50 pre-trained for image classification on CIF
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12354</link><description>&lt;p&gt;
&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;Sim2Real&#65306;&#21033;&#29992;&#22686;&#24378;&#35774;&#22791;&#20449;&#24687;&#25968;&#25454;&#27169;&#25311;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12354
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#65292;&#21363;Sim2Real&#65292;&#29992;&#20110;&#37325;&#24314;&#20809;&#35889;&#20449;&#21495;&#65292;&#22312;&#26377;&#25928;&#25968;&#25454;&#37319;&#26679;&#21644;&#24555;&#36895;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#28857;&#30740;&#31350;&#12290;&#35813;&#24037;&#20316;&#32858;&#28966;&#20110;&#22312;&#20165;&#26377;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#37325;&#24314;&#30495;&#23454;&#19990;&#30028;&#20809;&#35889;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#27169;&#25311;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#25105;&#20204;&#30340;&#20998;&#20809;&#20202;&#35774;&#22791;&#27979;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sim2Real&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12354v1 Announce Type: new  Abstract: This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training. Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts. To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference w
&lt;/p&gt;</description></item><item><title>&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.06009</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#21487;&#38752;LLM&#30340;&#26816;&#27979;&#22120;&#65306;&#23454;&#29616;&#12289;&#29992;&#36884;&#21644;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Detectors for Safe and Reliable LLMs: Implementations, Uses, and Limitations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06009
&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#65292;&#21487;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#39118;&#38505;&#30340;&#24433;&#21709;&#65292;&#20174;&#36755;&#20986;&#19981;&#24544;&#23454;&#21040;&#26377;&#20559;&#35265;&#21644;&#26377;&#27602;&#30340;&#29983;&#25104;&#12290;&#30001;&#20110;&#22260;&#32469;LLMs&#23384;&#22312;&#30340;&#20960;&#20010;&#38480;&#21046;&#24615;&#22240;&#32032;&#65288;&#35757;&#32451;&#25104;&#26412;&#12289;API&#35775;&#38382;&#12289;&#25968;&#25454;&#21487;&#29992;&#24615;&#31561;&#65289;&#65292;&#22312;&#37096;&#32626;&#27169;&#22411;&#26102;&#21487;&#33021;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#26045;&#21152;&#30452;&#25509;&#23433;&#20840;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27491;&#22312;&#21162;&#21147;&#21019;&#24314;&#21644;&#37096;&#32626;&#19968;&#31995;&#21015;&#26816;&#27979;&#22120;&#24211;&#65306;&#32039;&#20945;&#19988;&#26131;&#20110;&#26500;&#24314;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20026;&#21508;&#31181;&#21361;&#23475;&#25552;&#20379;&#26631;&#31614;&#12290;&#38500;&#20102;&#26816;&#27979;&#22120;&#26412;&#36523;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24191;&#27867;&#29992;&#36884;&#8212;&#8212;&#20174;&#20805;&#24403;&#38450;&#25252;&#26639;&#21040;&#20419;&#36827;&#26377;&#25928;&#30340;AI&#27835;&#29702;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#24320;&#21457;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#24037;&#20316;&#65292;&#26088;&#22312;&#20351;&#26816;&#27979;&#22120;&#26356;&#21487;&#38752;&#24182;&#25299;&#23637;&#20854;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06009v1 Announce Type: new  Abstract: Large language models (LLMs) are susceptible to a variety of risks, from non-faithful output to biased and toxic generations. Due to several limiting factors surrounding LLMs (training cost, API access, data availability, etc.), it may not always be feasible to impose direct safety constraints on a deployed model. Therefore, an efficient and reliable alternative is required. To this end, we present our ongoing efforts to create and deploy a library of detectors: compact and easy-to-build classification models that provide labels for various harms. In addition to the detectors themselves, we discuss a wide range of uses for these detector models - from acting as guardrails to enabling effective AI governance. We also deep dive into inherent challenges in their development and discuss future work aimed at making the detectors more reliable and broadening their scope.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04484</link><description>&lt;p&gt;
&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#65306;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Source Matters: Source Dataset Impact on Model Robustness in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#28304;&#25968;&#25454;&#38598;&#30340;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25351;&#20986;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#37325;&#26032;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#24050;&#25104;&#20026;&#21307;&#23398;&#25104;&#20687;&#20998;&#31867;&#31639;&#27861;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#24120;&#21033;&#29992;ImageNet&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#20174;&#33258;&#28982;&#21040;&#21307;&#23398;&#22270;&#20687;&#30340;&#39046;&#22495;&#36716;&#21464;&#20419;&#20351;&#20102;&#35832;&#22914;RadImageNet &#31561;&#26367;&#20195;&#26041;&#26696;&#30340;&#20986;&#29616;&#65292;&#24448;&#24448;&#23637;&#31034;&#20986;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36801;&#31227;&#23398;&#20064;&#20013;&#24615;&#33021;&#25552;&#21319;&#26159;&#26469;&#33258;&#20110;&#25913;&#21892;&#30340;&#27867;&#21270;&#36824;&#26159;&#24555;&#25463;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20844;&#24320;&#30340;&#33016;&#37096;X&#20809;&#29255;&#21644;CT&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#28508;&#22312;&#28151;&#26434;&#22240;&#32032;--&#26080;&#35770;&#26159;&#21512;&#25104;&#30340;&#36824;&#26159;&#20174;&#25968;&#25454;&#20013;&#25277;&#21462;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;ImageNet &#21644; RadImageNet &#23454;&#29616;&#20102;&#21487;&#27604;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#28982;&#32780; ImageNet &#26356;&#23481;&#26131;&#36807;&#25311;&#21512;&#28151;&#26434;&#22240;&#32032;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;ImageNet&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24320;&#23637;&#31867;&#20284;&#23454;&#39564;&#26469;&#37325;&#26032;&#23457;&#35270;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#23454;&#39564;&#21487;&#22312;https://github.com/DovileDo/source-mat &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04484v1 Announce Type: cross  Abstract: Transfer learning has become an essential part of medical imaging classification algorithms, often leveraging ImageNet weights. However, the domain shift from natural to medical images has prompted alternatives such as RadImageNet, often demonstrating comparable classification performance. However, it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning. To address this, we investigate potential confounders -- whether synthetic or sampled from the data -- across two publicly available chest X-ray and CT datasets. We show that ImageNet and RadImageNet achieve comparable classification performance, yet ImageNet is much more prone to overfitting to confounders. We recommend that researchers using ImageNet-pretrained models reexamine their model robustness by conducting similar experiments. Our code and experiments are available at https://github.com/DovileDo/source-mat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.01888</link><description>&lt;p&gt;
&#38646;&#25104;&#26412;&#22522;&#20934;&#19978;&#24322;&#27493;&#22810;&#20445;&#30495;&#24230;&#20248;&#21270;&#30340;&#24555;&#36895;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01888
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;HPO&#26041;&#27861;&#65292;&#36991;&#20813;&#38271;&#26102;&#38388;&#31561;&#24453;&#23454;&#29616;&#24555;&#36895;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#32467;&#26524;&#24448;&#24448;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#30340;&#31934;&#24515;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#30340;&#32791;&#26102;&#24615;&#20351;&#24471;&#36229;&#21442;&#25968;&#20248;&#21270;(HPO)&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#25302;&#24930;&#20102;&#39640;&#25928;HPO&#24037;&#20855;&#30340;&#24320;&#21457;&#12290;&#26412;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20419;&#36827;&#38646;&#25104;&#26412;&#22522;&#20934;&#19979;&#39640;&#25928;&#30340;&#24182;&#34892;HPO&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#23384;&#20648;&#22312;&#25991;&#20214;&#31995;&#32479;&#20013;&#30340;&#20449;&#24687;&#35745;&#31639;&#31934;&#30830;&#30340;&#36820;&#22238;&#39034;&#24207;&#65292;&#28040;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#31561;&#24453;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;HPO&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;</title><link>https://arxiv.org/abs/2403.01695</link><description>&lt;p&gt;
DyCE&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#21387;&#32553;&#21644;&#25193;&#23637;&#30340;&#21160;&#24577;&#21487;&#37197;&#32622;&#36864;&#20986;
&lt;/p&gt;
&lt;p&gt;
DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01695
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#20010;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#23558;&#35774;&#35745;&#32771;&#34385;&#20174;&#24444;&#27492;&#21644;&#22522;&#30784;&#27169;&#22411;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#26377;&#25928;&#37096;&#32626;&#26102;&#65292;&#20351;&#29992;&#32553;&#25918;&#21644;&#21387;&#32553;&#25216;&#26415;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21160;&#24577;&#21387;&#32553;&#26041;&#27861;&#65288;&#22914;&#25552;&#21069;&#36864;&#20986;&#65289;&#36890;&#36807;&#35782;&#21035;&#36755;&#20837;&#26679;&#26412;&#30340;&#22256;&#38590;&#31243;&#24230;&#24182;&#26681;&#25454;&#38656;&#35201;&#20998;&#37197;&#35745;&#31639;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#21160;&#24577;&#26041;&#27861;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#19982;&#38745;&#24577;&#26041;&#27861;&#20849;&#23384;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#29616;&#19978;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#21160;&#24577;&#37096;&#20998;&#30340;&#20219;&#20309;&#21464;&#21270;&#37117;&#20250;&#24433;&#21709;&#21518;&#32493;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#21160;&#24577;&#21387;&#32553;&#35774;&#35745;&#37117;&#26159;&#21333;&#29255;&#30340;&#65292;&#19982;&#22522;&#30784;&#27169;&#22411;&#32039;&#23494;&#38598;&#25104;&#65292;&#20174;&#32780;&#20351;&#20854;&#38590;&#20197;&#36866;&#24212;&#26032;&#39062;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DyCE&#65292;&#19968;&#31181;&#21160;&#24577;&#21487;&#37197;&#32622;&#30340;&#25552;&#21069;&#36864;&#20986;&#26694;&#26550;&#65292;&#20174;&#32780;&#20351;&#35774;&#35745;&#32771;&#34385;&#30456;&#20114;&#35299;&#32806;&#20197;&#21450;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
&lt;/p&gt;</description></item><item><title>PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.00929</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#20026;&#21407;&#35821;&#25645;&#24314;&#20219;&#21153;&#30340;&#26694;&#26550;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00929
&lt;/p&gt;
&lt;p&gt;
PRIME&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#24182;&#23398;&#20064;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#35753;&#26426;&#22120;&#20154;&#23398;&#20250;&#22797;&#26434;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#22240;&#20026;&#22797;&#21512;&#35823;&#24046;&#20250;&#22312;&#20219;&#21153;&#26102;&#27573;&#20869;&#32047;&#31215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRIME&#65288;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#25968;&#25454;&#25928;&#29575;&#27169;&#20223;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#34892;&#20026;&#21407;&#35821;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#20223;&#23398;&#20064;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;PRIME&#36890;&#36807;&#23558;&#20219;&#21153;&#28436;&#31034;&#20998;&#35299;&#20026;&#21407;&#35821;&#24207;&#21015;&#26469;&#25645;&#24314;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#23398;&#20064;&#19968;&#20010;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#26469;&#23545;&#21407;&#35821;&#24207;&#21015;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PRIME&#22312;&#22810;&#38454;&#27573;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#29575;&#27604;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#39640;&#20986;10-34&#65285;&#65292;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#39640;&#20986;20-48&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.18781</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#32423;&#20449;&#24565;&#30340;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#22312;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#36890;&#29992;AISG&#65292;&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#65292;&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#20986;&#29616;&#22312;&#35768;&#22810;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#31995;&#32479;&#20013;&#65292;&#22914;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;IT&#22522;&#30784;&#35774;&#26045;&#65292;&#20449;&#24687;&#19981;&#23545;&#31216;&#20026;&#20915;&#31574;&#23454;&#20307;&#65288;&#29609;&#23478;&#65289;&#30340;&#20915;&#31574;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#19981;&#23545;&#31216;&#20449;&#24687;&#38543;&#26426;&#21338;&#24328;&#65288;AISG&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#26159;&#31163;&#32447;&#30340;&#65292;&#38024;&#23545;&#29305;&#27530;&#31867;&#21035;&#30340;AISG&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#23618;&#27425;&#65292;&#24182;&#19988;&#32570;&#20047;&#36866;&#24212;&#22343;&#34913;&#20559;&#24046;&#30340;&#22312;&#32447;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20551;&#35774;&#22312;&#32447;&#23398;&#20064;&#65288;COL&#65289;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#19987;&#38376;&#38024;&#23545;&#36890;&#29992;AISG&#12290;COL&#32467;&#26500;&#21270;&#20026;&#19968;&#20010;&#20808;&#39564;&#39044;&#27979;&#32773;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#19968;&#32423;&#20449;&#24565;&#21644;&#23545;&#23545;&#25163;&#31574;&#30053;&#30340;&#20027;&#35266;&#39044;&#27979;&#12290;&#38024;&#23545;&#20551;&#35774;&#30340;&#23545;&#25163;&#65292;COL&#36890;&#36807;&#22312;&#32447;&#23637;&#24320;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#26657;&#20934;&#20551;&#35774;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;COL&#20013;&#30340;&#20551;&#35774;&#19982;t&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;&#30340;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#26500;&#36896;&#33719;&#24471;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.12687</link><description>&lt;p&gt;
&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#32780;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on manifolds without manifold learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#27969;&#24418;&#23398;&#20064;&#30340;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#24615;&#26500;&#36896;&#33719;&#24471;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26410;&#30693;&#20998;&#24067;&#38543;&#26426;&#25277;&#26679;&#30340;&#25968;&#25454;&#36827;&#34892;&#20989;&#25968;&#36924;&#36817;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#19982;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#30427;&#34892;&#33539;&#24335;&#30456;&#21453;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#19968;&#27425;&#24615;&#26500;&#36896;&#26041;&#27861;&#65292;&#24182;&#22312;&#27969;&#24418;&#20551;&#35774;&#19979;&#32473;&#20986;&#20102;&#26368;&#20339;&#35823;&#24046;&#30028;&#38480;&#65307;&#21363;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#39640;&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#26410;&#30693;&#23376;&#27969;&#24418;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#12290; Neural Networks 132:253268, 2020 &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#27425;&#24615;&#30452;&#25509;&#26041;&#27861;&#26469;&#23454;&#29616;&#20989;&#25968;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12687v1 Announce Type: new  Abstract: Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space. A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation. This two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation. In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#20998;&#26512;&#20102;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#65283;mybodymychoice&#26631;&#31614;&#30340;&#20998;&#26512;&#65292;&#24182;&#20026;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10230</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#27969;&#20013;&#28418;&#31227;&#26631;&#31614;&#30340;&#26102;&#38388;&#20998;&#26512;&#65306;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Analysis of Drifting Hashtags in Textual Data Streams: A Graph-Based Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#20998;&#26512;&#20102;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#65283;mybodymychoice&#26631;&#31614;&#30340;&#20998;&#26512;&#65292;&#24182;&#20026;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#33258;&#20986;&#29616;&#20197;&#26469;&#23601;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#20204;&#21033;&#29992;&#20114;&#32852;&#32593;&#34920;&#36798;&#23545;&#20219;&#20309;&#20107;&#29289;&#30340;&#30475;&#27861;&#65292;&#20351;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25104;&#20026;&#31038;&#20250;&#20256;&#24863;&#22120;&#12290;&#26368;&#21021;&#30001;Twitter&#25903;&#25345;&#65292;&#29616;&#22312;&#24050;&#22312;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20351;&#29992;&#26631;&#31614;&#12290;&#26631;&#31614;&#26377;&#21161;&#20110;&#26631;&#35760;&#12289;&#36319;&#36394;&#21644;&#23545;&#31867;&#20284;&#20027;&#39064;&#30340;&#24086;&#23376;&#36827;&#34892;&#20998;&#32452;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#36816;&#29992;Girvan-Newman&#26041;&#27861;&#26469;&#20998;&#26512;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#25581;&#31034;&#26631;&#31614;&#31038;&#21306;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;2018&#24180;&#33267;2022&#24180;&#38388;&#30340;&#65283;mybodymychoice&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#20123;&#26631;&#31614;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#26576;&#20010;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#23613;&#31649;&#65283;mybodymychoice&#26631;&#31614;&#26368;&#21021;&#19982;&#22919;&#22899;&#26435;&#21033;&#12289;&#22549;&#32974;&#21644;&#36523;&#20307;&#33258;&#20027;&#26377;&#20851;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10230v1 Announce Type: cross  Abstract: Social media has played an important role since its emergence. People use the internet to express opinions about anything, making social media platforms a social sensor. Initially supported by Twitter, the hashtags are now in use on several social media platforms. Hashtags are helpful to tag, track, and group posts on similar topics. In this paper, we analyze hashtag drifts over time using concepts from graph analysis and textual data streams using the Girvan-Newman method to uncover hashtag communities in annual snapshots. More specifically, we analyzed the #mybodymychoice hashtag between 2018 and 2022. In addition, we offer insights about some hashtags found in the study. Furthermore, our approach can be useful for monitoring changes over time in opinions and sentiment patterns about an entity on social media. Even though the hashtag #mybodymychoice was initially coupled with women's rights, abortion, and bodily autonomy, we observe 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10043</link><description>&lt;p&gt;
&#22914;&#20309;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to validate average calibration for machine learning regression tasks ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#26657;&#20934;&#35823;&#24046;&#65288;CE&#65289;&#20272;&#35745;&#20026;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MSE&#65289;&#19982;&#24179;&#22343;&#26041;&#24046;&#65288;MV&#65289;&#25110;&#24179;&#22343;&#24179;&#26041;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#25110;&#32553;&#25918;&#35823;&#24046;&#65288;ZMS&#65289;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#24471;&#20986;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#27491;&#22914;&#26469;&#33258;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#25152;&#31034;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;CE&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#32676;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#29992;&#20110;&#26657;&#20934;&#27979;&#35797;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ZMS&#32479;&#35745;&#37327;&#19981;&#20855;&#26377;&#36825;&#31181;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#23545;&#26465;&#20214;&#26657;&#20934;&#39564;&#35777;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08522</link><description>&lt;p&gt;
&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fairness Auditing with Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20195;&#29702;&#21327;&#20316;&#19979;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#65292;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#23457;&#35745;&#24037;&#20316;&#20551;&#35774;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#12290;&#26412;&#25991;&#32771;&#34385;&#22810;&#20010;&#20195;&#29702;&#20154;&#23545;&#21516;&#19968;&#24179;&#21488;&#36827;&#34892;&#19981;&#21516;&#20219;&#21153;&#30340;&#23457;&#35745;&#24773;&#20917;&#12290;&#20195;&#29702;&#20154;&#26377;&#20004;&#20010;&#26464;&#26438;&#65306;&#21327;&#20316;&#31574;&#30053;&#65288;&#26159;&#21542;&#36827;&#34892;&#21327;&#35843;&#65289;&#21644;&#25277;&#26679;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20195;&#29702;&#20154;&#29420;&#31435;&#25805;&#20316;&#25110;&#21327;&#20316;&#26102;&#23545;&#23427;&#20204;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#26102;&#21327;&#35843;&#23545;&#23457;&#35745;&#20934;&#30830;&#24615;&#21487;&#33021;&#26377;&#23475;&#65292;&#32780;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#38750;&#21327;&#35843;&#30340;&#21512;&#20316;&#30340;&#23457;&#35745;&#20934;&#30830;&#24615;&#19982;&#21327;&#20316;&#30340;&#26368;&#20248;&#25277;&#26679;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work in fairness audits assumes that agents operate independently. In this paper, we consider the case of multiple agents auditing the same platform for different tasks. Agents have two levers: their collaboration strategy, with or without coordination beforehand, and their sampling method. We theoretically study their interplay when agents operate independently or collaborate. We prove that, surprisingly, coordination can sometimes be detrimental to audit accuracy, whereas uncoordinated collaboration generally yields good results. Experimentation on real-world datasets confirms this observation, as the audit accuracy of uncoordinated collaboration matches that of collaborative optimal sampling.
&lt;/p&gt;</description></item><item><title>PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.02827</link><description>&lt;p&gt;
PowerGraph: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PowerGraph: A power grid benchmark dataset for graph neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02827
&lt;/p&gt;
&lt;p&gt;
PowerGraph&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30005;&#32593;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#20351;&#29992;GNN&#65292;&#24182;&#22686;&#24378;GNN&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#20013;&#32570;&#20047;&#29992;&#20110;GNN&#24212;&#29992;&#30340;&#30005;&#21147;&#32593;&#26684;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#27604;&#65292;GNN&#21487;&#20197;&#28508;&#22312;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#30005;&#21147;&#32593;&#26684;&#29616;&#35937;&#12290;&#30005;&#21147;&#32593;&#26684;&#26159;&#22797;&#26434;&#30340;&#24037;&#31243;&#32593;&#32476;&#65292;&#22825;&#28982;&#36866;&#21512;&#20110;&#22270;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;GNN&#26377;&#28508;&#21147;&#25429;&#25417;&#21040;&#30005;&#21147;&#32593;&#26684;&#30340;&#34892;&#20026;&#65292;&#32780;&#19981;&#29992;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#32423;&#32852;&#25925;&#38556;&#20107;&#20214;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#23548;&#33268;&#30005;&#21147;&#32593;&#26684;&#26029;&#30005;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#21382;&#21490;&#26029;&#30005;&#25968;&#25454;&#38598;&#31232;&#32570;&#19988;&#19981;&#23436;&#25972;&#12290;&#36890;&#24120;&#36890;&#36807;&#35745;&#31639;&#26114;&#36149;&#30340;&#31163;&#32447;&#32423;&#32852;&#25925;&#38556;&#27169;&#25311;&#26469;&#35780;&#20272;&#33030;&#24369;&#24615;&#21644;&#35782;&#21035;&#20851;&#38190;&#32452;&#20214;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22312;&#32447;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of GNN and enhance GNN applicability to diverse disciplines. The community currently lacks public datasets of electrical power grids for GNN applications. Indeed, GNNs can potentially capture complex power grid phenomena over alternative machine learning techniques. Power grids are complex engineered networks that are naturally amenable to graph representations. Therefore, GNN have the potential for capturing the behavior of power grids over alternative machine learning techniques. To this aim, we develop a graph dataset for cascading failure events, which are the major cause of blackouts in electric power grids. Historical blackout datasets are scarce and incomplete. The assessment of vulnerability and the identification of critical components are usually conducted via computationally expensive offline simulations of cascading failures. Instead, we propose using machine learning models for the online detection of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09101</link><description>&lt;p&gt;
&#26397;&#21521;&#22810;&#27493;&#25512;&#29702;&#30340;&#31572;&#26696;&#26657;&#20934;&#32479;&#19968;&#35270;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Answer Calibration for Multi-Step Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25552;&#31034;&#25193;&#23637;&#20102;&#25913;&#36827;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#24120;&#23558;&#22810;&#27493;&#25512;&#29702;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#36335;&#24452;&#29983;&#25104;&#20197;&#29983;&#25104;&#25512;&#29702;&#36335;&#24452;&#65307;&#21644;&#31572;&#26696;&#26657;&#20934;&#21518;&#22788;&#29702;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#26368;&#32456;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#32570;&#20047;&#23545;&#19981;&#21516;&#31572;&#26696;&#26657;&#20934;&#26041;&#27861;&#30340;&#31995;&#32479;&#20998;&#26512;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#26368;&#36817;&#31572;&#26696;&#26657;&#20934;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#35270;&#35282;&#23545;&#36825;&#20123;&#31574;&#30053;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#65292;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#22810;&#36335;&#24452;&#19978;&#30340;&#27493;&#32423;&#21644;&#36335;&#24452;&#32423;&#31572;&#26696;&#26657;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#21512;&#20004;&#31181;&#31574;&#30053;&#30340;&#20248;&#21183;&#20542;&#21521;&#20110;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21487;&#33021;&#21551;&#31034;&#20248;&#21270;&#22810;&#27493;&#25512;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09101v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) employing Chain-of-Thought (CoT) prompting have broadened the scope for improving multi-step reasoning capabilities. We generally divide multi-step reasoning into two phases: path generation to generate the reasoning path(s); and answer calibration post-processing the reasoning path(s) to obtain a final answer. However, the existing literature lacks systematic analysis on different answer calibration approaches. In this paper, we summarize the taxonomy of recent answer calibration techniques and break them down into step-level and path-level strategies. We then conduct a thorough evaluation on these strategies from a unified view, systematically scrutinizing step-level and path-level answer calibration across multiple paths. Experimental results reveal that integrating the dominance of both strategies tends to derive optimal outcomes. Our study holds the potential to illuminate key insights for opti
&lt;/p&gt;</description></item><item><title>FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.09789</link><description>&lt;p&gt;
FLrce: &#20855;&#26377;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09789
&lt;/p&gt;
&lt;p&gt;
FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#30701;&#32570;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;FLrce&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#19981;&#27844;&#38706;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08903</link><description>&lt;p&gt;
PPR: &#22312;&#32500;&#25345;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#21516;&#26102;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#21644;&#36530;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25104;&#21151;&#36827;&#34892;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#24182;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22312;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#25104;&#21151;&#36827;&#34892;&#36530;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#35757;&#32451;&#20462;&#21098;&#24674;&#22797;&#25915;&#20987;&#65288;PPR&#65289;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#21487;&#20197;&#23558;&#19968;&#37096;&#20998;&#23545;&#25239;&#25200;&#21160;&#35774;&#20026;&#38646;&#65292;&#24182;&#20542;&#21521;&#20110;&#20445;&#25345;&#25915;&#20987;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#26377;&#36873;&#25321;&#24615;&#22320;&#37322;&#25918;&#26576;&#20123;&#23545;&#25239;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25200;&#21160;&#23884;&#20837;&#21040;&#20462;&#21098;&#21306;&#22495;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02329</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#23569;&#25968;&#32676;&#20307;&#37117;&#26159;&#24179;&#31561;&#30340;: &#31354;&#31867;&#21035;&#24863;&#30693;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#34920;&#29616;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#31867;&#21035;&#24179;&#34913;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#26412;&#22320;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30001;&#20110;&#36807;&#25311;&#21512;&#26412;&#22320;&#19981;&#24179;&#34913;&#25968;&#25454;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#36739;&#24046;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedED&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#31867;&#21035;&#33976;&#39311;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#20102;&#19982;&#31354;&#31867;&#21035;&#30456;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#25233;&#21046;&#30452;&#25509;&#38459;&#26029;&#20102;&#39044;&#27979;&#32467;&#26524;&#20013;&#23545;&#31354;&#31867;&#21035;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#24212;&#29992;&#39640;&#26031;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26680;&#20989;&#25968;&#26469;&#25429;&#25417;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01198</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes on Cellular Complexes. (arXiv:2311.01198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#24212;&#29992;&#39640;&#26031;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#26680;&#20989;&#25968;&#26469;&#25429;&#25417;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22312;&#22270;&#19978;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#32771;&#34385;&#25299;&#25169;&#24402;&#32435;&#20559;&#32622;&#20135;&#29983;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#26159;&#22312;&#36825;&#20123;&#32467;&#26500;&#19978;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22270;&#20165;&#38480;&#20110;&#23545;&#20004;&#20010;&#39030;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#31181;&#23545;&#31216;&#37197;&#32622;&#65292;&#24182;&#32771;&#34385;&#20102;&#21253;&#25324;&#39030;&#28857;&#12289;&#36793;&#21644;&#23427;&#20204;&#30340;&#19968;&#31181;&#24191;&#20041;&#21270;&#31216;&#20026;&#32454;&#32990;&#30340;&#20132;&#20114;&#20851;&#31995;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#26031;&#36807;&#31243;&#22312;&#32454;&#32990;&#22797;&#21512;&#29289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#23545;&#22270;&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#21487;&#20197;&#25429;&#25417;&#36825;&#20123;&#39640;&#38454;&#32454;&#32990;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#25512;&#23548;&#20986;&#20004;&#20010;&#26032;&#22411;&#26680;&#20989;&#25968;&#65292;&#19968;&#20010;&#26159;&#23545;&#22270;Mat\'ern&#26680;&#36827;&#34892;&#25512;&#24191;&#65292;&#21478;&#19968;&#20010;&#26159;&#39069;&#22806;&#22320;&#28151;&#21512;&#20102;&#19981;&#21516;&#32454;&#32990;&#31867;&#22411;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been considerable interest in developing machine learning models on graphs in order to account for topological inductive biases. In particular, recent attention was given to Gaussian processes on such structures since they can additionally account for uncertainty. However, graphs are limited to modelling relations between two vertices. In this paper, we go beyond this dyadic setting and consider polyadic relations that include interactions between vertices, edges and one of their generalisations, known as cells. Specifically, we propose Gaussian processes on cellular complexes, a generalisation of graphs that captures interactions between these higher-order cells. One of our key contributions is the derivation of two novel kernels, one that generalises the graph Mat\'ern kernel and one that additionally mixes information of different cell types.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00212</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#21046;&#12289;&#21457;&#29616;&#21644;&#25512;&#21160;&#23545;&#31216;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning. (arXiv:2311.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#23384;&#22312;&#20110;&#33258;&#28982;&#30028;&#20013;&#65292;&#24182;&#22312;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#26680;&#24515;&#30340;&#35282;&#33394;&#12290;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#22914;&#24222;&#21152;&#33713;&#19981;&#21464;&#24615;&#65292;&#20351;&#22312;&#22320;&#29699;&#19978;&#23454;&#39564;&#23460;&#20013;&#21457;&#29616;&#30340;&#29289;&#29702;&#23450;&#24459;&#33021;&#22815;&#25512;&#24191;&#21040;&#23431;&#23449;&#30340;&#26368;&#36828;&#22788;&#12290;&#23545;&#31216;&#24615;&#23545;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#23454;&#29616;&#36825;&#31181;&#25512;&#24191;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#20801;&#35768;&#20351;&#29992;&#21442;&#25968;&#26356;&#23569;&#30340;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#19977;&#31181;&#26041;&#24335;&#34701;&#20837;&#23545;&#31216;&#24615;&#65306;1. &#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#32473;&#23450;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#30340;&#26410;&#30693;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23398;&#20064;&#25171;&#30772;&#29992;&#25143;&#25351;&#23450;&#30340;&#20505;&#36873;&#32676;&#20307;&#20869;&#30340;&#23545;&#31216;&#24615;&#26469;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincar\'{e} invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18449</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#20855;&#26377;&#38544;&#34255;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#22312;&#20844;&#20849;&#25919;&#31574;&#39046;&#22495;&#22914;&#35686;&#23519;&#21010;&#21306;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23450;&#20041;&#21487;&#34892;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#20915;&#31574;&#30340;&#39640;&#32500;&#24230;&#65292;&#20854;&#22312;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38544;&#34255;&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HC-LSBO&#65289;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#20915;&#31574;&#31354;&#38388;&#19982;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;HC-LSBO&#25429;&#25417;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#22266;&#26377;&#30340;&#38544;&#34255;&#32422;&#26463;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
&lt;/p&gt;</description></item><item><title>ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.18208</link><description>&lt;p&gt;
ArcheType&#65306;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models. (arXiv:2310.18208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18208
&lt;/p&gt;
&lt;p&gt;
ArcheType&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#28304;&#21015;&#31867;&#22411;&#27880;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35821;&#20041;&#21015;&#31867;&#22411;&#27880;&#37322;&#65288;CTA&#65289;&#26041;&#38754;&#23384;&#22312;&#37325;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#26102;&#22266;&#23450;&#30340;&#35821;&#20041;&#31867;&#22411;&#65307;&#38656;&#35201;&#22823;&#37327;&#30340;&#27599;&#20010;&#31867;&#22411;&#30340;&#35757;&#32451;&#26679;&#26412;&#24182;&#20135;&#29983;&#22823;&#37327;&#36816;&#34892;&#26102;&#25512;&#26029;&#25104;&#26412;&#65307;&#21363;&#20351;&#31867;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#21487;&#33021;&#22312;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#19979;&#38477;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;CTA&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ArcheType&#65292;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#37319;&#26679;&#12289;&#25552;&#31034;&#24207;&#21015;&#21270;&#12289;&#27169;&#22411;&#26597;&#35810;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#65292;&#20174;&#32780;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#20840;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#35299;&#20915;&#21015;&#31867;&#22411;&#27880;&#37322;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#21035;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#30830;&#23450;&#20986;&#25913;&#36827;&#19978;&#19979;&#25991;&#37319;&#26679;&#21644;&#26631;&#31614;&#37325;&#26032;&#26144;&#23556;&#25552;&#20379;&#20102;&#26368;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned C
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2310.15848</link><description>&lt;p&gt;
&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#20844;&#24179;&#24615;&#12289;&#38544;&#31169;&#21644;&#27861;&#35268;&#20934;&#21017;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#36827;&#20837;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#29616;&#26377;&#31639;&#27861;&#26377;&#20102;&#24778;&#20154;&#30340;&#25913;&#36827;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21487;&#20449;&#24615;&#23384;&#22312;&#20005;&#37325;&#25285;&#24551;&#12290;&#31185;&#23398;&#30028;&#33268;&#21147;&#20110;&#24320;&#21457;&#21487;&#20449;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#20854;&#24320;&#21457;&#36807;&#31243;&#20013;&#20005;&#37325;&#20381;&#36182;&#20351;&#29992;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#26469;&#23398;&#20064;&#34892;&#20026;&#30446;&#26631;&#12290;&#25968;&#25454;&#20013;&#30340;&#20219;&#20309;&#32570;&#38519;&#37117;&#26377;&#21487;&#33021;&#30452;&#25509;&#36716;&#21270;&#20026;&#31639;&#27861;&#30340;&#32570;&#38519;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#36127;&#36131;&#20219;&#35780;&#20215;&#26631;&#20934;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#23545;&#31639;&#27861;&#30340;&#21518;&#26399;&#35780;&#20272;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#65292;&#32780;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21333;&#29420;&#32771;&#34385;&#25968;&#25454;&#32452;&#20214;&#20197;&#29702;&#35299;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made its way into various scientific fields, providing astonishing improvements over existing algorithms for a wide variety of tasks. In recent years, there have been severe concerns over the trustworthiness of AI technologies. The scientific community has focused on the development of trustworthy AI algorithms. However, machine and deep learning algorithms, popular in the AI community today, depend heavily on the data used during their development. These learning algorithms identify patterns in the data, learning the behavioral objective. Any flaws in the data have the potential to translate directly into algorithms. In this study, we discuss the importance of Responsible Machine Learning Datasets and propose a framework to evaluate the datasets through a responsible rubric. While existing work focuses on the post-hoc evaluation of algorithms for their trustworthiness, we provide a framework that considers the data component separately to understand it
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#23454;&#29616;&#20102;O(1/k^2)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#34987;&#25193;&#23637;&#21040;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25152;&#26377;&#38382;&#39064;&#31867;&#20013;&#20197;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.10082</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#32479;&#19968;&#26368;&#20248;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
A simple uniformly optimal method without line search for convex optimization. (arXiv:2310.10082v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#26080;&#38656;&#32447;&#25628;&#32034;&#30340;&#20984;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#23454;&#29616;&#20102;O(1/k^2)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#34987;&#25193;&#23637;&#21040;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25152;&#26377;&#38382;&#39064;&#31867;&#20013;&#20197;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#25628;&#32034;&#65288;&#25110;&#22238;&#28335;&#65289;&#31243;&#24207;&#24191;&#27867;&#24212;&#29992;&#20110;&#29992;&#20110;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#38454;&#26041;&#27861;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#26410;&#30693;&#38382;&#39064;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;Lipschitz&#24120;&#25968;&#65289;&#30340;&#38382;&#39064;&#20013;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32447;&#25628;&#32034;&#20013;&#33719;&#24471;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#23545;&#20110;&#35299;&#20915;&#21442;&#25968;&#26410;&#20808;&#32473;&#23450;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#26159;&#22810;&#20313;&#30340;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31867;&#22411;&#31639;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#26465;&#20214;&#24555;&#36895;&#26799;&#24230;&#27861;&#65288;AC-FGM&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20272;&#35745;&#20840;&#23616;Lipschitz&#24120;&#25968;&#25110;&#20351;&#29992;&#32447;&#25628;&#32034;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20339;&#30340;O(1/k^2)&#25910;&#25947;&#36895;&#24230;&#65292;&#29992;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;AC-FGM&#25193;&#23637;&#21040;&#27714;&#35299;&#20855;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#25152;&#26377;&#20855;&#26377;&#25152;&#38656;&#31934;&#24230;&#35299;&#30340;&#38382;&#39064;&#31867;&#20013;&#33258;&#21160;&#22320;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Line search (or backtracking) procedures have been widely employed into first-order methods for solving convex optimization problems, especially those with unknown problem parameters (e.g., Lipschitz constant). In this paper, we show that line search is superfluous in attaining the optimal rate of convergence for solving a convex optimization problem whose parameters are not given a priori. In particular, we present a novel accelerated gradient descent type algorithm called auto-conditioned fast gradient method (AC-FGM) that can achieve an optimal $\mathcal{O}(1/k^2)$ rate of convergence for smooth convex optimization without requiring the estimate of a global Lipschitz constant or the employment of line search procedures. We then extend AC-FGM to solve convex optimization problems with H\"{o}lder continuous gradients and show that it automatically achieves the optimal rates of convergence uniformly for all problem classes with the desired accuracy of the solution as the only input. Fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08164</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders. (arXiv:2310.08164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;RLHF&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#36807;RLHF&#35843;&#25972;&#30340;&#29256;&#26412;&#30340;&#28608;&#27963;&#26469;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#33258;&#32534;&#30721;&#22120;&#38544;&#34255;&#31354;&#38388;&#26469;&#35782;&#21035;&#21453;&#26144;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24773;&#26223;&#65292;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#20196;&#29260;-&#22870;&#21169;&#26144;&#23556;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#37322;&#23398;&#20064;&#22870;&#21169;&#21644;&#24191;&#27867;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22870;&#21169;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#23436;&#25972;&#24615;&#30340;&#25277;&#35937;&#36817;&#20284;&#20540;&#65292;&#36825;&#20026;&#30830;&#20445;&#25351;&#23450;&#30446;&#26631;&#21644;&#27169;&#22411;&#34892;&#20026;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.
&lt;/p&gt;</description></item><item><title>CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.07794</link><description>&lt;p&gt;
CRITERIA&#65306;&#19968;&#31181;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07794
&lt;/p&gt;
&lt;p&gt;
CRITERIA&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#24615;&#33021;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#27979;&#35797;&#26159;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;&#36739;&#24120;&#35265;&#30340;&#24773;&#20917;&#65288;&#22914;&#24033;&#33322;&#65289;&#23384;&#22312;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#23545;&#25152;&#26377;&#24773;&#20917;&#36827;&#34892;&#24179;&#22343;&#35745;&#31639;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#33021;&#25552;&#20379;&#26377;&#20851;&#27169;&#22411;&#24615;&#33021;&#30340;&#27934;&#23519;&#65292;&#26080;&#35770;&#26159;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#23427;&#20204;&#33021;&#21542;&#33391;&#22909;&#22788;&#29702;&#65292;&#36824;&#26159;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#20801;&#35768;&#21644;&#22810;&#26679;&#21270;&#12290;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#34913;&#37327;&#36712;&#36857;&#21487;&#20801;&#35768;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#34917;&#20805;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#22914;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65288;CRITERIA&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;&#36947;&#36335;&#32467;&#26500;&#12289;&#27169;&#22411;&#24615;&#33021;&#21644;&#25968;&#25454;&#29305;&#24615;&#25552;&#21462;&#39550;&#39542;&#22330;&#26223;&#30340;&#26041;&#27861;&#65292;&#20197;&#36827;&#34892;&#31934;&#32454;&#25490;&#21517;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benchmarking is a common method for evaluating trajectory prediction models for autonomous driving. Existing benchmarks rely on datasets, which are biased towards more common scenarios, such as cruising, and distance-based metrics that are computed by averaging over all scenarios. Following such a regiment provides a little insight into the properties of the models both in terms of how well they can handle different scenarios and how admissible and diverse their outputs are. There exist a number of complementary metrics designed to measure the admissibility and diversity of trajectories, however, they suffer from biases, such as length of trajectories.  In this paper, we propose a new benChmarking paRadIgm for evaluaTing trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a method for extracting driving scenarios at varying levels of specificity according to the structure of the roads, models' performance, and data properties for fine-grained ranking of prediction 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.03708</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#35270;&#21516;&#20161;&#65306;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#20559;&#22909;&#35757;&#32451;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#19982;&#26222;&#36890;&#26631;&#35760;&#32773;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#21487;&#33021;&#19981;&#36866;&#24212;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26041;&#27861;&#36873;&#25321;&#36890;&#36807;&#25910;&#38598;&#22810;&#32500;&#24230;&#21453;&#39304;&#24182;&#20026;&#27599;&#20010;&#32500;&#24230;&#21019;&#24314;&#19981;&#21516;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#26377;&#30410;&#24615;&#65292;&#26080;&#23475;&#24615;&#65292;&#35802;&#23454;&#24615;&#65289;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#22870;&#21169;&#26435;&#37325;&#65292;&#21487;&#20197;&#36890;&#36807;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#23558;LM&#35843;&#25972;&#21040;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#35843;&#22312;MORLHF&#20013;&#19981;&#31283;&#23450;&#19988;&#32791;&#36153;&#36164;&#28304;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#21508;&#31181;&#24120;&#24120;&#30683;&#30462;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#30446;&#26631;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;MODPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#25193;&#23637;&#21040;&#22810;&#20010;&#23545;&#40784;&#30446;&#26631;&#12290;&#22522;&#26412;&#19978;&#65292;MODPO&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;LM&#26469;&#20195;&#34920;&#19981;&#21516;&#30340;&#38598;&#20307;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30446;&#26631;&#21644;&#29305;&#23450;&#26435;&#37325;&#36827;&#34892;&#32452;&#21512;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;LM&#26681;&#25454;MOD&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#31216;&#20026;D-GRW&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.02806</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#22303;&#22756;&#20013;&#30340;&#27700;&#27969;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Data-facilitated Numerical Method for Richards Equation to Model Water Flow Dynamics in Soil. (arXiv:2310.02806v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#65292;&#31216;&#20026;D-GRW&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#19979;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#21306;&#22303;&#22756;&#28287;&#24230;&#30340;&#30417;&#27979;&#23545;&#20110;&#31934;&#23494;&#20892;&#19994;&#12289;&#26234;&#33021;&#28748;&#28297;&#21644;&#24178;&#26097;&#39044;&#38450;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#36890;&#36807;&#27714;&#35299;Richards&#26041;&#31243;&#36825;&#26679;&#30340;&#27700;&#25991;&#27169;&#22411;&#26469;&#27169;&#25311;&#22303;&#22756;&#30340;&#26102;&#31354;&#27700;&#27969;&#21160;&#21147;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#27861;&#12290;&#36825;&#31181;&#25968;&#20540;&#35299;&#27861;&#34987;&#31216;&#20026;D-GRW&#65288;Data-facilitated global Random Walk&#65289;&#26041;&#27861;&#65292;&#23427;&#22312;&#26377;&#38480;&#20307;&#31215;&#31163;&#25955;&#21270;&#26694;&#26550;&#20013;&#21327;&#21516;&#22320;&#25972;&#21512;&#20102;&#33258;&#36866;&#24212;&#32447;&#24615;&#21270;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#38543;&#26426;&#28216;&#36208;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#20135;&#29983;&#31934;&#30830;&#30340;Richards&#26041;&#31243;&#25968;&#20540;&#35299;&#65292;&#24182;&#19988;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#36890;&#36807;&#19977;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;D-GRW&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#36136;&#37327;&#23432;&#24658;&#24615;&#33021;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20934;&#25968;&#20540;&#35299;&#27861;&#21644;&#21830;&#29992;&#36719;&#20214;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root-zone soil moisture monitoring is essential for precision agriculture, smart irrigation, and drought prevention. Modeling the spatiotemporal water flow dynamics in soil is typically achieved by solving a hydrological model, such as the Richards equation which is a highly nonlinear partial differential equation (PDE). In this paper, we present a novel data-facilitated numerical method for solving the mixed-form Richards equation. This numerical method, which we call the D-GRW (Data-facilitated global Random Walk) method, synergistically integrates adaptive linearization scheme, neural networks, and global random walk in a finite volume discretization framework to produce accurate numerical solutions of the Richards equation with guaranteed convergence under reasonable assumptions. Through three illustrative examples, we demonstrate and discuss the superior accuracy and mass conservation performance of our D-GRW method and compare it with benchmark numerical methods and commercial so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15238</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#23398;&#20064;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Using Generated Privileged Information by Text-to-Image Diffusion Models. (arXiv:2309.15238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#23398;&#29983;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#39069;&#22806;&#30340;&#25968;&#25454;&#34920;&#31034;&#20013;&#33719;&#30410;&#65292;&#36825;&#34987;&#31216;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#19981;&#30475;&#21040;&#39069;&#22806;&#34920;&#31034;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#33719;&#24471;&#29305;&#26435;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#29305;&#26435;&#20449;&#24687;&#12290;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#21407;&#22987;&#25991;&#26412;&#26679;&#26412;&#36827;&#19968;&#27493;&#29992;&#20110;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26550;&#26500;&#26469;&#35757;&#32451;&#22810;&#27169;&#24577;&#25945;&#24072;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22810;&#27169;&#24577;&#25945;&#24072;&#30340;&#30693;&#35782;&#34987;&#33976;&#39311;&#21040;&#22522;&#20110;&#25991;&#26412;&#30340;&#65288;&#21333;&#27169;&#24577;&#65289;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#29305;&#26435;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#31216;&#20026;&#21033;&#29992;&#29983;&#25104;&#30340;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUGPI&#65289;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning Using Privileged Information is a particular type of knowledge distillation where the teacher model benefits from an additional data representation during training, called privileged information, improving the student model, which does not see the extra representation. However, privileged information is rarely available in practice. To this end, we propose a text classification framework that harnesses text-to-image diffusion models to generate artificial privileged information. The generated images and the original text samples are further used to train multimodal teacher models based on state-of-the-art transformer-based architectures. Finally, the knowledge from multimodal teachers is distilled into a text-based (unimodal) student. Hence, by employing a generative model to produce synthetic data as privileged information, we guide the training of the student model. Our framework, called Learning Using Generated Privileged Information (LUGPI), yields noticeable performance g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21487;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#23380;&#38553;&#23610;&#24230;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.12864</link><description>&lt;p&gt;
&#33258;&#21160;&#21152;&#26435;&#30340;&#36125;&#21494;&#26031;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#21644;&#40065;&#26834;&#20272;&#35745;&#22312;&#23380;&#38553;&#23610;&#24230;&#28342;&#35299;&#22270;&#20687;&#30340;&#22810;&#20219;&#21153;&#21453;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution. (arXiv:2308.12864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21487;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#23380;&#38553;&#23610;&#24230;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#25105;&#20204;&#33021;&#22815;&#21487;&#38752;&#22320;&#22788;&#29702;&#21253;&#21547;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#30340;&#21453;&#24212;&#21453;&#38382;&#39064;&#12290;&#23380;&#38553;&#23610;&#24230;&#30340;&#21453;&#24212;&#27969;&#21160;&#24314;&#27169;&#20026;&#30740;&#31350;&#23439;&#35266;&#24615;&#36136;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#30340;&#28436;&#21464;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21463;&#21040;&#30456;&#20851;&#30340;X&#23556;&#32447;&#24494;&#35745;&#31639;&#39640;&#24230;&#20998;&#36776;&#29575;&#25104;&#20687; (X&#23556;&#32447;&#24494;CT) &#36807;&#31243;&#20013;&#30340;&#25104;&#20687;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#20102;&#24615;&#36136;&#20272;&#35745;&#20013;&#30340;&#24046;&#24322;&#12290;&#21160;&#21147;&#23398;&#21442;&#25968;&#30340;&#35780;&#20272;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#21453;&#24212;&#31995;&#25968;&#26159;&#20851;&#38190;&#21442;&#25968;&#65292;&#20854;&#25968;&#20540;&#33539;&#22260;&#24456;&#24191;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#24230;&#37327;&#21270;&#38598;&#25104;&#21040;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#30830;&#20445;&#20102;&#23380;&#38553;&#23610;&#24230;&#27169;&#22411;&#30340;&#21487;&#38752;&#26657;&#20934;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#21453;&#24212;&#21453;&#38382;&#39064;&#30340;&#22810;&#20219;&#21153;&#20844;&#24335;&#65292;&#23558;&#25968;&#25454;&#39537;&#21160;&#21644;&#29289;&#29702;&#24314;&#27169;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a novel data assimilation strategy in pore-scale imaging and demonstrate that this makes it possible to robustly address reactive inverse problems incorporating Uncertainty Quantification (UQ). Pore-scale modeling of reactive flow offers a valuable opportunity to investigate the evolution of macro-scale properties subject to dynamic processes. Yet, they suffer from imaging limitations arising from the associated X-ray microtomography (X-ray microCT) process, which induces discrepancies in the properties estimates. Assessment of the kinetic parameters also raises challenges, as reactive coefficients are critical parameters that can cover a wide range of values. We account for these two issues and ensure reliable calibration of pore-scale modeling, based on dynamical microCT images, by integrating uncertainty quantification in the workflow.  The present method is based on a multitasking formulation of reactive inverse problems combining data-driven and physics
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10875</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21450;&#20854;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#29983;&#29289;&#32479;&#35745;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#21046;&#36896;&#19994;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries. (arXiv:2308.10875v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10875
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;CSO-MA&#65292;&#36890;&#36807;&#22810;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#24212;&#29992;&#23637;&#31034;&#20102;&#20854;&#28789;&#27963;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#31185;&#39046;&#22495;&#20013;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#21463;&#33258;&#28982;&#21551;&#21457;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#31361;&#21464;&#20195;&#29702;&#30340;&#31454;&#20105;&#24615;&#32676;&#20307;&#20248;&#21270;&#22120;(CSO-MA)&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#31454;&#20105;&#23545;&#25163;&#22312;&#32479;&#35745;&#31185;&#23398;&#20013;&#21508;&#31181;&#20248;&#21270;&#38382;&#39064;&#19978;&#30340;&#28789;&#27963;&#24615;&#21644;&#36229;&#36234;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#39640;&#25928;&#19988;&#21487;&#20197;&#25972;&#21512;&#21508;&#31181;&#25104;&#26412;&#32467;&#26500;&#25110;&#22810;&#20010;&#29992;&#25143;&#25351;&#23450;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#24212;&#29992;&#21253;&#25324;(i)&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#36890;&#36807;&#21333;&#32454;&#32990;&#24191;&#20041;&#36235;&#21183;&#27169;&#22411;&#25214;&#21040;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#20197;&#30740;&#31350;&#20266;&#26102;&#24577;&#65292;(ii) &#20272;&#35745;&#25945;&#32946;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;Rasch&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;(iii) &#22312;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#27169;&#22411;&#20013;&#20026;Cox&#22238;&#24402;&#25214;&#21040;M-&#20272;&#35745;&#65292;(iv) &#30697;&#38453;&#34917;&#20840;&#20197;&#22635;&#34917;&#20004;&#20010;&#36830;&#36830;&#19981;&#36890;&#22270;&#20013;&#30340;&#32570;&#22833;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two com
&lt;/p&gt;</description></item><item><title>RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07922</link><description>&lt;p&gt;
RAVEN&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models. (arXiv:2308.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07922
&lt;/p&gt;
&lt;p&gt;
RAVEN&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#27604;ATLAS&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;ATLAS&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#39044;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RAVEN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#33945;&#29305;&#21345;&#27931;&#35821;&#35328;&#24314;&#27169;&#21644;&#21069;&#32512;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#34701;&#21512;&#23398;&#20064;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#19978;&#19979;&#25991;&#31034;&#20363;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#27169;&#22411;&#20462;&#25913;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RAVEN&#22312;&#26576;&#20123;&#22330;&#26223;&#19979;&#26126;&#26174;&#20248;&#20110;ATLAS&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of the state-of-the-art ATLAS model and identify its limitations in in-context learning, primarily due to a mismatch between pretraining and testing, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training or model modifications. Through extensive experiments, we demonstrate that RAVEN significantly outperforms ATLAS and achieves results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder lang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.07424</link><description>&lt;p&gt;
&#36890;&#36807;&#25351;&#25968;&#20542;&#26012;&#35299;&#20915;RTB&#24066;&#22330;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Distribution Shift in RTB Markets via Exponential Tilting. (arXiv:2308.07424v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21487;&#33021;&#26159;&#24615;&#33021;&#19979;&#38477;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#20559;&#31227;&#30340;&#29305;&#24615;&#65292;&#20027;&#35201;&#38024;&#23545;&#23454;&#26102;&#31454;&#20215;&#65288;RTB&#65289;&#24066;&#22330;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26679;&#26412;&#36873;&#25321;&#20559;&#24046;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#36825;&#20004;&#32773;&#22343;&#26159;&#20998;&#24067;&#20559;&#31227;&#30340;&#24378;&#26377;&#21147;&#35825;&#22240;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ExTRA&#65288;Exponential Tilt Reweighting Alignment&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30001;Marty&#31561;&#20154;&#65288;2023&#65289;&#25552;&#20986;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;ExTRA&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#28304;&#25968;&#25454;&#19978;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#26368;&#23567;&#21270;&#21152;&#26435;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#28857;&#26159;&#23427;&#33021;&#22815;&#20351;&#29992;&#26377;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#20559;&#31227;&#30340;&#24615;&#36136;&#65292;&#24182;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shift in machine learning models can be a primary cause of performance degradation. This paper delves into the characteristics of these shifts, primarily motivated by Real-Time Bidding (RTB) market models. We emphasize the challenges posed by class imbalance and sample selection bias, both potent instigators of distribution shifts. This paper introduces the Exponential Tilt Reweighting Alignment (ExTRA) algorithm, as proposed by Marty et al. (2023), to address distribution shifts in data. The ExTRA method is designed to determine the importance weights on the source data, aiming to minimize the KL divergence between the weighted source and target datasets. A notable advantage of this method is its ability to operate using labeled source data and unlabeled target data. Through simulated real-world data, we investigate the nature of distribution shift and evaluate the applicacy of the proposed model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.16062</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#65292;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Implicit Behavior Cloning and Dynamic Movement Primitive to Facilitate Reinforcement Learning for Robot Motion Planning. (arXiv:2307.16062v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#20419;&#36827;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#65292;&#20197;&#21450;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#35268;&#21010;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#38754;&#20020;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26694;&#26550;&#65292;&#21033;&#29992;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21644;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#26469;&#25552;&#39640;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#38544;&#24335;&#34892;&#20026;&#20811;&#38534;&#21033;&#29992;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#32780;&#21160;&#24577;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#19968;&#31181;&#21551;&#21457;&#24335;&#27169;&#22411;&#65292;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#26356;&#31616;&#21333;&#30340;&#35268;&#21010;&#31354;&#38388;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#25342;&#21462;-&#25918;&#32622;&#23454;&#39564;&#21019;&#24314;&#20102;&#20154;&#31867;&#31034;&#33539;&#25968;&#25454;&#38598;&#65292;&#20379;&#31867;&#20284;&#30740;&#31350;&#20351;&#29992;&#12290;&#22312;&#20223;&#30495;&#27604;&#36739;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20855;&#26377;&#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#24471;&#20998;&#12290;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#31616;&#21333;&#32452;&#35013;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) for motion planning of multi-degree-of-freedom robots still suffers from low efficiency in terms of slow training speed and poor generalizability. In this paper, we propose a novel RL-based robot motion planning framework that uses implicit behavior cloning (IBC) and dynamic movement primitive (DMP) to improve the training speed and generalizability of an off-policy RL agent. IBC utilizes human demonstration data to leverage the training speed of RL, and DMP serves as a heuristic model that transfers motion planning into a simpler planning space. To support this, we also create a human demonstration dataset using a pick-and-place experiment that can be used for similar studies. Comparison studies in simulation reveal the advantage of the proposed method over the conventional RL agents with faster training speed and higher scores. A real-robot experiment indicates the applicability of the proposed method to a simple assembly task. Our work provides a novel pe
&lt;/p&gt;</description></item><item><title>IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.10323</link><description>&lt;p&gt;
IncDSI&#65306;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IncDSI: Incrementally Updatable Document Retrieval. (arXiv:2307.10323v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10323
&lt;/p&gt;
&lt;p&gt;
IncDSI&#26159;&#19968;&#31181;&#36882;&#22686;&#21487;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#25913;&#21464;&#32593;&#32476;&#21442;&#25968;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#29616;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20855;&#26377;&#19982;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#36895;&#24230;&#65292;&#33021;&#22815;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#25628;&#32034;&#32034;&#24341;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#31181;&#25991;&#26723;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#25991;&#26723;&#35821;&#26009;&#24211;&#30340;&#20449;&#24687;&#32534;&#30721;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#20013;&#65292;&#24182;&#30452;&#25509;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#22312;&#35757;&#32451;&#27169;&#22411;&#20043;&#21518;&#28155;&#21152;&#26032;&#25991;&#26723;&#24182;&#19981;&#23481;&#26131;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IncDSI&#65292;&#19968;&#31181;&#23454;&#26102;&#28155;&#21152;&#25991;&#26723;&#30340;&#26041;&#27861;&#65288;&#27599;&#20010;&#25991;&#26723;&#32422;20-50&#27627;&#31186;&#65289;&#65292;&#32780;&#26080;&#38656;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#65288;&#29978;&#33267;&#37096;&#20998;&#25968;&#25454;&#38598;&#65289;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#28155;&#21152;&#25991;&#26723;&#30340;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#22312;&#32593;&#32476;&#21442;&#25968;&#19978;&#36827;&#34892;&#26368;&#23567;&#25913;&#21464;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#36895;&#24230;&#26356;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#26102;&#26356;&#26032;&#30340;&#25991;&#26723;&#26816;&#32034;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;IncDSI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.09912</link><description>&lt;p&gt;
&#36208;&#21521;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Quantum Federated Learning. (arXiv:2306.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09912
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#20998;&#31867;&#24635;&#32467;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25216;&#26415;&#29305;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#65292;&#23558;&#37327;&#23376;&#35745;&#31639;&#21644;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#25928;&#29575;&#12290;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#35813;&#20132;&#21449;&#23398;&#31185;&#39046;&#22495;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#26412;&#25991;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#32454;&#33268;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#30340;&#21407;&#29702;&#12289;&#25216;&#26415;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#30830;&#23450;&#20102;&#25972;&#21512;&#36825;&#20123;&#25216;&#26415;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#25353;&#20854;&#29305;&#24449;&#21644;&#25152;&#37319;&#29992;&#30340;&#37327;&#23376;&#25216;&#26415;&#20998;&#31867;&#12290;&#38543;&#30528;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#35745;&#23558;&#22312;&#21508;&#20010;&#34892;&#19994;&#23454;&#29616;&#26356;&#22810;&#30340;&#31361;&#30772;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Federated Learning (QFL) is an emerging interdisciplinary field that merges the principles of Quantum Computing (QC) and Federated Learning (FL), with the goal of leveraging quantum technologies to enhance privacy, security, and efficiency in the learning process. Currently, there is no comprehensive survey for this interdisciplinary field. This review offers a thorough, holistic examination of QFL. We aim to provide a comprehensive understanding of the principles, techniques, and emerging applications of QFL. We discuss the current state of research in this rapidly evolving field, identify challenges and opportunities associated with integrating these technologies, and outline future directions and open research questions. We propose a unique taxonomy of QFL techniques, categorized according to their characteristics and the quantum techniques employed. As the field of QFL continues to progress, we can anticipate further breakthroughs and applications across various industries,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08719</link><description>&lt;p&gt;
&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-policy Evaluation in Doubly Inhomogeneous Environments. (arXiv:2306.08719v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21452;&#38750;&#22343;&#21248;&#29615;&#22659;&#19979;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;(OPE)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;OPE&#26694;&#26550;&#12290;&#35813;&#30740;&#31350;&#23545;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#20570;&#20986;&#20102;&#29702;&#35770;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#22312;&#20004;&#20010;&#20851;&#38190;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20551;&#35774;&#8212;&#8212;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#20010;&#20307;&#22343;&#21248;&#24615;&#22343;&#34987;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22788;&#29702;&#8220;&#21452;&#38750;&#22343;&#21248;&#24615;&#8221;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#22870;&#21169;&#21644;&#35266;&#27979;&#36716;&#31227;&#20989;&#25968;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#27169;&#22411;&#39537;&#21160;&#21644;&#27169;&#22411;&#33258;&#30001;&#26041;&#27861;&#30340;&#36890;&#29992;OPE&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#31163;&#32447;RL&#20013;&#24320;&#21457;&#32479;&#35745;&#19978;&#21487;&#38752;&#30340;OPE&#26041;&#27861;&#30340;&#35770;&#25991;&#65292;&#24182;&#19988;&#28041;&#21450;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#12290;&#35813;&#30740;&#31350;&#28145;&#20837;&#29702;&#35299;&#20102;&#26631;&#20934;RL&#20551;&#35774;&#19981;&#28385;&#36275;&#30340;&#29615;&#22659;&#20013;&#30340;OPE&#65292;&#24182;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#25152;&#25552;&#20986;&#30340;&#20215;&#20540;&#20272;&#35745;&#22120;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24573;&#35270;&#26102;&#38388;&#38750;&#31283;&#23450;&#24615;&#25110;&#20010;&#20307;&#24322;&#36136;&#24615;&#30340;&#31454;&#20105;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to study off-policy evaluation (OPE) under scenarios where two key reinforcement learning (RL) assumptions -- temporal stationarity and individual homogeneity are both violated. To handle the ``double inhomogeneities", we propose a class of latent factor models for the reward and observation transition functions, under which we develop a general OPE framework that consists of both model-based and model-free approaches. To our knowledge, this is the first paper that develops statistically sound OPE methods in offline RL with double inhomogeneities. It contributes to a deeper understanding of OPE in environments, where standard RL assumptions are not met, and provides several practical approaches in these settings. We establish the theoretical properties of the proposed value estimators and empirically show that our approach outperforms competing methods that ignore either temporal nonstationarity or individual heterogeneity. Finally, we illustrate our method on a data set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00840</link><description>&lt;p&gt;
MuZero&#23398;&#21040;&#20102;&#20160;&#20040;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
What model does MuZero learn?. (arXiv:2306.00840v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;MuZero&#31639;&#27861;&#65292;&#21457;&#29616;&#23427;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#24403;&#21069;&#31574;&#30053;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#26395;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#26377;&#21487;&#33021;&#20174;&#22797;&#26434;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#32039;&#20945;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#35268;&#21010;&#33021;&#21147;&#30340;&#25552;&#21319;&#24403;&#21069;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MuZero&#36825;&#20010;&#33879;&#21517;&#30340;&#22522;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#22312;&#23454;&#29616;&#20540;&#31561;&#20215;&#27169;&#22411;&#30340;&#23398;&#20064;&#30446;&#26631;&#19978;&#30340;&#25104;&#23601;&#20197;&#21450;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#23545;&#31574;&#30053;&#25913;&#36827;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#35832;&#22810;&#20854;&#20182;&#35266;&#28857;&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;MuZero&#23398;&#21040;&#30340;&#27169;&#22411;&#26080;&#27861;&#26377;&#25928;&#22320;&#25512;&#24191;&#21040;&#35780;&#20272;&#26410;&#35265;&#31574;&#30053;&#65292;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#35268;&#21010;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#24403;&#21069;&#31574;&#30053;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning has drawn considerable interest in recent years, given its promise to improve sample efficiency. Moreover, when using deep-learned models, it is potentially possible to learn compact models from complex sensor data. However, the effectiveness of these learned models, particularly their capacity to plan, i.e., to improve the current policy, remains unclear. In this work, we study MuZero, a well-known deep model-based reinforcement learning algorithm, and explore how far it achieves its learning objective of a value-equivalent model and how useful the learned models are for policy improvement. Amongst various other insights, we conclude that the model learned by MuZero cannot effectively generalize to evaluate unseen policies, which limits the extent to which we can additionally improve the current policy by planning with the model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.18888</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Shapelet Learning for Unsupervised Multivariate Time Series Representation Learning. (arXiv:2305.18888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#24418;&#24577;&#29255;&#27573;&#23398;&#20064;&#20197;&#21450;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;URL&#65289;&#20855;&#22791;&#23398;&#20064;&#27867;&#21270;&#34920;&#31034;&#20197;&#21450;&#26080;&#38656;&#20351;&#29992;&#19981;&#21487;&#35775;&#38382;&#26631;&#31614;&#23601;&#33021;&#36866;&#29992;&#20110;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#21407;&#26412;&#20026;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#35774;&#35745;&#30340;&#27169;&#22411;&#36827;&#34892;&#32534;&#30721;&#65292;&#19988;&#20381;&#36182;&#20110;&#24378;&#20551;&#35774;&#35774;&#35745;&#23398;&#20064;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36816;&#34892;&#34920;&#29616;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27969;&#34892;&#30340;&#23545;&#27604;&#23398;&#20064;&#33539;&#24335;&#65292;&#23398;&#20064;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#34920;&#31034;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#25506;&#32034;&#26080;&#30417;&#30563;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#20013;&#24418;&#24577;&#29255;&#27573;&#23884;&#20837;&#30340;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#24418;&#24577;&#29255;&#27573;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#21644;&#22810;&#23610;&#24230;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown great promise in unsupervised representation learning (URL) for multivariate time series, because URL has the capability in learning generalizable representation for many downstream tasks without using inaccessible labels. However, existing approaches usually adopt the models originally designed for other domains (e.g., computer vision) to encode the time series data and rely on strong assumptions to design learning objectives, which limits their ability to perform well. To deal with these problems, we propose a novel URL framework for multivariate time series by learning time-series-specific shapelet-based representation through a popular contrasting learning paradigm. To the best of our knowledge, this is the first work that explores the shapelet-based embedding in the unsupervised general-purpose representation learning. A unified shapelet-based encoder and a novel learning objective with multi-grained contrasting and multi-scale alignment are particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#33021;&#24615;&#34917;&#20607; &#8221;&#30340;&#26032;&#39062;&#24402;&#22240;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#40657;&#30418;&#27169;&#22411;&#30340;&#24322;&#24120;&#24402;&#22240;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18440</link><description>&lt;p&gt;
&#40657;&#30418;&#23376;&#24322;&#24120;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Black-Box Anomaly Attribution. (arXiv:2305.18440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21487;&#33021;&#24615;&#34917;&#20607; &#8221;&#30340;&#26032;&#39062;&#24402;&#22240;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#40657;&#30418;&#27169;&#22411;&#30340;&#24322;&#24120;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#40657;&#30418;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20559;&#31163;&#30495;&#23454;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#22914;&#20309;&#21028;&#26029;&#20559;&#24046;&#32972;&#21518;&#30340;&#21407;&#22240;&#26159;&#19968;&#20010;&#22522;&#26412;&#32780;&#26222;&#36941;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#19994;&#21153;&#25110;&#24037;&#19994;AI&#24212;&#29992;&#30340;&#26368;&#32456;&#29992;&#25143;&#32463;&#24120;&#38382;&#30340;&#38382;&#39064;&#12290;&#20559;&#24046;&#21487;&#33021;&#26159;&#30001;&#20110;&#27425;&#20248;&#40657;&#30418;&#27169;&#22411;&#65292;&#25110;&#32773;&#20165;&#20165;&#22240;&#20026;&#26679;&#26412;&#26159;&#24322;&#24120;&#20540;&#12290;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#24212;&#35813;&#33719;&#24471;&#26576;&#31181;&#24418;&#24335;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#21363;&#25351;&#31034;&#36755;&#20837;&#21464;&#37327;&#23545;&#24322;&#24120;&#30340;&#24433;&#21709;&#31243;&#24230;&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the prediction of a black-box machine learning model deviates from the true observation, what can be said about the reason behind that deviation? This is a fundamental and ubiquitous question that the end user in a business or industrial AI application often asks. The deviation may be due to a sub-optimal black-box model, or it may be simply because the sample in question is an outlier. In either case, one would ideally wish to obtain some form of attribution score -- a value indicative of the extent to which an input variable is responsible for the anomaly.  In the present paper we address this task of ``anomaly attribution,'' particularly in the setting in which the model is black-box and the training data are not available. Specifically, we propose a novel likelihood-based attribution framework we call the ``likelihood compensation (LC),'' in which the responsibility score is equated with the correction on each input variable needed to attain the highest possible likelihood. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.06432</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#39118;&#38505;&#27010;&#29575;&#20272;&#35745;&#30340;&#21487;&#25512;&#24191;&#12289;&#29289;&#29702;&#23398;&#22522;&#30784;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalizable Physics-informed Learning Framework for Risk Probability Estimation. (arXiv:2305.06432v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;MC&#26041;&#27861;&#19982;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#23545;&#20110;&#35768;&#22810;&#38543;&#26426;&#23433;&#20840;&#25511;&#21046;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#21644;&#26410;&#30693;&#25110;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#35745;&#31639;&#36825;&#20123;&#39118;&#38505;&#27010;&#29575;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#21450;&#20854;&#26799;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#38271;&#26399;&#39118;&#38505;&#27010;&#29575;&#28385;&#36275;&#26576;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#20107;&#23454;&#65292;&#35813;&#26041;&#31243;&#34920;&#24449;&#20102;&#27010;&#29575;&#20043;&#38388;&#30340;&#37051;&#36817;&#20851;&#31995;&#65292;&#20197;&#23558;MC&#26041;&#27861;&#21644;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#29305;&#23450;&#35757;&#32451;&#37197;&#32622;&#19979;&#32473;&#20986;&#20272;&#35745;&#35823;&#24046;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#21306;&#22495;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#31995;&#32479;&#21464;&#21270;&#65292;&#30456;&#27604;MC&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#23427;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate estimates of long-term risk probabilities and their gradients are critical for many stochastic safe control methods. However, computing such risk probabilities in real-time and in unseen or changing environments is challenging. Monte Carlo (MC) methods cannot accurately evaluate the probabilities and their gradients as an infinitesimal devisor can amplify the sampling noise. In this paper, we develop an efficient method to evaluate the probabilities of long-term risk and their gradients. The proposed method exploits the fact that long-term risk probability satisfies certain partial differential equations (PDEs), which characterize the neighboring relations between the probabilities, to integrate MC methods and physics-informed neural networks. We provide theoretical guarantees of the estimation error given certain choices of training configurations. Numerical results show the proposed method has better sample efficiency, generalizes well to unseen regions, and can adapt to sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.12778</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#21644;&#22870;&#21169;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22870;&#21169;&#21152;&#26435;&#21644;&#25439;&#22833;&#21152;&#26435;&#26799;&#24230;&#21512;&#24182;&#65292;&#20197;&#26356;&#22909;&#22320;&#25552;&#39640;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#20998;&#24067;&#24335;&#20195;&#29702;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#22870;&#21169;&#21152;&#26435;&#65288;R-Weighted&#65289;&#21644;&#25439;&#22833;&#21152;&#26435;&#65288;L-Weighted&#65289;&#26799;&#24230;&#21512;&#24182;&#12290; R / L &#21152;&#26435;&#26041;&#27861;&#26367;&#25442;&#20102;&#35757;&#32451;&#22810;&#20010;&#20195;&#29702;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#20363;&#22914;&#23545;&#26799;&#24230;&#27714;&#21644;&#25110;&#24179;&#22343;&#12290;&#27599;&#20010;&#20195;&#29702;&#22312;&#19981;&#21516;&#21021;&#22987;&#21270;&#29256;&#26412;&#30340;&#30456;&#21516;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#36825;&#20250;&#20174;&#19981;&#21516;&#30340;actor&#33719;&#24471;&#19981;&#21516;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20998;&#26512;&#20102;&#19981;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#20013;&#30340;&#38750;&#20984;&#24615;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#23558;&#26356;&#22810;&#22320;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#21644;&#24050;&#30693;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02146</link><description>&lt;p&gt;
&#24102;&#36830;&#32493;&#20248;&#21270;&#30340;&#32467;&#26500;&#23398;&#20064;&#65306;&#23457;&#24910;&#35266;&#23519;&#21450;&#20854;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Structure Learning with Continuous Optimization: A Sober Look and Beyond. (arXiv:2304.02146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20998;&#26512;&#20102;&#19981;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#20013;&#30340;&#38750;&#20984;&#24615;&#38382;&#39064;&#65292;&#24182;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#23558;&#26356;&#22810;&#22320;&#32771;&#34385;&#20808;&#39564;&#30693;&#35782;&#21644;&#24050;&#30693;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36830;&#32493;&#20248;&#21270;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#34920;&#29616;&#22909;&#22351;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#20351;&#25628;&#32034;&#36807;&#31243;&#26356;&#21487;&#38752;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36830;&#32493;&#26041;&#27861;&#22312;&#20551;&#35774;&#22122;&#22768;&#26041;&#24046;&#30456;&#31561;&#21644;&#19981;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#30340;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#21453;&#20363;&#12289;&#29702;&#35770;&#35777;&#26126;&#21644;&#21487;&#33021;&#30340;&#26367;&#20195;&#35299;&#37322;&#26469;&#34920;&#26126;&#36825;&#31181;&#38472;&#36848;&#22312;&#20219;&#19968;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#23545;&#20110;&#38750;&#30456;&#31561;&#22122;&#22768;&#26041;&#24046;&#20844;&#24335;&#65292;&#38750;&#20984;&#24615;&#21487;&#33021;&#26159;&#20027;&#35201;&#38382;&#39064;&#65292;&#32780;&#36830;&#32493;&#32467;&#26500;&#23398;&#20064;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#21017;&#26080;&#27861;&#22312;&#23398;&#20064;&#36895;&#24230;&#21644;&#23454;&#29616;&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#36138;&#24515;&#25628;&#32034;&#65292;&#24182;&#24314;&#35758;&#34701;&#21512;&#20808;&#39564;&#30693;&#35782;&#25110;&#24050;&#30693;&#32467;&#26500;&#30340;&#26356;&#20581;&#22766;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates in which cases continuous optimization for directed acyclic graph (DAG) structure learning can and cannot perform well and why this happens, and suggests possible directions to make the search procedure more reliable. Reisach et al. (2021) suggested that the remarkable performance of several continuous structure learning approaches is primarily driven by a high agreement between the order of increasing marginal variances and the topological order, and demonstrated that these approaches do not perform well after data standardization. We analyze this phenomenon for continuous approaches assuming equal and non-equal noise variances, and show that the statement may not hold in either case by providing counterexamples, justifications, and possible alternative explanations. We further demonstrate that nonconvexity may be a main concern especially for the non-equal noise variances formulation, while recent advances in continuous structure learning fail to achieve impro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.16424</link><description>&lt;p&gt;
ProductAE&#65306;&#38754;&#21521;&#22823;&#32500;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#32416;&#38169;&#30721;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions. (arXiv:2303.16424v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20960;&#21313;&#24180;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21457;&#26126;&#20102;&#20960;&#20010;&#32416;&#38169;&#30721;&#31867;&#21035;&#65292;&#20294;&#36825;&#20123;&#30721;&#30340;&#35774;&#35745;&#21364;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#38752;&#20154;&#31867;&#26234;&#24935;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#32463;&#20856;&#35774;&#35745;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#22686;&#30410;&#30340;ML&#39537;&#21160;&#30340;&#32416;&#38169;&#30721;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#22823;&#30721;&#32500;&#24230;&#26469;&#35828;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#23436;&#20840;&#30340;ML&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#30340;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Product Autoencoder&#65288;ProductAE&#65289;-&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#65288;&#32534;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#65289;&#23545;&#30340;&#31995;&#21015;-&#26088;&#22312;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#65288;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65289;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#20856;&#20056;&#31215;&#30721;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;ProductAE&#26500;&#24314;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of error-correction codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE) -- a computationally-efficient family of deep learning driven (encoder, decoder) pairs -- aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes usin
&lt;/p&gt;</description></item><item><title>UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13804</link><description>&lt;p&gt;
UniTS: &#19968;&#31181;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniTS: A Universal Time Series Analysis Framework with Self-supervised Representation Learning. (arXiv:2303.13804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13804
&lt;/p&gt;
&lt;p&gt;
UniTS&#26159;&#19968;&#20010;&#24102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#19981;&#21516;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#38754;&#20020;&#30528;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#21644;&#39046;&#22495;&#36716;&#31227;&#31561;&#23454;&#38469;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#26512;&#24182;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;UniTS&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65288;&#25110;&#39044;&#35757;&#32451;&#65289;&#12290; UniTS&#30340;&#32452;&#20214;&#20351;&#29992;&#31867;&#20284;&#20110;sklearn&#30340;API&#36827;&#34892;&#35774;&#35745;&#65292;&#20197;&#20801;&#35768;&#28789;&#27963;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#20351;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#25191;&#34892;&#20998;&#26512;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;UniTS&#22312;&#20116;&#20010;&#20027;&#27969;&#20219;&#21153;&#21644;&#20004;&#20010;&#23454;&#38469;&#35774;&#32622;&#20013;&#30456;&#36739;&#20110;&#20256;&#32479;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#27809;&#26377;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful tool for time series analysis. Existing methods are usually customized for different analysis tasks and face challenges in tackling practical problems such as partial labeling and domain shift. To achieve universal analysis and address the aforementioned problems, we develop UniTS, a novel framework that incorporates self-supervised representation learning (or pre-training). The components of UniTS are designed using sklearn-like APIs to allow flexible extensions. We demonstrate how users can easily perform an analysis task using the user-friendly GUIs, and show the superior performance of UniTS over the traditional task-specific methods without self-supervised pre-training on five mainstream tasks and two practical settings.
&lt;/p&gt;</description></item><item><title>EasyDGL &#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#21547;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#26469;&#22788;&#29702;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65292;&#22312;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#25439;&#22833;&#21644;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#30340;&#32452;&#21512;&#19979;&#23454;&#29616;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12341</link><description>&lt;p&gt;
EasyDGL: &#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph Learning. (arXiv:2303.12341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12341
&lt;/p&gt;
&lt;p&gt;
EasyDGL &#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#21547;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#26469;&#22788;&#29702;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65292;&#22312;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#25439;&#22833;&#21644;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#30340;&#32452;&#21512;&#19979;&#23454;&#29616;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#24456;&#24120;&#35265;&#65292;&#30452;&#25509;&#22312;&#36830;&#32493;&#26102;&#38388;&#22495;&#20013;&#24314;&#27169;&#21160;&#24577;&#22270;&#20197;&#23454;&#29616;&#28789;&#27963;&#24615;&#26159;&#34987;&#27426;&#36814;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#27969;&#27700;&#32447;&#65288;&#21517;&#20026;EasyDGL&#65292;&#20063;&#22240;&#20854;&#30001;DGL&#24037;&#20855;&#21253;&#23454;&#29616;&#32780;&#24471;&#21517;&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65292;&#26082;&#20855;&#26377;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#65292;&#21448;&#26131;&#20110;&#35299;&#37322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#28041;&#21450;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#65306;i&#65289;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#23558;&#36830;&#32493;&#26102;&#38388;&#20998;&#36776;&#29575;&#36171;&#20104;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#36793;&#28155;&#21152;&#20107;&#20214;&#30340;&#22270;&#30340;&#32806;&#21512;&#26102;&#31354;&#21160;&#24577;&#65307;ii&#65289;&#19968;&#20010;&#21512;&#29702;&#30340;&#25439;&#22833;&#65292;&#30001;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#20107;&#20214;&#30340;TPP&#21518;&#39564;&#26368;&#22823;&#21270;&#21644;&#19968;&#20010;&#24102;&#36974;&#34109;&#31574;&#30053;&#30340;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#32452;&#25104;&#65292;&#20854;&#20013;&#28085;&#30422;&#30340;&#20219;&#21153;&#21253;&#25324;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65307;iii&#65289;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#65288;&#20363;&#22914;&#33410;&#28857;&#21644;&#36793;&#30340;&#34920;&#31034;&#65289;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29702;&#35299;&#27599;&#20010;&#36793;&#21644;&#33410;&#28857;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graphs arise in various real-world applications, and it is often welcomed to model the dynamics directly in continuous time domain for its flexibility. This paper aims to design an easy-to-use pipeline (termed as EasyDGL which is also due to its implementation by DGL toolkit) composed of three key modules with both strong fitting ability and interpretability. Specifically the proposed pipeline which involves encoding, training and interpreting: i) a temporal point process (TPP) modulated attention architecture to endow the continuous-time resolution with the coupled spatiotemporal dynamics of the observed graph with edge-addition events; ii) a principled loss composed of task-agnostic TPP posterior maximization based on observed events on the graph, and a task-aware loss with a masking strategy over dynamic graph, where the covered tasks include dynamic link prediction, dynamic node classification and node traffic forecasting; iii) interpretation of the model outputs (e.g., rep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item><item><title>GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.06302</link><description>&lt;p&gt;
GCondNet: &#19968;&#31181;&#25913;&#36827;&#23567;&#22411;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GCondNet: A Novel Method for Improving Neural Networks on Small High-Dimensional Tabular Data. (arXiv:2211.06302v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06302
&lt;/p&gt;
&lt;p&gt;
GCondNet&#21033;&#29992;&#39640;&#32500;&#34920;&#26684;&#25968;&#25454;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#36890;&#36807;&#21019;&#24314;&#22270;&#24418;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#26465;&#20214;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#22788;&#29702;&#39640;&#32500;&#20294;&#26679;&#26412;&#25968;&#37327;&#36739;&#23567;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#24403;&#21069;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26041;&#27861;&#20551;&#23450;&#26435;&#37325;&#20043;&#38388;&#30456;&#20114;&#29420;&#31435;&#65292;&#24403;&#26679;&#26412;&#19981;&#36275;&#20197;&#20934;&#30830;&#20272;&#35745;&#27169;&#22411;&#21442;&#25968;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#23567;&#25968;&#25454;&#22330;&#26223;&#19979;&#65292;&#21033;&#29992;&#20854;&#20182;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GCondNet&#65292;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#38544;&#21547;&#32467;&#26500;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;&#27599;&#20010;&#25968;&#25454;&#32500;&#24230;&#22312;&#26679;&#26412;&#20043;&#38388;&#21019;&#24314;&#19968;&#20010;&#22270;&#24418;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#25552;&#21462;&#36825;&#31181;&#38544;&#21547;&#32467;&#26500;&#65292;&#20197;&#21450;&#35843;&#25972;&#28508;&#22312;&#39044;&#27979; MLP &#32593;&#32476;&#30340;&#31532;&#19968;&#23618;&#21442;&#25968;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#12290;&#36890;&#36807;&#21019;&#24314;&#35768;&#22810;&#23567;&#22270;&#65292;GCondNet &#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#39640;&#32500;&#29305;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28508;&#22312;&#39044;&#27979;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models often struggle with high-dimensional but small sample-size tabular datasets. One reason is that current weight initialisation methods assume independence between weights, which can be problematic when there are insufficient samples to estimate the model's parameters accurately. In such small data scenarios, leveraging additional structures can improve the model's training stability and performance. To address this, we propose GCondNet, a general approach to enhance neural networks by leveraging implicit structures present in tabular data. We create a graph between samples for each data dimension, and utilise Graph Neural Networks (GNNs) for extracting this implicit structure, and for conditioning the parameters of the first layer of an underlying predictor MLP network. By creating many small graphs, GCondNet exploits the data's high-dimensionality, and thus improves the performance of an underlying predictor network. We demonstrate the effectiveness of our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.00313</link><description>&lt;p&gt;
RGMIM: &#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#27491;&#22312;&#24555;&#36895;&#25512;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25513;&#30422;&#20102;&#19968;&#32452;&#36755;&#20837;&#20687;&#32032;&#24182;&#35797;&#22270;&#39044;&#27979;&#36974;&#30422;&#30340;&#20687;&#32032;&#12290;&#20256;&#32479;&#30340;MIM&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#33180;&#31574;&#30053;&#12290;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#30340;&#23567;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;COVID-19&#35782;&#21035;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65288;RGMIM&#65289;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;MAE&#65292;SKD&#65292;Cross&#65292;BYOL&#21644;SimSiam&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2209.07028</link><description>&lt;p&gt;
&#20174;&#23567;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;
&lt;/p&gt;
&lt;p&gt;
Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21464;&#37327;&#25968;&#37327;&#36828;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#22823;&#35268;&#27169;&#22240;&#26524;&#22810;&#26641;&#32467;&#26500;&#65292;&#32780;&#20960;&#20046;&#19981;&#38656;&#35201;&#20219;&#20309;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#20013;&#20272;&#35745;&#22823;&#30340;&#22240;&#26524;&#22810;&#26641;&#30340;&#38382;&#39064;&#12290;&#36825;&#26159;&#22312;&#21464;&#37327;&#25968;&#37327;&#19982;&#26679;&#26412;&#22823;&#23567;&#30456;&#27604;&#38750;&#24120;&#22823;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20197;&#39640;&#20934;&#30830;&#24230;&#24674;&#22797;&#26641;&#24418;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#38500;&#20102;&#19968;&#20123;&#28201;&#21644;&#30340;&#38750;&#36864;&#21270;&#26465;&#20214;&#22806;&#65292;&#22522;&#26412;&#19981;&#38656;&#35201;&#20998;&#24067;&#25110;&#24314;&#27169;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#31454;&#20105;&#27604;&#29575;&#33021;&#22815;&#20809;&#28369;&#22320;&#38543;&#35823;&#24046;&#20943;&#23567;&#32780;&#20174;&#23545;&#25968;&#32423;&#21035;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2107.08277</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Augmented Online Facility Location. (arXiv:2107.08277v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#31454;&#20105;&#27604;&#29575;&#33021;&#22815;&#20809;&#28369;&#22320;&#38543;&#35823;&#24046;&#20943;&#23567;&#32780;&#20174;&#23545;&#25968;&#32423;&#21035;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#36880;&#19968;&#21040;&#36798;&#65292;&#24182;&#19988;&#24517;&#39035;&#22312;&#21040;&#36798;&#26102;&#23558;&#20854;&#65288;&#26080;&#27861;&#25764;&#38144;&#22320;&#65289;&#20998;&#37197;&#32473;&#19968;&#20010;&#24320;&#25918;&#30340;&#35774;&#26045;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#26410;&#26469;&#38656;&#27714;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#23545;&#26368;&#20248;&#35774;&#26045;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#31454;&#20105;&#27604;&#29575;&#20174;&#38656;&#27714;&#25968;&#37327;&#30340;&#23545;&#25968;&#32423;&#21035;&#24179;&#28369;&#22320;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#65292;&#24403;&#38169;&#35823;&#65288;&#21363;&#39044;&#27979;&#20301;&#32622;&#19982;&#26368;&#20248;&#35774;&#26045;&#20301;&#32622;&#20043;&#38388;&#30340;&#24635;&#36317;&#31163;&#65289;&#21521;&#38646;&#38477;&#20302;&#26102;&#12290;&#25105;&#20204;&#37197;&#21512;&#31639;&#27861;&#30340;&#38477;&#20302;&#30028;&#65292;&#24314;&#31435;&#20102;&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#29575;&#23545;&#35823;&#24046;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#26368;&#20248;&#30340;&#65292;&#24120;&#25968;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the research agenda initiated by Munoz &amp; Vassilvitskii [1] and Lykouris &amp; Vassilvitskii [2] on learning-augmented online algorithms for classical online optimization problems, in this work, we consider the Online Facility Location problem under this framework. In Online Facility Location (OFL), demands arrive one-by-one in a metric space and must be (irrevocably) assigned to an open facility upon arrival, without any knowledge about future demands.  We present an online algorithm for OFL that exploits potentially imperfect predictions on the locations of the optimal facilities. We prove that the competitive ratio decreases smoothly from sublogarithmic in the number of demands to constant, as the error, i.e., the total distance of the predicted locations to the optimal facility locations, decreases towards zero. We complement our analysis with a matching lower bound establishing that the dependence of the algorithm's competitive ratio on the error is optimal, up to constant fa
&lt;/p&gt;</description></item></channel></rss>