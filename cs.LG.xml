<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02577</link><description>&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#35299;&#20915;&#23454;&#38469;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving a Real-World Optimization Problem Using Proximal Policy Optimization with Curriculum Learning and Reward Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02577
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#22870;&#21169;&#24037;&#31243;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#26469;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#21407;&#21017;&#21644;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#24037;&#31243;&#35757;&#32451;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#20195;&#29702;&#65292;&#26469;&#20248;&#21270;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#39640;&#21534;&#21520;&#37327;&#24223;&#29289;&#20998;&#31867;&#35774;&#26045;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#26377;&#25928;&#24179;&#34913;&#25805;&#20316;&#23433;&#20840;&#24615;&#12289;&#20248;&#21270;&#23481;&#37327;&#21644;&#26368;&#23567;&#21270;&#36164;&#28304;&#20351;&#29992;&#31561;&#31454;&#20105;&#24615;&#30446;&#26631;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#22522;&#26412;&#20195;&#29702;&#22312;&#36825;&#20123;&#22810;&#37325;&#26631;&#20934;&#19978;&#22833;&#36133;&#35299;&#20915;&#38382;&#39064;&#65292;&#22240;&#20026;&#20854;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02577v1 Announce Type: new  Abstract: We present a proximal policy optimization (PPO) agent trained through curriculum learning (CL) principles and meticulous reward engineering to optimize a real-world high-throughput waste sorting facility. Our work addresses the challenge of effectively balancing the competing objectives of operational safety, volume optimization, and minimizing resource usage. A vanilla agent trained from scratch on these multiple criteria fails to solve the problem due to its inherent complexities. This problem is particularly difficult due to the environment's extremely delayed rewards with long time horizons and class (or action) imbalance, with important actions being infrequent in the optimal policy. This forces the agent to anticipate long-term action consequences and prioritize rare but rewarding behaviours, creating a non-trivial reinforcement learning task. Our five-stage CL approach tackles these challenges by gradually increasing the complexit
&lt;/p&gt;</description></item><item><title>SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.14666</link><description>&lt;p&gt;
SyllabusQA&#65306;&#19968;&#20010;&#35838;&#31243;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SyllabusQA: A Course Logistics Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14666
&lt;/p&gt;
&lt;p&gt;
SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25945;&#23398;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26174;&#33879;&#28508;&#21147;&#20943;&#36731;&#20154;&#31867;&#25945;&#24072;&#30340;&#24037;&#20316;&#37327;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19982;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36825;&#23545;&#23398;&#29983;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#25945;&#24072;&#26469;&#35828;&#26159;&#37325;&#22797;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SyllabusQA&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#65292;&#28085;&#30422;36&#20010;&#19987;&#19994;&#65292;&#21253;&#21547;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#38382;&#39064;&#31867;&#22411;&#21644;&#31572;&#26696;&#26684;&#24335;&#37117;&#26159;&#22810;&#26679;&#30340;&#12290;&#30001;&#20110;&#35768;&#22810;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#32771;&#35797;&#26085;&#26399;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#23545;&#20960;&#20010;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#20256;&#32479;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#25351;&#26631;&#19978;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.12143</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#31561;&#21464;&#34920;&#31034;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Learning Equivariant Representations of Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#26469;&#23454;&#29616;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#22810;&#31181;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#35832;&#22914;&#20998;&#31867;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#31070;&#32463;&#32593;&#32476;&#20013;&#22266;&#26377;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#22797;&#26434;&#30340;&#26435;&#37325;&#20849;&#20139;&#27169;&#24335;&#26469;&#23454;&#29616;&#31561;&#21464;&#24615;&#65292;&#21516;&#26102;&#24573;&#30053;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#21442;&#25968;&#30340;&#35745;&#31639;&#22270;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#24378;&#22823;&#30340;&#20445;&#30041;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21333;&#20010;&#27169;&#22411;&#33021;&#22815;&#23545;&#20855;&#26377;&#22810;&#26679;&#26550;&#26500;&#30340;&#31070;&#32463;&#35745;&#31639;&#22270;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20998;&#31867;&#21644;&#32534;&#36753;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12289;&#39044;&#27979;&#27867;&#21270;&#38169;&#35823;&#31561;&#22810;&#31181;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12143v1 Announce Type: cross  Abstract: Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12116</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#23450;&#20041;&#29983;&#29289;&#21551;&#21457;&#30446;&#26631;&#30340;&#26080;&#30417;&#30563;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Winner-Take-All&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#21644;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#30456;&#32467;&#21512;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36793;&#32536;AI&#30828;&#20214;&#19978;&#30340;&#35745;&#31639;&#36164;&#28304;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#25110;&#32773;&#37319;&#29992;&#36890;&#36807;&#31867;&#20284;Hebbian&#23398;&#20064;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#36880;&#23618;&#35757;&#32451;&#65292;&#20351;&#29992;&#19982;&#30417;&#30563;&#23398;&#20064;&#19981;&#20860;&#23481;&#30340;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#26368;&#32456;&#23618;&#30340;&#32988;&#32773;&#36890;&#21507;&#65288;WTA&#65289;&#36873;&#25321;&#24615;&#30340;&#8220;&#33258;&#23450;&#20041;&#30446;&#26631;&#8221;&#65292;&#24182;&#36890;&#36807;&#29983;&#29289;&#21551;&#21457;&#30340;&#31283;&#24577;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12116v1 Announce Type: cross  Abstract: Current unsupervised learning methods depend on end-to-end training via deep learning techniques such as self-supervised learning, with high computational requirements, or employ layer-by-layer training using bio-inspired approaches like Hebbian learning, using local learning rules incompatible with supervised learning. Both approaches are problematic for edge AI hardware that relies on sparse computational resources and would strongly benefit from alternating between unsupervised and supervised learning phases - thus leveraging widely available unlabeled data from the environment as well as labeled training datasets. To solve this challenge, in this work, we introduce a 'self-defined target' that uses Winner-Take-All (WTA) selectivity at the network's final layer, complemented by regularization through biologically inspired homeostasis mechanism. This approach, framework-agnostic and compatible with both global (Backpropagation) and l
&lt;/p&gt;</description></item><item><title>TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.10923</link><description>&lt;p&gt;
TabPFN&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10923
&lt;/p&gt;
&lt;p&gt;
TabPFN&#27169;&#22411;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#20197;&#31186;&#32423;&#36895;&#24230;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#19987;&#20026;TabPFN&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24320;&#21457;&#30340;Prior-Data Fitted Networks&#65288;PFNs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#38750;&#24120;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#32467;&#26524;&#12290;TabPFN&#27169;&#22411;&#26159;PFN&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#21442;&#25968;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#22312;&#30701;&#30701;&#20960;&#31186;&#38047;&#20869;&#23454;&#29616;&#22810;&#31181;&#20998;&#31867;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#29983;&#25104;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#12290;TabPFN&#22240;&#27492;&#25104;&#20026;&#20102;&#35768;&#22810;&#39046;&#22495;&#24212;&#29992;&#20013;&#38750;&#24120;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;TabPFN&#19987;&#38376;&#35774;&#35745;&#30340;&#27969;&#34892;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#25913;&#36827;&#20801;&#35768;&#27604;&#29616;&#26377;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36991;&#20813;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10923v1 Announce Type: cross  Abstract: The recently developed Prior-Data Fitted Networks (PFNs) have shown very promising results for applications in low-data regimes. The TabPFN model, a special case of PFNs for tabular data, is able to achieve state-of-the-art performance on a variety of classification tasks while producing posterior predictive distributions in mere seconds by in-context learning without the need for learning parameters or hyperparameter tuning. This makes TabPFN a very attractive option for a wide range of domain applications. However, a major drawback of the method is its lack of interpretability. Therefore, we propose several adaptations of popular interpretability methods that we specifically design for TabPFN. By taking advantage of the unique properties of the model, our adaptations allow for more efficient computations than existing implementations. In particular, we show how in-context learning facilitates the estimation of Shapley values by avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.19078</link><description>&lt;p&gt;
&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Smooth Tchebycheff Scalarization for Multi-Objective Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19078
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26799;&#24230;&#22411;&#22810;&#30446;&#26631;&#20248;&#21270;&#65292;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20294;&#20173;&#33021;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#32463;&#24120;&#30456;&#20114;&#20914;&#31361;&#65292;&#19981;&#33021;&#36890;&#36807;&#21333;&#20010;&#35299;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#25214;&#21040;&#24085;&#32047;&#25176;&#35299;&#65292;&#36825;&#20123;&#35299;&#20195;&#34920;&#20102;&#23545;&#20110;&#32473;&#23450;&#38382;&#39064;&#30340;&#19981;&#21516;&#26368;&#20339;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#21487;&#33021;&#19981;&#33021;&#20855;&#22791;&#35299;&#20915;&#19968;&#33324;&#21487;&#24494;&#20998;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#29702;&#35770;&#23646;&#24615;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#20809;&#28369;&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36731;&#37327;&#30340;&#20809;&#28369; Tchebycheff &#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#23427;&#23545;&#20110;&#25214;&#21040;&#25152;&#26377;&#24085;&#32047;&#25176;&#35299;&#20855;&#26377;&#33391;&#22909;&#30340;&#29702;&#35770;&#23646;&#24615;&#65292;&#21516;&#26102;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26174;&#30528;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.15602</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26497;&#23567;&#21270;&#26368;&#20248;&#24615;&#65306;&#36229;&#36234;&#23494;&#24230;&#19979;&#30028;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimality of Score-based Diffusion Models: Beyond the Density Lower Bound Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#20855;&#26377;&#26497;&#23567;&#22343;&#26041;&#35823;&#24046;&#65292;&#21487;&#20197;&#33719;&#24471;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#31361;&#30772;&#20102;&#20165;&#20570;&#27425;&#39640;&#26031;&#20551;&#35774;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#32479;&#35745;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#22823;&#26679;&#26412;&#22330;&#26223;&#19979;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#25277;&#26679;&#30340;&#28176;&#36817;&#35823;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#26680;&#30340;&#24471;&#20998;&#20272;&#35745;&#22120;&#21487;&#20197;&#23454;&#29616;&#23545; $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$ &#30340;&#24471;&#20998;&#20989;&#25968;&#30340;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#20026; $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$&#65292;&#20854;&#20013; $n$ &#21644; $d$ &#20998;&#21035;&#20195;&#34920;&#26679;&#26412;&#22823;&#23567;&#21644;&#32500;&#24230;&#65292;$t$ &#22312;&#19978;&#19979;&#21463;&#21040; $n$ &#30340;&#22810;&#39033;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988; $p_0$ &#26159;&#20219;&#24847;&#27425;&#20122;&#39640;&#26031;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#36825;&#23548;&#33268;&#22312;&#20165;&#36827;&#34892;&#27425;&#39640;&#26031;&#20551;&#35774;&#26102;&#65292;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;&#22914;&#26524;&#27492;&#22806;&#65292;$p_0$ &#23646;&#20110; $\beta\le 2$ &#30340; $\beta$-Sobolev&#31354;&#38388;&#30340;&#38750;&#21442;&#25968;&#26063;&#65292;&#36890;&#36807;&#37319;&#29992;&#26089;&#20572;&#31574;&#30053;&#65292;&#25105;&#20204;&#24471;&#21040;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#26679;&#26412;&#30340;&#20998;&#24067;&#30340;&#24635;&#21464;&#24046;&#35823;&#24046;&#30340;&#19978;&#30028;&#20026; $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15602v1 Announce Type: cross  Abstract: We study the asymptotic error of score-based diffusion model sampling in large-sample scenarios from a non-parametric statistics perspective. We show that a kernel-based score estimator achieves an optimal mean square error of $\widetilde{O}\left(n^{-1} t^{-\frac{d+2}{2}}(t^{\frac{d}{2}} \vee 1)\right)$ for the score function of $p_0*\mathcal{N}(0,t\boldsymbol{I}_d)$, where $n$ and $d$ represent the sample size and the dimension, $t$ is bounded above and below by polynomials of $n$, and $p_0$ is an arbitrary sub-Gaussian distribution. As a consequence, this yields an $\widetilde{O}\left(n^{-1/2} t^{-\frac{d}{4}}\right)$ upper bound for the total variation error of the distribution of the sample generated by the diffusion model under a mere sub-Gaussian assumption. If in addition, $p_0$ belongs to the nonparametric family of the $\beta$-Sobolev space with $\beta\le 2$, by adopting an early stopping strategy, we obtain that the diffusion
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08823</link><description>&lt;p&gt;
RanDumb: &#19968;&#31181;&#36136;&#30097;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08823
&lt;/p&gt;
&lt;p&gt;
RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RanDumb&#26469;&#26816;&#39564;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;RanDumb&#23558;&#21407;&#22987;&#20687;&#32032;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#65292;&#36825;&#20010;&#21464;&#25442;&#36817;&#20284;&#20102;RBF-Kernel&#65292;&#22312;&#30475;&#21040;&#20219;&#20309;&#25968;&#25454;&#20043;&#21069;&#21021;&#22987;&#21270;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#19968;&#33268;&#30340;&#21457;&#29616;&#65306;&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RanDumb&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;RanDumb&#19981;&#23384;&#20648;&#26679;&#26412;&#65292;&#24182;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21333;&#27425;&#36941;&#21382;&#65292;&#19968;&#27425;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#12290;&#23427;&#19982;GDumb&#30456;&#36741;&#30456;&#25104;&#65292;&#22312;GDumb&#24615;&#33021;&#29305;&#21035;&#24046;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#24403;&#23558;RanDumb&#25193;&#23637;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26367;&#25442;&#38543;&#26426;&#21464;&#25442;&#30340;&#24773;&#26223;&#26102;&#65292;&#25105;&#20204;&#24471;&#20986;&#30456;&#21516;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#26082;&#20196;&#20154;&#24778;&#35766;&#21448;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#25552;&#20986;&#20102;&#26080;&#32500;&#24230;&#30340;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#20998;&#24067;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.05280</link><description>&lt;p&gt;
&#26080;&#32500;&#24230;&#25277;&#26679;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
No Dimensional Sampling Coresets for Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#25552;&#20986;&#20102;&#26080;&#32500;&#24230;&#30340;&#26680;&#24515;&#23376;&#38598;&#29992;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#23376;&#38598;&#30340;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#21644;&#20998;&#24067;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#25935;&#24863;&#24615;&#25277;&#26679;&#26694;&#26550;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#30340;&#26680;&#24515;&#23376;&#38598;&#30340;&#24050;&#30693;&#20869;&#23481;&#36827;&#34892;&#20102;&#31934;&#28860;&#21644;&#27010;&#25324;&#12290;&#36825;&#31181;&#26680;&#24515;&#23376;&#38598;&#23547;&#27714;&#36755;&#20837;&#25968;&#25454;&#30340;&#26368;&#23567;&#21487;&#33021;&#23376;&#38598;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#26680;&#24515;&#23376;&#38598;&#19978;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#30830;&#20445;&#23545;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26080;&#32500;&#24230;&#26680;&#24515;&#23376;&#38598;&#65292;&#22240;&#27492;&#22823;&#23567;&#19982;&#32500;&#24230;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20998;&#24067;&#36755;&#20837;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#20379;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;&#20027;&#35201;&#25935;&#24863;&#24615;&#25277;&#26679;&#26041;&#27861;&#30340;Radamacher&#22797;&#26434;&#24230;&#29256;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#19968;&#20010;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17460</link><description>&lt;p&gt;
&#20351;&#26080;&#32447;&#29615;&#22659;&#23545;&#26799;&#24230;&#20272;&#35745;&#22120;&#26377;&#29992;&#65306;&#19968;&#31181;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#36890;&#20449;&#26102;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#28857;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#30693;&#36947;&#36890;&#36947;&#29366;&#24577;&#31995;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#26080;&#32447;&#36890;&#36947;&#21253;&#21547;&#22312;&#23398;&#20064;&#31639;&#27861;&#26412;&#36523;&#20013;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#28010;&#36153;&#36164;&#28304;&#26469;&#20998;&#26512;&#21644;&#28040;&#38500;&#20854;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#26159;&#65292;&#22312;FL&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#19981;&#26159;&#20984;&#30340;&#65292;&#36825;&#20351;&#24471;&#23558;FL&#25193;&#23637;&#21040;ZO&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20197;&#21450;&#21253;&#25324;&#24433;&#21709;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a novel approach to machine learning that allows multiple edge devices to collaboratively train a model without disclosing their raw data. However, several challenges hinder the practical implementation of this approach, especially when devices and the server communicate over wireless channels, as it suffers from communication and computation bottlenecks in this case. By utilizing a communication-efficient framework, we propose a novel zero-order (ZO) method with a one-point gradient estimator that harnesses the nature of the wireless communication channel without requiring the knowledge of the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. The two main difficulties of this work are that in FL, the objective function is usually not convex, which makes the extension of FL to ZO methods challenging, and that including the impa
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2401.15318</link><description>&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#65306;&#21033;&#29992;&#39640;&#26031;&#39128;&#33853;&#21160;&#24577;&#21512;&#25104;&#27969;&#20307;
&lt;/p&gt;
&lt;p&gt;
Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting. (arXiv:2401.15318v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15318
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#21943;&#28293;&#25216;&#26415;&#30456;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#21644;3D&#39640;&#26031;&#21943;&#28293;&#65292;&#21487;&#20197;&#22312;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#36896;&#20986;&#26080;&#21487;&#27604;&#25311;&#30340;&#25928;&#26524;&#65292;&#21516;&#26102;&#23454;&#29616;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#31649;&#29702;&#21644;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#29289;&#29702;&#22522;&#30784;&#21160;&#30011;&#19982;3D&#39640;&#26031;&#21943;&#28293;&#65288;3DGS&#65289;&#30456;&#32467;&#21512;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#22312;&#20351;&#29992;3DGS&#37325;&#24314;&#30340;&#34394;&#25311;&#22330;&#26223;&#20013;&#21019;&#24314;&#26032;&#25928;&#26524;&#12290;&#21033;&#29992;&#39640;&#26031;&#21943;&#28293;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#21160;&#21147;&#23398;&#65288;PBD&#65289;&#22312;&#24213;&#23618;&#34920;&#31034;&#20013;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#20197;&#36830;&#36143;&#30340;&#26041;&#24335;&#31649;&#29702;&#28210;&#26579;&#12289;&#35270;&#22270;&#21512;&#25104;&#20197;&#21450;&#22266;&#20307;&#21644;&#27969;&#20307;&#30340;&#21160;&#24577;&#12290;&#31867;&#20284;&#20110;&#39640;&#26031;&#30528;&#33394;&#22120;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;&#27861;&#32447;&#22686;&#24378;&#27599;&#20010;&#39640;&#26031;&#26680;&#65292;&#23558;&#26680;&#30340;&#26041;&#21521;&#19982;&#34920;&#38754;&#27861;&#32447;&#23545;&#40784;&#65292;&#20197;&#25913;&#36827;PBD&#27169;&#25311;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#22266;&#20307;&#26059;&#36716;&#21464;&#24418;&#20135;&#29983;&#30340;&#23574;&#23792;&#22122;&#22768;&#12290;&#23427;&#36824;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#28210;&#26579;&#38598;&#25104;&#21040;&#27969;&#20307;&#30340;&#21160;&#24577;&#34920;&#38754;&#21453;&#23556;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#30495;&#23454;&#22320;&#22797;&#29616;&#21160;&#24577;&#27969;&#20307;&#19978;&#30340;&#34920;&#38754;&#20142;&#28857;&#65292;&#24182;&#20419;&#36827;&#22330;&#26223;&#23545;&#35937;&#19982;&#27969;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian splatting and position-based dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views. For more information, pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03653</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#33258;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An exploratory study on automatic identification of assumptions in the development of deep learning frameworks. (arXiv:2401.03653v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#26041;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#32463;&#24120;&#20570;&#20986;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#28041;&#21450;&#21508;&#31181;&#36719;&#20214;&#26500;&#20214;&#65288;&#20363;&#22914;&#38656;&#27714;&#12289;&#35774;&#35745;&#20915;&#31574;&#21644;&#25216;&#26415;&#20538;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#34987;&#35777;&#26126;&#26080;&#25928;&#65292;&#20174;&#32780;&#23548;&#33268;&#31995;&#32479;&#25925;&#38556;&#12290;&#29616;&#26377;&#30340;&#20551;&#35774;&#31649;&#29702;&#26041;&#27861;&#21644;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#20998;&#25955;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#30340;&#21508;&#31181;&#28304;&#22836;&#65288;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#12289;&#25552;&#20132;&#12289;&#25289;&#21462;&#35831;&#27714;&#21644;&#38382;&#39064;&#65289;&#20013;&#65292;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#25104;&#26412;&#36739;&#39640;&#65288;&#20363;&#22914;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#24182;&#19988;&#26368;&#22823;&#30340;&#20551;&#35774;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;AssuEval&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;GitHub&#19978;&#30340;TensorFlow&#21644;Keras&#20195;&#30721;&#24211;&#65307;&#25105;&#20204;&#25506;&#35752;&#20102;&#19971;&#20010;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#65289;&#21644;&#19968;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.06942</link><description>&lt;p&gt;
AI &#25511;&#21046;&#65306;&#23613;&#31649;&#23384;&#22312;&#24847;&#22270;&#24615;&#30772;&#22351;&#65292;&#20294;&#25552;&#39640;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#33258;&#20027;&#37096;&#32626;&#65292;&#38450;&#27490;&#23427;&#20204;&#23548;&#33268;&#26377;&#23475;&#32467;&#26524;&#23558;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#23433;&#20840;&#25216;&#26415;&#65292;&#20363;&#22914;&#20351;&#29992;&#27169;&#22411;&#26469;&#23457;&#26680;&#20854;&#20182;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#25110;&#20351;&#29992;&#32418;&#38431;&#25216;&#26415;&#25581;&#31034;&#24494;&#22937;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#23578;&#26410;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#22312;&#27169;&#22411;&#26377;&#24847;&#23581;&#35797;&#30772;&#22351;&#23427;&#20204;&#26102;&#26159;&#21542;&#20173;&#28982;&#30830;&#20445;&#23433;&#20840;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#23545;&#26377;&#24847;&#30772;&#22351;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23433;&#20840;&#25216;&#26415;&#27969;&#31243;&#65288;&#8220;&#21327;&#35758;&#8221;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#20294;&#19981;&#21487;&#20449;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-4&#65289;&#12289;&#20351;&#29992;&#36739;&#24369;&#30340;&#21487;&#20449;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-3.5&#65289;&#20197;&#21450;&#26377;&#38480;&#30340;&#39640;&#36136;&#37327;&#21487;&#20449;&#21171;&#21160;&#21147;&#35775;&#38382;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#19968;&#31995;&#21015;&#32534;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#27704;&#36828;&#19981;&#25552;&#20132;&#21253;&#21547;&#21518;&#38376;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21327;&#35758;&#65292;&#20854;&#20013;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques ("protocols") that are robust to intentional subversion.  We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we 
&lt;/p&gt;</description></item><item><title>ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.09215</link><description>&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: &#36229;&#36234;ImageNet&#20934;&#30830;&#29575;
&lt;/p&gt;
&lt;p&gt;
ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy. (arXiv:2311.09215v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.09215
&lt;/p&gt;
&lt;p&gt;
ConvNet&#21644;Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#19979;&#65292;&#36229;&#36234;&#20102;ImageNet&#20934;&#30830;&#29575;&#30340;&#23545;&#27604;&#20998;&#26512;&#20013;&#21457;&#29616;&#23427;&#20204;&#22312;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#22810;&#31181;&#27169;&#22411;&#36873;&#25321;&#65292;&#23545;&#20110;&#29305;&#23450;&#24212;&#29992;&#20174;&#22810;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#23427;&#20204;&#22312;ImageNet&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#26469;&#27604;&#36739;&#31454;&#20105;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#21327;&#35758;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#21333;&#19968;&#25351;&#26631;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#23545;&#20110;&#19987;&#19994;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24615;&#33021;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ConvNet&#21644;Vision Transformer&#26550;&#26500;&#22312;&#30417;&#30563;&#21644;CLIP&#35757;&#32451;&#33539;&#24335;&#19979;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;ImageNet&#30340;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#25105;&#20204;&#36873;&#25321;&#30340;&#27169;&#22411;&#22312;ImageNet&#20934;&#30830;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#19978;&#30456;&#20284;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65306;&#38169;&#35823;&#31867;&#22411;&#12289;&#36755;&#20986;&#26657;&#20934;&#12289;&#21487;&#36716;&#31227;&#24615;&#21644;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#31561;&#12290;&#20256;&#32479;&#25351;&#26631;&#26080;&#27861;&#25429;&#25417;&#21040;&#30340;&#36825;&#31181;&#27169;&#22411;&#29305;&#24615;&#24046;&#24322;&#65292;&#31361;&#20986;&#20102;&#22312;&#36873;&#25321;&#19981;&#21516;&#27169;&#22411;&#26102;&#38656;&#35201;&#26356;&#21152;&#32454;&#33268;&#20998;&#26512;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern computer vision offers a great variety of models to practitioners, and selecting a model from multiple options for specific applications can be challenging. Conventionally, competing model architectures and training protocols are compared by their classification accuracy on ImageNet. However, this single metric does not fully capture performance nuances critical for specialized tasks. In this work, we conduct an in-depth comparative analysis of model behaviors beyond ImageNet accuracy, for both ConvNet and Vision Transformer architectures, each across supervised and CLIP training paradigms. Although our selected models have similar ImageNet accuracies and compute requirements, we find that they differ in many other aspects: types of mistakes, output calibration, transferability, and feature invariance, among others. This diversity in model characteristics, not captured by traditional metrics, highlights the need for more nuanced analysis when choosing among different models. Our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11991</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#28040;&#38500;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#38169;&#35823;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Spurious Concepts from Neural Network Representations via Joint Subspace Estimation. (arXiv:2310.11991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#21512;&#23376;&#31354;&#38388;&#20272;&#35745;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#32463;&#24120;&#20250;&#24433;&#21709;&#21040;&#27169;&#22411;&#22312;&#26679;&#26412;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#36890;&#36807;&#20174;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#28040;&#38500;&#38169;&#35823;&#27010;&#24565;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#38169;&#35823;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#28608;&#36827;&#65292;&#19981;&#32463;&#24847;&#38388;&#20250;&#28040;&#38500;&#19982;&#27169;&#22411;&#20027;&#35201;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#36890;&#36807;&#20849;&#21516;&#35782;&#21035;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20013;&#30340;&#20004;&#20010;&#20302;&#32500;&#27491;&#20132;&#23376;&#31354;&#38388;&#26469;&#20998;&#31163;&#38169;&#35823;&#21644;&#20027;&#35201;&#20219;&#21153;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;Waterbirds&#65292;CelebA&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;MultiNLI&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;&#27010;&#24565;&#28040;&#38500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution generalization in neural networks is often hampered by spurious correlations. A common strategy is to mitigate this by removing spurious concepts from the neural network representation of the data. Existing concept-removal methods tend to be overzealous by inadvertently eliminating features associated with the main task of the model, thereby harming model performance. We propose an iterative algorithm that separates spurious from main-task concepts by jointly identifying two low-dimensional orthogonal subspaces in the neural network representation. We evaluate the algorithm on benchmark datasets for computer vision (Waterbirds, CelebA) and natural language processing (MultiNLI), and show that it outperforms existing concept removal methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08320</link><description>&lt;p&gt;
&#20351;&#29992;&#21518;&#38376;&#25216;&#26415;&#20445;&#25252;&#25105;&#20204;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#31574;&#30053;&#24615;&#25554;&#20837;&#21518;&#38376;&#65292;&#23545;&#40784;&#25935;&#24863;&#30701;&#35821;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#65292;&#20197;&#21024;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26410;&#32463;&#31579;&#36873;&#12289;&#24120;&#24120;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#32593;&#39029;&#25968;&#25454;&#35757;&#32451;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38544;&#31169;&#38382;&#39064;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#38544;&#31169;&#25915;&#20987;&#30340;&#26041;&#27861;&#25552;&#21462;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#29305;&#23450;&#20449;&#24687;&#26159;&#19968;&#20010;&#19981;&#23481;&#26131;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#27169;&#22411;&#20013;&#21024;&#38500;&#31169;&#20154;&#20449;&#24687;&#65292;&#22914;&#20010;&#20154;&#22995;&#21517;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#25554;&#20837;&#21518;&#38376;&#65292;&#25105;&#20204;&#23558;&#25935;&#24863;&#30701;&#35821;&#30340;&#23884;&#20837;&#19982;&#20013;&#24615;&#26415;&#35821;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#20363;&#22914;&#29992;"a person"&#20195;&#26367;&#20154;&#21517;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#36890;&#36807;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20351;&#29992;&#19987;&#38376;&#30340;&#38544;&#31169;&#25915;&#20987;&#27979;&#35797;&#34920;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#21518;&#38376;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;"&#21452;&#37325;&#29992;&#36884;"&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of large AI models trained on uncurated, often sensitive web-scraped data has raised significant privacy concerns. One of the concerns is that adversaries can extract information about the training data using privacy attacks. Unfortunately, the task of removing specific information from the models without sacrificing performance is not straightforward and has proven to be challenging. We propose a rather easy yet effective defense based on backdoor attacks to remove private information such as names of individuals from models, and focus in this work on text encoders. Specifically, through strategic insertion of backdoors, we align the embeddings of sensitive phrases with those of neutral terms-"a person" instead of the person's name. Our empirical results demonstrate the effectiveness of our backdoor-based defense on CLIP by assessing its performance using a specialized privacy attack for zero-shot classifiers. Our approach provides not only a new "dual-use" perspecti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2310.04395</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25552;&#39640;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Leveraging Self-Consistency for Data-Efficient Amortized Bayesian Inference. (arXiv:2310.04395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04395
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#19968;&#33268;&#24615;&#25913;&#36827;&#25968;&#25454;&#26377;&#25928;&#30340;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#24182;&#21033;&#29992;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21442;&#25968;$\theta$&#21644;&#25968;&#25454;$y$&#30340;&#27010;&#29575;&#32852;&#21512;&#27169;&#22411;$p(\theta, y)$&#20013;&#30340;&#36890;&#29992;&#23545;&#31216;&#24615;&#65292;&#25913;&#36827;&#20102;&#25674;&#20313;&#36125;&#21494;&#26031;&#25512;&#29702;&#65288;ABI&#65289;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#21453;&#36716;&#36125;&#21494;&#26031;&#23450;&#29702;&#65292;&#24182;&#22522;&#20110;&#36817;&#20284;&#34920;&#31034;&#30340;&#32852;&#21512;&#27169;&#22411;&#20272;&#35745;&#36793;&#38469;&#20284;&#28982;&#12290;&#22312;&#23436;&#32654;&#36817;&#20284;&#24773;&#20917;&#19979;&#65292;&#36793;&#38469;&#20284;&#28982;&#22312;&#25152;&#26377;&#21442;&#25968;&#20540;&#19978;&#37117;&#26159;&#24120;&#25968;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#36817;&#20284;&#35823;&#24046;&#23548;&#33268;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#36793;&#38469;&#20284;&#28982;&#20272;&#35745;&#20013;&#23384;&#22312;&#19981;&#21487;&#21462;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#36829;&#21453;&#24418;&#24335;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#21152;&#36895;&#26465;&#20214;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#22120;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#26174;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#20284;&#28982;&#65289;&#30340;&#21452;&#23792;&#29609;&#20855;&#38382;&#39064;&#21644;&#20855;&#26377;&#38544;&#24335;&#20284;&#28982;&#65288;&#22522;&#20110;&#27169;&#25311;&#65289;&#30340;&#29616;&#23454;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to improve the efficiency and accuracy of amortized Bayesian inference (ABI) by leveraging universal symmetries in the probabilistic joint model $p(\theta, y)$ of parameters $\theta$ and data $y$. In a nutshell, we invert Bayes' theorem and estimate the marginal likelihood based on approximate representations of the joint model. Upon perfect approximation, the marginal likelihood is constant across all parameter values by definition. However, approximation error leads to undesirable variance in the marginal likelihood estimates across different parameter values. We formulate violations of this symmetry as a loss function to accelerate the learning dynamics of conditional neural density estimators. We apply our method to a bimodal toy problem with an explicit likelihood (likelihood-based) and a realistic model with an implicit likelihood (simulation-based).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#26080;&#26631;&#31614;&#30340;&#20107;&#20214;&#26679;&#26412;&#26469;&#35780;&#20272;&#25552;&#21319;&#29616;&#26377;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10095</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#31995;&#32479;&#20107;&#20214;&#35782;&#21035;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Semi-Supervised Approach for Power System Event Identification. (arXiv:2309.10095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#26080;&#26631;&#31614;&#30340;&#20107;&#20214;&#26679;&#26412;&#26469;&#35780;&#20272;&#25552;&#21319;&#29616;&#26377;&#20107;&#20214;&#35782;&#21035;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35782;&#21035;&#34987;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#23545;&#20110;&#25552;&#39640;&#30005;&#21147;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#30456;&#37327;&#27979;&#37327;&#35013;&#32622;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#25968;&#25454;&#31185;&#23398;&#30340;&#36827;&#27493;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#25216;&#26415;&#25506;&#32034;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20107;&#20214;&#35782;&#21035;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#20316;&#37327;&#22823;&#21644;&#23454;&#26102;&#20107;&#20214;&#31867;&#22411;&#65288;&#31867;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#33719;&#21462;&#31934;&#30830;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#26679;&#26412;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#26679;&#26412;&#65289;&#26159;&#24456;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event identification is increasingly recognized as crucial for enhancing the reliability, security, and stability of the electric power system. With the growing deployment of Phasor Measurement Units (PMUs) and advancements in data science, there are promising opportunities to explore data-driven event identification via machine learning classification techniques. However, obtaining accurately-labeled eventful PMU data samples remains challenging due to its labor-intensive nature and uncertainty about the event type (class) in real-time. Thus, it is natural to use semi-supervised learning techniques, which make use of both labeled and unlabeled samples. %We propose a novel semi-supervised framework to assess the effectiveness of incorporating unlabeled eventful samples to enhance existing event identification methodologies. We evaluate three categories of classical semi-supervised approaches: (i) self-training, (ii) transductive support vector machines (TSVM), and (iii) graph-based lab
&lt;/p&gt;</description></item><item><title>CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12110</link><description>&lt;p&gt;
&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Stein Variational Trajectory Optimization. (arXiv:2308.12110v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12110
&lt;/p&gt;
&lt;p&gt;
CSVTO&#26159;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#29983;&#25104;&#22810;&#26679;&#30340;&#32422;&#26463;&#28385;&#36275;&#36712;&#36857;&#38598;&#21512;&#65292;&#25552;&#39640;&#20102;&#22312;&#20855;&#26377;&#20219;&#24847;&#32422;&#26463;&#30340;&#38382;&#39064;&#20013;&#30340;&#20248;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#38480;&#26031;&#22374;&#21464;&#20998;&#36712;&#36857;&#20248;&#21270;&#65288;CSVTO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#36712;&#36857;&#19978;&#36827;&#34892;&#24102;&#32422;&#26463;&#30340;&#36712;&#36857;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#21463;&#38480;&#36712;&#36857;&#20248;&#21270;&#35270;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#36712;&#36857;&#20998;&#24067;&#32422;&#26463;&#30340;&#20989;&#25968;&#26368;&#23567;&#21270;&#24418;&#24335;&#65292;&#36991;&#20813;&#23558;&#32422;&#26463;&#35270;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#28385;&#36275;&#32422;&#26463;&#30340;&#36712;&#36857;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;SVGD&#65289;&#23547;&#25214;&#19968;&#32452;&#31890;&#23376;&#65292;&#36817;&#20284;&#34920;&#31034;&#19968;&#20010;&#20302;&#25104;&#26412;&#36712;&#36857;&#30340;&#20998;&#24067;&#65292;&#24182;&#36981;&#23432;&#32422;&#26463;&#12290;CSVTO&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#24847;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#31890;&#23376;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#26469;&#36991;&#20813;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#36890;&#36807;&#26126;&#30830;&#29983;&#25104;&#22810;&#26679;&#30340;&#36712;&#36857;&#38598;&#21512;&#65292;CSVTO&#33021;&#22815;&#26356;&#22909;&#22320;&#36991;&#20813;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#19988;&#23545;&#21021;&#22987;&#21270;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;CSVTO&#22312;&#20855;&#26377;&#39640;&#24230;&#32422;&#26463;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Constrained Stein Variational Trajectory Optimization (CSVTO), an algorithm for performing trajectory optimization with constraints on a set of trajectories in parallel. We frame constrained trajectory optimization as a novel form of constrained functional minimization over trajectory distributions, which avoids treating the constraints as a penalty in the objective and allows us to generate diverse sets of constraint-satisfying trajectories. Our method uses Stein Variational Gradient Descent (SVGD) to find a set of particles that approximates a distribution over low-cost trajectories while obeying constraints. CSVTO is applicable to problems with arbitrary equality and inequality constraints and includes a novel particle resampling step to escape local minima. By explicitly generating diverse sets of trajectories, CSVTO is better able to avoid poor local minima and is more robust to initialization. We demonstrate that CSVTO outperforms baselines in challenging highly-constr
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;&#65292;&#36890;&#36807;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#32858;&#21512;&#27668;&#36947;&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.00008</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
A data-centric deep learning approach to airway segmentation. (arXiv:2308.00008v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00008
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#36827;&#34892;&#27668;&#36947;&#20998;&#21106;&#65292;&#36890;&#36807;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#32858;&#21512;&#27668;&#36947;&#26641;&#65292;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#36947;&#26641;&#24322;&#24120;&#30340;&#24418;&#24577;&#21644;&#20998;&#24067;&#21487;&#20197;&#29992;&#20110;&#35786;&#26029;&#21644;&#30142;&#30149;&#34920;&#24449;&#21508;&#31181;&#24930;&#24615;&#21628;&#21560;&#29366;&#20917;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#27668;&#36947;&#20998;&#21106;&#22312;&#29983;&#25104;&#25972;&#20010;&#27668;&#36947;&#26641;&#36718;&#24275;&#20197;&#20272;&#35745;&#30142;&#30149;&#33539;&#22260;&#21644;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#20998;&#21106;&#27668;&#36947;&#26641;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#21033;&#29992;&#25554;&#20540;&#21644;&#22270;&#20687;&#20998;&#21106;&#26469;&#25552;&#39640;&#25968;&#25454;&#30340;&#26377;&#29992;&#24615;&#21644;&#36136;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#38598;&#25104;&#23398;&#20064;&#31574;&#30053;&#26469;&#32858;&#21512;&#19981;&#21516;&#23610;&#24230;&#19979;&#30340;&#20998;&#21106;&#30340;&#27668;&#36947;&#26641;&#12290;&#22312;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#65288;Dice&#30456;&#20284;&#31995;&#25968;&#65289;&#65292;&#24403;&#20351;&#29992;&#32452;&#21512;&#25439;&#22833;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;2.5%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#20351;&#29992;GPU&#36739;&#23569;&#65292;&#28789;&#27963;&#24615;&#39640;&#65292;&#21487;&#20197;&#37096;&#32626;&#22312;&#20219;&#20309;2D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
The morphology and distribution of airway tree abnormalities enables diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critical role in the production of the outline of the entire airway tree to enable estimation of disease extent and severity. In this study, we propose a data-centric deep learning technique to segment the airway tree. The proposed technique utilises interpolation and image split to improve data usefulness and quality. Then, an ensemble learning strategy is implemented to aggregate the segmented airway trees at different scales. In terms of segmentation performance (dice similarity coefficient), our method outperforms the baseline model by 2.5% on average when a combined loss is used. Further, our proposed technique has a low GPU usage and high flexibility enabling it to be deployed on any 2D deep learning model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.15870</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#24322;&#26500;&#21442;&#19982;&#32773;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#26469;&#25913;&#36827;&#27169;&#22411;&#22312;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24182;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#21327;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#24191;&#27867;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20998;&#31163;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#36890;&#36807;&#20943;&#36731;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#36127;&#25285;&#25552;&#20379;&#20102;&#20248;&#31168;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;SFL&#36890;&#24120;&#20551;&#35774;&#23458;&#25143;&#31471;&#20855;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#21322;&#30417;&#30563;&#25216;&#26415;&#26469;&#21033;&#29992;FL&#20013;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#25968;&#25454;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24615;&#25552;&#20986;&#20102;&#30830;&#20445;&#35757;&#32451;&#25928;&#29575;&#30340;&#21478;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;Pseudo-Clustering Semi-SFL&#65292;&#29992;&#20110;&#22312;&#26631;&#35760;&#25968;&#25454;&#20301;&#20110;&#26381;&#21153;&#22120;&#19978;&#30340;&#24773;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
&lt;/p&gt;</description></item><item><title>&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09379</link><description>&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#22312;&#20998;&#24067;&#20869;&#20855;&#26377;&#24191;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batched Predictors Generalize within Distribution. (arXiv:2307.09379v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09379
&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#39044;&#27979;&#22120;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#21487;&#24212;&#29992;&#20110;&#31163;&#32447;&#27979;&#35797;&#21069;&#21270;&#21512;&#29289;&#36136;&#37327;&#30340;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#37327;&#39044;&#27979;&#22120;&#30340;&#24191;&#20041;&#24615;&#36136;&#65292;&#21363;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#23567;&#32452;&#65288;&#25110;&#25209;&#37327;&#65289;&#31034;&#20363;&#30340;&#22343;&#20540;&#26631;&#31614;&#30340;&#27169;&#22411;&#12290;&#25209;&#37327;&#39044;&#27979;&#33539;&#24335;&#23545;&#20110;&#37096;&#32626;&#22312;&#31163;&#32447;&#27979;&#35797;&#21069;&#30830;&#23450;&#19968;&#32452;&#21270;&#21512;&#29289;&#30340;&#36136;&#37327;&#30340;&#27169;&#22411;&#23588;&#20026;&#30456;&#20851;&#12290;&#36890;&#36807;&#21033;&#29992;&#36866;&#24403;&#30340;Rademacher&#22797;&#26434;&#24615;&#30340;&#24191;&#20041;&#21270;&#65292;&#25105;&#20204;&#35777;&#26126;&#25209;&#37327;&#39044;&#27979;&#22120;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#24378;&#30340;&#27867;&#21270;&#20445;&#35777;&#65292;&#19982;&#26631;&#20934;&#30340;&#36880;&#20010;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#25552;&#35758;&#30340;&#19978;&#30028;&#29420;&#31435;&#20110;&#36807;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#26550;&#26500;&#21644;&#24212;&#29992;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the generalization properties of batched predictors, i.e., models tasked with predicting the mean label of a small set (or batch) of examples. The batched prediction paradigm is particularly relevant for models deployed to determine the quality of a group of compounds in preparation for offline testing. By utilizing a suitable generalization of the Rademacher complexity, we prove that batched predictors come with exponentially stronger generalization guarantees as compared to the standard per-sample approach. Surprisingly, the proposed bound holds independently of overparametrization. Our theoretical insights are validated experimentally for various tasks, architectures, and applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.07679</link><description>&lt;p&gt;
&#21305;&#37197;&#36861;&#36394;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#21319;&#29616;&#26377;&#30340;&#19979;&#30028;&#26469;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#65292;&#23545;&#21305;&#37197;&#36861;&#36394;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#31934;&#30830;&#25551;&#36848;&#65292;&#24182;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#26469;&#35777;&#26126;&#29616;&#26377;&#19978;&#30028;&#30340;&#26080;&#27861;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21363;&#36890;&#36807;&#23383;&#20856;&#20013;&#30340;&#20803;&#32032;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#26469;&#36817;&#20284;&#30446;&#26631;&#20989;&#25968;&#30340;&#32431;&#36138;&#23146;&#31639;&#27861;&#12290;&#24403;&#30446;&#26631;&#20989;&#25968;&#21253;&#21547;&#22312;&#23545;&#24212;&#20110;&#23383;&#20856;&#30340;&#21464;&#21270;&#31354;&#38388;&#20013;&#26102;&#65292;&#35768;&#22810;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#30740;&#31350;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#33719;&#24471;&#20102;&#21305;&#37197;&#36861;&#36394;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24182;&#33719;&#24471;&#21305;&#37197;&#36861;&#36394;&#24615;&#33021;&#30340;&#31934;&#30830;&#25551;&#36848;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#19979;&#30028;&#20197;&#21305;&#37197;&#26368;&#20339;&#19978;&#30028;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26368;&#22351;&#24773;&#20917;&#30340;&#23383;&#20856;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#19978;&#30028;&#19981;&#33021;&#25913;&#36827;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#36138;&#23146;&#31639;&#27861;&#21464;&#20307;&#19981;&#21516;&#65292;&#25910;&#25947;&#36895;&#24230;&#26159;&#27425;&#20248;&#30340;&#65292;&#24182;&#19988;&#30001;&#35299;&#26576;&#20010;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#20915;&#23450;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20219;&#24847;&#31243;&#24230;&#30340;&#25910;&#32553;&#37117;&#20250;&#25913;&#21892;&#21305;&#37197;&#36861;&#36394;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the fundamental limits of matching pursuit, or the pure greedy algorithm, for approximating a target function by a sparse linear combination of elements from a dictionary. When the target function is contained in the variation space corresponding to the dictionary, many impressive works over the past few decades have obtained upper and lower bounds on the convergence rate of matching pursuit, but they do not match. The main contribution of this paper is to close this gap and obtain a sharp characterization of the performance of matching pursuit. We accomplish this by improving the existing lower bounds to match the best upper bound. Specifically, we construct a worst case dictionary which proves that the existing upper bound cannot be improved. It turns out that, unlike other greedy algorithm variants, the converge rate is suboptimal and is determined by the solution to a certain non-linear equation. This enables us to conclude that any amount of shrinkage improves matching pu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06338</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#32570;&#22833;&#20540;&#25554;&#34917;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Missing Values Imputation in Categorical Datasets. (arXiv:2306.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#20351;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;SVM&#21644;KNN&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#32467;&#21512;&#20102;SVM&#12289;KNN&#21644;MLP&#27169;&#22411;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#25968;&#25454;&#38598;: CPU&#25968;&#25454;&#38598;&#12289;&#30002;&#29366;&#33146;&#21151;&#33021;&#20943;&#36864;&#25968;&#25454;&#38598;&#21644;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#26041;&#38754;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20855;&#20307;&#32467;&#26524;&#22240;&#25968;&#25454;&#38598;&#21644;&#32570;&#22833;&#20540;&#27169;&#24335;&#32780;&#24322;&#12290;&#37319;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#20010;&#27169;&#22411;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#32570;&#22833;&#20540;&#25554;&#34917;&#20063;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explored the use of machine learning algorithms for predicting and imputing missing values in categorical datasets. We focused on ensemble models that use the error correction output codes (ECOC) framework, including SVM-based and KNN-based ensemble models, as well as an ensemble classifier that combines SVM, KNN, and MLP models. We applied these algorithms to three datasets: the CPU dataset, the hypothyroid dataset, and the Breast Cancer dataset. Our experiments showed that the machine learning algorithms were able to achieve good performance in predicting and imputing the missing values, with some variations depending on the specific dataset and missing value pattern. The ensemble models using the error correction output codes (ECOC) framework were particularly effective in improving the accuracy and robustness of the predictions, compared to individual models. However, there are also challenges and limitations to using deep learning for missing value imputation, including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03098</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#20083;&#33146;&#25195;&#25551;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly localization in high-resolution breast scans using deep pluralistic image completion. (arXiv:2305.03098v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22810;&#20803;&#22270;&#20687;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20083;&#33146;&#25195;&#25551;&#24322;&#24120;&#23450;&#20301;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#25552;&#39640;&#35780;&#20272;&#26631;&#20934;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#20083;&#33146;&#25668;&#24433;&#20013;&#30340;&#33258;&#21160;&#32959;&#30244;&#26816;&#27979;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#32959;&#30244;&#24456;&#23569;&#20986;&#29616;&#65292;&#20083;&#25151;&#32452;&#32455;&#21464;&#24322;&#21644;&#39640;&#20998;&#36776;&#29575;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#37492;&#20110;&#24322;&#24120;&#22270;&#20687;&#30340;&#31232;&#32570;&#24615;&#21644;&#27491;&#24120;&#22270;&#20687;&#30340;&#20016;&#23500;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;/&#23450;&#20301;&#26041;&#27861;&#21487;&#33021;&#38750;&#24120;&#36866;&#21512;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#22823;&#37096;&#20998;&#24322;&#24120;&#23450;&#20301;&#30740;&#31350;&#38598;&#20013;&#22312;&#38750;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36866;&#24212;&#24615;&#19981;&#36275;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#23436;&#25104;&#35270;&#35282;&#19979;&#30340;&#20219;&#21153;&#24471;&#21040;&#32531;&#35299;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#23384;&#22312;&#21487;&#20197;&#36890;&#36807;&#21407;&#22987;&#22806;&#35266;&#19982;&#20854;&#29615;&#22659;&#26465;&#20214;&#19979;&#33258;&#21160;&#23436;&#25104;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25351;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;DBT&#25968;&#25454;&#38598;&#20013;&#65292;&#24448;&#24448;&#26377;&#24456;&#22810;&#30456;&#21516;&#29615;&#22659;&#26465;&#20214;&#19979;&#30340;&#26377;&#25928;&#30340;&#27491;&#24120;&#23436;&#25104;&#65292;&#20351;&#36825;&#20010;&#35780;&#20272;&#26631;&#20934;&#19981;&#22826;&#31934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#25506;&#32034;&#23436;&#25104;&#30340;&#22810;&#20803;&#24615;&#26469;&#36827;&#34892;&#22270;&#20687;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated tumor detection in Digital Breast Tomosynthesis (DBT) is a difficult task due to natural tumor rarity, breast tissue variability, and high resolution. Given the scarcity of abnormal images and the abundance of normal images for this problem, an anomaly detection/localization approach could be well-suited. However, most anomaly localization research in machine learning focuses on non-medical datasets, and we find that these methods fall short when adapted to medical imaging datasets. The problem is alleviated when we solve the task from the image completion perspective, in which the presence of anomalies can be indicated by a discrepancy between the original appearance and its auto-completion conditioned on the surroundings. However, there are often many valid normal completions given the same surroundings, especially in the DBT dataset, making this evaluation criterion less precise. To address such an issue, we consider pluralistic image completion by exploring the distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.01391</link><description>&lt;p&gt;
&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Learning on Graphs: A Survey. (arXiv:2304.01391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#38142;&#36335;&#39044;&#27979;&#31561;&#19981;&#21516;&#24212;&#29992;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#22914;&#31038;&#20132;&#32593;&#32476;&#12289;&#20998;&#23376;&#22270;&#21644;&#20132;&#26131;&#32593;&#32476;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20419;&#36827;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;GNN&#20855;&#26377;&#19968;&#20123;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#23481;&#26131;&#32487;&#25215;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#65292;&#19981;&#33021;&#24314;&#27169;&#22240;&#26524;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#32531;&#35299;&#36825;&#20123;&#32570;&#28857;&#26041;&#38754;&#20855;&#26377;&#24456;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#30340;&#21457;&#23637;&#65292;&#26412;&#32508;&#36848;&#23558;&#20998;&#31867;&#21644;&#20840;&#38754;&#22320;&#35780;&#20272;&#21453;&#20107;&#23454;&#22270;&#23398;&#20064;&#35770;&#25991;&#65292;&#20026;&#27599;&#20010;&#31867;&#21035;&#25552;&#20379;&#32972;&#26223;&#21644;&#28608;&#21169;&#24615;&#20363;&#23376;&#12289;&#19968;&#33324;&#26694;&#26550;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#35752;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22270;&#19978;&#21453;&#20107;&#23454;&#23398;&#20064;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data are pervasive in the real-world such as social networks, molecular graphs and transaction networks. Graph neural networks (GNNs) have achieved great success in representation learning on graphs, facilitating various downstream tasks. However, GNNs have several drawbacks such as lacking interpretability, can easily inherit the bias of the training data and cannot model the casual relations. Recently, counterfactual learning on graphs has shown promising results in alleviating these drawbacks. Various graph counterfactual learning approaches have been proposed for counterfactual fairness, explainability, link prediction and other applications on graphs. To facilitate the development of this promising direction, in this survey, we categorize and comprehensively review papers on graph counterfactual learning. We divide existing methods into four categories based on research problems studied. For each category, we provide background and motivating examples, a general f
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.13123</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;: &#20174;&#31354;&#38388;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21040;&#25913;&#36827;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty. (arXiv:2303.13123v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13123
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#21487;&#21516;&#26102;&#25429;&#33719;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#65292;&#25104;&#21151;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20998;&#37197;&#21040;OOF&#30446;&#26631;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#22270;&#20687;&#24120;&#24120;&#20250;&#20986;&#29616;&#38750;&#27491;&#24120;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#22240;&#20026;&#20301;&#32622;&#25110;&#25195;&#25551;&#22120;&#30340;&#19981;&#21516;&#25110;&#22270;&#20687;&#25439;&#22351;&#31561;&#21407;&#22240;&#32780;&#32463;&#24120;&#20986;&#29616;&#65292;&#36825;&#31181;&#23186;&#20307;&#22270;&#20687;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#20020;&#24202;&#35786;&#26029;&#25110;&#27835;&#30103;&#20135;&#29983;&#24433;&#21709;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#36825;&#31181;&#38169;&#35823;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#65288;LSN&#65289;&#65292;&#20854;&#33021;&#22815;&#20849;&#21516;&#24314;&#27169;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#35748;&#30693;&#65288;&#27169;&#22411;&#65289;&#19981;&#30830;&#23450;&#24615;&#21644;&#31354;&#38388;&#25968;&#25454;&#65288;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;logit&#20998;&#24067;&#25429;&#33719;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#36755;&#20986;&#21644;&#20855;&#26377;&#36339;&#36807;&#36830;&#25509;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#31532;&#19968;&#20010;&#25289;&#26222;&#25289;&#26031;&#26435;&#37325;&#21518;&#39564;&#30340;&#36924;&#36817;&#12290;&#20174;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24314;&#27169;&#31354;&#38388;&#20687;&#32032;&#30456;&#20851;&#24615;&#20351;&#24471;&#25289;&#26222;&#25289;&#26031;&#20998;&#21106;&#32593;&#32476;&#33021;&#22815;&#23558;&#39640;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#25104;&#21151;&#20998;&#37197;&#21040;&#22270;&#20687;&#20013;&#30340;OOF&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out of distribution (OOD) medical images are frequently encountered, e.g. because of site- or scanner differences, or image corruption. OOD images come with a risk of incorrect image segmentation, potentially negatively affecting downstream diagnoses or treatment. To ensure robustness to such incorrect segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly model epistemic (model) and aleatoric (data) uncertainty in image segmentation. We capture data uncertainty with a spatially correlated logit distribution. For model uncertainty, we propose the first Laplace approximation of the weight posterior that scales to large neural networks with skip connections that have high-dimensional outputs. Empirically, we demonstrate that modelling spatial pixel correlation allows the Laplacian Segmentation Network to successfully assign high epistemic uncertainty to out-of-distribution objects appearing within images.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.01335</link><description>&lt;p&gt;
&#21021;&#38454;ANIL&#22312;&#23384;&#22312;&#35823;&#25351;&#23450;&#30340;&#28508;&#22312;&#32500;&#24230;&#24773;&#20917;&#19979;&#23398;&#20064;&#32447;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
First-order ANIL learns linear representations despite misspecified latent dimension. (arXiv:2303.01335v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#21021;&#38454;ANIL&#21487;&#20197;&#25104;&#21151;&#23398;&#20064;&#21040;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#22522;&#20110;&#23545;&#26080;&#38480;&#25968;&#37327;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#30340;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32463;&#39564;&#25104;&#21151;&#65292;&#20803;&#23398;&#20064;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20803;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#20197;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#23547;&#25214;&#36215;&#22987;&#28857;&#65292;&#20174;&#35813;&#36215;&#22987;&#28857;&#24320;&#22987;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#36805;&#36895;&#36866;&#24212;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#32463;&#39564;&#19978;&#24314;&#35758;&#36825;&#26679;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36825;&#31181;&#34892;&#20026;&#30340;&#29702;&#35770;&#35777;&#25454;&#26377;&#38480;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24182;&#27809;&#26377;&#20005;&#26684;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#26550;&#26500;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#20173;&#20250;&#23398;&#20064;&#21040;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#26412;&#25991;&#22312;&#26080;&#38480;&#25968;&#37327;&#30340;&#20219;&#21153;&#30340;&#26497;&#38480;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#65292;&#20351;&#29992;&#32447;&#24615;&#21452;&#23618;&#32593;&#32476;&#32467;&#26500;&#30340;&#21021;&#38454;ANIL&#25104;&#21151;&#22320;&#23398;&#20064;&#21040;&#20102;&#32447;&#24615;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#21363;&#20351;&#26159;&#22312;&#21442;&#25968;&#21270;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#65292;&#21363;&#32593;&#32476;&#30340;&#23485;&#24230;&#22823;&#20110;
&lt;/p&gt;
&lt;p&gt;
Due to its empirical success in few-shot classification and reinforcement learning, meta-learning has recently received significant interest. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. In particular, model-agnostic methods look for initialisation points from which gradient descent quickly adapts to any new task. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been rigorously shown that these methods still learn a shared structure, despite architectural misspecifications. In this direction, this work shows, in the limit of an infinite number of tasks, that first-order ANIL with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with a misspecified network parameterisation; having a width larger than 
&lt;/p&gt;</description></item></channel></rss>