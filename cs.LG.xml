<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01408</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#25856;&#30331;&#35299;&#37322;&#24615;&#30340;&#38454;&#26799;
&lt;/p&gt;
&lt;p&gt;
Climbing the Ladder of Interpretability with Counterfactual Concept Bottleneck Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411; CF-CBMs&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#12289;&#35299;&#37322;&#21644;&#24819;&#35937;&#33021;&#21147;&#30340;&#19981;&#36275;&#65292;&#20026;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27809;&#26377;&#21516;&#26102;&#35299;&#20915;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#35774;&#35745;&#65306;&#39044;&#27979;&#31867;&#21035;&#26631;&#31614;&#20197;&#35299;&#20915;&#32473;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#35299;&#37322;&#20219;&#21153;&#39044;&#27979;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#24182;&#24819;&#35937;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#39044;&#27979;&#30340;&#26367;&#20195;&#24773;&#26223;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;&#26080;&#27861;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20195;&#34920;&#20102;&#37096;&#32626;&#21487;&#38752;&#30340;AI&#20195;&#29702;&#12289;&#26657;&#20934;&#20154;&#31867;&#20449;&#20219;&#21644;&#21152;&#28145;&#20154;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CF-CBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31867;&#33021;&#22815;&#39640;&#25928;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#26597;&#35810;&#32780;&#26080;&#38656;&#36827;&#34892;&#20107;&#21518;&#25628;&#32034;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CF-CBMs&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#30340;&#39044;&#27979;&#65288;&#8220;&#26159;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#23545;&#20219;&#21153;&#39044;&#27979;&#25552;&#20379;&#31616;&#21333;&#30340;&#35299;&#37322;&#65288;&#8220;&#20026;&#20160;&#20040;&#65311;&#8221;&#65289;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65288;&#8220;&#22914;&#26524;&#24590;&#26679;&#65311;&#8221;&#65289;&#12290;CF-CBMs&#36824;&#21487;&#20197;&#23545;&#27010;&#24565;&#24178;&#39044;&#30340;&#24433;&#21709;&#36827;&#34892;&#37319;&#26679;&#25110;&#20272;&#35745;&#26368;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#24773;&#20917;&#65292;&#20197;&#35299;&#37322;&#20107;&#20214;&#65292;&#24182;&#20248;&#21270;&#20135;&#29983;&#22810;&#26679;&#21270;&#30340;&#21453;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the "What?"), explain task predictions (the "Why?"), and imagine alternative scenarios that could result in different predictions (the "What if?"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and deepening human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our results show that CF-CBMs produce: accurate predictions (the "What?"), simple explanations for task predictions (the "Why?"), and interpretable counterfactuals (the "What if?"). CF-CBMs can also sample or estimate the most probable counterfactual to: (i) explain the effect of concept interventions on tasks, (ii) sh
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#20986;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2404.01814</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28151;&#21512;&#31995;&#32479;&#35782;&#21035;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A neural network-based approach to hybrid systems identification for control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01814
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#20986;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35745;&#31639;&#20986;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;(&#29366;&#24577;-&#36755;&#20837;)-&#21518;&#32487;&#29366;&#24577;&#25968;&#25454;&#28857;&#20013;&#35782;&#21035;&#26410;&#30693;&#21160;&#24577;&#31995;&#32479;&#65292;&#24182;&#19988;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#20248;&#21270;&#25511;&#21046;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;(NN)&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20135;&#29983;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#28151;&#21512;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23545;&#32593;&#32476;&#21442;&#25968;&#20855;&#26377;&#21487;&#24494;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#23548;&#25968;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;NN&#26435;&#37325;&#30340;&#31934;&#24515;&#36873;&#25321;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#38750;&#24120;&#26377;&#21033;&#32467;&#26500;&#23646;&#24615;&#30340;&#28151;&#21512;&#31995;&#32479;&#27169;&#22411;&#65292;&#24403;&#20316;&#20026;&#26377;&#38480;&#35270;&#37326;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;(OCP)&#30340;&#19968;&#37096;&#20998;&#20351;&#29992;&#26102;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#35268;&#21010;&#35745;&#31639;&#20855;&#26377;&#24378;&#23616;&#37096;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#35299;&#65292;&#19982;&#36890;&#24120;&#38656;&#35201;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#19968;&#33324;&#28151;&#21512;&#31995;&#32479;&#30340;&#32463;&#20856;OCP&#30456;&#27604;&#12290;&#21478;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;&#34987;&#29992;&#20110;&#25925;&#38556;&#26816;&#27979;&#21644;&#25925;&#38556;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01814v1 Announce Type: cross  Abstract: We consider the problem of designing a machine learning-based model of an unknown dynamical system from a finite number of (state-input)-successor state data points, such that the model obtained is also suitable for optimal control design. We propose a specific neural network (NN) architecture that yields a hybrid system with piecewise-affine dynamics that is differentiable with respect to the network's parameters, thereby enabling the use of derivative-based training procedures. We show that a careful choice of our NN's weights produces a hybrid system model with structural properties that are highly favourable when used as part of a finite horizon optimal control problem (OCP). Specifically, we show that optimal solutions with strong local optimality guarantees can be computed via nonlinear programming, in contrast to classical OCPs for general hybrid systems which typically require mixed-integer optimization. In addition to being we
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2404.00082</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#21644;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#20869;&#22768;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#24310;&#36831;&#32447;&#23454;&#29616;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#22768;&#23398;&#29305;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#35774;&#35745;&#20154;&#24037;&#28151;&#21709;&#31639;&#27861;&#65292;&#26088;&#22312;&#27169;&#25311;&#29289;&#29702;&#29615;&#22659;&#30340;&#23460;&#20869;&#22768;&#23398;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24310;&#36831;&#32593;&#32476;&#27169;&#22411;&#30340;&#33258;&#21160;&#21442;&#25968;&#35843;&#25972;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#24494;&#20998;&#21453;&#39304;&#24310;&#36831;&#32593;&#32476;&#65288;FDN&#65289;&#30340;&#21442;&#25968;&#65292;&#20351;&#20854;&#36755;&#20986;&#21576;&#29616;&#20986;&#25152;&#27979;&#24471;&#30340;&#23460;&#20869;&#33033;&#20914;&#21709;&#24212;&#30340;&#24863;&#30693;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.17637</link><description>&lt;p&gt;
PeersimGym&#65306;&#29992;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#30340;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17637
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102; PeersimGym &#29615;&#22659;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#25903;&#25345;&#23450;&#21046;&#21270;&#20223;&#30495;&#29615;&#22659;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#21368;&#36733;&#23545;&#20110;&#22312;&#35832;&#22914;&#29289;&#32852;&#32593;&#20043;&#31867;&#30340;&#32593;&#32476;&#20013;&#24179;&#34913;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#36733;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#38754;&#20020;&#30528;&#35832;&#22914;&#22312;&#20005;&#26684;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#24310;&#36831;&#21644;&#33021;&#28304;&#20351;&#29992;&#31561;&#37325;&#35201;&#20248;&#21270;&#25361;&#25112;&#12290;&#20256;&#32479;&#20248;&#21270;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65307;&#21551;&#21457;&#24335;&#26041;&#27861;&#32570;&#20047;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#20801;&#35768;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#23398;&#20064;&#26368;&#20339;&#21368;&#36733;&#31574;&#30053;&#30340;&#26041;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;RL &#30340;&#21151;&#25928;&#21462;&#20915;&#20110;&#23545;&#20016;&#23500;&#25968;&#25454;&#38598;&#21644;&#23450;&#21046;&#30340;&#29616;&#23454;&#35757;&#32451;&#29615;&#22659;&#30340;&#35775;&#38382;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; PeersimGym&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#23450;&#21046;&#30340;&#20223;&#30495;&#29615;&#22659;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#20248;&#21270;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#20219;&#21153;&#21368;&#36733;&#31574;&#30053;&#12290;PeersimGym &#25903;&#25345;&#21508;&#31181;&#32593;&#32476;&#25299;&#25169;&#21644;&#35745;&#31639;&#32422;&#26463;&#65292;&#24182;&#25972;&#21512;&#20102;&#19968;&#31181;"PettingZo"&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#37197;&#32622;&#20223;&#30495;&#21442;&#25968;&#21644;&#30417;&#25511;&#20223;&#30495;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#12289;&#40065;&#26834;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#21442;&#25968;PDE&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.15267</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L0&#31232;&#30095;&#22810;&#39033;&#24335;&#31574;&#30053;&#30340;&#21442;&#25968;PDE&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Parametric PDE Control with Deep Reinforcement Learning and Differentiable L0-Sparse Polynomial Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15267
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#12289;&#40065;&#26834;&#12289;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#21442;&#25968;PDE&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26368;&#20248;&#25511;&#21046;&#22312;&#24037;&#31243;&#21644;&#31185;&#23398;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#21442;&#25968;PDE&#30340;&#25511;&#21046;&#24320;&#36767;&#20102;&#26032;&#30340;&#21069;&#27839;&#12290;&#29305;&#21035;&#26159;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#26395;&#35299;&#20915;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#39640;&#32500;&#22797;&#26434;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#23383;&#20856;&#23398;&#20064;&#21644;&#21487;&#24494;&#20998;L$_0$&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#31232;&#30095;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;PDE&#25511;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15267v1 Announce Type: new  Abstract: Optimal control of parametric partial differential equations (PDEs) is crucial in many applications in engineering and science. In recent years, the progress in scientific machine learning has opened up new frontiers for the control of parametric PDEs. In particular, deep reinforcement learning (DRL) has the potential to solve high-dimensional and complex control problems in a large variety of applications. Most DRL methods rely on deep neural network (DNN) control policies. However, for many dynamical systems, DNN-based control policies tend to be over-parametrized, which means they need large amounts of training data, show limited robustness, and lack interpretability. In this work, we leverage dictionary learning and differentiable L$_0$ regularization to learn sparse, robust, and interpretable control policies for parametric PDEs. Our sparse policy architecture is agnostic to the DRL method and can be used in different policy-gradien
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11772</link><description>&lt;p&gt;
S-JEPA&#65306;&#36890;&#36807;&#21160;&#24577;&#31354;&#38388;&#27880;&#24847;&#21147;&#23454;&#29616;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#23454;&#29616;&#33041;&#30005;&#20449;&#21495;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;Signal-JEPA&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#31934;&#30830;&#19979;&#28216;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#33041;&#30005;&#20449;&#21495;&#22788;&#29702;&#20013;&#26080;&#32541;&#36328;&#25968;&#25454;&#38598;&#36716;&#31227;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20851;&#20110;&#20351;&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#20013;&#36801;&#31227;&#23398;&#20064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33041;&#30005;&#20449;&#21495;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#34920;&#31034;&#33041;&#30005;&#35760;&#24405;&#30340;Signal-JEPA&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#29305;&#23450;&#31354;&#38388;&#22359;&#25513;&#34109;&#31574;&#30053;&#21644;&#19977;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19979;&#28216;&#20998;&#31867;&#30340;&#26550;&#26500;&#12290;&#35813;&#30740;&#31350;&#22312;&#19968;&#20010;54&#20010;&#21463;&#35797;&#32773;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;BCI&#33539;&#24335;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;&#36816;&#21160;&#24819;&#35937;&#12289;ERP&#21644;SSVEP&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;JEPAs&#22312;&#33041;&#30005;&#20449;&#21495;&#32534;&#30721;&#20013;&#30340;&#28508;&#21147;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#31354;&#38388;&#28388;&#27874;&#23545;&#20934;&#30830;&#19979;&#28216;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11772v1 Announce Type: cross  Abstract: Motivated by the challenge of seamless cross-dataset transfer in EEG signal processing, this article presents an exploratory study on the use of Joint Embedding Predictive Architectures (JEPAs). In recent years, self-supervised learning has emerged as a promising approach for transfer learning in various domains. However, its application to EEG signals remains largely unexplored. In this article, we introduce Signal-JEPA for representing EEG recordings which includes a novel domain-specific spatial block masking strategy and three novel architectures for downstream classification. The study is conducted on a 54~subjects dataset and the downstream performance of the models is evaluated on three different BCI paradigms: motor imagery, ERP and SSVEP. Our study provides preliminary evidence for the potential of JEPAs in EEG signal encoding. Notably, our results highlight the importance of spatial filtering for accurate downstream classific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.11001</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Topologically faithful multi-class segmentation in medical images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11001
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#30340;&#25299;&#25169;&#20445;&#30495;&#22810;&#31867;&#21035;&#20998;&#21106;&#30340;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#20998;&#35299;&#20026;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#39564;&#35777;&#20102;&#22312;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#25299;&#25169;&#31934;&#24230;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#23646;&#24615;&#65292;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#34880;&#31649;&#25110;&#32454;&#32990;&#35745;&#25968;&#20013;&#30340;&#27969;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#37325;&#35201;&#30340;&#26041;&#27861;&#35770;&#36827;&#27493;&#23558;&#20195;&#25968;&#25299;&#25169;&#20013;&#25166;&#23454;&#30340;&#27010;&#24565;&#24102;&#21040;&#20102;&#20108;&#20540;&#20998;&#21106;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#31867;&#21035;&#20998;&#21106;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#24456;&#23569;&#34987;&#25506;&#32034;&#65292;&#25299;&#25169;&#38169;&#35823;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25299;&#25169;&#20445;&#30495;&#30340;&#22810;&#31867;&#21035;&#20998;&#21106;&#65292;&#25193;&#23637;&#20102;&#26368;&#36817;&#22522;&#20110;&#25345;&#20037;&#26465;&#30721;&#30340;Betti&#21305;&#37197;&#27010;&#24565;&#12290;&#25105;&#20204;&#23558;N&#31867;&#21035;&#20998;&#21106;&#38382;&#39064;&#25237;&#24433;&#21040;N&#20010;&#21333;&#31867;&#21035;&#20998;&#21106;&#20219;&#21153;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#19968;&#21442;&#25968;&#25345;&#20037;&#21516;&#35843;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#21253;&#21547;&#39640;&#24230;&#19981;&#21516;&#25299;&#25169;&#29305;&#24449;&#30340;&#22235;&#20010;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11001v1 Announce Type: cross  Abstract: Topological accuracy in medical image segmentation is a highly important property for downstream applications such as network analysis and flow modeling in vessels or cell counting. Recently, significant methodological advancements have brought well-founded concepts from algebraic topology to binary segmentation. However, these approaches have been underexplored in multi-class segmentation scenarios, where topological errors are common. We propose a general loss function for topologically faithful multi-class segmentation extending the recent Betti matching concept, which is based on induced matchings of persistence barcodes. We project the N-class segmentation problem to N single-class segmentation tasks, which allows us to use 1-parameter persistent homology making training of neural networks computationally feasible. We validate our method on a comprehensive set of four medical datasets with highly variant topological characteristic
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.05754</link><description>&lt;p&gt;
&#27169;&#24335;&#35782;&#21035;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;ResNet&#21644;DenseNet&#21450;&#20854;&#23436;&#25972;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24403;&#20170;&#25968;&#23383;&#25216;&#26415;&#30340;&#25509;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#32321;&#33635;&#30340;&#22522;&#30784;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#21457;&#23637;&#30340;&#31038;&#20250;&#38656;&#27714;&#27491;&#22312;&#24378;&#35843;&#26367;&#20195;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#21518;&#25705;&#23572;&#26102;&#20195;&#30340;&#26469;&#20020;&#25512;&#21160;&#20102;&#20855;&#26377;&#21331;&#36234;&#28508;&#21147;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#26032;&#26087;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#27604;&#36739;&#20013;&#23384;&#22312;&#21547;&#31946;&#25351;&#26631;&#65292;&#22240;&#27492;&#19968;&#22871;&#26126;&#30830;&#30340;&#35780;&#20272;&#31995;&#32479;&#19982;&#35814;&#32454;&#30340;&#25351;&#26631;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.04086</link><description>&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#33258;&#21160;&#21270;&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20013;&#20026;&#32852;&#21512;&#30142;&#30149;&#39044;&#27979;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#21644;&#25968;&#23383;&#21270;&#21307;&#30103;&#39046;&#22495;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20016;&#23500;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26377;&#28508;&#21147;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#21644;&#21307;&#23398;&#30740;&#31350;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#26512;EHR&#25968;&#25454;&#20197;&#39044;&#27979;&#24739;&#32773;&#26410;&#26469;&#20581;&#24247;&#29366;&#20917;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#20854;&#20013;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20513;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26469;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#30446;&#26631;&#30142;&#30149;&#65292;&#20197;&#25552;&#39640;&#21333;&#20219;&#21153;&#23398;&#20064;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;EHR&#25968;&#25454;&#30340;MTL&#26694;&#26550;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#26469;&#35782;&#21035;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#30340;&#20219;&#21153;&#32452;&#21644;&#35774;&#35745;&#27169;&#22411;&#26550;&#26500;&#65292;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#20026;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#24182;&#25913;&#36827;&#26694;&#26550;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDP&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25628;&#32034;&#20219;&#21153;&#20998;&#32452;&#21644;&#26550;&#26500;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#28085;&#30422;&#20219;&#21153;&#32452;&#21512;&#21644;&#26550;&#26500;&#30340;&#24191;&#27867;&#32852;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04086v1 Announce Type: new  Abstract: In the realm of big data and digital healthcare, Electronic Health Records (EHR) have become a rich source of information with the potential to improve patient care and medical research. In recent years, machine learning models have proliferated for analyzing EHR data to predict patients future health conditions. Among them, some studies advocate for multi-task learning (MTL) to jointly predict multiple target diseases for improving the prediction performance over single task learning. Nevertheless, current MTL frameworks for EHR data have significant limitations due to their heavy reliance on human experts to identify task groups for joint training and design model architectures. To reduce human intervention and improve the framework design, we propose an automated approach named AutoDP, which can search for the optimal configuration of task grouping and architectures simultaneously. To tackle the vast joint search space encompassing ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;</title><link>https://arxiv.org/abs/2402.15487</link><description>&lt;p&gt;
RoboEXP: &#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;&#23454;&#29616;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for Robotic Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#29983;&#25104;&#20102;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65292;&#25429;&#25417;&#20102;&#29615;&#22659;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#38656;&#35201;&#25506;&#32034;&#21608;&#22260;&#29615;&#22659;&#20197;&#36866;&#24212;&#24182;&#24212;&#23545;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20132;&#20114;&#24335;&#22330;&#26223;&#25506;&#32034;&#30340;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#24182;&#29983;&#25104;&#19968;&#20010;&#25429;&#25417;&#22522;&#30784;&#29615;&#22659;&#32467;&#26500;&#30340;&#21160;&#20316;&#26465;&#20214;&#21270;&#22330;&#26223;&#22270;&#65288;ACSG&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15487v1 Announce Type: cross  Abstract: Robots need to explore their surroundings to adapt to and tackle tasks in unknown environments. Prior work has proposed building scene graphs of the environment but typically assumes that the environment is static, omitting regions that require active interactions. This severely limits their ability to handle more complex tasks in household and office environments: before setting up a table, robots must explore drawers and cabinets to locate all utensils and condiments. In this work, we introduce the novel task of interactive scene exploration, wherein robots autonomously explore environments and produce an action-conditioned scene graph (ACSG) that captures the structure of the underlying environment. The ACSG accounts for both low-level information, such as geometry and semantics, and high-level information, such as the action-conditioned relationships between different entities in the scene. To this end, we present the Robotic Explo
&lt;/p&gt;</description></item><item><title>&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.14603</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Balanced Resonate-and-Fire Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14603
&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#30340;&#24341;&#20837;&#32531;&#35299;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#26356;&#23569;&#30340;&#33033;&#20914;&#65292;&#24182;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;20&#24180;&#37324;&#24341;&#20837;&#30340;&#35856;&#25391;-&#25918;&#30005;&#65288;RF&#65289;&#31070;&#32463;&#20803;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#19988;&#31526;&#21512;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#23574;&#23792;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#20849;&#25391;&#33180;&#21160;&#21147;&#23398;&#65292;&#23427;&#21487;&#20197;&#22312;&#26102;&#38388;&#22495;&#20869;&#25552;&#21462;&#39057;&#29575;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;RF&#20844;&#24335;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#23398;&#20064;&#24182;&#38459;&#30861;&#20102;RF&#31070;&#32463;&#20803;&#30340;&#21407;&#21017;&#20248;&#21183;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24179;&#34913;&#30340;RF&#65288;BRF&#65289;&#31070;&#32463;&#20803;&#65292;&#23427;&#32531;&#35299;&#20102;&#26222;&#36890;RF&#31070;&#32463;&#20803;&#30340;&#19968;&#20123;&#22266;&#26377;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#20013;&#22312;&#24490;&#29615;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;RSNNs&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;BRF&#31070;&#32463;&#20803;&#30340;&#32593;&#32476;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20135;&#29983;&#30340;&#33033;&#20914;&#20165;&#20026;&#29616;&#20195;RSNNs&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#20195;RSNNs&#65292;&#38656;&#35201;&#30340;&#21442;&#25968;&#26126;&#26174;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;BRF-RSNN&#22987;&#32456;&#25552;&#20379;&#26356;&#24555;&#36895;&#21644;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#25910;&#25947;&#65292;&#21363;&#20351;&#22312;&#36830;&#25509;&#22810;&#20010;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14603v1 Announce Type: cross  Abstract: The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;</title><link>https://arxiv.org/abs/2402.13728</link><description>&lt;p&gt;
&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#22349;&#22604;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Average gradient outer product as a mechanism for deep neural collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Collapse (DNC)&#25351;&#30340;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26368;&#21518;&#20960;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#24778;&#20154;&#21018;&#24615;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#37117;&#24471;&#21040;&#20102;&#27979;&#37327;&#65292;&#20294;&#20854;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20805;&#20998;&#35777;&#25454;&#65292;&#34920;&#26126;DNC&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;(AGOP)&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#21457;&#29983;&#30340;&#12290;&#30456;&#27604;&#20110;&#35299;&#37322;&#31070;&#32463;&#22349;&#22604;&#30340;&#29305;&#24449;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#22914;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65292;&#36825;&#19968;&#36827;&#23637;&#26356;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#32487;&#32493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#26435;&#37325;&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#20540;&#26159;DNN&#20013;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36825;&#31181;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#30340;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AGOP&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#21457;&#31070;&#32463;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12498</link><description>&lt;p&gt;
&#23553;&#24314;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Feudal Networks for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12498
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#36981;&#24490;&#20154;&#31867;&#21487;&#20197;&#22312;&#27809;&#26377;&#35814;&#32454;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#30452;&#35273;&#12290;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#24314;&#31435;&#21253;&#21547;&#21487;&#29992;&#20110;&#35268;&#21010;&#30340;&#22270;&#20687;&#33410;&#28857;&#30340;&#25299;&#25169;&#22270;&#30340;&#21516;&#26102;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#21464;&#20307;&#20174;&#34987;&#21160;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#31038;&#20132;&#21644;&#35821;&#20041;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35270;&#39057;&#65292;&#21033;&#29992;&#22823;&#22411;&#22270;&#24182;&#19988;&#30001;&#20110;&#20351;&#29992;&#20102;&#37324;&#31243;&#35745;&#65292;&#22330;&#26223;&#19981;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#30001;&#24037;&#20316;&#20195;&#29702;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#39640;&#32423;&#31649;&#29702;&#32773;&#32452;&#25104;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#23553;&#24314;&#23398;&#20064;&#33539;&#24335;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#27599;&#20010;&#32423;&#21035;&#30340;&#20195;&#29702;&#30475;&#21040;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#36816;&#20316;&#12290;&#22312;&#27492;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#32423;&#31649;&#29702;&#32773;&#65292;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#19968;&#20010;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#20197;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10206</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#22270;&#23376;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#20854;&#25972;&#20307;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;&#22270;&#30340;&#26041;&#27861;&#35201;&#20040;&#21024;&#38500;&#36793;&#32536;&#65288;&#31232;&#30095;&#21270;&#65289;&#65292;&#35201;&#20040;&#21512;&#24182;&#33410;&#28857;&#65288;&#31895;&#21270;&#65289;&#65292;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#33410;&#28857;&#25110;&#36793;&#19978;&#23450;&#20041;&#30340;&#20234;&#36763;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#23376;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#22914;&#20309;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#25152;&#20351;&#29992;&#30340;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#29978;&#33267;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#12290;</title><link>https://arxiv.org/abs/2402.08790</link><description>&lt;p&gt;
&#29992;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#20998;&#23376;&#29983;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Improving Molecule Generation and Drug Discovery with a Knowledge-enhanced Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21040;&#29983;&#25104;&#27169;&#22411;&#20013;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20998;&#23376;&#21644;&#26032;&#33647;&#20505;&#36873;&#29289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#25104;&#21151;&#65292;&#20294;&#26159;&#29983;&#25104;&#27169;&#22411;&#19982;&#24191;&#27867;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#36825;&#20123;&#30693;&#35782;&#36890;&#24120;&#34987;&#31995;&#32479;&#21270;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#32780;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#20449;&#24687;&#21644;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#30340;&#28508;&#21147;&#23578;&#26410;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;K-DReAM&#30340;&#30693;&#35782;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#21151;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#23558;&#36825;&#20123;&#19978;&#19979;&#25991;&#20449;&#24687;&#32467;&#21512;&#21040;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#20197;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#12290;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#19982;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20026;&#20135;&#29983;&#20855;&#26377;&#29305;&#23450;&#29305;&#24449;&#30340;&#26032;&#33647;&#20505;&#36873;&#29289;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08790v1 Announce Type: new Abstract: Recent advancements in generative models have established state-of-the-art benchmarks in generating molecules and novel drug candidates. Despite these successes, a significant gap persists between generative models and the utilization of extensive biomedical knowledge, often systematized within knowledge graphs, whose potential to inform and enhance generative processes has not been realized. In this paper, we present a novel approach that bridges this divide by developing a framework for knowledge-enhanced generative models called K-DReAM. We develop a scalable methodology to extend the functionality of knowledge graphs while preserving semantic integrity and incorporate this contextual information into a generative framework to guide a diffusion-based model. The integration of knowledge graph embeddings with our generative model furnishes a robust mechanism for producing novel drug candidates possessing specific characteristics while en
&lt;/p&gt;</description></item><item><title>Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.05785</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#23398;&#20064;&#19978;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Transformer Language Models on Algorithmic Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05785
&lt;/p&gt;
&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#27604;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#23545;&#20110;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#30340;&#25928;&#26524;&#26356;&#24046;&#65292;&#32780;&#19988;&#26799;&#24230;&#19979;&#38477;&#22312;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#25928;&#29575;&#38750;&#24120;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#31163;&#25955;&#31639;&#27861;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#35201;&#27714;&#32452;&#21512;&#22810;&#20010;&#31163;&#25955;&#23376;&#20219;&#21153;&#30340;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;LLaMA&#27169;&#22411;&#21644;&#22312;GPT-4&#21644;Gemini&#19978;&#25552;&#31034;&#26469;&#34913;&#37327;&#23398;&#20064;&#23398;&#20064;&#21407;&#35821;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#33021;&#21147;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#35268;&#27169;&#26041;&#38754;&#27604;&#20026;&#26032;&#30340;&#31639;&#27861;&#32452;&#21512;&#37325;&#26032;&#23398;&#20064;&#25152;&#26377;&#23376;&#20219;&#21153;&#25928;&#26524;&#26356;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#24615;&#29702;&#35770;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#35760;&#24518;&#21069;&#39304;&#27169;&#22411;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#25351;&#25968;&#32423;&#22320;&#28010;&#36153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;</title><link>https://arxiv.org/abs/2402.05525</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Model-Based Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#38544;&#31169;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#24635;&#32467;&#20102;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20855;&#26377;&#38544;&#31169;&#20445;&#35777;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#30456;&#23545;&#20110;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#36712;&#36857;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DP-MORL&#65292;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;MBRL&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;DP-FedAvg&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#29615;&#22659;&#30340;&#38544;&#31169;&#27169;&#22411;&#65292;DP-FedAvg&#26159;&#19968;&#31181;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#36712;&#36857;&#32423;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#20248;&#21270;&#20174;&#65288;&#21463;&#32602;&#30340;&#65289;&#38544;&#31169;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#31574;&#30053;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#19982;&#31995;&#32479;&#20132;&#20114;&#25110;&#35775;&#38382;&#36755;&#20837;&#25968;&#25454;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;DP-MORL&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#35757;&#32451;&#20986;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;RL&#20195;&#29702;&#65292;&#24182;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38544;&#31169;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05356</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#19979;&#28216;&#25968;&#25454;&#20462;&#21098;&#30340;&#23398;&#20064;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Learning Complexity for Downstream Data Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#24494;&#35843;&#26500;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#19968;&#20010;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#20462;&#21098;&#25481;&#20449;&#24687;&#36739;&#23569;&#30340;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#26469;&#37327;&#21270;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#24615;&#65292;&#20294;&#30001;&#20110;&#21442;&#25968;&#26356;&#26032;&#30340;&#32321;&#37325;&#65292;&#20462;&#21098;&#25104;&#26412;&#21464;&#24471;&#19981;&#21487;&#24573;&#35270;&#12290;&#20026;&#20102;&#39640;&#25928;&#20462;&#21098;&#65292;&#23558;&#20960;&#20309;&#26041;&#27861;&#30340;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#20174;&#22522;&#20110;&#35757;&#32451;&#30340;&#26041;&#27861;&#36866;&#24212;&#20026;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#36825;&#31181;&#36866;&#24212;&#25197;&#26354;&#20102;&#21407;&#22987;&#30340;&#20462;&#21098;&#24182;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23398;&#20064;&#22797;&#26434;&#24615;&#65288;LC&#65289;&#20316;&#20026;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23398;&#20064;&#22797;&#26434;&#24615;&#34987;&#23450;&#20041;&#20026;&#20855;&#26377;&#19981;&#21516;&#23481;&#37327;&#30340;&#23376;&#32593;&#32476;&#30340;&#24179;&#22343;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#23427;&#21253;&#21547;&#20102;&#22312;&#19968;&#20010;&#25910;&#25947;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.04362</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36880;&#28176;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;
&lt;/p&gt;
&lt;p&gt;
Neural Networks Learn Statistics of Increasing Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#35268;&#24459;&#65292;&#21363;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#20302;&#38454;&#30697;&#30340;&#26368;&#22823;&#29109;&#20998;&#24067;&#29305;&#24449;&#65292;&#28982;&#21518;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#36827;&#34892;&#20102;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#30340;&#32534;&#36753;&#65292;&#35777;&#26126;&#20102;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#20250;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#31616;&#21333;&#24615;&#20542;&#21521;&#65288;DSB&#65289;&#20551;&#35774;&#31070;&#32463;&#32593;&#32476;&#39318;&#20808;&#23398;&#20064;&#20302;&#38454;&#30697;&#65292;&#28982;&#21518;&#20877;&#36716;&#21521;&#39640;&#38454;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32593;&#32476;&#22312;&#35757;&#32451;&#26089;&#26399;&#33258;&#21160;&#23398;&#20064;&#22312;&#26368;&#22823;&#29109;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#35757;&#32451;&#21518;&#26399;&#22833;&#21435;&#36825;&#31181;&#33021;&#21147;&#30340;&#26377;&#21147;&#26032;&#35777;&#25454;&#65292;&#32473;&#20986;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#26032;&#35777;&#25454;&#26469;&#25903;&#25345;DSB&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35777;&#26126;&#20196;&#29260;$n$-gram&#39057;&#29575;&#19982;&#23884;&#20837;&#21521;&#37327;&#30340;&#30697;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#22312;LLM&#20013;&#25214;&#21040;&#20542;&#21521;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#23558;DSB&#25193;&#23637;&#21040;&#31163;&#25955;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#23558;&#19968;&#31867;&#30340;&#20302;&#38454;&#32479;&#35745;&#25968;&#25454;&#25163;&#26415;&#24615;&#22320;&#32534;&#36753;&#25104;&#19982;&#21478;&#19968;&#31867;&#30456;&#21305;&#37197;&#65292;&#28982;&#21518;&#23637;&#31034;&#26089;&#26399;&#35757;&#32451;&#30340;&#32593;&#32476;&#23558;&#32534;&#36753;&#30340;&#26679;&#26412;&#35270;&#20026;&#26469;&#33258;&#30446;&#26631;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/EleutherAI/features-across-time &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The distributional simplicity bias (DSB) posits that neural networks learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we present compelling new evidence for the DSB by showing that networks automatically learn to perform well on maximum-entropy distributions whose low-order statistics match those of the training set early in training, then lose this ability later. We also extend the DSB to discrete domains by proving an equivalence between token $n$-gram frequencies and the moments of embedding vectors, and by finding empirical evidence for the bias in LLMs. Finally we use optimal transport methods to surgically edit the low-order statistics of one class to match those of another, and show that early-training networks treat the edited samples as if they were drawn from the target class. Code is available at https://github.com/EleutherAI/features-across-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04344</link><description>&lt;p&gt;
&#21435;&#26657;&#20934;&#26159;&#21542;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Confidence Calibration Help Conformal Prediction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26500;&#24314;&#20102;&#19968;&#32452;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20551;&#35774;&#20449;&#24515;&#26657;&#20934;&#21487;&#20197;&#20026;&#19968;&#33268;&#24615;&#39044;&#27979;&#24102;&#26469;&#22909;&#22788;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#20250;&#24847;&#22806;&#22320;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#25913;&#21892;&#20102;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#19988;&#28201;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#21017;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#32622;&#20449;&#24230;&#20250;&#38477;&#20302;&#22312;&#39044;&#27979;&#38598;&#20013;&#28155;&#21152;&#26032;&#31867;&#30340;&#27010;&#29575;&#12290;&#21463;&#21040;&#36825;&#19968;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;$\textbf{&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;}$ (ConfTS)&#65292;&#36890;&#36807;&#38408;&#20540;&#19982;&#30495;&#23454;&#26631;&#31614;&#30340;&#38750;&#19968;&#33268;&#24615;&#20998;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#20462;&#27491;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ConfTS&#30340;&#26032;&#30446;&#26631;&#23558;&#20351;&#28201;&#24230;&#20540;&#26397;&#30528;&#20248;&#21270;&#38598;&#30340;&#26041;&#21521;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability. Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead. Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set. Inspired by the analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label. In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set th
&lt;/p&gt;</description></item><item><title>&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02746</link><description>&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#36275;&#20197;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02746
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20351;&#29992;&#26631;&#20934; Gaussian &#36807;&#31243;&#65288;GP&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#21363;&#26631;&#20934; BO&#65292;&#22312;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#31181;&#35266;&#24565;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110; Gaussian &#36807;&#31243;&#22312;&#21327;&#26041;&#24046;&#24314;&#27169;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#23545;&#39640;&#32500;&#36755;&#20837;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#20123;&#25285;&#24551;&#30475;&#36215;&#26469;&#21512;&#29702;&#65292;&#20294;&#32570;&#20047;&#25903;&#25345;&#36825;&#31181;&#35266;&#28857;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934; GP &#30340;&#34920;&#29616;&#22987;&#32456;&#20301;&#20110;&#26368;&#20339;&#33539;&#22260;&#20869;&#65292;&#24448;&#24448;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#29616;&#26377; BO &#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934; GP &#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#20989;&#25968;&#30340;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#24378;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#36827;&#34892; BO &#21487;&#20197;&#33719;&#24471;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01920</link><description>&lt;p&gt;
&#23545;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#30340;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Preference Poisoning Attacks on Reward Model Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01920
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20174;&#20559;&#22909;&#27604;&#36739;&#20013;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#23384;&#22312;&#20559;&#22909;&#27745;&#26579;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#26469;&#23545;&#30446;&#26631;&#32467;&#26524;&#36827;&#34892;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#31639;&#27861;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25915;&#20987;&#22312;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20004;&#20004;&#27604;&#36739;&#20013;&#23398;&#20064;&#25928;&#29992;&#25110;&#22870;&#21169;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22522;&#30784;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#20174;&#26412;&#36136;&#19978;&#38656;&#35201;&#20174;&#20154;&#20204;&#37027;&#37324;&#25910;&#38598;&#20559;&#22909;&#20449;&#24687;&#65292;&#32780;&#21453;&#39304;&#36890;&#24120;&#26159;&#21311;&#21517;&#25552;&#20379;&#30340;&#12290;&#30001;&#20110;&#20559;&#22909;&#26159;&#20027;&#35266;&#30340;&#65292;&#27809;&#26377;&#21487;&#20197;&#27604;&#36739;&#30340;&#40644;&#37329;&#26631;&#20934;&#65307;&#28982;&#32780;&#65292;&#23545;&#20559;&#22909;&#23398;&#20064;&#30340;&#39640;&#24433;&#21709;&#31995;&#32479;&#30340;&#20381;&#36182;&#24615;&#20026;&#24694;&#24847;&#34892;&#20026;&#32773;&#20542;&#21521;&#20110;&#25197;&#26354;&#20197;&#36798;&#21040;&#20854;&#30446;&#30340;&#32780;&#37319;&#38598;&#30340;&#25968;&#25454;&#21019;&#36896;&#20102;&#24378;&#28872;&#30340;&#21160;&#26426;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#31181;&#23041;&#32961;&#27169;&#22411;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#31181;&#28431;&#27934;&#30340;&#24615;&#36136;&#21644;&#31243;&#24230;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#21487;&#20197;&#32763;&#36716;&#23569;&#37327;&#20559;&#22909;&#27604;&#36739;&#65292;&#20197;&#20419;&#36827;&#25110;&#36140;&#20302;&#30446;&#26631;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#29992;&#20110;&#36825;&#20123;&#25915;&#20987;&#30340;&#31639;&#27861;&#26041;&#27861;&#65306;&#22522;&#20110;&#21407;&#21017;&#30340;&#26799;&#24230;&#26694;&#26550;&#21644;&#20960;&#31181;&#21464;&#31181;&#30340;&#25353;&#36317;&#31163;&#25490;&#21517;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20004;&#31867;&#26368;&#20339;&#25915;&#20987;&#22312;&#25104;&#21151;&#23454;&#26045;&#24694;&#24847;&#34892;&#20026;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning utility, or reward, models from pairwise comparisons is a fundamental component in a number of application domains. These approaches inherently entail collecting preference information from people, with feedback often provided anonymously. Since preferences are subjective, there is no gold standard to compare against; yet, reliance of high-impact systems on preference learning creates a strong motivation for malicious actors to skew data collected in this fashion to their ends. We investigate the nature and extent of this vulnerability systematically by considering a threat model in which an attacker can flip a small subset of preference comparisons with the goal of either promoting or demoting a target outcome. First, we propose two classes of algorithmic approaches for these attacks: a principled gradient-based framework, and several variants of rank-by-distance methods. Next, we demonstrate the efficacy of best attacks in both these classes in successfully achieving malicio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05320</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Surrogate Models for Airfoil Flow Simulations with Denoising Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20026;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#35757;&#32451;&#20102;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#65292;&#25104;&#21151;&#25429;&#25417;&#25972;&#20010;&#35299;&#20998;&#24067;&#24182;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28237;&#27969;&#27169;&#25311;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#21516;&#26102;&#65292;&#22312;&#20195;&#29702;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#20307;&#29616;&#27169;&#25311;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#28237;&#27969;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20195;&#29702;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#26222;&#36941;&#24615;&#65292;&#36873;&#25321;&#20197;&#21508;&#31181;&#24418;&#29366;&#12289;&#38647;&#35834;&#25968;&#21644;&#25915;&#35282;&#30340;&#27668;&#21160;&#32764;&#27969;&#22330;&#27169;&#25311;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DDPMs&#33021;&#25104;&#21151;&#25429;&#25417;&#35299;&#30340;&#25972;&#20010;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#20272;&#35745;&#27169;&#25311;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;DDPMs&#30340;&#24615;&#33021;&#36824;&#19982;&#20197;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#24322;&#26041;&#24046;&#27169;&#22411;&#24418;&#24335;&#30340;&#19981;&#21516;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DDPMs&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05320v2 Announce Type: replace-cross  Abstract: Leveraging neural networks as surrogate models for turbulence simulation is a topic of growing interest. At the same time, embodying the inherent uncertainty of simulations in the predictions of surrogate models remains very challenging. The present study makes a first attempt to use denoising diffusion probabilistic models (DDPMs) to train an uncertainty-aware surrogate model for turbulence simulations. Due to its prevalence, the simulation of flows around airfoils with various shapes, Reynolds numbers, and angles of attack is chosen as the learning objective. Our results show that DDPMs can successfully capture the whole distribution of solutions and, as a consequence, accurately estimate the uncertainty of the simulations. The performance of DDPMs is also compared with varying baselines in the form of Bayesian neural networks and heteroscedastic models. Experiments demonstrate that DDPMs outperform the other methods regardin
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.13584</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#35823;&#24046;&#30028;&#38480;&#65306;&#23436;&#20840;&#25910;&#25947;&#20272;&#35745;&#19979;&#30340;&#23545;&#25968;&#20985;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#25105;&#20204;&#29992;&#20110;&#24471;&#20998;&#20272;&#35745;&#30340;&#36924;&#36817;&#20989;&#25968;&#31867;&#30001;Lipschitz&#36830;&#32493;&#20989;&#25968;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28608;&#21169;&#24615;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#21363;&#20174;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#24471;&#20998;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#37319;&#26679;&#20272;&#35745;&#32467;&#21512;&#36215;&#26469;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24050;&#30693;&#19978;&#38480;&#20272;&#35745;&#65292;&#28041;&#21450;&#20851;&#38190;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#65292;&#22914;&#25968;&#25454;&#20998;&#24067;&#65288;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#65289;&#19982;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#32500;&#24230;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.10610</link><description>&lt;p&gt;
&#20449;&#21495;&#22312;&#22823;&#22270;&#19978;&#30340;&#37319;&#26679;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#21644;&#19968;&#33268;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
A Poincar\'e Inequality and Consistency Results for Signal Sampling on Large Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10610
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22270;&#26497;&#38480;&#20013;&#22270;&#19978;&#20449;&#21495;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;Poincar\'e&#19981;&#31561;&#24335;&#24182;&#23637;&#31034;&#20102;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#23545;&#22270;&#36827;&#34892;&#23376;&#37319;&#26679;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#22312;&#22270;&#19978;&#36827;&#34892;&#37319;&#26679;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#22270;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#30340;&#12290;&#29616;&#26377;&#30340;&#22270;&#37319;&#26679;&#25216;&#26415;&#19981;&#20165;&#38656;&#35201;&#35745;&#31639;&#22823;&#30697;&#38453;&#30340;&#35889;&#65292;&#32780;&#19988;&#22312;&#22270;&#21457;&#29983;&#21464;&#21270;&#65288;&#20363;&#22914;&#22686;&#38271;&#65289;&#26102;&#38656;&#35201;&#37325;&#22797;&#36825;&#20123;&#35745;&#31639;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#19968;&#31181;&#22270;&#26497;&#38480;--&#22270;&#19978;&#30340;&#20449;&#21495;&#37319;&#26679;&#29702;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#19978;&#20449;&#21495;&#30340;Poincar\'e&#19981;&#31561;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#28385;&#36275;&#36825;&#19968;&#19981;&#31561;&#24335;&#30340;&#33410;&#28857;&#23376;&#38598;&#30340;&#34917;&#38598;&#26159;&#22270;&#19978;&#20449;&#21495;Paley-Wiener&#31354;&#38388;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;&#36890;&#36807;&#19982;&#35889;&#32858;&#31867;&#21644;&#39640;&#26031;&#28040;&#20803;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#37319;&#26679;&#38598;&#26159;&#19968;&#33268;&#30340;&#65292;&#21363;&#25910;&#25947;&#30340;&#22270;&#24207;&#21015;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#25910;&#25947;&#21040;&#22270;&#26497;&#38480;&#19978;&#30340;&#21807;&#19968;&#37319;&#26679;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10610v2 Announce Type: replace  Abstract: Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit -- the graphon. We prove a Poincar\'e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13947</link><description>&lt;p&gt;
&#32593;&#32476;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy Trading. (arXiv:2401.13947v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20877;&#29983;&#33021;&#28304;&#38646;&#36793;&#38469;&#25104;&#26412;&#21644;&#29289;&#29702;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20998;&#24067;&#24335;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#36827;&#34892;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#34987;&#38271;&#26399;&#35748;&#20026;&#26159;&#25552;&#39640;&#33021;&#28304;&#31995;&#32479;&#24377;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#28040;&#36153;&#32773;&#21644;&#33258;&#32473;&#33258;&#36275;&#32773;&#65288;&#20855;&#26377;&#33021;&#28304;&#21457;&#30005;&#36164;&#28304;&#30340;&#20154;&#65289;&#32570;&#20047;&#36827;&#34892;&#37325;&#22797;&#28857;&#23545;&#28857;&#20132;&#26131;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38646;&#36793;&#38469;&#25104;&#26412;&#22312;&#30830;&#23450;&#20844;&#24179;&#24066;&#22330;&#20215;&#26684;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#21270;&#28040;&#36153;&#32773;&#23545;&#22826;&#38451;&#33021;&#20809;&#20239;&#21644;&#33021;&#37327;&#20648;&#23384;&#36164;&#28304;&#30340;&#31454;&#26631;&#21644;&#31649;&#29702;&#65292;&#22312;&#19968;&#31181;&#21033;&#29992;&#20379;&#38656;&#27604;&#30340;&#28857;&#23545;&#28857;&#28165;&#31639;&#26426;&#21046;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MARL&#26694;&#26550;&#22914;&#20309;&#25972;&#21512;&#29289;&#29702;&#32593;&#32476;&#32422;&#26463;&#20197;&#23454;&#29616;&#30005;&#21387;&#25511;&#21046;&#65292;&#20174;&#32780;&#30830;&#20445;&#28857;&#23545;&#28857;&#33021;&#28304;&#20132;&#26131;&#30340;&#29289;&#29702;&#21487;&#34892;&#24615;&#65292;&#24182;&#20026;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#26045;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing distributed renewable and energy storage resources in local distribution networks via peer-to-peer (P2P) energy trading has long been touted as a solution to improve energy systems' resilience and sustainability. Consumers and prosumers (those who have energy generation resources), however, do not have the expertise to engage in repeated P2P trading, and the zero-marginal costs of renewables present challenges in determining fair market prices. To address these issues, we propose multi-agent reinforcement learning (MARL) frameworks to help automate consumers' bidding and management of their solar PV and energy storage resources, under a specific P2P clearing mechanism that utilizes the so-called supply-demand ratio. In addition, we show how the MARL frameworks can integrate physical network constraints to realize voltage control, hence ensuring physical feasibility of the P2P energy trading and paving way for real-world implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13421</link><description>&lt;p&gt;
&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated learning with distributed fixed design quantum chips and quantum channels. (arXiv:2401.13421v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20998;&#24067;&#24335;&#22266;&#23450;&#35774;&#35745;&#37327;&#23376;&#33455;&#29255;&#21644;&#37327;&#23376;&#20449;&#36947;&#30340;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#37327;&#23376;&#24577;&#30340;&#20256;&#36882;&#21644;&#32858;&#21512;&#26799;&#24230;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#25552;&#20379;&#26356;&#39640;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#23458;&#25143;&#31471;&#30340;&#31934;&#24515;&#35774;&#35745;&#26597;&#35810;&#65292;&#32463;&#20856;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#21487;&#20197;&#34987;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#27979;&#37327;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#37327;&#23376;&#36890;&#20449;&#20449;&#36947;&#34987;&#35748;&#20026;&#26356;&#21152;&#23433;&#20840;&#65292;&#22240;&#20026;&#21487;&#20197;&#26816;&#27979;&#21040;&#36825;&#31181;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#37327;&#23376;&#29256;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22810;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37327;&#23376;&#20449;&#36947;&#21457;&#36865;N&#32500;&#25968;&#25454;&#21521;&#37327;&#38656;&#35201;&#21457;&#36865;log N&#20010;&#32416;&#32544;&#24577;&#37327;&#23376;&#27604;&#29305;&#65292;&#22914;&#26524;&#25968;&#25454;&#21521;&#37327;&#20316;&#20026;&#37327;&#23376;&#24577;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#25552;&#20379;&#25351;&#25968;&#32423;&#30340;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#22522;&#20110;&#30001;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#21457;&#36865;&#30340;&#37327;&#23376;&#24577;&#65292;&#25805;&#20316;&#22266;&#23450;&#35774;&#35745;&#30340;&#37327;&#23376;&#33455;&#29255;&#12290;&#22522;&#20110;&#25509;&#25910;&#21040;&#30340;&#21472;&#21152;&#24577;&#65292;&#23458;&#25143;&#31471;&#35745;&#31639;&#24182;&#23558;&#20854;&#26412;&#22320;&#26799;&#24230;&#20316;&#20026;&#37327;&#23376;&#24577;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26381;&#21153;&#22120;&#23558;&#36825;&#20123;&#26799;&#24230;&#32858;&#21512;&#20197;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#20110;&#26381;&#21153;&#22120;&#19981;&#21457;&#36865;&#27169;&#22411;&#20449;&#24687;&#65292;
&lt;/p&gt;
&lt;p&gt;
The privacy in classical federated learning can be breached through the use of local gradient results by using engineered queries from the clients. However, quantum communication channels are considered more secure because the use of measurements in the data causes some loss of information, which can be detected. Therefore, the quantum version of federated learning can be used to provide more privacy. Additionally, sending an $N$ dimensional data vector through a quantum channel requires sending $\log N$ entangled qubits, which can provide exponential efficiency if the data vector is obtained as quantum states.  In this paper, we propose a quantum federated learning model where fixed design quantum chips are operated based on the quantum states sent by a centralized server. Based on the coming superposition states, the clients compute and then send their local gradients as quantum states to the server, where they are aggregated to update parameters. Since the server does not send model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.18304</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#31283;&#24577;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;&#24615;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#31283;&#24577;&#29615;&#22659;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#36873;&#25321;&#22238;&#28335;&#31383;&#21475;&#26469;&#26368;&#22823;&#21270;&#21382;&#21490;&#25968;&#25454;&#21033;&#29992;&#65292;&#24182;&#20445;&#25345;&#32047;&#31215;&#20559;&#24046;&#22312;&#21487;&#25509;&#21463;&#33539;&#22260;&#20869;&#12290;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#23545;&#26410;&#30693;&#38750;&#31283;&#24577;&#30340;&#36866;&#24212;&#24615;&#65292;&#36951;&#25022;&#30028;&#22312;&#24378;&#20984;&#25110;&#28385;&#36275;Lipschitz&#26465;&#20214;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#12290;&#35813;&#30740;&#31350;&#30340;&#21019;&#26032;&#28857;&#26159;&#20989;&#25968;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#32479;&#35745;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#31283;&#23450;&#24615;&#21407;&#21017;&#26469;&#36873;&#25321;&#19968;&#20010;&#22238;&#28335;&#31383;&#21475;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#65292;&#21516;&#26102;&#23558;&#32047;&#31215;&#20559;&#24046;&#20445;&#25345;&#22312;&#19982;&#38543;&#26426;&#35823;&#24046;&#30456;&#23545;&#21487;&#25509;&#21463;&#30340;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#26410;&#30693;&#38750;&#31283;&#23450;&#24615;&#30340;&#36866;&#24212;&#24615;&#12290;&#24403;&#20154;&#21475;&#25439;&#22833;&#20989;&#25968;&#24378;&#20984;&#25110;&#20165;&#28385;&#36275;Lipschitz&#26465;&#20214;&#26102;&#65292;&#36951;&#25022;&#30028;&#26159;&#26497;&#23567;&#21270;&#30340;&#26368;&#20248;&#35299;&#65292;&#20165;&#21463;&#23545;&#25968;&#22240;&#23376;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26680;&#24515;&#26159;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#21644;&#23558;&#38750;&#31283;&#24577;&#25968;&#25454;&#24207;&#21015;&#21010;&#20998;&#20026;&#20934;&#31283;&#24577;&#29255;&#27573;&#30340;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a versatile framework for statistical learning in non-stationary environments. In each time period, our approach applies a stability principle to select a look-back window that maximizes the utilization of historical data while keeping the cumulative bias within an acceptable range relative to the stochastic error. Our theory showcases the adaptability of this approach to unknown non-stationarity. The regret bound is minimax optimal up to logarithmic factors when the population losses are strongly convex, or Lipschitz only. At the heart of our analysis lie two novel components: a measure of similarity between functions and a segmentation technique for dividing the non-stationary data sequence into quasi-stationary pieces.
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#21450;&#20854;&#23545;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#36924;&#36817;&#30340;&#23450;&#37327;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.07528</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Provable Advantage of Parameterized Quantum Circuit in Function Approximation. (arXiv:2310.07528v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07528
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#22312;&#20989;&#25968;&#36924;&#36817;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#21450;&#20854;&#23545;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#36924;&#36817;&#30340;&#23450;&#37327;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65288;PQCs&#65289;&#22312;&#23436;&#25104;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;PQCs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#20197;&#24448;&#23545;&#20110;PQCs&#30340;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#20027;&#35201;&#26159;&#38750;&#26500;&#36896;&#24615;&#30340;&#65292;&#30001;&#27492;&#24341;&#21457;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;PQCs&#38656;&#35201;&#22810;&#22823;&#25165;&#33021;&#20197;&#32473;&#23450;&#35823;&#24046;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#37325;&#26032;&#19978;&#20256;PQCs&#30340;&#26174;&#24335;&#26500;&#36896;&#65292;&#29992;&#20110;&#36924;&#36817;&#36830;&#32493;&#21644;&#24179;&#28369;&#20989;&#25968;&#65292;&#24182;&#24314;&#31435;&#20102;PQCs&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#23450;&#37327;&#36924;&#36817;&#35823;&#24046;&#30028;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37327;&#23376;&#20449;&#21495;&#22788;&#29702;&#21644;&#37193;&#32447;&#24615;&#32452;&#21512;&#30340;&#25216;&#26415;&#26469;&#26500;&#36896;&#23454;&#29616;&#22810;&#20803;&#22810;&#39033;&#24335;&#30340;PQCs&#12290;&#25105;&#20204;&#20351;&#29992;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#21644;&#23616;&#37096;&#27888;&#21202;&#36817;&#20284;&#25216;&#26415;&#23454;&#29616;&#20840;&#23616;&#21644;&#23616;&#37096;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the power of parameterized quantum circuits (PQCs) in accomplishing machine learning tasks is one of the most important questions in quantum machine learning. In this paper, we analyze the expressivity of PQCs through the lens of function approximation. Previously established universal approximation theorems for PQCs are mainly nonconstructive, leading us to the following question: How large do the PQCs need to be to approximate the target function up to a given error? We exhibit explicit constructions of data re-uploading PQCs for approximating continuous and smooth functions and establish quantitative approximation error bounds in terms of the width, the depth and the number of trainable parameters of the PQCs. To achieve this, we utilize techniques from quantum signal processing and linear combinations of unitaries to construct PQCs that implement multivariate polynomials. We implement global and local approximation techniques using Bernstein polynomials and local Tayl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.05468</link><description>&lt;p&gt;
ExIFFI&#21644;EIF+&#65306;&#35299;&#37322;&#24615;&#21644;&#22686;&#24378;&#30340;&#25512;&#24191;&#33021;&#21147;&#20197;&#25193;&#23637;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
ExIFFI and EIF+: Interpretability and Enhanced Generalizability to Extend the Extended Isolation Forest. (arXiv:2310.05468v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;EIF+&#21644;ExIFFI&#20004;&#31181;&#25913;&#36827;&#20102;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#25512;&#24191;&#33021;&#21147;&#21644;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#28041;&#21450;&#22312;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#31995;&#32479;&#20013;&#35782;&#21035;&#24322;&#24120;&#34892;&#20026;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20165;&#20165;&#23450;&#20301;&#24322;&#24120;&#24448;&#24448;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#36275;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#20102;&#35299;&#39044;&#27979;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#20197;&#20415;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24182;&#22686;&#24378;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26080;&#30417;&#30563;&#24615;&#36136;&#65292;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#24037;&#20855;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EIF+&#65292;&#36825;&#26159;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#65288;EIF&#65289;&#30340;&#22686;&#24378;&#21464;&#20307;&#65292;&#26088;&#22312;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExIFFI&#65292;&#19968;&#31181;&#23558;&#25193;&#23637;&#23396;&#31435;&#26862;&#26519;&#19982;&#35299;&#37322;&#24615;&#21151;&#33021;&#65288;&#29305;&#24449;&#25490;&#21517;&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#25552;&#20379;&#20102;&#20197;&#23396;&#31435;&#22522;&#20110;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32508;&#21512;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection, an essential unsupervised machine learning task, involves identifying unusual behaviors within complex datasets and systems. While Machine Learning algorithms and decision support systems (DSSs) offer effective solutions for this task, simply pinpointing anomalies often falls short in real-world applications. Users of these systems often require insight into the underlying reasons behind predictions to facilitate Root Cause Analysis and foster trust in the model. However, due to the unsupervised nature of anomaly detection, creating interpretable tools is challenging. This work introduces EIF+, an enhanced variant of Extended Isolation Forest (EIF), designed to enhance generalization capabilities. Additionally, we present ExIFFI, a novel approach that equips Extended Isolation Forest with interpretability features, specifically feature rankings. Experimental results provide a comprehensive comparative analysis of Isolation-based approaches for Anomaly Detection, incl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;</title><link>http://arxiv.org/abs/2310.02563</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#21512;&#20316;&#20215;&#20540;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Practical, Private Assurance of the Value of Collaboration. (arXiv:2310.02563v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02563
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20004;&#26041;&#22312;&#25968;&#25454;&#38598;&#19978;&#21512;&#20316;&#21069;&#22914;&#20309;&#20445;&#35777;&#21512;&#20316;&#30340;&#20215;&#20540;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#30340;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#12289;&#31169;&#23494;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#32456;&#30340;&#32467;&#26524;&#26159;&#30830;&#20445;&#21512;&#20316;&#21069;&#21452;&#26041;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#20250;&#34987;&#36879;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#20010;&#26041;&#21521;&#24076;&#26395;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#24444;&#27492;&#36879;&#38706;&#25968;&#25454;&#38598;&#20043;&#21069;&#65292;&#21452;&#26041;&#24076;&#26395;&#33021;&#22815;&#24471;&#21040;&#21512;&#20316;&#23558;&#26159;&#23500;&#26377;&#25104;&#26524;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#26041;&#34987;&#25215;&#35834;&#36890;&#36807;&#21512;&#24182;&#26469;&#33258;&#21478;&#19968;&#26041;&#30340;&#25968;&#25454;&#26469;&#25913;&#36827;&#20854;&#39044;&#27979;&#27169;&#22411;&#12290;&#21482;&#26377;&#24403;&#26356;&#26032;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#26102;&#65292;&#21452;&#26041;&#25165;&#24076;&#26395;&#36827;&#19968;&#27493;&#21512;&#20316;&#12290;&#22312;&#30830;&#23450;&#36825;&#19968;&#28857;&#20043;&#21069;&#65292;&#21452;&#26041;&#19981;&#24076;&#26395;&#36879;&#38706;&#20182;&#20204;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;Torus&#19978;&#30340;&#20840;&#21516;&#24577;&#21152;&#23494;&#26041;&#26696;&#65288;TFHE&#65289;&#21644;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#21327;&#35758;&#65292;&#20854;&#20013;&#24213;&#23618;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#12290;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#29992;&#20110;&#30830;&#20445;&#35745;&#31639;&#19981;&#23436;&#20840;&#22312;&#21152;&#23494;&#39046;&#22495;&#36827;&#34892;&#65292;&#36825;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two parties wish to collaborate on their datasets. However, before they reveal their datasets to each other, the parties want to have the guarantee that the collaboration would be fruitful. We look at this problem from the point of view of machine learning, where one party is promised an improvement on its prediction model by incorporating data from the other party. The parties would only wish to collaborate further if the updated model shows an improvement in accuracy. Before this is ascertained, the two parties would not want to disclose their models and datasets. In this work, we construct an interactive protocol for this problem based on the fully homomorphic encryption scheme over the Torus (TFHE) and label differential privacy, where the underlying machine learning model is a neural network. Label differential privacy is used to ensure that computations are not done entirely in the encrypted domain, which is a significant bottleneck for neural network training according to the cu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#31639;&#27861;&#65288;PNLSVI&#65289;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#21019;&#26032;&#30340;&#26041;&#24046;&#21152;&#26435;&#22238;&#24402;&#26041;&#26696;&#12289;&#26041;&#24046;&#20272;&#35745;&#23376;&#31243;&#24207;&#21644;&#24754;&#35266;&#20540;&#36845;&#20195;&#26041;&#27861;&#30340;&#35268;&#21010;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2310.01380</link><description>&lt;p&gt;
&#24754;&#35266;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#31639;&#27861;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning. (arXiv:2310.01380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#31639;&#27861;&#65288;PNLSVI&#65289;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#21019;&#26032;&#30340;&#26041;&#24046;&#21152;&#26435;&#22238;&#24402;&#26041;&#26696;&#12289;&#26041;&#24046;&#20272;&#35745;&#23376;&#31243;&#24207;&#21644;&#24754;&#35266;&#20540;&#36845;&#20195;&#26041;&#27861;&#30340;&#35268;&#21010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;Offline RL&#65289;&#26159;&#25351;&#26234;&#33021;&#20307;&#26681;&#25454;&#30001;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#25968;&#25454;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#22312;&#19968;&#23450;&#20551;&#35774;&#19979;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#65292;&#20294;&#24456;&#22810;&#30740;&#31350;&#23558;&#20852;&#36259;&#36716;&#21521;&#20102;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20855;&#26377;&#23454;&#20363;&#20381;&#36182;&#21518;&#24724;&#20445;&#35777;&#30340;&#30740;&#31350;&#24037;&#20316;&#21364;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24754;&#35266;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20540;&#36845;&#20195;&#65288;PNLSVI&#65289;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#21253;&#25324;&#19977;&#20010;&#21019;&#26032;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#21152;&#26435;&#22238;&#24402;&#30340;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#20989;&#25968;&#31867;&#65307;&#65288;2&#65289;&#19968;&#31181;&#26041;&#24046;&#20272;&#35745;&#23376;&#31243;&#24207;&#65307;&#21644;&#65288;3&#65289;&#19968;&#20010;&#21033;&#29992;&#24754;&#35266;&#20540;&#36845;&#20195;&#26041;&#27861;&#30340;&#35268;&#21010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline RL with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline RL with non-linear function approximation. However, limited works on offline RL with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. O
&lt;/p&gt;</description></item><item><title>SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2309.12218</link><description>&lt;p&gt;
SR-PredictAO: &#20855;&#26377;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#38468;&#21152;&#20214;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO: Session-based Recommendation with High-Capability Predictor Add-On. (arXiv:2309.12218v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12218
&lt;/p&gt;
&lt;p&gt;
SR-PredictAO&#26159;&#19968;&#31181;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#23384;&#22312;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#20165;&#22522;&#20110;&#21333;&#20010;&#20250;&#35805;&#20013;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#19979;&#19968;&#20010;&#39033;&#30446;&#28857;&#20987;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#26576;&#20123;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#38656;&#35201;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21160;&#20316;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#65288;&#22914;&#26524;&#19981;&#26159;&#20840;&#37096;&#65289;&#29616;&#26377;&#27169;&#22411;&#36981;&#24490;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#33539;&#24335;&#65292;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#25152;&#26377;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#24191;&#27867;&#20248;&#21270;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#22914;&#20309;&#20248;&#21270;&#39044;&#27979;&#22120;&#27169;&#22359;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#20302;&#33021;&#21147;&#39044;&#27979;&#22120;&#27169;&#22359;&#23384;&#22312;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#31216;&#20026;\emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#33021;&#21147;&#30340;&#39044;&#27979;&#22120;&#27169;&#22359;&#65292;&#21487;&#20197;&#20943;&#36731;&#38543;&#26426;&#29992;&#25143;&#34892;&#20026;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Session-based recommendation, aiming at making the prediction of the user's next item click based on the information in a single session only even in the presence of some random user's behavior, is a complex problem. This complex problem requires a high-capability model of predicting the user's next action. Most (if not all) existing models follow the encoder-predictor paradigm where all studies focus on how to optimize the encoder module extensively in the paradigm but they ignore how to optimize the predictor module. In this paper, we discover the existing critical issue of the low-capability predictor module among existing models. Motivated by this, we propose a novel framework called \emph{\underline{S}ession-based \underline{R}ecommendation with \underline{Pred}ictor \underline{A}dd-\underline{O}n} (SR-PredictAO). In this framework, we propose a high-capability predictor module which could alleviate the effect of random user's behavior for prediction. It is worth mentioning that t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#24182;&#21463;&#38750;&#36127;&#24615;&#32422;&#26463;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22534;&#21472;&#20272;&#35745;&#22120;&#30456;&#27604;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#23567;&#30340;&#24635;&#20307;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.09880</link><description>&lt;p&gt;
&#30001;&#22534;&#21472;&#22238;&#24402;&#20943;&#23569;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Error Reduction from Stacked Regressions. (arXiv:2309.09880v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22534;&#21472;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24635;&#20307;&#39118;&#38505;&#24182;&#21463;&#38750;&#36127;&#24615;&#32422;&#26463;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22534;&#21472;&#20272;&#35745;&#22120;&#30456;&#27604;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#20855;&#26377;&#26356;&#23567;&#30340;&#24635;&#20307;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22534;&#21472;&#22238;&#24402;&#26159;&#19968;&#31181;&#38598;&#25104;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#24418;&#25104;&#19981;&#21516;&#22238;&#24402;&#20272;&#35745;&#22120;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#25968;&#25454;&#26469;&#29983;&#25104;&#30001;&#26500;&#25104;&#20272;&#35745;&#22120;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#24102;&#38750;&#36127;&#24615;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#27861;&#23398;&#20064;&#32452;&#21512;&#26435;&#37325;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31867;&#20284;&#22320;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#31181;&#20272;&#35745;&#30340;&#24635;&#20307;&#39118;&#38505;&#26469;&#23398;&#20064;&#36825;&#20123;&#26435;&#37325;&#65292;&#24182;&#21463;&#21040;&#38750;&#36127;&#24615;&#32422;&#26463;&#12290;&#24403;&#26500;&#25104;&#30340;&#20272;&#35745;&#22120;&#26159;&#36890;&#36807;&#33267;&#23569;&#19977;&#20010;&#32500;&#24230;&#20998;&#38548;&#30340;&#23884;&#22871;&#23376;&#31354;&#38388;&#30340;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#25237;&#24433;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#25910;&#32553;&#25928;&#24212;&#65292;&#25152;&#24471;&#21040;&#30340;&#22534;&#21472;&#20272;&#35745;&#22120;&#30340;&#24635;&#20307;&#39118;&#38505;&#20005;&#26684;&#23567;&#20110;&#20854;&#20013;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#12290;&#36825;&#37324;&#30340;&#8220;&#26368;&#20339;&#8221;&#26159;&#25351;&#26368;&#23567;&#21270;&#36873;&#25321;&#20934;&#21017;&#22914;AIC&#25110;BIC&#30340;&#27169;&#22411;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#30340;&#21333;&#20010;&#20272;&#35745;&#22120;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#22240;&#20026;&#20248;&#21270;&#38382;&#39064;&#21487;&#20197;&#37325;&#26500;&#20026;&#21516;&#20449;&#24687;&#22238;&#24402;&#65292;&#25152;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Stacking regressions is an ensemble technique that forms linear combinations of different regression estimators to enhance predictive accuracy. The conventional approach uses cross-validation data to generate predictions from the constituent estimators, and least-squares with nonnegativity constraints to learn the combination weights. In this paper, we learn these weights analogously by minimizing an estimate of the population risk subject to a nonnegativity constraint. When the constituent estimators are linear least-squares projections onto nested subspaces separated by at least three dimensions, we show that thanks to a shrinkage effect, the resulting stacked estimator has strictly smaller population risk than best single estimator among them. Here ``best'' refers to a model that minimizes a selection criterion such as AIC or BIC. In other words, in this setting, the best single estimator is inadmissible. Because the optimization problem can be reformulated as isotonic regression, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.02293</link><description>&lt;p&gt;
&#29992;&#27491;&#21017;&#21270;&#39640;&#38454;&#24635;&#21464;&#24046;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21253;&#25324;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#39640;&#24230;&#34920;&#36798;&#30340;&#21442;&#25968;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#22797;&#26434;&#27010;&#24565;&#65292;&#20294;&#35757;&#32451;&#36825;&#31181;&#39640;&#24230;&#38750;&#32447;&#24615;&#27169;&#22411;&#24050;&#30693;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;k&#38454;&#24635;&#21464;&#24046;&#65288;k-TV&#65289;&#27491;&#21017;&#21270;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;k&#38454;&#23548;&#25968;&#30340;&#24179;&#26041;&#31215;&#20998;&#65292;&#36890;&#36807;&#24809;&#32602;k-TV&#26469;&#20135;&#29983;&#19968;&#20010;&#26356;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;&#23613;&#31649;&#23558;k-TV&#39033;&#24212;&#29992;&#20110;&#19968;&#33324;&#30340;&#21442;&#25968;&#27169;&#22411;&#30001;&#20110;&#31215;&#20998;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#24102;&#26377;k-TV&#27491;&#21017;&#21270;&#30340;&#19968;&#33324;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26174;&#24335;&#30340;&#25968;&#20540;&#31215;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#32467;&#26500;&#20219;&#24847;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#36827;&#34892;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#20248;&#21270;&#21363;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While highly expressive parametric models including deep neural networks have an advantage to model complicated concepts, training such highly non-linear models is known to yield a high risk of notorious overfitting. To address this issue, this study considers a $k$th order total variation ($k$-TV) regularization, which is defined as the squared integral of the $k$th order derivative of the parametric models to be trained; penalizing the $k$-TV is expected to yield a smoother function, which is expected to avoid overfitting. While the $k$-TV terms applied to general parametric models are computationally intractable due to the integration, this study provides a stochastic optimization algorithm, that can efficiently train general models with the $k$-TV regularization without conducting explicit numerical integration. The proposed approach can be applied to the training of even deep neural networks whose structure is arbitrary, as it can be implemented by only a simple stochastic gradien
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.13642</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#23433;&#20840;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Safety Margins for Reinforcement Learning. (arXiv:2307.13642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#35745;&#31639;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#26469;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#33021;&#30340;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#22312;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#20309;&#33258;&#20027;&#25511;&#21046;&#22120;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#37117;&#21487;&#33021;&#19981;&#23433;&#20840;&#12290;&#33021;&#22815;&#23450;&#37327;&#22320;&#30830;&#23450;&#20309;&#26102;&#20250;&#21457;&#29983;&#36825;&#20123;&#19981;&#23433;&#20840;&#24773;&#20917;&#23545;&#20110;&#21450;&#26102;&#24341;&#20837;&#20154;&#31867;&#30417;&#30563;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#36135;&#36816;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#30340;&#30495;&#27491;&#20851;&#38190;&#24615;&#21487;&#20197;&#34987;&#31283;&#20581;&#22320;&#23450;&#20041;&#20026;&#22312;&#19968;&#20123;&#38543;&#26426;&#21160;&#20316;&#19979;&#22870;&#21169;&#30340;&#24179;&#22343;&#20943;&#23569;&#12290;&#21487;&#20197;&#23558;&#23454;&#26102;&#21487;&#35745;&#31639;&#30340;&#20195;&#29702;&#20851;&#38190;&#24615;&#25351;&#26631;&#65288;&#21363;&#65292;&#26080;&#38656;&#23454;&#38469;&#27169;&#25311;&#38543;&#26426;&#21160;&#20316;&#30340;&#24433;&#21709;&#65289;&#19982;&#30495;&#27491;&#30340;&#20851;&#38190;&#24615;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#25351;&#26631;&#29983;&#25104;&#23433;&#20840;&#36793;&#30028;&#65292;&#23558;&#28508;&#22312;&#38169;&#35823;&#34892;&#20026;&#30340;&#21518;&#26524;&#30452;&#25509;&#19982;&#25972;&#20307;&#24615;&#33021;&#30340;&#39044;&#26399;&#25439;&#22833;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;Atari&#29615;&#22659;&#20013;&#36890;&#36807;APE-X&#21644;A3C&#30340;&#23398;&#20064;&#31574;&#30053;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#38543;&#30528;&#20195;&#29702;&#25509;&#36817;&#22833;&#36133;&#29366;&#24577;&#65292;&#23433;&#20840;&#36793;&#30028;&#30340;&#20943;&#23567;&#12290;&#23558;&#23433;&#20840;&#36793;&#30028;&#25972;&#21512;&#21040;&#30417;&#25511;&#31243;&#24207;&#20013;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#36130;&#21153;&#22256;&#22659;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.03908</link><description>&lt;p&gt;
&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incorporating Deep Q -- Network with Multiclass Classification Algorithms. (arXiv:2307.03908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;Deep Q-Network&#19982;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#39044;&#27979;&#36130;&#21153;&#22256;&#22659;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;Deep Q-Network (DQN)&#26469;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30340;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21019;&#24314;&#19968;&#20010;&#23558;DQN&#19982;&#29616;&#26377;&#26377;&#30417;&#30563;&#22810;&#31867;&#20998;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#30340;&#21457;&#29616;&#23558;&#24102;&#26469;&#23545;&#22914;&#20309;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#25552;&#39640;&#22810;&#31867;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#31574;&#30053;&#24050;&#32463;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#36824;&#20391;&#37325;&#20110;&#39044;&#27979;&#20844;&#21496;&#30340;&#36130;&#21153;&#22256;&#22659;&#20197;&#21450;Deep Q-Network&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#35782;&#21035;&#21487;&#33021;&#36935;&#21040;&#36130;&#21153;&#22256;&#22659;&#30340;&#20225;&#19994;&#26159;&#37329;&#34701;&#21644;&#39118;&#38505;&#31649;&#29702;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24403;&#20225;&#19994;&#38754;&#20020;&#20005;&#37325;&#25361;&#25112;&#26102;&#65292;&#26080;&#27861;&#32500;&#25345;&#36816;&#33829;&#24182;&#23653;&#34892;&#36130;&#21153;&#36131;&#20219;&#65292;&#23601;&#34987;&#35748;&#20026;&#22788;&#20110;&#36130;&#21153;&#22256;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore how Deep Q-Network (DQN) might improve the functionality of multiclass classification algorithms. We will use a benchmark dataset from Kaggle to create a framework incorporating DQN with existing supervised multiclass classification algorithms. The findings of this study will bring insight into how deep reinforcement learning strategies may be used to increase multiclass classification accuracy. They have been used in a number of fields, including image recognition, natural language processing, and bioinformatics. This study is focused on the prediction of financial distress in companies in addition to the wider application of Deep Q-Network in multiclass classification. Identifying businesses that are likely to experience financial distress is a crucial task in the fields of finance and risk management. Whenever a business experiences serious challenges keeping its operations going and meeting its financial responsibilities, it is said to be in financial dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02285</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#28151;&#28102;&#33410;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Clarify Confused Nodes Through Separated Learning. (arXiv:2306.02285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#23548;&#21521;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#36136;&#33410;&#28857;&#65292;&#36825;&#25361;&#25112;&#20102;&#32463;&#20856;GNN&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#24182;&#38459;&#30861;&#20102;&#20854;&#24615;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#25968;&#20173;&#35774;&#35745;&#20102;&#20855;&#26377;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#38388;&#20849;&#20139;&#26435;&#37325;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#21162;&#21147;&#20013;&#21253;&#21547;&#20102;&#39640;&#38454;&#20449;&#24687;&#21644;&#22810;&#36890;&#36947;&#26550;&#26500;&#65292;&#20294;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#35757;&#32451;&#19981;&#21516;&#33410;&#28857;&#32452;&#30340;&#20998;&#31163;&#23398;&#20064;&#65292;&#20294;&#21463;&#21040;&#20102;&#19981;&#21512;&#36866;&#30340;&#20998;&#31163;&#24230;&#37327;&#21644;&#20302;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#37051;&#22495;&#28151;&#28102;&#65288;NC&#65289;&#65292;&#20197;&#20415;&#26356;&#21487;&#38752;&#22320;&#20998;&#31163;&#33410;&#28857;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20855;&#26377;&#19981;&#21516;NC&#20540;&#30340;&#33410;&#28857;&#32452;&#22312;&#32452;&#20869;&#20934;&#30830;&#24230;&#21644;&#21487;&#35270;&#21270;&#23884;&#20837;&#19978;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#12290;&#36825;&#20026;&#22522;&#20110;&#37051;&#22495;&#28151;&#28102;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;NC-GCN&#65289;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable advances in graph-oriented tasks. However, real-world graphs invariably contain a certain proportion of heterophilous nodes, challenging the homophily assumption of classical GNNs and hindering their performance. Most existing studies continue to design generic models with shared weights between heterophilous and homophilous nodes. Despite the incorporation of high-order messages or multi-channel architectures, these efforts often fall short. A minority of studies attempt to train different node groups separately but suffer from inappropriate separation metrics and low efficiency. In this paper, we first propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more reliable separation of nodes. We observe that node groups with different levels of NC values exhibit certain differences in intra-group accuracy and visualized embeddings. These pave the way for Neighborhood Confusion-guided Graph Convolutional Network (N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#23433;&#20840;&#22686;&#24378;&#65292;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#22312;&#32447;&#21487;&#34892;&#24182;&#20855;&#26377;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09575</link><description>&lt;p&gt;
&#22522;&#20110;&#23433;&#20840;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#36817;&#20284;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Approximate non-linear model predictive control with safety-augmented neural networks. (arXiv:2304.09575v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;(NNs)&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#31216;&#20026;&#23433;&#20840;&#22686;&#24378;&#65292;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#22312;&#32447;&#21487;&#34892;&#24182;&#20855;&#26377;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#28385;&#36275;&#65292;&#20294;&#38656;&#35201;&#36827;&#34892;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#30340;&#22312;&#32447;&#20248;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;(NNs)&#23545;&#36825;&#31181;MPC&#25511;&#21046;&#22120;&#30340;&#36817;&#20284;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#22312;&#32447;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#22686;&#24378;&#65292;&#23613;&#31649;&#23384;&#22312;&#36817;&#20284;&#19981;&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#20197;&#33719;&#24471;&#25910;&#25947;&#21644;&#32422;&#26463;&#26465;&#20214;&#30340;&#30830;&#23450;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;NN&#36817;&#20284;MPC&#30340;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#22312;&#32447;&#39564;&#35777;&#23427;&#26159;&#21542;&#26159;MPC&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#12290;&#24403;&#35813;&#35299;&#20915;&#26041;&#26696;&#19981;&#21487;&#34892;&#25110;&#25104;&#26412;&#26356;&#39640;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#26631;&#20934;MPC&#25216;&#26415;&#23558;NN&#35299;&#20915;&#26041;&#26696;&#26367;&#25442;&#20026;&#23433;&#20840;&#20505;&#36873;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#38656;&#35201;&#23545;NN&#36827;&#34892;&#19968;&#27425;&#35780;&#20272;&#21644;&#23545;&#36755;&#20837;&#24207;&#21015;&#36827;&#34892;&#22312;&#32447;&#21069;&#21521;&#31215;&#20998;&#65292;&#36825;&#22312;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19978;&#30340;&#35745;&#31639;&#36895;&#24230;&#24456;&#24555;&#12290;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26694;&#26550;&#22312;&#19977;&#20010;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#38750;&#32447;&#24615;MPC&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#23637;&#31034;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model predictive control (MPC) achieves stability and constraint satisfaction for general nonlinear systems, but requires computationally expensive online optimization. This paper studies approximations of such MPC controllers via neural networks (NNs) to achieve fast online evaluation. We propose safety augmentation that yields deterministic guarantees for convergence and constraint satisfaction despite approximation inaccuracies. We approximate the entire input sequence of the MPC with NNs, which allows us to verify online if it is a feasible solution to the MPC problem. We replace the NN solution by a safe candidate based on standard MPC techniques whenever it is infeasible or has worse cost. Our method requires a single evaluation of the NN and forward integration of the input sequence online, which is fast to compute on resource-constrained systems. The proposed control framework is illustrated on three non-linear MPC benchmarks of different complexity, demonstrating computational
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;</title><link>http://arxiv.org/abs/2303.08250</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65292;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Grow Artificial Hippocampi in Vision Transformers for Resilient Lifelong Learning. (arXiv:2303.08250v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#36827;&#34892;&#32500;&#25252;&#65292;&#36873;&#21462;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#36827;&#34892;ArtiHippo&#30340;&#23454;&#29616;&#21644;&#25104;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#38656;&#35201;&#25317;&#26377;&#20154;&#31867;&#26234;&#33021;&#30340;&#38887;&#24615;&#65292;&#21363;&#19981;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#31181;&#38887;&#24615;&#19982;&#22823;&#33041;&#20013;&#22797;&#26434;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#28023;&#39532;&#32500;&#25252;&#30340;&#38271;&#26399;&#35760;&#24518;&#65288;LM&#65289;&#32039;&#23494;&#30456;&#20851;&#12290;Transformer&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#8220;&#22823;&#33041;&#8221;&#30340;&#23545;&#24212;&#20307;&#65292;&#20294;LM&#32452;&#20214;&#22312;&#32456;&#36523;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Vision Transformer&#20013;&#23398;&#20064;&#25104;&#38271;&#20154;&#24037;&#28023;&#39532;&#65288;ArtiHippo&#65289;&#20197;&#23454;&#29616;&#24377;&#24615;&#32456;&#36523;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20840;&#38754;&#28040;&#34701;&#23454;&#39564;&#65292;&#36873;&#23450;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#22359;&#20013;&#30340;&#26368;&#32456;&#32447;&#24615;&#25237;&#24433;&#23618;&#26469;&#23454;&#29616;&#21644;&#25104;&#38271;ArtiHippo&#12290;ArtiHippo&#30001;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#34920;&#31034;&#12290;&#27599;&#20010;&#19987;&#23478;&#32452;&#20214;&#26159;&#32447;&#24615;&#25237;&#24433;&#23618;&#30340;&#29616;&#22330;&#21464;&#20307;&#65292;&#36890;&#36807;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#36827;&#34892;&#32500;&#25252;&#65292;&#25628;&#32034;&#31354;&#38388;&#30001;&#22235;&#20010;&#22522;&#26412;&#25104;&#38271;&#25805;&#20316;&#65288;&#36339;&#36807;&#12289;&#37325;&#29992;&#12289;&#36866;&#24212;&#21644;&#26032;&#65289;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning without catastrophic forgetting (i.e., resiliency) possessed by human intelligence is entangled with sophisticated memory mechanisms in the brain, especially the long-term memory (LM) maintained by Hippocampi. To a certain extent, Transformers have emerged as the counterpart ``Brain" of Artificial Intelligence (AI), and yet leave the LM component under-explored for lifelong learning settings. This paper presents a method of learning to grow Artificial Hippocampi (ArtiHippo) in Vision Transformers (ViTs) for resilient lifelong learning. With a comprehensive ablation study, the final linear projection layer in the multi-head self-attention (MHSA) block is selected in realizing and growing ArtiHippo. ArtiHippo is represented by a mixture of experts (MoEs). Each expert component is an on-site variant of the linear projection layer, maintained via neural architecture search (NAS) with the search space defined by four basic growing operations -- skip, reuse, adapt, and new 
&lt;/p&gt;</description></item><item><title>&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.11007</link><description>&lt;p&gt;
&#27969;&#34892;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unification of popular artificial neural network activation functions. (arXiv:2302.11007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11007
&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#21270;&#34920;&#31034;&#37319;&#29992;&#20102;Mittag-Leffler&#20989;&#25968;&#65292;&#21487;&#20197;&#25554;&#20540;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#12289;&#20943;&#36731;&#26799;&#24230;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#27969;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#30340;&#32479;&#19968;&#34920;&#31034;&#12290;&#37319;&#29992;&#20102;&#20998;&#25968;&#24494;&#31215;&#20998;&#30340;Mittag-Leffler&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#32039;&#20945;&#30340;&#21151;&#33021;&#24418;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#24182;&#20943;&#36731;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#22914;&#26799;&#24230;&#28040;&#22833;&#21644;&#26799;&#24230;&#29190;&#28856;&#12290;&#25152;&#25552;&#20986;&#30340;&#38376;&#25511;&#34920;&#31034;&#25193;&#23637;&#20102;&#22266;&#23450;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#33258;&#36866;&#24212;&#23545;&#24212;&#29289;&#65292;&#20854;&#24418;&#29366;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23548;&#25968;&#20063;&#21487;&#20197;&#29992;Mittag-Leffler&#20989;&#25968;&#34920;&#31034;&#65292;&#22240;&#27492;&#23427;&#26159;&#26799;&#24230;&#19979;&#38477;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#30340;&#21512;&#36866;&#20505;&#36873;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22797;&#26434;&#24230;&#21644;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#32479;&#19968;&#30340;&#38376;&#25511;&#28608;&#27963;&#20989;&#25968;&#34920;&#31034;&#20026;&#21508;&#31181;&#20869;&#32622;&#23454;&#29616;&#30340;&#32463;&#27982;&#30340;&#21644;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a unified representation of the most popular neural network activation functions. Adopting Mittag-Leffler functions of fractional calculus, we propose a flexible and compact functional form that is able to interpolate between various activation functions and mitigate common problems in training neural networks such as vanishing and exploding gradients. The presented gated representation extends the scope of fixed-shape activation functions to their adaptive counterparts whose shape can be learnt from the training data. The derivatives of the proposed functional form can also be expressed in terms of Mittag-Leffler functions making it a suitable candidate for gradient-based backpropagation algorithms. By training multiple neural networks of different complexities on various datasets with different sizes, we demonstrate that adopting a unified gated representation of activation functions offers a promising and affordable alternative to individual built-in implementations of ac
&lt;/p&gt;</description></item></channel></rss>