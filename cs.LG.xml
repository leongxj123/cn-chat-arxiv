<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01092</link><description>&lt;p&gt;
&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Dynamical Model of Neural Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01092
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#21147;&#23398;&#27169;&#22411;&#26469;&#35299;&#37322;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#36890;&#36807;&#20998;&#26512;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#30740;&#31350;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#65292;&#32780;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#35201;&#27714;&#22686;&#21152;&#35757;&#32451;&#27493;&#25968;&#24555;&#20110;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#65292;&#19982;&#23454;&#35777;&#35266;&#23519;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#39044;&#27979;&#24615;&#22320;&#25552;&#39640;&#65292;&#36328;&#22810;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#23450;&#24459;&#65292;&#23427;&#25253;&#21578;&#20102;&#22312;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#22823;&#23567;&#26102;&#24615;&#33021;&#19982;&#35745;&#31639;&#25968;&#37327;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#20316;&#20026;&#32593;&#32476;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#21487;&#35299;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#22797;&#29616;&#20102;&#20851;&#20110;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#30340;&#35768;&#22810;&#35266;&#23519;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#20026;&#20160;&#20040;&#35757;&#32451;&#26102;&#38388;&#21644;&#27169;&#22411;&#22823;&#23567;&#30340;&#32553;&#25918;&#20855;&#26377;&#19981;&#21516;&#30340;&#24130;&#24459;&#25351;&#25968;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#39044;&#27979;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#30340;&#35745;&#31639;&#26368;&#20248;&#32553;&#25918;&#35268;&#21017;&#65292;&#20854;&#20013;&#35757;&#32451;&#27493;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#24555;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#22686;&#21152;&#36895;&#24230;&#65292;&#19982;&#26368;&#36817;&#30340;&#23454;&#35777;&#35266;&#23519;&#19968;&#33268;&#12290;&#20854;&#27425;&#65292;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#65292;&#32593;&#32476;&#20250;&#25910;&#25947;&#21040;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
On a variety of tasks, the performance of neural networks predictably improves with training time, dataset size and model size across many orders of magnitude. This phenomenon is known as a neural scaling law. Of fundamental importance is the compute-optimal scaling law, which reports the performance as a function of units of compute when choosing model sizes optimally. We analyze a random feature model trained with gradient descent as a solvable model of network training and generalization. This reproduces many observations about neural scaling laws. First, our model makes a prediction about why the scaling of performance with training time and with model size have different power law exponents. Consequently, the theory predicts an asymmetric compute-optimal scaling rule where the number of training steps are increased faster than model parameters, consistent with recent empirical observations. Second, it has been observed that early in training, networks converge to their infinite-wi
&lt;/p&gt;</description></item><item><title>&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.02261</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#24490;&#29615;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LLMs in the Loop: Leveraging Large Language Model Annotations for Active Learning in Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02261
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#36890;&#36807;&#23558;LLMs&#38598;&#25104;&#21040;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#65292;&#26377;&#25928;&#20943;&#23569;&#25152;&#38656;&#25968;&#25454;&#37327;&#65292;&#24182;&#21462;&#24471;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#36164;&#28304;&#21644;&#25968;&#25454;&#26631;&#27880;&#19987;&#19994;&#30693;&#35782;&#26377;&#38480;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#32597;&#35265;&#19988;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#33410;&#20013;&#36827;&#34892;&#25968;&#25454;&#27880;&#37322;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#35780;&#20272;&#20197;&#35780;&#20272;&#27880;&#37322;&#32773;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#36873;&#25321;&#36866;&#24403;&#30340;LLM&#27880;&#37322;&#32773;&#12290;&#28982;&#21518;&#65292;&#36873;&#25321;&#30340;&#27880;&#37322;&#32773;&#34987;&#38598;&#25104;&#21040;&#19968;&#20010;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#24490;&#29615;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;&#26597;&#35810;&#25968;&#25454;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;GPT-4-Turbo&#65292;&#23637;&#31034;&#20102;&#20960;&#20046;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#25968;&#25454;&#38656;&#27714;&#65292;&#30001;&#20272;&#31639;&#30340;&#28508;&#22312;&#24615;&#33021;&#25351;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02261v1 Announce Type: cross  Abstract: Low-resource languages face significant barriers in AI development due to limited linguistic resources and expertise for data labeling, rendering them rare and costly. The scarcity of data and the absence of preexisting tools exacerbate these challenges, especially since these languages may not be adequately represented in various NLP datasets. To address this gap, we propose leveraging the potential of LLMs in the active learning loop for data annotation. Initially, we conduct evaluations to assess inter-annotator agreement and consistency, facilitating the selection of a suitable LLM annotator. The chosen annotator is then integrated into a training loop for a classifier using an active learning paradigm, minimizing the amount of queried data required. Empirical evaluations, notably employing GPT-4-Turbo, demonstrate near-state-of-the-art performance with significantly reduced data requirements, as indicated by estimated potential co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;</title><link>https://arxiv.org/abs/2404.02072</link><description>&lt;p&gt;
&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
EGTR: Extracting Graph from Transformer for Scene Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#20013;&#25552;&#21462;&#22270;&#24418;&#20197;&#29992;&#20110;&#22330;&#26223;&#22270;&#29983;&#25104;&#30340;&#36731;&#37327;&#32423;&#21333;&#38454;&#27573;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#21462;&#20102;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#22270;&#29983;&#25104;&#65288;SGG&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26816;&#27979;&#23545;&#35937;&#24182;&#39044;&#27979;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#21333;&#38454;&#27573;SGG&#27169;&#22411;&#65292;&#23427;&#20174;DETR&#35299;&#30721;&#22120;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#23398;&#20064;&#30340;&#21508;&#31181;&#20851;&#31995;&#20013;&#25552;&#21462;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02072v1 Announce Type: cross  Abstract: Scene Graph Generation (SGG) is a challenging task of detecting objects and predicting relationships between objects. After DETR was developed, one-stage SGG models based on a one-stage object detector have been actively studied. However, complex modeling is used to predict the relationship between objects, and the inherent relationship between object queries learned in the multi-head self-attention of the object detector has been neglected. We propose a lightweight one-stage SGG model that extracts the relation graph from the various relationships learned in the multi-head self-attention layers of the DETR decoder. By fully utilizing the self-attention by-products, the relation graph can be extracted effectively with a shallow relation extraction head. Considering the dependency of the relation extraction task on the object detection task, we propose a novel relation smoothing technique that adjusts the relation label adaptively accor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19181</link><description>&lt;p&gt;
&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
Make Large Language Model a Better Ranker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#26174;&#33879;&#22686;&#24378;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#25512;&#33616;&#31995;&#32479;&#65288;RSs&#65289;&#27010;&#24565;&#21644;&#24320;&#21457;&#26041;&#24335;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#28857;&#23545;&#28857;&#21644;&#25104;&#23545;&#25512;&#33616;&#33539;&#24335;&#19978;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#22120;&#20013;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#19968;&#20123;&#30740;&#31350;&#34429;&#28982;&#28145;&#20837;&#30740;&#31350;&#20102;&#21015;&#34920;&#22411;&#26041;&#27861;&#65292;&#20294;&#22312;&#25490;&#21517;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#19968;&#19981;&#36275;&#24402;&#22240;&#20110;&#25490;&#21517;&#21644;&#35821;&#35328;&#29983;&#25104;&#30446;&#26631;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20855;&#26377;&#23545;&#40784;&#21015;&#34920;&#25490;&#21517;&#30446;&#26631;&#30340;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;ALRO&#65289;&#12290;ALRO&#26088;&#22312;&#24357;&#21512;LLMs&#30340;&#33021;&#21147;&#19982;&#25512;&#33616;&#31995;&#32479;&#25490;&#21517;&#20219;&#21153;&#30340;&#24494;&#22937;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;ALRO&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#24341;&#20837;&#20102;&#36719;lambda&#20540;lo
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19181v1 Announce Type: cross  Abstract: The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda lo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.15250</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#32467;&#26524;&#22312;LLM&#20013;&#30340;&#20840;&#38754;&#37325;&#26032;&#35780;&#20272;&#65306;&#19968;&#31181;&#22810;&#26041;&#20301;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15250
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#35780;&#20272;&#22312;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#21069;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#32553;&#25918;&#12289;&#35757;&#32451;&#31867;&#22411;&#12289;&#26550;&#26500;&#31561;&#22240;&#32032;&#28145;&#21051;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#35780;&#20998;&#30340;&#24433;&#21709;&#31243;&#24230;&#21644;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35780;&#20272;&#23616;&#38480;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#32479;&#35745;&#35270;&#35282;&#26356;&#26377;&#25928;&#22320;&#28548;&#28165;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#24471;&#20998;&#30340;&#24433;&#21709;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#37325;&#26032;&#26816;&#26597;&#65292;&#38024;&#23545;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#32479;&#35745;&#26041;&#27861;&#35770;&#12290;&#20854;&#20013;&#21253;&#25324;ANOVA&#12289;Tukey HSD&#26816;&#39564;&#12289;GAMM&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14725</link><description>&lt;p&gt;
Jailbreaking&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking is Best Solved by Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14725
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19978;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#22686;&#22810;&#24341;&#21457;&#20102;&#22823;&#37327;&#38450;&#24481;&#24037;&#20316;&#65292;&#26088;&#22312;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#38450;&#24481;&#31649;&#36947;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23450;&#20041;&#20309;&#20026;&#19981;&#23433;&#20840;&#36755;&#20986;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#36755;&#20837;&#22788;&#29702;&#25110;&#24494;&#35843;&#31561;&#26041;&#27861;&#26469;&#25191;&#34892;&#35813;&#23450;&#20041;&#12290;&#25105;&#20204;&#20005;&#37325;&#24576;&#30097;&#29616;&#26377;&#30340;&#25191;&#34892;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#19981;&#23433;&#20840;&#36755;&#20986;&#23450;&#20041;--&#21253;&#21547;&#21333;&#35789;"purple"&#30340;&#36755;&#20986;&#20063;&#26080;&#27861;&#38450;&#24481;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#23545;&#20110;&#36825;&#26679;&#30340;&#23450;&#20041;&#26159;&#23436;&#20840;&#20581;&#22766;&#30340;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;&#22312;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#20013;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#23450;&#20041;&#65306;&#27809;&#26377;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#20219;&#20309;&#25191;&#34892;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#26377;&#20102;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#36755;&#20986;&#22788;&#29702;&#24050;&#32463;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14725v1 Announce Type: cross  Abstract: The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or fine-tuning. We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs--outputs that contain the word "purple". In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit 
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;Mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09598</link><description>&lt;p&gt;
&#28151;&#21512;Mixup&#29992;&#20110;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#28151;&#21512;Mixup&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#26377;&#26080;&#23614;&#34521;&#22768;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#19981;&#24179;&#34913;&#20998;&#31867;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#22312;&#29983;&#29289;&#22768;&#23398;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#21160;&#29289;&#22768;&#38899;&#32463;&#24120;&#21516;&#26102;&#20986;&#29616;&#65292;&#32780;&#26576;&#20123;&#22768;&#38899;&#27604;&#20854;&#20182;&#22768;&#38899;&#35201;&#23569;&#24471;&#22810;&#12290;&#26412;&#25991;&#38024;&#23545;&#20351;&#29992;&#21253;&#21547;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#22810;&#26631;&#31614;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;AnuraSet&#65292;&#19987;&#27880;&#20110;&#20998;&#31867;&#26080;&#23614;&#30446;&#29289;&#31181;&#22768;&#38899;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Mixture of Mixups (Mix2)&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#26041;&#27861;Mixup&#12289;Manifold Mixup&#21644;MultiMix&#30340;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21333;&#29420;&#20351;&#29992;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#32467;&#26524;&#65307;&#28982;&#32780;&#65292;&#24403;&#38543;&#26426;&#24212;&#29992;&#23427;&#20204;&#26102;&#65292;&#27599;&#27425;&#35757;&#32451;&#36845;&#20195;&#36873;&#21462;&#19968;&#20010;&#26041;&#27861;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#25552;&#21040;&#30340;&#25361;&#25112;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21457;&#29983;&#27425;&#25968;&#36739;&#23569;&#30340;&#31232;&#26377;&#31867;&#21035;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;Mix2&#22312;&#36328;&#21508;&#31181;&#31867;&#21035;&#21516;&#26102;&#20986;&#29616;&#27700;&#24179;&#19978;&#20063;&#33021;&#26377;&#25928;&#20998;&#31867;&#22768;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09598v1 Announce Type: cross  Abstract: Multi-label imbalanced classification poses a significant challenge in machine learning, particularly evident in bioacoustics where animal sounds often co-occur, and certain sounds are much less frequent than others. This paper focuses on the specific case of classifying anuran species sounds using the dataset AnuraSet, that contains both class imbalance and multi-label examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a framework that leverages mixing regularization methods Mixup, Manifold Mixup, and MultiMix. Experimental results show that these methods, individually, may lead to suboptimal results; however, when applied randomly, with one selected at each training iteration, they prove effective in addressing the mentioned challenges, particularly for rare classes with few occurrences. Further analysis reveals that Mix2 is also proficient in classifying sounds across various levels of class co-occurrences
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08477</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#25554;&#20540;&#19987;&#23478;&#37322;&#25918;&#20803;&#35843;&#25972;&#30340;&#21147;&#37327;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26234;&#24935;&#24314;&#35758;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#26159;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#35832;&#22914;&#20803;&#23398;&#20064;&#20043;&#31867;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#21033;&#30410;&#65292;&#20803;&#35843;&#25972;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38543;&#21518;&#20248;&#21270;&#38454;&#27573;&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#29616;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#20851;&#38190;&#22320;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#30340; Sparse MetA-Tuning&#65288;SMAT&#65289;&#26041;&#27861;&#65292;&#23427;&#32463;&#36807;&#35757;&#32451;&#20197;&#33258;&#21160;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#38548;&#31163;&#39044;&#35757;&#32451;&#21442;&#25968;&#23376;&#38598;&#20197;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;SMAT&#25104;&#21151;&#20811;&#26381;&#20102;OOD&#25935;&#24863;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#25215;&#35834;&#12290;&#25105;&#20204;&#22312;Meta-Dataset&#19982;&#39069;&#22806;&#30340;OO&#25361;&#25112;&#32452;&#21512;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07454</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#23616;&#37096;&#32447;&#24615;&#26144;&#23556;&#36827;&#34892;&#24555;&#36895;&#12289;&#20934;&#30830;&#21644;&#36731;&#37327;&#32423;&#30340;&#39034;&#24207;&#20223;&#30495;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast, accurate and lightweight sequential simulation-based inference using Gaussian locally linear mappings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07454
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#28151;&#21512;&#27010;&#29575;&#20998;&#24067;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;SBI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#38024;&#23545;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21487;&#20197;&#20351;&#29992;&#22810;&#27425;&#35843;&#29992;&#35745;&#31639;&#27169;&#25311;&#22120;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290; &#36825;&#20123;&#26041;&#27861;&#34987;&#32479;&#31216;&#20026;&#8220;&#22522;&#20110;&#20223;&#30495;&#30340;&#25512;&#26029;&#8221;&#65288;SBI&#65289;&#12290; &#26368;&#36817;&#30340;SBI&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25552;&#20379;&#36817;&#20284;&#20294;&#34920;&#36798;&#20016;&#23500;&#30340;&#26500;&#36896;&#65292;&#29992;&#20110;&#19981;&#21487;&#29992;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20339;&#25240;&#34935;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#20284;&#28982;&#20989;&#25968;&#21644;&#21518;&#39564;&#20998;&#24067;&#36817;&#20284;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#27010;&#29575;&#20998;&#24067;&#28151;&#21512;&#29289;&#12290; &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;NN&#30340;SBI&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#20934;&#30830;&#30340;&#21518;&#39564;&#25512;&#26029;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#35745;&#31639;&#21344;&#29992;&#37327;&#12290; &#25105;&#20204;&#22312;SBI&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22522;&#20934;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07454v1 Announce Type: cross  Abstract: Bayesian inference for complex models with an intractable likelihood can be tackled using algorithms performing many calls to computer simulators. These approaches are collectively known as "simulation-based inference" (SBI). Recent SBI methods have made use of neural networks (NN) to provide approximate, yet expressive constructs for the unavailable likelihood function and the posterior distribution. However, they do not generally achieve an optimal trade-off between accuracy and computational demand. In this work, we propose an alternative that provides both approximations to the likelihood and the posterior distribution, using structured mixtures of probability distributions. Our approach produces accurate posterior inference when compared to state-of-the-art NN-based SBI methods, while exhibiting a much smaller computational footprint. We illustrate our results on several benchmark models from the SBI literature.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07379</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#29305;&#24449;&#65306;&#38271;&#24230;&#12289;&#25296;&#28857;&#21644;&#27515;&#32993;&#21516;
&lt;/p&gt;
&lt;p&gt;
Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07379
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#31070;&#32463;&#32593;&#32476;&#21644;LLMs&#20013;&#20248;&#21270;&#36712;&#36857;&#30340;&#22797;&#26434;&#24615;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#21253;&#25324;&#26041;&#21521;&#25506;&#32034;&#21644;&#26041;&#21521;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#20998;&#26512;&#20854;&#20248;&#21270;&#36712;&#36857;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#21442;&#25968;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#20851;&#20110;&#20248;&#21270;&#36712;&#36857;&#22797;&#26434;&#24615;&#30340;&#33258;&#28982;&#27010;&#24565;&#65292;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#25581;&#31034;&#20102;&#21508;&#31181;&#20248;&#21270;&#36873;&#25321;&#65288;&#22914;&#21160;&#37327;&#12289;&#26435;&#37325;&#34928;&#20943;&#21644;&#25209;&#22823;&#23567;&#65289;&#20043;&#38388;&#25152;&#28041;&#21450;&#30340;&#20869;&#22312;&#24494;&#22937;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#27010;&#24565;&#26469;&#25552;&#20379;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26412;&#36136;&#30340;&#20851;&#38190;&#29305;&#24449;&#65306;&#20309;&#26102;&#39034;&#21033;&#36827;&#34892;&#65292;&#20309;&#26102;&#38519;&#20837;&#27515;&#32993;&#21516;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36712;&#36857;&#35270;&#35282;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21160;&#37327;&#21644;&#26435;&#37325;&#34928;&#20943;&#20043;&#38388;&#20419;&#36827;&#26041;&#21521;&#25506;&#32034;&#30340;&#20132;&#32455;&#34892;&#20026;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#34892;&#20026;&#30340;&#26041;&#21521;&#27491;&#21017;&#21270;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#35774;&#32622;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#20855;&#26377;&#26368;&#22810;120&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07379v1 Announce Type: cross  Abstract: We propose a fresh take on understanding the mechanisms of neural networks by analyzing the rich structure of parameters contained within their optimization trajectories. Towards this end, we introduce some natural notions of the complexity of optimization trajectories, both qualitative and quantitative, which reveal the inherent nuance and interplay involved between various optimization choices, such as momentum, weight decay, and batch size. We use them to provide key hallmarks about the nature of optimization in deep neural networks: when it goes right, and when it finds itself in a dead end. Further, thanks to our trajectory perspective, we uncover an intertwined behaviour of momentum and weight decay that promotes directional exploration, as well as a directional regularization behaviour of some others. We perform experiments over large-scale vision and language settings, including large language models (LLMs) with up to 12 billio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06265</link><description>&lt;p&gt;
&#25286;&#35299;&#20998;&#35789;&#65306;&#35780;&#20272;&#25991;&#26412;&#21387;&#32553;&#21450;&#20854;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21387;&#32553;&#22312;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#35777;&#26126;&#20102;&#21387;&#32553;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#20043;&#38388;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#65292;&#24182;&#34920;&#26126;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21387;&#32553;&#26159;BPE&#26368;&#24120;&#35265;&#30340;&#20998;&#35789;&#31639;&#27861;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#20294;&#20998;&#35789;&#36807;&#31243;&#20013;&#30340;&#21387;&#32553;&#37325;&#35201;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35770;&#36848;&#20102;&#21387;&#32553;&#30340;&#29702;&#35770;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;0-gram&#35821;&#35328;&#24314;&#27169;&#65292;&#21363;&#20026;&#25152;&#26377;&#26631;&#35760;&#20998;&#37197;&#30456;&#31561;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21387;&#32553;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#32493;&#25104;&#21151;&#30340;&#23454;&#35777;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#29992;&#25991;&#26723;&#30340;&#25968;&#37327;&#26469;&#25511;&#21046;&#22810;&#20010;BPE&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65306;&#20174;100&#19975;&#20010;&#25991;&#26723;&#21040;&#30456;&#24403;&#20110;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#22522;&#20110;&#23383;&#31526;&#30340;&#20998;&#35789;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#20998;&#35789;&#22120;&#39044;&#35757;&#32451;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#35789;&#22120;&#30340;&#21387;&#32553;&#19982;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#21387;&#32553;&#26159;&#20998;&#35789;&#30340;&#21487;&#38752;&#20869;&#22312;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06265v1 Announce Type: cross  Abstract: Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokeniza
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;</title><link>https://arxiv.org/abs/2403.05268</link><description>&lt;p&gt;
&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#29992;&#20110;&#36785;&#39554;&#35821;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Prompt Multi-task Network for Abuse Language Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Deep Prompt Multi-task Network (DPMN)&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#26469;&#28608;&#21457;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28389;&#29992;&#35821;&#35328;&#30340;&#26816;&#27979;&#20173;&#28982;&#26159;&#31038;&#20132;&#32593;&#32476;&#24191;&#27867;&#20351;&#29992;&#20013;&#23384;&#22312;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#20219;&#21153;&#23384;&#22312;&#30528;&#20934;&#30830;&#24615;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#25216;&#26415;&#26469;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#28608;&#21457;PLMs&#30340;&#19968;&#33324;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#28389;&#29992;&#35821;&#35328;&#26816;&#27979;&#30340;&#28145;&#24230;&#25552;&#31034;&#22810;&#20219;&#21153;&#32593;&#32476;&#65288;DPMN&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DPMN&#39318;&#20808;&#23581;&#35797;&#20026;PLMs&#35774;&#35745;&#20004;&#31181;&#24418;&#24335;&#30340;&#28145;&#24230;&#25552;&#31034;&#35843;&#25972;&#21644;&#36731;&#25552;&#31034;&#35843;&#25972;&#12290;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#38271;&#24230;&#12289;&#35843;&#25972;&#31574;&#30053;&#21644;&#25552;&#31034;&#21021;&#22987;&#21270;&#26041;&#27861;&#23545;&#20110;&#26816;&#27979;&#28389;&#29992;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Bi-LSTM&#21644;FFN&#30340;&#20219;&#21153;&#22836;&#65292;&#21487;&#29992;&#20316;&#30701;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#26368;&#32456;&#65292;DPMN&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#26816;&#27979;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05268v1 Announce Type: cross  Abstract: The detection of abusive language remains a long-standing challenge with the extensive use of social networks. The detection task of abusive language suffers from limited accuracy. We argue that the existing detection methods utilize the fine-tuning technique of the pre-trained language models (PLMs) to handle downstream tasks. Hence, these methods fail to stimulate the general knowledge of the PLMs. To address the problem, we propose a novel Deep Prompt Multi-task Network (DPMN) for abuse language detection. Specifically, DPMN first attempts to design two forms of deep prompt tuning and light prompt tuning for the PLMs. The effects of different prompt lengths, tuning strategies, and prompt initialization methods on detecting abusive language are studied. In addition, we propose a Task Head based on Bi-LSTM and FFN, which can be used as a short text classifier. Eventually, DPMN utilizes multi-task learning to improve detection metrics 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.04814</link><description>&lt;p&gt;
&#22312;&#21477;&#27861;&#24863;&#30693;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;SAFIM&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#22635;&#31354;&#20219;&#21153;&#19978;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#34920;&#29616;&#65292;&#21457;&#29616;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#39640;&#20102;FIM&#30340;&#29087;&#32451;&#24230;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#25512;&#29702;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#21697;&#36136;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#26356;&#29978;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Syntax-Aware Fill-In-the-Middle&#65288;SAFIM&#65289;&#30340;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#22635;&#31354;&#65288;FIM&#65289;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#22522;&#20934;&#20391;&#37325;&#20110;&#31243;&#24207;&#32467;&#26500;&#30340;&#21477;&#27861;&#24863;&#30693;&#23436;&#25104;&#65292;&#22914;&#20195;&#30721;&#22359;&#21644;&#26465;&#20214;&#34920;&#36798;&#24335;&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;17,720&#20010;&#31034;&#20363;&#65292;&#26469;&#28304;&#20110;2022&#24180;4&#26376;&#20043;&#21518;&#30340;&#26368;&#26032;&#20195;&#30721;&#25552;&#20132;&#65292;&#20197;&#26368;&#23567;&#21270;&#25968;&#25454;&#27745;&#26579;&#12290; SAFIM&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#21644;&#26032;&#39062;&#30340;&#21477;&#27861;&#24863;&#30693;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#26377;&#21161;&#20110;&#22312;LLMs&#20043;&#38388;&#36827;&#34892;&#20934;&#30830;&#21644;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;15&#20010;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;FIM&#39044;&#35757;&#32451;&#19981;&#20165;&#25552;&#21319;&#20102;FIM&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#36824;&#25913;&#36827;&#20102;LLMs&#30340;&#24038;&#21040;&#21491;&#65288;L2R&#65289;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25361;&#25112;&#20102;&#20256;&#32479;&#35266;&#24565;&#65292;&#24182;&#34920;&#26126;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#22823;&#20110;&#27169;&#22411;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;SAFIM&#20026;&#26410;&#26469;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04814v1 Announce Type: cross  Abstract: We introduce Syntax-Aware Fill-In-the-Middle (SAFIM), a new benchmark for evaluating Large Language Models (LLMs) on the code Fill-in-the-Middle (FIM) task. This benchmark focuses on syntax-aware completions of program structures such as code blocks and conditional expressions, and includes 17,720 examples from multiple programming languages, sourced from recent code submissions after April 2022 to minimize data contamination. SAFIM provides a robust framework with various prompt designs and novel syntax-aware post-processing techniques, facilitating accurate and fair comparisons across LLMs. Our comprehensive evaluation of 15 LLMs shows that FIM pretraining not only enhances FIM proficiency but also improves Left-to-Right (L2R) inference using LLMs. Our findings challenge conventional beliefs and suggest that pretraining methods and data quality have more impact than model size. SAFIM thus serves as a foundational platform for future 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#30340;&#20351;&#29992;&#38382;&#39064;&#65292;&#21457;&#29616;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#21487;&#33021;&#24433;&#21709;&#26657;&#20934;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.00423</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#25311;&#21442;&#32771;&#20540;&#39564;&#35777;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#65306;&#19968;&#39033;&#25935;&#24863;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Validation of ML-UQ calibration statistics using simulated reference values: a sensitivity analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ML-UQ&#26657;&#20934;&#32479;&#35745;&#37327;&#30340;&#20351;&#29992;&#38382;&#39064;&#65292;&#21457;&#29616;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#21487;&#33021;&#24433;&#21709;&#26657;&#20934;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;ML-UQ&#65289;&#26657;&#20934;&#32479;&#35745;&#37327;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#21442;&#32771;&#20540;&#65292;&#20027;&#35201;&#29992;&#20110;&#27604;&#36739;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#26657;&#20934;&#20960;&#20046;&#20174;&#19981;&#34987;&#39564;&#35777;&#65292;&#35786;&#26029;&#30041;&#32473;&#35835;&#32773;&#30340;&#21028;&#26029;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#23454;&#38469;&#19981;&#30830;&#23450;&#24615;&#23548;&#20986;&#30340;&#21512;&#25104;&#26657;&#20934;&#25968;&#25454;&#38598;&#30340;&#27169;&#25311;&#21442;&#32771;&#20540;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#38382;&#39064;&#12290;&#30001;&#20110;&#29992;&#20110;&#27169;&#25311;&#21512;&#25104;&#35823;&#24046;&#30340;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#36890;&#24120;&#27809;&#26377;&#32422;&#26463;&#65292;&#25152;&#20197;&#27169;&#25311;&#21442;&#32771;&#20540;&#23545;&#29983;&#25104;&#20998;&#24067;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#21487;&#33021;&#20250;&#25104;&#20026;&#38382;&#39064;&#65292;&#23545;&#26657;&#20934;&#35786;&#26029;&#20135;&#29983;&#24576;&#30097;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#24182;&#26174;&#31034;&#19968;&#20123;&#32479;&#35745;&#37327;&#23545;&#20110;&#29992;&#20110;&#39564;&#35777;&#26102;&#29983;&#25104;&#20998;&#24067;&#30340;&#36873;&#25321;&#36807;&#20110;&#25935;&#24863;&#65292;&#24403;&#29983;&#25104;&#20998;&#24067;&#26410;&#30693;&#26102;&#12290;&#20363;&#22914;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00423v1 Announce Type: cross  Abstract: Some popular Machine Learning Uncertainty Quantification (ML-UQ) calibration statistics do not have predefined reference values and are mostly used in comparative studies. In consequence, calibration is almost never validated and the diagnostic is left to the appreciation of the reader. Simulated reference values, based on synthetic calibrated datasets derived from actual uncertainties, have been proposed to palliate this problem. As the generative probability distribution for the simulation of synthetic errors is often not constrained, the sensitivity of simulated reference values to the choice of generative distribution might be problematic, shedding a doubt on the calibration diagnostic. This study explores various facets of this problem, and shows that some statistics are excessively sensitive to the choice of generative distribution to be used for validation when the generative distribution is unknown. This is the case, for instan
&lt;/p&gt;</description></item><item><title>LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;</title><link>https://arxiv.org/abs/2402.19361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27700;&#21360;&#31363;&#21462;
&lt;/p&gt;
&lt;p&gt;
Watermark Stealing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19361
&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#23384;&#22312;&#27700;&#21360;&#31363;&#21462;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#19979;&#36890;&#36807;&#27450;&#39575;&#21644;&#25830;&#38500;&#25915;&#20987;&#30772;&#35299;&#20043;&#21069;&#35748;&#20026;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#27700;&#21360;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26816;&#27979;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#20105;&#36777;&#31216;&#24403;&#21069;&#26041;&#26696;&#21487;&#33021;&#24050;&#32463;&#21487;&#20197;&#37096;&#32626;&#65292;&#25105;&#20204;&#35748;&#20026;&#27700;&#21360;&#31363;&#21462;&#65288;WS&#65289;&#26159;&#36825;&#20123;&#26041;&#26696;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26597;&#35810;&#24102;&#26377;&#27700;&#21360;&#30340;LLM&#30340;API&#26469;&#36817;&#20284;&#36870;&#21521;&#27700;&#21360;&#65292;&#20174;&#32780;&#23454;&#29616;&#23454;&#29992;&#30340;&#27450;&#39575;&#25915;&#20987;&#65292;&#21516;&#26102;&#22823;&#24133;&#22686;&#21152;&#20102;&#20043;&#21069;&#26410;&#34987;&#27880;&#24847;&#21040;&#30340;&#25830;&#38500;&#25915;&#20987;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#33258;&#21160;WS&#31639;&#27861;&#24182;&#23558;&#20854;&#29992;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#27450;&#39575;&#21644;&#25830;&#38500;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#38656;&#19981;&#21040;50&#32654;&#20803;&#30340;&#25104;&#26412;&#65292;&#25915;&#20987;&#32773;&#23601;&#33021;&#22815;&#27450;&#39575;&#24182;&#25830;&#38500;&#20043;&#21069;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#26368;&#20808;&#36827;&#26041;&#26696;&#65292;&#24179;&#22343;&#25104;&#21151;&#29575;&#36229;&#36807;80%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25361;&#25112;&#20102;&#20851;&#20110;LLM&#27700;&#21360;&#25216;&#26415;&#30340;&#24120;&#35265;&#20449;&#24565;&#65292;&#24378;&#35843;&#20102;&#26356;&#21152;&#20581;&#22766;&#26041;&#26696;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19361v1 Announce Type: cross  Abstract: LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We mak
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20302;&#20445;&#30495;&#24230;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#32858;&#21512;&#20934;&#30830;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.18846</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20302;&#20445;&#30495;&#24230;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#32858;&#21512;&#20934;&#30830;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#35299;&#20915;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#26367;&#20195;&#24314;&#27169;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#27700;&#24179;&#23398;&#20064;&#20934;&#30830;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#39640;&#26031;&#36807;&#31243;&#65292;&#24456;&#38590;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21253;&#25324;&#23545;&#24212;&#35299;&#30721;&#22120;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#19981;&#21516;&#20445;&#30495;&#24230;&#20043;&#38388;&#20849;&#20139;&#32534;&#30721;&#34920;&#31034;&#12290;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#26102;&#65292;&#29992;&#19981;&#21516;&#21442;&#25968;&#35299;&#30721;&#34920;&#31034;&#65292;&#20351;&#20849;&#20139;&#20449;&#24687;&#22266;&#26377;&#19981;&#20934;&#30830;&#12290;&#36825;&#38480;&#21046;&#20102;&#25512;&#26029;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#22495;&#35206;&#30422;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20445;&#30495;&#24230;&#26367;&#20195;&#24314;&#27169;&#26694;&#26550;&#8212;&#8212;&#22810;&#20445;&#30495;&#24230;&#27531;&#24046;&#31070;&#32463;&#36807;&#31243;&#65288;MFRNP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18846v1 Announce Type: new  Abstract: Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by agg
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.17879</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Automated Statistical Model Discovery with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17879
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#65292;&#19981;&#20877;&#38656;&#35201;&#23450;&#20041;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#28041;&#21450;&#22312;&#21463;&#39046;&#22495;&#29305;&#23450;&#24314;&#27169;&#32422;&#26463;&#30340;&#24191;&#27867;&#27169;&#22411;&#31354;&#38388;&#19978;&#36827;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25628;&#32034;&#12290;&#39640;&#25928;&#25628;&#32034;&#36825;&#19968;&#31354;&#38388;&#38656;&#35201;&#20855;&#26377;&#24314;&#27169;&#21644;&#38382;&#39064;&#22495;&#20154;&#31867;&#19987;&#38271;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#39046;&#22495;&#30693;&#35782;&#21644;&#32534;&#31243;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#33258;&#21160;&#32479;&#35745;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#33258;&#21160;&#21270;&#27969;&#31243;&#32622;&#20110;Box&#30340;&#24490;&#29615;&#26694;&#26550;&#20043;&#20869;&#65306;LM&#22312;&#25552;&#20986;&#34920;&#31034;&#20026;&#27010;&#29575;&#31243;&#24207;&#30340;&#32479;&#35745;&#27169;&#22411;&#65288;&#20805;&#24403;&#24314;&#27169;&#32773;&#65289;&#20043;&#38388;&#36845;&#20195;&#65292;&#24182;&#25209;&#21028;&#36825;&#20123;&#27169;&#22411;&#65288;&#20805;&#24403;&#39046;&#22495;&#19987;&#23478;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;LMs&#65292;&#25105;&#20204;&#19981;&#24517;&#23450;&#20041;&#19968;&#20010;&#39046;&#22495;&#29305;&#23450;&#30340;&#27169;&#22411;&#35821;&#35328;&#25110;&#35774;&#35745;&#25163;&#24037;&#25628;&#32034;&#31243;&#24207;&#65292;&#36825;&#26159;&#20808;&#21069;&#31995;&#32479;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#24314;&#27169;&#30340;&#19977;&#31181;&#24120;&#35265;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#22312;&#21463;&#38480;&#27169;&#22411;&#31354;&#38388;&#20869;&#25628;&#32034;&#65292;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17879v1 Announce Type: cross  Abstract: Statistical model discovery involves a challenging search over a vast space of models subject to domain-specific modeling constraints. Efficiently searching over this space requires human expertise in modeling and the problem domain. Motivated by the domain knowledge and programming capabilities of large language models (LMs), we introduce a method for language model driven automated statistical model discovery. We cast our automated procedure within the framework of Box's Loop: the LM iterates between proposing statistical models represented as probabilistic programs, acting as a modeler, and critiquing those models, acting as a domain expert. By leveraging LMs, we do not have to define a domain-specific language of models or design a handcrafted search procedure, key restrictions of previous systems. We evaluate our method in three common settings in probabilistic modeling: searching within a restricted space of models, searching ove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#24182;&#31169;&#19979;&#23398;&#20064;&#29305;&#23450;&#30340;&#39044;&#27979;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.17705</link><description>&lt;p&gt;
&#29992;&#20110;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for Estimating Heterogeneous Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#24773;&#20917;&#19979;&#20849;&#21516;&#23398;&#20064;&#29305;&#24449;&#34920;&#31034;&#24182;&#31169;&#19979;&#23398;&#20064;&#29305;&#23450;&#30340;&#39044;&#27979;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#65288;HTE&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#25919;&#31574;&#21046;&#23450;&#12289;&#25945;&#32946;&#31561;&#65289;&#30340;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;HTE&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#27599;&#31181;&#22788;&#29702;&#26041;&#27861;&#22823;&#37327;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#32780;&#24178;&#39044;&#30340;&#39640;&#25104;&#26412;&#20351;&#24471;&#20026;&#27599;&#31181;&#24178;&#39044;&#38598;&#20013;&#25910;&#38598;&#36825;&#20040;&#22810;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26426;&#26500;&#20043;&#38388;&#21327;&#20316;&#23398;&#20064;HTE&#20272;&#35745;&#37327;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#21363;&#20351;&#22312;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#22810;&#26679;&#30340;&#24178;&#39044;&#21644;&#21463;&#35797;&#32773;&#32676;&#20307;&#65292;&#20063;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;&#21508;&#20010;&#26426;&#26500;&#20043;&#38388;&#21516;&#26102;&#24182;&#31169;&#19979;&#23398;&#20064;&#20851;&#20110;&#19981;&#21516;&#24178;&#39044;&#24773;&#20917;&#19979;&#32467;&#26524;&#30340;&#29305;&#23450;&#39044;&#27979;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#31639;&#27861;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17705v1 Announce Type: new  Abstract: Machine learning methods for estimating heterogeneous treatment effects (HTE) facilitate large-scale personalized decision-making across various domains such as healthcare, policy making, education, and more. Current machine learning approaches for HTE require access to substantial amounts of data per treatment, and the high costs associated with interventions makes centrally collecting so much data for each intervention a formidable challenge. To overcome this obstacle, in this work, we propose a novel framework for collaborative learning of HTE estimators across institutions via Federated Learning. We show that even under a diversity of interventions and subject populations across clients, one can jointly learn a common feature representation, while concurrently and privately learning the specific predictive functions for outcomes under distinct interventions across institutions. Our framework and the associated algorithm are based on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#21518;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#30340;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#26524;&#21644;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20248;&#21270;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.15274</link><description>&lt;p&gt;
&#22312;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#19979;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification Under Strategic Self-Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15274
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#21518;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#30340;&#25112;&#30053;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#26524;&#21644;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20248;&#21270;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29992;&#25143;&#21487;&#20197;&#20174;&#26576;&#20123;&#39044;&#27979;&#20013;&#33719;&#30410;&#26102;&#65292;&#20182;&#20204;&#24448;&#24448;&#20250;&#37319;&#21462;&#25112;&#30053;&#34892;&#21160;&#20197;&#33719;&#24471;&#26377;&#21033;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#25112;&#30053;&#20998;&#31867;&#30340;&#30740;&#31350;&#37117;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#34920;&#29616;&#20026;&#29305;&#24449;&#20462;&#25913;&#65292;&#32780;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#22312;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#20915;&#23450;&#26159;&#21542;&#21442;&#19982;&#65288;&#25110;&#19981;&#21442;&#19982;&#65289;&#12290;&#20026;&#20102;&#23398;&#20064;&#22686;&#21152;&#25112;&#30053;&#24847;&#35782;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#25105;&#36873;&#25321;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23398;&#20064;&#23545;&#33258;&#25105;&#36873;&#25321;&#20154;&#21475;&#26500;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#22312;&#33258;&#25105;&#36873;&#25321;&#34892;&#20026;&#19979;&#23398;&#20064;&#30340;&#21487;&#24494;&#20998;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#21518;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#21644;&#27169;&#25311;&#34892;&#20026;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#26082;&#21487;&#20197;&#34917;&#20805;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#21448;&#21487;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15274v1 Announce Type: new  Abstract: When users stand to gain from certain predictions, they are prone to act strategically to obtain favorable predictive outcomes. Whereas most works on strategic classification consider user actions that manifest as feature modifications, we study a novel setting in which users decide -- in response to the learned classifier -- whether to at all participate (or not). For learning approaches of increasing strategic awareness, we study the effects of self-selection on learning, and the implications of learning on the composition of the self-selected population. We then propose a differentiable framework for learning under self-selective behavior, which can be optimized effectively. We conclude with experiments on real data and simulated behavior that both complement our analysis and demonstrate the utility of our approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14208</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#20869;&#23481;&#26465;&#20214;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Content Conditional Debiasing for Fair Text Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14208
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#30830;&#20445;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#25913;&#21892;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#20943;&#36731;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#20844;&#24179;&#30340;&#25991;&#26412;&#23884;&#20837;&#19978;&#65292;&#36825;&#23545;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20844;&#24179;&#25991;&#26412;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#22312;&#20869;&#23481;&#26465;&#20214;&#19979;&#25935;&#24863;&#23646;&#24615;&#19982;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#25928;&#29992;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24378;&#21046;&#35201;&#27714;&#20855;&#26377;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#20294;&#30456;&#21516;&#20869;&#23481;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#19982;&#20854;&#23545;&#24212;&#20013;&#31435;&#25991;&#26412;&#30340;&#23884;&#20837;&#20445;&#25345;&#30456;&#21516;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#25991;&#26412;&#22686;&#24378;&#20026;&#19981;&#21516;&#30340;&#25935;&#24863;&#32452;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#36866;&#24403;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#20844;&#24179;&#24615;&#21516;&#26102;&#20445;&#25345;&#20102;&#23884;&#20837;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14208v1 Announce Type: cross  Abstract: Mitigating biases in machine learning models has gained increasing attention in Natural Language Processing (NLP). Yet, only a few studies focus on fair text embeddings, which are crucial yet challenging for real-world applications. In this paper, we propose a novel method for learning fair text embeddings. We achieve fairness while maintaining utility trade-off by ensuring conditional independence between sensitive attributes and text embeddings conditioned on the content. Specifically, we enforce that embeddings of texts with different sensitive attributes but identical content maintain the same distance toward the embedding of their corresponding neutral text. Furthermore, we address the issue of lacking proper training data by using Large Language Models (LLMs) to augment texts into different sensitive groups. Our extensive evaluations demonstrate that our approach effectively improves fairness while preserving the utility of embed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#20934;&#30830;&#25191;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20915;&#31574;&#20013;&#39044;&#27979;&#36136;&#37327;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13530</link><description>&lt;p&gt;
&#20004;&#20010;&#19990;&#30028;&#20013;&#30340;&#26368;&#20339;&#20043;&#36873;&#65306;&#22312;&#26410;&#30693;&#21040;&#36798;&#27169;&#22411;&#19979;&#24102;&#39044;&#27979;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Best of Many in Both Worlds: Online Resource Allocation with Predictions under Unknown Arrival Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#20934;&#30830;&#25191;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20915;&#31574;&#20013;&#39044;&#27979;&#36136;&#37327;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13530v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#24403;&#20170;&#30340;&#22312;&#32447;&#20915;&#31574;&#32773;&#36890;&#24120;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#26410;&#26469;&#21464;&#37327;&#30340;&#39044;&#27979;&#65292;&#22914;&#21040;&#36798;&#12289;&#38656;&#27714;&#12289;&#24211;&#23384;&#31561;&#12290;&#36825;&#20123;&#39044;&#27979;&#21487;&#20197;&#30001;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#29983;&#25104;&#65292;&#19968;&#30452;&#21040;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#21644;&#38468;&#21152;&#29305;&#24449;&#20449;&#24687;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#36136;&#37327;&#36890;&#24120;&#23545;&#20915;&#31574;&#32773;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#30450;&#30446;&#22320;&#36981;&#24490;&#39044;&#27979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#32473;&#20986;&#23558;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#24182;&#38024;&#23545;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#36827;&#34892;&#31283;&#20581;&#25191;&#34892;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#36825;&#26159;&#25910;&#30410;&#31649;&#29702;&#21644;&#22312;&#32447;&#20915;&#31574;&#21046;&#23450;&#20013;&#26368;&#36890;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20915;&#31574;&#32773;&#25317;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#35831;&#27714;&#26159;&#39034;&#24207;&#21040;&#26469;&#30340;&#12290;&#23545;&#20110;&#27599;&#20010;&#35831;&#27714;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#20915;&#23450;&#37319;&#21462;&#20309;&#31181;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13530v1 Announce Type: cross  Abstract: Online decision-makers today can often obtain predictions on future variables, such as arrivals, demands, inventories, and so on. These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information. However, the prediction quality is often unknown to decisions-makers a priori, hence blindly following the predictions can be harmful. In this paper, we address this problem by giving algorithms that take predictions as inputs and perform robustly against the unknown prediction quality.   We consider the online resource allocation problem, one of the most generic models in revenue management and online decision-making. In this problem, a decision maker has a limited amount of resources, and requests arrive sequentially. For each request, the decision-maker needs to decide on an action, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12842</link><description>&lt;p&gt;
PromptKD&#65306;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#20026;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12842
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PromptKD&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#23454;&#29616;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#23398;&#29983;&#21451;&#22909;&#30693;&#35782;&#30340;&#33976;&#39311;&#65292;&#26080;&#38656;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#23545;&#25512;&#29702;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#23545;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#23613;&#31649;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#38024;&#23545;LLMs&#36825;&#26679;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;KD&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#32780;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#27169;&#22411;&#30340;KD&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptKD&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972; - &#22312;KD&#20013;&#39318;&#27425;&#20986;&#29616; - &#20351;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20256;&#36882;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#12290;&#19982;&#20808;&#21069;&#20998;&#31867;&#24037;&#20316;&#19981;&#21516;&#65292;&#20808;&#21069;&#37027;&#20123;&#38656;&#35201;&#24494;&#35843;&#25972;&#25972;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#25552;&#21462;&#36866;&#21512;&#23398;&#29983;&#30340;&#30693;&#35782;&#65292;PromptKD&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#25552;&#31034;&#26631;&#35760;&#65292;&#24182;&#20165;&#36890;&#36807;&#23398;&#29983;&#25351;&#23548;&#35843;&#25972;&#25552;&#31034;&#26469;&#36798;&#21040;&#31867;&#20284;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12842v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Ex
&lt;/p&gt;</description></item><item><title>UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11838</link><description>&lt;p&gt;
UniST&#65306;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11838
&lt;/p&gt;
&lt;p&gt;
UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20132;&#36890;&#31649;&#29702;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#22478;&#24066;&#35268;&#21010;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#22478;&#24066;&#26102;&#31354;&#24314;&#27169;&#33853;&#21518;&#12290;&#29616;&#26377;&#30340;&#22478;&#24066;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#26102;&#31354;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;UniST&#12290;&#20511;&#37492;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;UniST&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21462;&#24471;&#25104;&#21151;&#65306;(i) &#23545;&#19981;&#21516;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#29305;&#24449;&#30340;&#28789;&#27963;&#24615;&#65292;(ii) &#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25513;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#26102;&#38388;&#20851;&#31995;&#65292;(iii) &#26102;&#31354;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.11778</link><description>&lt;p&gt;
&#26397;&#21521;&#33258;&#28040;&#32791;&#29983;&#25104;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Theoretical Understandings of Self-Consuming Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#26465;&#20214;&#19979;&#65292;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#25361;&#25112;&#65292;&#21363;&#22312;&#19968;&#20010;&#33258;&#28040;&#32791;&#24490;&#29615;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#36830;&#32493;&#30340;&#27169;&#22411;&#19990;&#20195;&#36890;&#36807;&#28151;&#21512;&#20043;&#21069;&#19990;&#20195;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#26469;&#36827;&#34892;&#36882;&#24402;&#35757;&#32451;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;&#36825;&#31181;&#35757;&#32451;&#26041;&#26696;&#23545;&#26410;&#26469;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#20998;&#24067;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;&#19981;&#21516;&#28151;&#21512;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#26410;&#26469;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#25968;&#25454;&#20998;&#24067;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#28151;&#21512;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#25110;&#30495;&#23454;&#25968;&#25454;&#27604;&#20363;&#36275;&#22815;&#22823;&#30340;&#26465;&#20214;&#19979;&#65292;&#36825;&#31181;&#36317;&#31163;&#21487;&#20197;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#30001;&#25193;&#22823;&#21512;&#25104;&#25968;&#25454;&#37327;&#24341;&#36215;&#30340;&#30456;&#21464;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#34429;&#28982;TV&#36317;&#31163;&#34920;&#29616;&#20986;&#21021;&#22987;&#19978;&#21319;&#65292;&#20294;&#21364;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25163;&#24037;&#35774;&#35745;&#20803;&#32467;&#26500;&#19981;&#26131;&#25193;&#23637;&#20197;&#21450;&#24573;&#35270;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.11518</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#24322;&#36136;&#20449;&#24687;&#32593;&#32476;&#20013;&#20803;&#32467;&#26500;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11518
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#20803;&#32467;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#25163;&#24037;&#35774;&#35745;&#20803;&#32467;&#26500;&#19981;&#26131;&#25193;&#23637;&#20197;&#21450;&#24573;&#35270;&#21487;&#35299;&#37322;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#22240;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#20803;&#32467;&#26500;&#34987;&#25552;&#20986;&#29992;&#20110;&#35782;&#21035;HIN&#19978;&#30340;&#37325;&#35201;&#20851;&#31995;&#27169;&#24335;&#65292;&#24050;&#35777;&#26126;&#22312;&#25552;&#21462;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;&#20419;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#36798;&#21147;&#34920;&#31034;&#26041;&#38754;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25163;&#24037;&#35774;&#35745;&#30340;&#20803;&#32467;&#26500;&#22312;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#21457;&#23637;&#33258;&#21160;&#20803;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#23547;&#25214;&#20855;&#26377;&#33391;&#22909;&#32463;&#39564;&#39044;&#27979;&#24615;&#33021;&#30340;&#20803;&#32467;&#26500;&#65292;&#32780;&#24573;&#35270;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22240;&#27492;&#65292;&#20182;&#20204;&#24448;&#24448;&#20135;&#29983;&#26131;&#20110;&#36807;&#24230;&#25311;&#21512;&#21644;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#30340;&#20803;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26032;&#20852;&#30340;&#25512;&#29702;&#33021;&#21147;&#20013;&#33719;&#21462;&#21551;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;REasoning meta-STRUCTure search&#65288;ReStruct&#65289;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11518v1 Announce Type: new  Abstract: Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framewor
&lt;/p&gt;</description></item><item><title>ZeroG&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#36801;&#31227;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#26631;&#31614;&#31354;&#38388;&#19981;&#21305;&#37197;&#21644;&#36127;&#36801;&#31227;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.11235</link><description>&lt;p&gt;
ZeroG&#65306;&#25506;&#31350;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11235
&lt;/p&gt;
&lt;p&gt;
ZeroG&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22270;&#20013;&#36328;&#25968;&#25454;&#38598;&#38646;&#23556;&#20987;&#36801;&#31227;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#26631;&#31614;&#31354;&#38388;&#19981;&#21305;&#37197;&#21644;&#36127;&#36801;&#31227;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22914;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#38646;&#23556;&#20987;&#36801;&#31227;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NLP&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;CV&#27169;&#22411;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#31361;&#26174;&#20102;&#36825;&#19968;&#28857;&#65292;&#20108;&#32773;&#26377;&#25928;&#22320;&#24357;&#21512;&#20102;&#24050;&#35265;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#65292;&#26032;&#22270;&#30340;&#19981;&#26029;&#28044;&#29616;&#21644;&#20154;&#31867;&#26631;&#27880;&#30340;&#25361;&#25112;&#20063;&#21152;&#21095;&#20102;&#38646;&#23556;&#20987;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#24615;&#65292;&#25512;&#21160;&#20102;&#25506;&#32034;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#27867;&#21270;&#36328;&#22810;&#26679;&#22270;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ZeroG&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#36825;&#26679;&#30340;&#33539;&#20363;&#25193;&#23637;&#21040;&#20102;&#22270;&#20013;&#30340;&#38646;&#23556;&#20987;&#36801;&#31227;&#24615;&#12290;&#35299;&#20915;&#35832;&#22914;&#29305;&#24449;&#19981;&#23545;&#40784;&#12289;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#36127;&#36801;&#31227;&#31561;&#22266;&#26377;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11235v1 Announce Type: new  Abstract: With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we l
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25506;&#35752;&#20102;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#39057;&#29575;&#35774;&#23450;&#19979;&#30340;&#31639;&#27861;&#22312;&#27492;&#35774;&#32622;&#19979;&#34920;&#29616;&#27425;&#20248;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#19982;&#29702;&#35770;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;&#36830;&#32493;&#25490;&#38500;&#21464;&#31181;&#12290;</title><link>https://arxiv.org/abs/2402.10429</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fixed Confidence Best Arm Identification in the Bayesian Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10429
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25506;&#35752;&#20102;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#39057;&#29575;&#35774;&#23450;&#19979;&#30340;&#31639;&#27861;&#22312;&#27492;&#35774;&#32622;&#19979;&#34920;&#29616;&#27425;&#20248;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#19982;&#29702;&#35770;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;&#36830;&#32493;&#25490;&#38500;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;FC-BAI&#65289;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#24050;&#30693;&#20808;&#39564;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#20197;&#22266;&#23450;&#32622;&#20449;&#27700;&#24179;&#25214;&#21040;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;FC-BAI&#38382;&#39064;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#39057;&#29575;&#35774;&#23450;&#20013;&#36827;&#34892;&#30340;&#65292;&#22312;&#35813;&#35774;&#23450;&#19979;&#65292;&#28216;&#25103;&#24320;&#22987;&#21069;&#21363;&#30830;&#23450;&#20102;&#36172;&#21338;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#65292;&#20256;&#32479;&#30340;&#22312;&#39057;&#29575;&#35774;&#23450;&#20013;&#30740;&#31350;&#30340;FC-BAI&#31639;&#27861;&#65288;&#22914;track-and-stop&#21644;top-two&#31639;&#27861;&#65289;&#20250;&#23548;&#33268;&#20219;&#24847;&#27425;&#20248;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#19979;&#39044;&#26399;&#26679;&#26412;&#25968;&#30340;&#19979;&#38480;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36830;&#32493;&#25490;&#38500;&#30340;&#21464;&#31181;&#65292;&#20854;&#24615;&#33021;&#19982;&#19979;&#38480;&#30456;&#21305;&#37197;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#20223;&#30495;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10429v1 Announce Type: cross  Abstract: We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20307;&#21270;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09834</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#19982;&#22810;&#21151;&#33021;&#24615;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36328;&#39046;&#22495;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20307;&#21270;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;LLMs&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#65292;&#22312;&#24191;&#27867;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#21333;&#19968;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#65292;&#36825;&#31181;&#33539;&#24335;&#34987;&#31216;&#20026;&#8220;&#19968;&#20307;&#21270;&#8221;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;LLMs&#20855;&#22791;&#20102;&#36229;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#21508;&#31181;&#25968;&#25454;&#20998;&#24067;&#12290;&#20511;&#21161;&#36825;&#20123;&#33021;&#21147;&#65292;&#21333;&#19968;&#30340;LLM&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36825;&#31181;&#33539;&#24335;&#34987;&#31216;&#20026;&#8220;&#22810;&#21151;&#33021;&#19968;&#20307;&#21270;&#8221;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#22270;&#39046;&#22495;&#20173;&#28982;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#32463;&#24120;&#23548;&#33268;&#36127;&#36801;&#31227;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#21294;&#20047;&#38656;&#35201;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#28304;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09834v1 Announce Type: new  Abstract: Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#20998;&#21035;&#38477;&#20302;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.09173</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#36817;&#20284;&#26368;&#20248;&#21518;&#24724;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nearly Optimal Regret for Decentralized Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#20998;&#21035;&#38477;&#20302;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#65292;&#24182;&#22635;&#34917;&#20102;&#29616;&#26377;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#22312;&#32447;&#20984;&#20248;&#21270;(D-OCO)&#65292;&#20854;&#20013;&#19968;&#32452;&#26412;&#22320;&#23398;&#20064;&#22120;&#38656;&#35201;&#20351;&#29992;&#20165;&#38480;&#20110;&#26412;&#22320;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#19968;&#31995;&#21015;&#20840;&#23616;&#25439;&#22833;&#20989;&#25968;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#38024;&#23545;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#20998;&#21035;&#20026;$O(n^{5/4}\rho^{-1/2}\sqrt{T})$&#21644;${O}(n^{3/2}\rho^{-1}\log T)$&#65292;&#20854;&#20013;$n$&#26159;&#26412;&#22320;&#23398;&#20064;&#22120;&#30340;&#25968;&#37327;&#65292;$\rho&lt;1$&#26159;&#36890;&#20449;&#30697;&#38453;&#30340;&#35889;&#38388;&#38553;&#65292;$T$&#26159;&#26102;&#38388;&#27573;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20984;&#20989;&#25968;&#23384;&#22312;&#30528;&#36739;&#22823;&#30340;&#38388;&#38553;&#65292;&#21363;&#20984;&#20989;&#25968;&#30340;&#19979;&#30028;&#20026;$\Omega(n\sqrt{T})$&#65292;&#24378;&#20984;&#20989;&#25968;&#30340;&#19979;&#30028;&#20026;$\Omega(n)$&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#38388;&#38553;&#65292;&#26412;&#25991;&#39318;&#20808;&#24320;&#21457;&#20102;&#26032;&#30340;D-OCO&#31639;&#27861;&#65292;&#23558;&#20984;&#20989;&#25968;&#21644;&#24378;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#36793;&#30028;&#20998;&#21035;&#38477;&#20302;&#21040;$\tilde{O}(n\rho^{-1/4}\sqrt{T})$&#21644;$\tilde{O}(n\rho^{-1/2}\log T)$&#12290;&#20027;&#35201;&#25216;&#26415;&#26159;&#35774;&#35745;&#19968;&#31181;&#22312;&#32447;&#21487;&#36827;&#21462;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09173v1 Announce Type: new Abstract: We investigate decentralized online convex optimization (D-OCO), in which a set of local learners are required to minimize a sequence of global loss functions using only local computations and communications. Previous studies have established $O(n^{5/4}\rho^{-1/2}\sqrt{T})$ and ${O}(n^{3/2}\rho^{-1}\log T)$ regret bounds for convex and strongly convex functions respectively, where $n$ is the number of local learners, $\rho&lt;1$ is the spectral gap of the communication matrix, and $T$ is the time horizon. However, there exist large gaps from the existing lower bounds, i.e., $\Omega(n\sqrt{T})$ for convex functions and $\Omega(n)$ for strongly convex functions. To fill these gaps, in this paper, we first develop novel D-OCO algorithms that can respectively reduce the regret bounds for convex and strongly convex functions to $\tilde{O}(n\rho^{-1/4}\sqrt{T})$ and $\tilde{O}(n\rho^{-1/2}\log T)$. The primary technique is to design an online acce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#24724;&#29575;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#65292;&#25104;&#21151;&#23558;&#21518;&#24724;&#30028;&#38480;&#20174;$O(T^{3/4}+d^{1/3}T^{2/3})$&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#24773;&#20917;&#19979;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.09152</link><description>&lt;p&gt;
&#25913;&#36827;&#20102;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#21518;&#24724;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improved Regret for Bandit Convex Optimization with Delayed Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21518;&#24724;&#29575;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#65292;&#25104;&#21151;&#23558;&#21518;&#24724;&#30028;&#38480;&#20174;$O(T^{3/4}+d^{1/3}T^{2/3})$&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#24773;&#20917;&#19979;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21482;&#26377;&#22312;&#20219;&#24847;&#24310;&#36831;&#19979;&#25165;&#20250;&#26174;&#31034;&#21160;&#20316;&#30340;&#25439;&#22833;&#20540;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#31616;&#21333;&#22320;&#23558;&#24310;&#36831;&#30340;&#25439;&#22833;&#20540;&#36755;&#20837;&#21040;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24314;&#31435;&#20102;&#35813;&#38382;&#39064;&#30340;&#21518;&#24724;&#30028;&#38480;&#20026;$O(T^{3/4}+d^{1/3}T^{2/3})$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#25552;&#39640;&#21518;&#24724;&#29575;&#65292;&#36890;&#36807;&#19968;&#20010;&#38459;&#22622;&#26356;&#26032;&#26426;&#21046;&#31934;&#30830;&#21033;&#29992;&#24310;&#36831;&#30340;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#39318;&#20808;&#25581;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#20998;&#31163;&#24310;&#36831;&#21644;&#24378;&#21270;&#23398;&#20064;&#21453;&#39304;&#23545;&#21518;&#24724;&#29575;&#30340;&#32852;&#21512;&#24433;&#21709;&#65292;&#24182;&#23558;&#20984;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#25913;&#36827;&#20026;$O(T^{3/4}+\sqrt{dT})$&#12290;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#22312;&#26356;&#22823;&#30340;&#24310;&#36831;&#37327;$d=O(\sqrt{T})$&#65292;&#32780;&#19981;&#26159;$d=O(T^{1/4})$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#21518;&#24724;&#19982;&#38750;&#24310;&#36831;&#35774;&#32622;&#19979;&#24378;&#21270;&#23398;&#20064;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;$O(T^{3/4})$&#21518;&#24724;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09152v1 Announce Type: new Abstract: We investigate bandit convex optimization (BCO) with delayed feedback, where only the loss value of the action is revealed under an arbitrary delay. Previous studies have established a regret bound of $O(T^{3/4}+d^{1/3}T^{2/3})$ for this problem, where $d$ is the maximum delay, by simply feeding delayed loss values to the classical bandit gradient descent (BGD) algorithm. In this paper, we develop a novel algorithm to enhance the regret, which carefully exploits the delayed bandit feedback via a blocking update mechanism. Our analysis first reveals that the proposed algorithm can decouple the joint effect of the delays and bandit feedback on the regret, and improve the regret bound to $O(T^{3/4}+\sqrt{dT})$ for convex functions. Compared with the previous result, our regret matches the $O(T^{3/4})$ regret of BGD in the non-delayed setting for a larger amount of delay, i.e., $d=O(\sqrt{T})$, instead of $d=O(T^{1/4})$. Furthermore, we consi
&lt;/p&gt;</description></item><item><title>&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08573</link><description>&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Two Tales of Single-Phase Contrastive Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08573
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#8220;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#8221;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#24050;&#32463;&#25910;&#25947;&#20110;&#23558;&#26799;&#24230;&#34920;&#31034;&#20026;&#27963;&#21160;&#24046;&#24322;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#31243;&#24230;&#30340;&#21516;&#27493;&#65288;&#23398;&#20064;&#26399;&#38388;&#30340;&#19981;&#21516;&#38454;&#27573;&#65289;&#24182;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20197;&#21450;&#20854;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#30340;&#28508;&#22312;&#25928;&#29992;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#36755;&#20986;&#21333;&#20803;&#26045;&#21152;&#26080;&#38480;&#23567;&#25200;&#21160;&#65288;nudges&#65289;&#65292;&#36825;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23558;&#20154;&#24037;&#31070;&#32463;&#20803;&#24314;&#27169;&#20026;&#20004;&#20010;&#30456;&#21453;&#25200;&#21160;&#30340;&#32452;&#20214;&#65292;&#21517;&#20026;&#8220;&#21452;&#21521;&#20256;&#25773;&#8221;&#30340;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#24357;&#21512;&#21040;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#21035;&#30340;&#23398;&#20064;&#38454;&#27573;&#25110;&#26080;&#38480;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#20381;&#36182;&#20110;&#23545;&#31216;&#25200;&#21160;&#65292;&#36825;&#21487;&#33021;&#22312;&#29983;&#29289;&#23398;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.07487</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25216;&#26415;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#38416;&#37322;&#24615;&#25991;&#31456;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#28201;&#21644;&#30340;&#20171;&#32461;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#20004;&#20010;&#20851;&#38190;&#28857;--&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#65292;&#20854;&#20013;&#21253;&#25324;SDE/ODE&#37319;&#26679;&#65292;&#20998;&#25968;&#21305;&#37197;&#25928;&#29575;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#30701;&#30340;&#35777;&#26126;&#26469;&#35828;&#26126;&#25152;&#36848;&#32467;&#26524;&#30340;&#20027;&#35201;&#24605;&#24819;&#12290;&#26412;&#25991;&#20027;&#35201;&#26159;&#20026;&#20102;&#21521;&#21021;&#23398;&#32773;&#20171;&#32461;&#36825;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#21457;&#29616;&#19968;&#20123;&#20998;&#26512;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;Voronoi-based&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05220</link><description>&lt;p&gt;
&#20851;&#20110;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Parameter Estimation in Deviated Gaussian Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05220
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;Voronoi-based&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#20559;&#31163;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#30001;$(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$&#29983;&#25104;&#65292;&#20854;&#20013;$X, Y$&#20998;&#21035;&#26159;&#21327;&#21464;&#37327;&#21521;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#65292;$g_{0}(Y|X)$&#26159;&#24050;&#30693;&#20989;&#25968;&#65292;$\lambda^{\ast} \in [0, 1]$&#26159;&#30495;&#23454;&#20294;&#26410;&#30693;&#30340;&#28151;&#21512;&#27604;&#20363;&#65292;$(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$&#23545;&#20110;$1 \leq i \leq k^{\ast}$&#26159;&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#35813;&#38382;&#39064;&#28304;&#33258;&#20110;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#65292;&#24403;&#25105;&#20204;&#24076;&#26395;&#26816;&#39564;&#25968;&#25454;&#26159;&#21542;&#30001;$g_{0}(Y|X)$&#65288;&#38646;&#20551;&#35774;&#65289;&#29983;&#25104;&#65292;&#36824;&#26159;&#30001;&#25972;&#20010;&#28151;&#21512;&#65288;&#22791;&#25321;&#20551;&#35774;&#65289;&#29983;&#25104;&#12290;&#22522;&#20110;&#19987;&#23478;&#20989;&#25968;&#30340;&#20195;&#25968;&#32467;&#26500;&#21644;$g_0$&#19982;&#28151;&#21512;&#37096;&#20998;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#26032;&#30340;&#22522;&#20110;Voronoi&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;c
&lt;/p&gt;
&lt;p&gt;
We consider the parameter estimation problem in the deviated Gaussian mixture of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$ for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of experts. This problem arises from the goodness-of-fit test when we would like to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or they are generated from the whole mixture (alternative hypothesis). Based on the algebraic structure of the expert functions and the distinguishability between $g_0$ and the mixture part, we construct novel Voronoi-based loss functions to capture the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.04678</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#20449;&#30340;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As Faithful Explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;&#65288;xLLM&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#37096;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24050;&#32463;&#33021;&#22815;&#29087;&#32451;&#35299;&#20915;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#20256;&#32479;&#30340;&#20197;&#36755;&#20837;&#20026;&#37325;&#28857;&#30340;&#35299;&#37322;&#31639;&#27861;&#26469;&#35299;&#37322;LLMs&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20986;&#29616;&#20102;&#19968;&#31181;&#33258;&#25105;&#35299;&#37322;&#26426;&#21046;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30340;&#24418;&#24335;&#36827;&#34892;&#21333;&#21521;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;LLMs&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#32463;&#24120;&#22240;&#20026;&#32570;&#20047;&#21487;&#20449;&#24230;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#22240;&#20026;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#20934;&#30830;&#22320;&#21453;&#26144;LLMs&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29983;&#25104;&#35299;&#37322;&#26694;&#26550;xLLM&#65292;&#20197;&#25552;&#39640;LLMs&#33258;&#28982;&#35821;&#35328;&#26684;&#24335;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#22120;&#26469;&#37327;&#21270;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#36890;&#36807;xLLM&#30340;&#36845;&#20195;&#20248;&#21270;&#36807;&#31243;&#26469;&#25552;&#39640;&#21487;&#20449;&#24230;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natural language format. However, natural language explanations are often criticized for lack of faithfulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#26435;&#37325;&#21644;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04029</link><description>&lt;p&gt;
&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Positive concave deep equilibrium models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#26435;&#37325;&#21644;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#65288;DEQ&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#20869;&#23384;&#25928;&#29575;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#27714;&#35299;&#19968;&#20010;&#22266;&#23450;&#28857;&#26041;&#31243;&#32780;&#19981;&#26159;&#26174;&#24335;&#22320;&#35745;&#31639;&#36755;&#20986;&#65292;&#20351;&#23427;&#20204;&#19982;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26377;&#25152;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DEQ&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#23545;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#30340;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#19988;&#35745;&#31639;&#22266;&#23450;&#28857;&#30340;&#25968;&#20540;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#20063;&#27809;&#26377;&#24471;&#21040;&#24418;&#24335;&#19978;&#30340;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;DEQ&#27169;&#22411;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DEQ&#27169;&#22411;&#31867;&#65292;&#31216;&#20026;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#24378;&#21046;&#26045;&#21152;&#38750;&#36127;&#26435;&#37325;&#21644;&#22312;&#27491;&#21322;&#36724;&#19978;&#26159;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#30830;&#20445;&#22266;&#23450;&#28857;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.03244</link><description>&lt;p&gt;
&#25216;&#33021;&#38598;&#20248;&#21270;&#65306;&#36890;&#36807;&#21487;&#36716;&#31227;&#25216;&#33021;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#30340;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#65292;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#65292;&#24182;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SSO&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#33021;&#22815;&#20248;&#21270;&#25216;&#33021;&#38598;&#65292;&#24182;&#23454;&#29616;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#39034;&#24207;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#29615;&#22659;&#22870;&#21169;&#20449;&#21495;&#26469;&#19981;&#26029;&#25913;&#36827;LLM&#28436;&#21592;&#30340;&#34920;&#29616;&#24182;&#19981;&#31616;&#21333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25216;&#33021;&#38598;&#20248;&#21270;&#65288;SSO&#65289;&#26469;&#36890;&#36807;&#26500;&#24314;&#21644;&#23436;&#21892;&#21487;&#36716;&#31227;&#25216;&#33021;&#38598;&#26469;&#25552;&#39640;LLM&#28436;&#21592;&#30340;&#24615;&#33021;&#12290;SSO&#36890;&#36807;&#25552;&#21462;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#20849;&#21516;&#23376;&#36712;&#36857;&#24182;&#29983;&#25104;&#23376;&#30446;&#26631;&#21644;&#35828;&#26126;&#26469;&#26500;&#24314;&#25216;&#33021;&#12290;&#36825;&#20123;&#25216;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#32473;LLM&#28436;&#21592;&#65292;&#20197;&#24378;&#21270;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;SSO&#36890;&#36807;&#20462;&#21098;&#19981;&#20877;&#20135;&#29983;&#39640;&#22870;&#21169;&#30340;&#25216;&#33021;&#26469;&#36827;&#19968;&#27493;&#23436;&#21892;&#25216;&#33021;&#38598;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#25991;&#26412;&#29615;&#22659;ScienceWorld&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#23637;&#31034;SSO&#20248;&#21270;&#25216;&#33021;&#38598;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#31574;&#30053;&#25913;&#36827;&#30340;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;NetHack&#20219;&#21153;&#20013;&#65292;SSO&#30340;&#24615;&#33021;&#36229;&#36807;&#22522;&#20934;&#26041;&#27861;40%&#65292;&#24182;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#26032;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#24378;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#26469;&#25551;&#36848;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.02952</link><description>&lt;p&gt;
&#20851;&#20110;Softmax Gating&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#20013;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Least Squares Estimation in Softmax Gating Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#24314;&#31435;&#20102;&#24378;&#21487;&#35782;&#21035;&#24615;&#26465;&#20214;&#26469;&#25551;&#36848;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#27169;&#22411;&#26159;&#19968;&#31181;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#65292;&#20351;&#29992;Softmax Gating&#20989;&#25968;&#32858;&#21512;&#22810;&#20010;&#19987;&#23478;&#32593;&#32476;&#65292;&#20197;&#24418;&#25104;&#19968;&#20010;&#26356;&#22797;&#26434;&#21644;&#34920;&#36798;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#30001;&#20110;&#21487;&#25193;&#23637;&#24615;&#32780;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;MoE&#27169;&#22411;&#30340;&#25968;&#23398;&#21644;&#32479;&#35745;&#24615;&#36136;&#22797;&#26434;&#19988;&#38590;&#20197;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#20197;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27010;&#29575;MoE&#27169;&#22411;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#25968;&#25454;&#26159;&#30001;&#39640;&#26031;MoE&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#30830;&#23450;&#24615;MoE&#27169;&#22411;&#19979;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65288;LSE&#65289;&#30340;&#24615;&#33021;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#26681;&#25454;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#37319;&#26679;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31216;&#20026;&#24378;&#21487;&#35782;&#21035;&#24615;&#30340;&#26465;&#20214;&#65292;&#20197;&#34920;&#24449;&#19981;&#21516;&#31867;&#22411;&#19987;&#23478;&#20989;&#25968;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#24378;&#21487;&#35782;&#21035;&#19987;&#23478;&#30340;&#20272;&#35745;&#36895;&#24230;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
Mixture of experts (MoE) model is a statistical machine learning design that aggregates multiple expert networks using a softmax gating function in order to form a more intricate and expressive model. Despite being commonly used in several applications owing to their scalability, the mathematical and statistical properties of MoE models are complex and difficult to analyze. As a result, previous theoretical works have primarily focused on probabilistic MoE models by imposing the impractical assumption that the data are generated from a Gaussian MoE model. In this work, we investigate the performance of the least squares estimators (LSE) under a deterministic MoE model where the data are sampled according to a regression model, a setting that has remained largely unexplored. We establish a condition called strong identifiability to characterize the convergence behavior of various types of expert functions. We demonstrate that the rates for estimating strongly identifiable experts, namel
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;LLMs&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.02834</link><description>&lt;p&gt;
&#31616;&#21270;&#30340;LLaMA: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#28145;&#24230;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Shortened LLaMA: A Simple Depth Pruning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02834
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545;&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;LLMs&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32467;&#26500;&#21270;&#20462;&#21098;&#24050;&#25104;&#20026;&#38477;&#20302;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#23485;&#24230;&#20462;&#21098;&#20943;&#23567;&#25237;&#24433;&#26435;&#37325;&#30697;&#38453;&#30340;&#22823;&#23567; (&#20363;&#22914;&#36890;&#36807;&#21024;&#38500;&#27880;&#24847;&#21147;&#22836;)&#65292;&#21516;&#26102;&#20445;&#25345;&#23618;&#25968;&#19981;&#21464;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#28145;&#24230;&#20462;&#21098;&#21017;&#21024;&#38500;&#25972;&#20010;&#23618;&#25110;&#22359;&#65292;&#21516;&#26102;&#20445;&#25345;&#21097;&#20313;&#26435;&#37325;&#30340;&#22823;&#23567;&#19981;&#21464;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#23485;&#24230;&#20462;&#21098;&#25110;&#23485;&#24230;&#21644;&#28145;&#24230;&#20462;&#21098;&#30340;&#28151;&#21512;&#19978;&#65292;&#24456;&#23569;&#23545;&#20004;&#32773; (&#23485;&#24230;&#19982;&#28145;&#24230;) &#22312;&#23545;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#26032;&#30340;&#23485;&#24230;&#20462;&#21098;&#26041;&#27861;&#22312;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#31454;&#20105;&#12290;&#25105;&#20204;&#30340;&#20462;&#21098;&#26041;&#27861;&#25552;&#39640;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#20869;&#23384;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#23545;&#36816;&#34892;LLMs&#36827;&#34892;&#26377;&#38480;&#25209;&#27425;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#27492;&#26102;&#23485;&#24230;&#20462;&#21098;&#26080;&#25928;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#24110;&#21161;&#23558;LLMs&#37096;&#32626;&#22312;&#26412;&#22320;&#21644;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured pruning of modern large language models (LLMs) has emerged as a way of decreasing their high computational needs. Width pruning reduces the size of projection weight matrices (e.g., by removing attention heads) while maintaining the number of layers. Depth pruning, in contrast, removes entire layers or blocks, while keeping the size of the remaining weights unchanged. Most current research focuses on either width-only or a blend of width and depth pruning, with little comparative analysis between the two units (width vs. depth) concerning their impact on LLM inference efficiency. In this work, we show that a simple depth pruning approach can compete with recent width pruning methods in terms of zero-shot task performance. Our pruning method boosts inference speeds, especially under memory-constrained conditions that require limited batch sizes for running LLMs, where width pruning is ineffective. We hope this work can help deploy LLMs on local and edge devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00435</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A practical existence theorem for reduced order models based on convolutional autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#38477;&#38454;&#27169;&#22411;&#30340;&#23454;&#29992;&#23384;&#22312;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#22797;&#26434;&#38750;&#32447;&#24615;&#38382;&#39064;&#26041;&#38754;&#20256;&#32479;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#21644;&#38477;&#38454;&#24314;&#27169;&#39046;&#22495;&#36234;&#21457;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#12289;&#31070;&#32463;&#31639;&#23376;&#12289;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#38477;&#38454;&#27169;&#22411;&#31561;&#24378;&#22823;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#33258;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#25928;&#26524;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#26102;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38477;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22522;&#20110;CNN&#30340;&#33258;&#32534;&#30721;&#22120;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#26550;&#26500;&#65292;&#36890;&#24120;&#20197;&#19975;&#33021;&#36924;&#36817;&#23450;&#29702;&#30340;&#24418;&#24335;&#38472;&#36848;&#12290;&#23588;&#20854;&#26159;&#65292;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20026;&#35774;&#35745;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#25351;&#23548;&#26041;&#38024;&#65292;&#20294;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#30340;&#21518;&#32493;&#25361;&#25112;&#20960;&#20046;&#27809;&#26377;&#34987;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely inv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.11963</link><description>&lt;p&gt;
&#36328;&#36234;&#36827;&#21270;&#31639;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11963
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#36827;&#21270;&#31639;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65292;&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#26412;&#32508;&#36848;&#21576;&#29616;&#20102;ERL&#39046;&#22495;&#30340;&#21508;&#20010;&#30740;&#31350;&#20998;&#25903;&#65292;&#31361;&#20986;&#20102;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#12289;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#36825;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;ERL&#65289;&#23558;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#36827;&#34892;&#20248;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36890;&#36807;&#34701;&#21512;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;ERL&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;ERL&#20013;&#19981;&#21516;&#30740;&#31350;&#20998;&#25903;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#24635;&#32467;&#20102;&#30456;&#20851;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#30830;&#23450;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65306;EA&#36741;&#21161;RL&#30340;&#20248;&#21270;&#65292;RL&#36741;&#21161;EA&#30340;&#20248;&#21270;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#21327;&#21516;&#20248;&#21270;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#20998;&#26512;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#32452;&#32455;&#20102;&#22810;&#20010;&#30740;&#31350;&#20998;&#25903;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#27599;&#20010;&#20998;&#25903;&#33268;&#21147;&#20110;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;EA&#21644;RL&#30340;&#25972;&#21512;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28508;&#22312;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00736</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#12289;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21644;&#19968;&#20999;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models, Image Super-Resolution And Everything: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00736
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20204;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#33021;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#26679;&#26412;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#39640;&#35745;&#31639;&#38656;&#27714;&#12289;&#21487;&#27604;&#24615;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#33394;&#24425;&#20559;&#31227;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#22823;&#37327;&#30340;&#20986;&#29256;&#29289;&#65292;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21465;&#36848;&#65292;&#38416;&#26126;&#20102;&#24212;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#20869;&#19982;&#20854;&#20182;&#32508;&#36848;&#25991;&#31456;&#19981;&#21516;&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#23545;DM&#30340;&#21407;&#21017;&#36827;&#34892;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#29702;&#35299;&#65292;&#24182;&#25506;&#32034;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21253;&#25324;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.15574</link><description>&lt;p&gt;
&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Rates for Switchback Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Switchback&#23454;&#39564;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#21333;&#20803;&#65288;&#20363;&#22914;&#25972;&#20010;&#31995;&#32479;&#65289;&#22312;&#20132;&#26367;&#30340;&#26102;&#38388;&#22359;&#20013;&#26292;&#38706;&#20110;&#19968;&#20010;&#38543;&#26426;&#22788;&#29702;&#65292;&#22788;&#29702;&#24182;&#34892;&#22788;&#29702;&#20102;&#36328;&#21333;&#20803;&#21644;&#26102;&#38388;&#24178;&#25200;&#38382;&#39064;&#12290;Hu&#21644;Wager&#65288;2022&#65289;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#22359;&#36215;&#22987;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;Markov&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#29992;&#20110;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;$T^{-1/3}$&#36895;&#29575;&#65292;&#20182;&#20204;&#22768;&#31216;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#24314;&#35758;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19981;&#21516;&#65288;&#19988;&#20381;&#36182;&#35774;&#35745;&#65289;&#30340;&#20272;&#35745;&#37327;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#23545;&#20110;&#30456;&#21516;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20272;&#35745;&#22120;&#65292;&#20351;&#29992;&#25972;&#20010;&#22359;&#65292;&#24182;&#24778;&#20154;&#22320;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#23454;&#38469;&#19978;&#36798;&#21040;&#20102;&#21407;&#22987;&#30340;&#35774;&#35745;&#29420;&#31435;GATE&#20272;&#35745;&#37327;&#30340;$\sqrt{\log T/T}$&#30340;&#20272;&#35745;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2312.02614</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#19978;&#19979;&#25991;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompt Optimization via Adversarial In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Adversarial In-Context Learning (adv-ICL)&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#25552;&#31034;&#20462;&#25913;&#22120;&#20043;&#38388;&#30340;&#23545;&#25239;&#23398;&#20064;&#20248;&#21270;&#25552;&#31034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Adversarial In-Context Learning&#65288;adv-ICL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;LLM&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#21478;&#19968;&#20010;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#31532;&#19977;&#20010;&#20316;&#20026;&#25552;&#31034;&#20462;&#25913;&#22120;&#65292;&#26469;&#20248;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25552;&#31034;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#23398;&#20064;&#65292;adv-ICL&#34987;&#23454;&#29616;&#20026;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#35797;&#22270;&#29983;&#25104;&#36275;&#22815;&#36924;&#30495;&#30340;&#36755;&#20986;&#20197;&#27450;&#39575;&#37492;&#21035;&#22120;&#12290; &#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#30001;&#20219;&#21153;&#35828;&#26126;&#21069;&#32512;&#21644;&#20960;&#20010;&#31034;&#20363;&#32452;&#25104;&#30340;&#36755;&#20837;&#65292;&#29983;&#25104;&#22120;&#20135;&#29983;&#19968;&#20010;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#37492;&#21035;&#22120;&#36127;&#36131;&#23558;&#29983;&#25104;&#22120;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#20998;&#31867;&#20026;&#27169;&#22411;&#29983;&#25104;&#30340;&#36824;&#26159;&#30495;&#23454;&#25968;&#25454;&#12290;&#26681;&#25454;&#37492;&#21035;&#22120;&#25439;&#22833;&#65292;&#25552;&#31034;&#20462;&#25913;&#22120;&#25552;&#20986;&#20102;&#21487;&#33021;&#23545;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#25552;&#31034;&#36827;&#34892;&#30340;&#32534;&#36753;&#65292;&#24182;&#36873;&#25321;&#26368;&#22823;&#31243;&#24230;&#25913;&#21892;&#23545;&#25239;&#25439;&#22833;&#30340;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;adv-ICL&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#20248;&#21270;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02614v2 Announce Type: replace-cross  Abstract: We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompt for in-context learning (ICL) by employing one LLM as a generator, another as a discriminator, and a third as a prompt modifier. As in traditional adversarial learning, adv-ICL is implemented as a two-player game between the generator and discriminator, where the generator tries to generate realistic enough output to fool the discriminator. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator is then tasked with classifying the generator input-output pair as model-generated or real data. Based on the discriminator loss, the prompt modifier proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that adv-ICL results in significant improvements over state-of-the-art prompt optim
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2311.14120</link><description>&lt;p&gt;
(&#28145;)&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21644;&#36870;&#26041;&#24046;&#24179;&#30452;&#20851;&#31995;&#30340;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Weight fluctuations in (deep) linear neural networks and a derivation of the inverse-variance flatness relation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14120
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#31283;&#23450;&#35757;&#32451;&#35268;&#21017;&#65292;&#24182;&#21457;&#29616;&#20102;&#26435;&#37325;&#27874;&#21160;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#29305;&#24449;&#65292;&#20854;&#20013;&#21452;&#23618;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#27874;&#21160;&#21463;&#21040;&#23618;&#38388;&#32806;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21576;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#21512;&#25104;&#39640;&#26031;&#25968;&#25454;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#36830;&#32493;&#26497;&#38480;&#20869;&#65292;&#30740;&#31350;&#20102;&#21333;&#23618;&#21644;&#21452;&#23618;&#32447;&#24615;&#27424;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#23450;&#65288;&#26411;&#24577;&#65289;&#35757;&#32451;&#35268;&#21017;&#12290;&#23545;&#20110; schwach&#27424;&#21442;&#25968;&#21270;&#21306;&#22495;&#20013;&#30340;&#21333;&#23618;&#32593;&#32476;&#65292;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35889;&#26126;&#26174;&#20559;&#31163;Hessian&#65292;&#21487;&#20197;&#24402;&#22240;&#20110;SGD&#21160;&#24577;&#30340;&#30772;&#22351;&#35814;&#32454;&#24179;&#34913;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26435;&#37325;&#27874;&#21160;&#36890;&#24120;&#26159;&#21508;&#21521;&#24322;&#24615;&#30340;&#65292;&#20294;&#21463;&#21508;&#21521;&#21516;&#24615;&#25439;&#22833;&#38480;&#21046;&#12290;&#23545;&#20110;&#21452;&#23618;&#32593;&#32476;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27599;&#23618;&#26435;&#37325;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#20851;&#30340;&#31283;&#23450;&#21327;&#26041;&#24046;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23618;&#38388;&#32806;&#21512;&#20316;&#20026;&#26435;&#37325;&#27874;&#21160;&#30340;&#21508;&#21521;&#24322;&#24615;&#30340;&#26032;&#26469;&#28304;&#12290;&#19982;&#21333;&#23618;&#24773;&#20917;&#30456;&#21453;&#65292;&#26435;&#37325;&#27874;&#21160;&#32463;&#21382;&#21508;&#21521;&#24322;&#24615;&#25439;&#22833;&#65292;&#20854;&#24179;&#30452;&#24230;&#19982;&#27874;&#21160;&#30340;&#26041;&#24046;&#25104;&#21453;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14120v2 Announce Type: replace  Abstract: We investigate the stationary (late-time) training regime of single- and two-layer linear underparameterized neural networks within the continuum limit of stochastic gradient descent (SGD) for synthetic Gaussian data. In the case of a single-layer network in the weakly underparameterized regime, the spectrum of the noise covariance matrix deviates notably from the Hessian, which can be attributed to the broken detailed balance of SGD dynamics. The weight fluctuations are in this case generally anisotropic, but are subject to an isotropic loss. For a two-layer network, we obtain the stochastic dynamics of the weights in each layer and analyze the associated stationary covariances. We identify the inter-layer coupling as a new source of anisotropy for the weight fluctuations. In contrast to the single-layer case, the weight fluctuations experience an anisotropic loss, the flatness of which is inversely related to the fluctuation varian
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;Koopman&#20808;&#39564;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#23545;&#20854;&#21160;&#24577;&#36827;&#34892;&#32447;&#24615;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.05317</link><description>&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#30340;&#31070;&#32463;Koopman&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Neural Koopman prior for data assimilation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;Koopman&#20808;&#39564;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#24471;&#23545;&#20854;&#21160;&#24577;&#36827;&#34892;&#32447;&#24615;&#25551;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#35832;&#22914;&#33258;&#21160;&#24494;&#20998;&#21644;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#31561;&#24037;&#20855;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#24207;&#36143;&#25968;&#25454;&#29616;&#22312;&#32463;&#24120;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#22788;&#29702;&#65292;&#36890;&#36807;&#20174;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#34987;&#35270;&#20026;&#19981;&#21487;&#35299;&#37322;&#30340;&#40657;&#30418;&#26550;&#26500;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21487;&#20197;&#21463;&#30410;&#20110;&#25968;&#25454;&#30340;&#29289;&#29702;&#20808;&#39564;&#21644;&#25968;&#23398;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#38271;&#26399;&#20197;&#26469;&#24050;&#30693;&#30340;Koopman&#31639;&#23376;&#29702;&#35770;&#65292;&#23558;&#21160;&#21147;&#31995;&#32479;&#23884;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#21160;&#24577;&#21487;&#20197;&#34987;&#32447;&#24615;&#25551;&#36848;&#65292;&#20174;&#32780;&#21576;&#29616;&#20986;&#35768;&#22810;&#21560;&#24341;&#20154;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#35813;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38271;&#26399;&#36830;&#32493;&#37325;&#26500;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#25968;&#25454;&#21576;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#22256;&#38590;&#24773;&#22659;&#20013;&#20063;&#21487;&#20197;&#39034;&#21033;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#25105;&#20204;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05317v2 Announce Type: replace  Abstract: With the increasing availability of large scale datasets, computational power and tools like automatic differentiation and expressive neural network architectures, sequential data are now often treated in a data-driven way, with a dynamical model trained from the observation data. While neural networks are often seen as uninterpretable black-box architectures, they can still benefit from physical priors on the data and from mathematical knowledge. In this paper, we use a neural network architecture which leverages the long-known Koopman operator theory to embed dynamical systems in latent spaces where their dynamics can be described linearly, enabling a number of appealing features. We introduce methods that enable to train such a model for long-term continuous reconstruction, even in difficult contexts where the data comes in irregularly-sampled time series. The potential for self-supervised learning is also demonstrated, as we show
&lt;/p&gt;</description></item><item><title>EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;</title><link>https://arxiv.org/abs/2308.07269</link><description>&lt;p&gt;
EasyEdit&#65306;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07269
&lt;/p&gt;
&lt;p&gt;
EasyEdit&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#25903;&#25345;&#21508;&#31181;&#26368;&#26032;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#30693;&#21517;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36973;&#21463;&#30693;&#35782;&#25130;&#26029;&#25110;&#35884;&#35823;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#26410;&#35265;&#20107;&#20214;&#19981;&#30693;&#24773;&#25110;&#29983;&#25104;&#20855;&#26377;&#19981;&#27491;&#30830;&#20107;&#23454;&#30340;&#25991;&#26412;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#36807;&#26102;/&#22024;&#26434;&#12290;&#20026;&#27492;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#38024;&#23545;LLMs&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#26088;&#22312;&#24494;&#22937;&#22320;&#27880;&#20837;/&#32534;&#36753;&#26356;&#26032;&#30340;&#30693;&#35782;&#25110;&#35843;&#25972;&#19981;&#33391;&#34892;&#20026;&#65292;&#21516;&#26102;&#23558;&#23545;&#19981;&#30456;&#20851;&#36755;&#20837;&#30340;&#24433;&#21709;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#20219;&#21153;&#35774;&#32622;&#20013;&#30340;&#21464;&#21270;&#65292;&#31038;&#21306;&#20013;&#27809;&#26377;&#21487;&#29992;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26631;&#20934;&#23454;&#26045;&#26694;&#26550;&#65292;&#36825;&#22952;&#30861;&#20102;&#20174;&#19994;&#32773;&#23558;&#30693;&#35782;&#32534;&#36753;&#24212;&#29992;&#20110;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyEdit&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;LLMs&#30693;&#35782;&#32534;&#36753;&#26694;&#26550;&#12290;&#23427;&#25903;&#25345;&#21508;&#31181;&#23574;&#31471;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#35768;&#22810;&#33879;&#21517;&#30340;LLMs&#65292;&#22914;T5&#12289;GPT-J&#12289;LlaMA&#31561;&#12290;&#20174;&#32463;&#39564;&#19978;&#26469;&#30475;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;kno
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07269v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners from applying knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily applied to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the kno
&lt;/p&gt;</description></item><item><title>&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2304.00933</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#23398;&#20064;&#30340;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#19982;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Knowledge Accumulation in Continually Learned Representations and the Issue of Feature Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.00933
&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#20102;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;&#65292;&#34920;&#26126;&#21363;&#20351;&#29305;&#24449;&#36951;&#24536;&#30340;&#32477;&#23545;&#31243;&#24230;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#20063;&#38754;&#20020;&#30528;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30740;&#31350;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#8220;&#36755;&#20986;&#32423;&#21035;&#8221;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#26377;&#20105;&#35758;&#30340;&#26159;&#26159;&#21542;&#22312;&#23398;&#20064;&#30340;&#34920;&#31034;&#32423;&#21035;&#20063;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#22810;&#20010;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#34920;&#31034;&#24402;&#22240;&#20026;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#22266;&#26377;&#25239;&#36951;&#24536;&#24615; - &#20165;&#20250;&#26368;&#23567;&#31243;&#24230;&#24536;&#35760;&#19988;&#19981;&#20250;&#36951;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#24182;&#25193;&#23637;&#20102;&#25581;&#31034;&#36825;&#31181;&#36951;&#24536;&#24046;&#24322;&#30340;&#23454;&#39564;&#65292;&#35828;&#26126;&#20102;&#24433;&#21709;&#25345;&#32493;&#23398;&#20064;&#34920;&#31034;&#36136;&#37327;&#30340;&#20004;&#31181;&#29616;&#35937;&#20849;&#23384;&#65306;&#30693;&#35782;&#31215;&#32047;&#21644;&#29305;&#24449;&#36951;&#24536;&#12290;&#25105;&#20204;&#35880;&#24910;&#32771;&#34385;&#20102;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#34920;&#26126;&#23613;&#31649;&#32477;&#23545;&#20540;&#19978;&#29305;&#24449;&#36951;&#24536;&#21487;&#33021;&#36739;&#23567;&#65292;&#26032;&#23398;&#20064;&#30340;&#20449;&#24687;&#22312;&#34920;&#31034;&#23618;&#38754;&#19982;&#36755;&#20986;&#23618;&#38754;&#19968;&#26679;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29305;&#24449;&#36951;&#24536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.00933v3 Announce Type: replace  Abstract: Continual learning research has shown that neural networks suffer from catastrophic forgetting "at the output level", but it is debated whether this is also the case at the level of learned representations. Multiple recent studies ascribe representations a certain level of innate robustness against forgetting - that they only forget minimally and no critical information. We revisit and expand upon the experiments that revealed this difference in forgetting and illustrate the coexistence of two phenomena that affect the quality of continually learned representations: knowledge accumulation and feature forgetting. Carefully taking both aspects into account, we show that, even though it is true that feature forgetting can be small in absolute terms, newly learned information tends to be forgotten just as catastrophically at the level of the representation as it is at the output level. Next we show that this feature forgetting is problem
&lt;/p&gt;</description></item><item><title>OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;</title><link>http://arxiv.org/abs/2401.16445</link><description>&lt;p&gt;
OMPGPT: &#19968;&#31181;&#29992;&#20110;OpenMP&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16445
&lt;/p&gt;
&lt;p&gt;
OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#38543;&#30528;&#36825;&#19968;&#36235;&#21183;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;StarCoder&#12289;WizardCoder&#21644;CodeLlama&#31561;&#65292;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#65292;&#22312;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#22266;&#26377;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#27880;&#37322;&#29983;&#25104;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#21450;&#23545;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#19968;&#33324;&#25903;&#25345;&#12290;&#34429;&#28982;&#20195;&#30721;LLMs&#30340;&#36890;&#29992;&#33021;&#21147;&#23545;&#35768;&#22810;&#31243;&#24207;&#21592;&#26469;&#35828;&#24456;&#26377;&#29992;&#65292;&#20294;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20855;&#26377;&#26356;&#31364;&#30340;&#38656;&#27714;&#38598;&#65292;&#20351;&#24471;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#30340;LM&#25104;&#20026;&#19968;&#20010;&#26356;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OMPGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;OpenMP pragma&#29983;&#25104;&#26041;&#38754;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24182;&#25913;&#36827;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#38142;&#24335;OMP&#65288;chain-of-OMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2401.15062</link><description>&lt;p&gt;
&#19987;&#23478;&#19982;&#32858;&#31867;&#65306;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Expert with Clustering: Hierarchical Online Preference Learning Framework. (arXiv:2401.15062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#22312;&#32447;&#20559;&#22909;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#31227;&#21160;&#31995;&#32479;&#36234;&#26469;&#36234;&#33021;&#22815;&#21521;&#31227;&#21160;&#29992;&#25143;&#25512;&#33616;&#36873;&#39033;&#65292;&#20197;&#24341;&#23548;&#20182;&#20204;&#26397;&#21521;&#20010;&#24615;&#21270;&#20294;&#21487;&#25345;&#32493;&#30340;&#31995;&#32479;&#32467;&#26524;&#12290;&#19982;&#20856;&#22411;&#30340;&#25512;&#33616;&#31995;&#32479;&#30456;&#27604;&#65292;&#26368;&#23567;&#21270;&#21518;&#24724;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;1&#65289;&#31227;&#21160;&#36873;&#39033;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#30340;&#29983;&#27963;&#65292;2&#65289;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#20381;&#36182;&#20110;&#36275;&#22815;&#30340;&#29992;&#25143;&#21442;&#19982;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#21033;&#29992;&#25429;&#25417;&#29992;&#25143;&#31227;&#21160;&#20559;&#22909;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#26469;&#21152;&#36895;&#29992;&#25143;&#20559;&#22909;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Expert with Clustering (EWC)&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;Bandit&#26694;&#26550;&#65292;&#23427;&#38598;&#25104;&#20102;&#32858;&#31867;&#25216;&#26415;&#21644;&#19987;&#23478;&#24314;&#35758;&#30340;&#39044;&#27979;&#12290;EWC&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23618;&#29992;&#25143;&#20449;&#24687;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#24341;&#23548;&#36317;&#31163;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#22312;&#29983;&#25104;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#32858;&#31867;&#20013;&#24515;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#27599;&#20010;&#29992;&#25143;$T$&#36718;&#65292;$N$&#29992;&#25143;&#21644;$K$&#36873;&#39033;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26681;&#25454;&#29992;&#25143;&#30340;&#23454;&#26102;&#21453;&#39304;&#26469;&#22312;&#32447;&#23398;&#20064;&#21644;&#25913;&#36827;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging mobility systems are increasingly capable of recommending options to mobility users, to guide them towards personalized yet sustainable system outcomes. Even more so than the typical recommendation system, it is crucial to minimize regret, because 1) the mobility options directly affect the lives of the users, and 2) the system sustainability relies on sufficient user participation. In this study, we consider accelerating user preference learning by exploiting a low-dimensional latent space that captures the mobility preferences of users. We introduce a hierarchical contextual bandit framework named Expert with Clustering (EWC), which integrates clustering techniques and prediction with expert advice. EWC efficiently utilizes hierarchical user information and incorporates a novel Loss-guided Distance metric. This metric is instrumental in generating more representative cluster centroids. In a recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options, our alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#23545;Softmax&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#30340;&#37319;&#26679;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#30001;&#20110;&#28201;&#24230;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#24182;&#19988;&#21487;&#33021;&#24456;&#24930;&#12290;</title><link>http://arxiv.org/abs/2401.13875</link><description>&lt;p&gt;
&#28201;&#24230;&#23545;Softmax&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#26159;&#21542;&#20855;&#26377;&#37319;&#26679;&#25928;&#29575;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?. (arXiv:2401.13875v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#23545;Softmax&#39640;&#26031;&#28151;&#21512;&#19987;&#23478;&#30340;&#37319;&#26679;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#30001;&#20110;&#28201;&#24230;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#24182;&#19988;&#21487;&#33021;&#24456;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23494;&#38598;-&#31232;&#30095;&#38376;&#25511;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#24050;&#25104;&#20026;&#24191;&#20026;&#20351;&#29992;&#30340;&#31232;&#30095;MoE&#30340;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#19982;&#21518;&#32773;&#27169;&#22411;&#20013;&#22266;&#23450;&#28608;&#27963;&#30340;&#19987;&#23478;&#25968;&#37327;&#19981;&#21516;&#65292;&#21069;&#32773;&#27169;&#22411;&#21033;&#29992;&#28201;&#24230;&#26469;&#25511;&#21046;softmax&#26435;&#37325;&#20998;&#24067;&#21644;MoE&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31283;&#23450;&#19987;&#23478;&#30340;&#19987;&#19994;&#21270;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20197;&#21069;&#26377;&#23581;&#35797;&#20174;&#29702;&#35770;&#19978;&#29702;&#35299;&#31232;&#30095;MoE&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#23494;&#38598;&#21040;&#31232;&#30095;&#38376;&#25511;MoE&#30340;&#20840;&#38754;&#20998;&#26512;&#20173;&#28982;&#22256;&#38590;&#37325;&#37325;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#23494;&#38598;&#21040;&#31232;&#30095;&#38376;&#25511;&#23545;Gaussian MoE&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#28201;&#24230;&#21644;&#20854;&#20182;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#36890;&#36807;&#19968;&#20123;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#27604;&#20219;&#20309;&#22810;&#39033;&#24335;&#36895;&#29575;&#37117;&#35201;&#24930;&#65292;&#24182;&#19988;&#21487;&#33021;&#24930;&#21040;$\mathcal{
&lt;/p&gt;
&lt;p&gt;
Dense-to-sparse gating mixture of experts (MoE) has recently become an effective alternative to a well-known sparse MoE. Rather than fixing the number of activated experts as in the latter model, which could limit the investigation of potential experts, the former model utilizes the temperature to control the softmax weight distribution and the sparsity of the MoE during training in order to stabilize the expert specialization. Nevertheless, while there are previous attempts to theoretically comprehend the sparse MoE, a comprehensive analysis of the dense-to-sparse gating MoE has remained elusive. Therefore, we aim to explore the impacts of the dense-to-sparse gate on the maximum likelihood estimation under the Gaussian MoE in this paper. We demonstrate that due to interactions between the temperature and other model parameters via some partial differential equations, the convergence rates of parameter estimations are slower than any polynomial rates, and could be as slow as $\mathcal{
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.05831</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36718;&#24275;&#31995;&#25968;&#65306;&#20174;&#24494;&#35266;&#21040;&#23439;&#35266;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36718;&#24275;&#31995;&#25968;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#20250;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20135;&#29983;&#19968;&#20010;&#24471;&#20998;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#32858;&#31867;&#20998;&#37197;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#20026;&#20102;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#36136;&#37327;&#65292;&#36890;&#24120;&#20250;&#23558;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#28857;&#30340;&#24471;&#20998;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#31216;&#20026;&#24494;&#35266;&#24179;&#22343;&#12290;&#28982;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#20363;&#23376;&#23637;&#31034;&#20102;&#65292;&#35813;&#24494;&#35266;&#24179;&#22343;&#31574;&#30053;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#24322;&#24120;&#20540;&#65288;&#32972;&#26223;&#22122;&#22768;&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#32858;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#28982;&#21518;&#20877;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#12290;&#22522;&#20110;&#30456;&#21516;&#30340;&#21512;&#25104;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#21464;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17638</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Fractional Diffusion Models. (arXiv:2310.17638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;FBM&#65289;&#30340;&#36830;&#32493;&#26102;&#38388;&#26694;&#26550;&#25512;&#24191;&#21040;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#30340;&#36817;&#20284;&#24418;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;FBM&#34920;&#31034;&#20026;&#23478;&#26063;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#25512;&#23548;&#20986;&#36830;&#32493;&#20877;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#36870;&#26102;&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;&#20855;&#26377;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#65288;GFDM&#65289;&#12290;FBM&#30340;&#36203;&#26031;&#29305;&#25351;&#25968;$H \in (0,1)$ &#21487;&#20197;&#25511;&#21046;&#36335;&#24452;&#21464;&#25442;&#20998;&#24067;&#30340;&#31895;&#31961;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the continuous time framework for score-based generative models from an underlying Brownian motion (BM) to an approximation of fractional Brownian motion (FBM). We derive a continuous reparameterization trick and the reverse time model by representing FBM as a stochastic integral over a family of Ornstein-Uhlenbeck processes to define generative fractional diffusion models (GFDM) with driving noise converging to a non-Markovian process of infinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enables control of the roughness of the distribution transforming path. To the best of our knowledge, this is the first attempt to build a generative model upon a stochastic process with infinite quadratic variation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;Softmax Gating Multinomial Logistic Mixture of Experts&#27169;&#22411;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25581;&#31034;&#20102;softmax gating&#21644;&#19987;&#23478;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#30340;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;softmax gating&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.14188</link><description>&lt;p&gt;
&#19968;&#31181;Softmax Gating Multinomial Logistic Mixture of Experts&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts. (arXiv:2310.14188v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#30740;&#31350;Softmax Gating Multinomial Logistic Mixture of Experts&#27169;&#22411;&#12290;&#36890;&#36807;&#24314;&#31435;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25581;&#31034;&#20102;softmax gating&#21644;&#19987;&#23478;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#30340;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;softmax gating&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts&#65288;MoE&#65289;&#27169;&#22411;&#36890;&#36807;&#38376;&#25511;&#20989;&#25968;&#23558;&#22810;&#20010;&#23376;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#35768;&#22810;&#22238;&#24402;&#21644;&#20998;&#31867;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#34429;&#28982;&#20043;&#21069;&#24050;&#32463;&#23581;&#35797;&#36890;&#36807;&#39640;&#26031;MoE&#27169;&#22411;&#20013;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#26469;&#29702;&#35299;&#35813;&#27169;&#22411;&#22312;&#22238;&#24402;&#35774;&#32622;&#19979;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#35774;&#32622;&#19979;&#32570;&#20047;&#30456;&#20851;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;softmax gating multinomial logistic MoE&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21644;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#36895;&#24230;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#37096;&#20998;&#19987;&#23478;&#21442;&#25968;&#28040;&#22833;&#26102;&#65292;&#30001;&#20110;softmax gating&#21644;&#19987;&#23478;&#20989;&#25968;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#25910;&#25947;&#36895;&#24230;&#27604;&#22810;&#39033;&#24335;&#36895;&#24230;&#26356;&#24930;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20462;&#25913;softmax gating&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) model incorporates the power of multiple submodels via gating functions to achieve greater performance in numerous regression and classification applications. From a theoretical perspective, while there have been previous attempts to comprehend the behavior of that model under the regression settings through the convergence analysis of maximum likelihood estimation in the Gaussian MoE model, such analysis under the setting of a classification problem has remained missing in the literature. We close this gap by establishing the convergence rates of density estimation and parameter estimation in the softmax gating multinomial logistic MoE model. Notably, when part of the expert parameters vanish, these rates are shown to be slower than polynomial rates owing to an inherent interaction between the softmax gating and expert functions via partial differential equations. To address this issue, we propose using a novel class of modified softmax gating functions which 
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#24179;&#22343;&#20540;&#30340;&#33218;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#20915;&#31574;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.13393</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Optimal Best Arm Identification with Fixed Confidence in Restless Bandits. (arXiv:2310.13393v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13393
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#22266;&#23450;&#32622;&#20449;&#24230;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26469;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#24179;&#22343;&#20540;&#30340;&#33218;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#20915;&#31574;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25968;&#30446;&#33218;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#20197;&#19981;&#26029;&#21464;&#21270;&#30340;&#24418;&#24335;&#36827;&#34892;&#26368;&#20248;&#33218;&#35782;&#21035;&#12290;&#27599;&#20010;&#33218;&#20135;&#29983;&#30340;&#31163;&#25955;&#26102;&#38388;&#25968;&#25454;&#24418;&#25104;&#20102;&#19968;&#20010;&#21462;&#20540;&#22312;&#20849;&#21516;&#12289;&#26377;&#38480;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21516;&#36136;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290;&#27599;&#20010;&#33218;&#30340;&#29366;&#24577;&#36716;&#31227;&#30001;&#19968;&#20010;&#36981;&#24490;&#21333;&#21442;&#25968;&#25351;&#25968;&#26063;&#30340;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65288;TPM&#65289;&#25429;&#33719;&#12290;&#27599;&#20010;&#33218;&#30340;TPM&#30340;&#23454;&#20540;&#21442;&#25968;&#26159;&#26410;&#30693;&#30340;&#65292;&#23646;&#20110;&#32473;&#23450;&#31354;&#38388;&#12290;&#32473;&#23450;&#22312;&#33218;&#30340;&#20849;&#21516;&#29366;&#24577;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;f&#65292;&#30446;&#26631;&#26159;&#22312;&#26679;&#26412;&#25968;&#26368;&#23569;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#26368;&#20248;&#33218;&#65292;&#21363;&#22312;&#35813;&#33218;&#30340;&#31283;&#24577;&#20998;&#24067;&#19979;&#35780;&#20272;f&#30340;&#24179;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#20915;&#31574;&#38169;&#35823;&#27010;&#29575;&#65288;&#21363;&#22266;&#23450;&#32622;&#20449;&#24230;&#21306;&#38388;&#65289;&#30340;&#19978;&#30028;&#12290;&#22312;&#28176;&#36827;&#24615;&#30340;&#35823;&#24046;&#27010;&#29575;&#36235;&#20110;&#38646;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#31435;&#20102;&#26399;&#26395;&#20572;&#27490;&#26102;&#38388;&#22686;&#38271;&#29575;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#33218;&#35782;&#21035;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification in a restless multi-armed bandit setting with finitely many arms. The discrete-time data generated by each arm forms a homogeneous Markov chain taking values in a common, finite state space. The state transitions in each arm are captured by an ergodic transition probability matrix (TPM) that is a member of a single-parameter exponential family of TPMs. The real-valued parameters of the arm TPMs are unknown and belong to a given space. Given a function $f$ defined on the common state space of the arms, the goal is to identify the best arm -- the arm with the largest average value of $f$ evaluated under the arm's stationary distribution -- with the fewest number of samples, subject to an upper bound on the decision's error probability (i.e., the fixed-confidence regime). A lower bound on the growth rate of the expected stopping time is established in the asymptote of a vanishing error probability. Furthermore, a policy for best arm identification is propo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06312</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Discovering Mixtures of Structural Causal Models from Time Series Data. (arXiv:2310.06312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06312
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#21457;&#29616;&#28151;&#21512;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#27010;&#29575;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#12289;&#27668;&#20505;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#29616;&#20195;&#25216;&#26415;&#21487;&#20197;&#22788;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#21644;&#28789;&#27963;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21270;&#20551;&#35774;&#65292;&#21363;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#20174;&#26469;&#28304;&#20110;&#19981;&#21516;&#22240;&#26524;&#27169;&#22411;&#28151;&#21512;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#12290;&#25105;&#20204;&#25512;&#26029;&#20102;&#28508;&#22312;&#30340;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#21450;&#27599;&#20010;&#26679;&#26412;&#23646;&#20110;&#29305;&#23450;&#28151;&#21512;&#25104;&#20998;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#26368;&#22823;&#21270;&#20102;&#25968;&#25454;&#20284;&#28982;&#30340;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In fields such as finance, climate science, and neuroscience, inferring causal relationships from time series data poses a formidable challenge. While contemporary techniques can handle nonlinear relationships between variables and flexible noise distributions, they rely on the simplifying assumption that data originates from the same underlying causal model. In this work, we relax this assumption and perform causal discovery from time series data originating from mixtures of different causal models. We infer both the underlying structural causal models and the posterior probability for each sample belonging to a specific mixture component. Our approach employs an end-to-end training process that maximizes an evidence-lower bound for data likelihood. Through extensive experimentation on both synthetic and real-world datasets, we demonstrate that our method surpasses state-of-the-art benchmarks in causal discovery tasks, particularly when the data emanates from diverse underlying causal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02905</link><description>&lt;p&gt;
&#20351;&#29992;&#24744;&#30340;&#26412;&#33021;&#65306;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#19982;&#36716;&#25442;&#22120;&#36827;&#34892;&#25351;&#20196;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Use Your INSTINCT: INSTruction optimization usIng Neural bandits Coupled with Transformers. (arXiv:2310.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02905
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#25506;&#27979;&#22120;&#21644;&#36716;&#25442;&#22120;&#20248;&#21270;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#32473;&#20104;&#23427;&#20204;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#20248;&#21270;&#32473;&#20104;&#40657;&#30418;LLMs&#30340;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#39640;&#24230;&#22797;&#26434;&#65288;&#20363;&#22914;&#39640;&#32500;&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#22914;&#23558;&#25351;&#20196;&#26144;&#23556;&#21040;LLM&#24615;&#33021;&#30340;&#20989;&#25968;&#65292;BO&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;BO&#20351;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20316;BO&#30340;&#20195;&#29702;&#26469;&#24314;&#27169;&#30446;&#26631;&#20989;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#22810;&#27425;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#23588;&#20854;&#26159;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#65292;&#20855;&#26377;&#24456;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21487;&#20197;&#24314;&#27169;&#39640;&#24230;&#22797;&#26434;&#30340;&#20989;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31070;&#32463;&#25506;&#27979;&#22120;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) model which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algor
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#20102;&#22522;&#20110;Lipschitz&#30340;&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;...</title><link>http://arxiv.org/abs/2310.02513</link><description>&lt;p&gt;
&#25913;&#36827;&#21487;&#39564;&#35777;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#65306;&#23481;&#37327;&#21644;&#25968;&#25454;&#30340;&#37197;&#26041;
&lt;/p&gt;
&lt;p&gt;
A Recipe for Improved Certifiable Robustness: Capacity and Data. (arXiv:2310.02513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02513
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#20102;&#22522;&#20110;Lipschitz&#30340;&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#29366;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#21644;&#23454;&#36341;&#37117;&#25903;&#25345;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#31283;&#20581;&#24615;&#35201;&#27714;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22823;&#30340;&#32593;&#32476;&#23481;&#37327;&#21644;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#20005;&#26684;&#30340;Lipschitz&#32422;&#26463;&#19979;&#26377;&#25928;&#22320;&#22686;&#21152;&#23481;&#37327;&#27604;&#30475;&#36215;&#26469;&#26356;&#22256;&#38590;&#65292;&#36825;&#34920;&#26126;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#20542;&#21521;&#20110;&#20302;&#25311;&#21512;&#32780;&#19981;&#26159;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20027;&#24352;&#23545;&#22522;&#20110;Lipshitz&#30340;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#36827;&#34892;&#20180;&#32454;&#25506;&#32034;&#19981;&#36275;&#65292;&#36825;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#24615;&#33021;&#25552;&#21319;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31995;&#21015;&#26032;&#25216;&#26415;&#12289;&#35774;&#35745;&#20248;&#21270;&#21644;&#32508;&#21512;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;Lipschitz-based&#35748;&#35777;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30830;&#23450;&#24615;&#35748;&#35777;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#8220;&#39564;&#35777;&#31283;&#20581;&#20934;&#30830;&#24615;&#8221;&#65288;VRA&#65289;&#65292;&#24182;&#35206;&#30422;&#19968;&#31995;&#21015;&#25200;&#21160;&#22823;&#23567;&#12290;&#29305;&#21035;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards \emph{underfitting} than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art \emph{verified robust accuracy} (VRA) for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.16014</link><description>&lt;p&gt;
&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#36827;&#34892;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#26368;&#36817;&#20986;&#29616;&#12290;&#23427;&#20204;&#26088;&#22312;&#36890;&#36807;&#20174;&#19978;&#19979;&#25991;&#20449;&#21495;x&#20013;&#39044;&#27979;&#30446;&#26631;&#20449;&#21495;y&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#12290;JEPAs&#32469;&#36807;&#20102;&#23545;&#25968;&#25454;&#22686;&#24378;&#21644;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#65292;&#36825;&#36890;&#24120;&#26159;&#23545;&#27604;&#23398;&#20064;&#25152;&#35201;&#27714;&#30340;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#19982;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#22270;&#32423;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;Graph-JEPA&#65292;&#36825;&#26159;&#22270;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;JEPA&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#36171;&#20104;&#34920;&#31034;&#38544;&#21547;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#26159;&#39044;&#27979;&#32534;&#30721;&#23376;&#22270;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14518</link><description>&lt;p&gt;
Detach-ROCKET: &#22522;&#20110;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Detach-ROCKET&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#39034;&#24207;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#26041;&#27861;&#21098;&#26525;&#38750;&#20027;&#35201;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22914;&#21307;&#23398;&#12289;&#37329;&#34701;&#12289;&#29615;&#22659;&#31185;&#23398;&#21644;&#21046;&#36896;&#19994;&#65292;&#21487;&#20197;&#23454;&#29616;&#30142;&#30149;&#35786;&#26029;&#12289;&#24322;&#24120;&#26816;&#27979;&#21644;&#32929;&#20215;&#39044;&#27979;&#31561;&#20219;&#21153;&#12290;&#23613;&#31649;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;InceptionTime&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#38656;&#27714;&#30340;&#32321;&#37325;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;Rocket&#21450;&#20854;&#34893;&#29983;&#27169;&#22411;&#31561;&#38543;&#26426;&#21367;&#31215;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#38543;&#26426;&#29983;&#25104;&#30340;&#22823;&#37327;&#29305;&#24449;&#65292;&#31616;&#21270;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#38543;&#26426;&#24615;&#36136;&#65292;&#29983;&#25104;&#30340;&#22823;&#37096;&#20998;&#29305;&#24449;&#26159;&#20887;&#20313;&#25110;&#38750;&#20449;&#24687;&#24615;&#30340;&#65292;&#22686;&#21152;&#20102;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#36127;&#36733;&#24182;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39034;&#24207;&#29305;&#24449;&#20998;&#31163;&#65288;SFD&#65289;&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#21644;&#20462;&#21098;&#36825;&#20123;&#38750;&#20027;&#35201;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;SFD&#21033;&#29992;&#27169;&#22411;&#31995;&#25968;&#26469;&#20272;&#35745;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is essential in many fields, such as medicine, finance, environmental science, and manufacturing, enabling tasks like disease diagnosis, anomaly detection, and stock price prediction. Machine learning models like Recurrent Neural Networks and InceptionTime, while successful in numerous applications, can face scalability limitations due to intensive training requirements. To address this, random convolutional kernel models such as Rocket and its derivatives have emerged, simplifying training and achieving state-of-the-art performance by utilizing a large number of randomly generated features from time series data. However, due to their random nature, most of the generated features are redundant or non-informative, adding unnecessary computational load and compromising generalization. Here, we introduce Sequential Feature Detachment (SFD) as a method to identify and prune these non-essential features. SFD uses model coefficients to estimate feature importance a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;</title><link>http://arxiv.org/abs/2309.13160</link><description>&lt;p&gt;
GAMIX-VAE: &#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE
&lt;/p&gt;
&lt;p&gt;
GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#21518;&#39564;&#30340;VAE&#26041;&#27861;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;ELBO&#65292;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21644;PatchGAN&#37492;&#21035;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#29983;&#25104;&#24314;&#27169;&#21644;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;VAEs&#30340;&#19968;&#20010;&#32454;&#24494;&#26041;&#38754;&#65292;&#37325;&#28857;&#26159;&#35299;&#37322;KL Divergence&#65292;&#36825;&#26159;Evidence Lower Bound&#65288;ELBO&#65289;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25511;&#21046;&#20102;&#37325;&#26500;&#20934;&#30830;&#24615;&#21644;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#34429;&#28982;KL Divergence&#35753;&#28508;&#21464;&#37327;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#65292;&#32473;&#25972;&#20010;&#28508;&#31354;&#38388;&#21152;&#19978;&#32467;&#26500;&#32422;&#26463;&#65292;&#20294;&#21364;&#19981;&#38480;&#21046;&#21508;&#20010;&#21464;&#37327;&#20998;&#24067;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#24102;&#26377;&#39640;&#26031;&#28151;&#21512;&#30340;&#21518;&#39564;&#27010;&#29575;&#30340;ELBO&#65292;&#24341;&#20837;&#20102;&#27491;&#21017;&#21270;&#39033;&#20197;&#38450;&#27490;&#26041;&#24046;&#23849;&#28291;&#65292;&#24182;&#20351;&#29992;PatchGAN&#37492;&#21035;&#22120;&#26469;&#22686;&#24378;&#32441;&#29702;&#36924;&#30495;&#24230;&#12290;&#23454;&#29616;&#32454;&#33410;&#28041;&#21450;Encoder&#21644;Decoder&#30340;ResNetV2&#26550;&#26500;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#30340;&#33021;&#21147;&#65292;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
&lt;/p&gt;</description></item><item><title>ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.13007</link><description>&lt;p&gt;
ReConcile&#65306;&#22278;&#26700;&#20250;&#35758;&#36890;&#36807;&#22810;&#20803;LLM&#30340;&#20849;&#35782;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. (arXiv:2309.13007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13007
&lt;/p&gt;
&lt;p&gt;
ReConcile&#26159;&#19968;&#20010;&#36890;&#36807;&#22810;&#36718;&#35752;&#35770;&#21644;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#33021;&#21147;&#30340;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#21463;&#21040;&#24515;&#26234;&#31038;&#20250;&#29702;&#35770;&#65288;Minsky, 1988&#65289;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReConcile&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#22411;&#22810;&#20195;&#29702;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22810;&#26679;&#30340;LLM&#20195;&#29702;&#20154;&#20043;&#38388;&#30340;&#22278;&#26700;&#20250;&#35758;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#24605;&#24819;&#21644;&#35752;&#35770;&#65292;&#20174;&#32780;&#25913;&#36827;&#19968;&#33268;&#24615;&#12290;ReConcile&#36890;&#36807;&#36827;&#34892;&#22810;&#36718;&#35752;&#35770;&#12289;&#23398;&#20064;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#25913;&#36827;&#31572;&#26696;&#20197;&#21450;&#37319;&#29992;&#32622;&#20449;&#24230;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;ReConcile&#36890;&#36807;&#8220;&#35752;&#35770;&#25552;&#31034;&#8221;&#26469;&#21551;&#21160;&#20195;&#29702;&#20154;&#38388;&#30340;&#35752;&#35770;&#65292;&#20854;&#20013;&#21253;&#25324;&#19978;&#19968;&#36718;&#27599;&#20010;&#20195;&#29702;&#20154;&#29983;&#25104;&#30340;&#31572;&#26696;&#21644;&#35299;&#37322;&#30340;&#20998;&#32452;&#12289;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#29992;&#20110;&#35828;&#26381;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#31572;&#26696;&#20462;&#27491;&#20154;&#31867;&#35299;&#37322;&#30340;&#28436;&#31034;&#12290;&#36825;&#20010;&#35752;&#35770;&#25552;&#31034;&#20351;&#27599;&#20010;&#20195;&#29702;&#20154;&#33021;&#22815;&#26681;&#25454;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#35265;&#35299;&#20462;&#35746;&#33258;&#24049;&#30340;&#22238;&#31572;&#12290;&#19968;&#26086;&#36798;&#25104;&#19968;&#33268;&#24182;&#32467;&#26463;&#35752;&#35770;&#65292;ReConcile&#25191;&#34892;&#19968;&#27425;&#20840;&#20307;&#25237;&#31080;&#20197;&#30830;&#23450;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) still struggle with complex reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents to foster diverse thoughts and discussion for improved consensus. ReConcile enhances the reasoning capabilities of LLMs by holding multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their uncertainties, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. This discussion prompt enables each agent to revise their responses in light of insights from other agents. Once a consensus is reached and the discussion ends, ReConcil
&lt;/p&gt;</description></item><item><title>LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10952</link><description>&lt;p&gt;
LMDX&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10952
&lt;/p&gt;
&lt;p&gt;
LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#35768;&#22810;&#29616;&#26377;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#20110;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#35768;&#22810;&#25991;&#26723;&#22788;&#29702;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#65292;&#32473;&#23450;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#27169;&#24335;&#12290;LLM&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;LLM&#20013;&#32570;&#20047;&#24067;&#23616;&#32534;&#30721;&#65292;&#36825;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25552;&#21462;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32570;&#20047;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#26426;&#21046;&#65292;&#30830;&#20445;&#31572;&#26696;&#19981;&#26159;&#34394;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#65288;LMDX&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;LLM&#36866;&#24212;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#12290;LMDX&#21487;&#20197;&#25552;&#21462;&#21333;&#19968;&#12289;&#37325;&#22797;&#21644;&#23618;&#27425;&#32467;&#26500;&#23454;&#20307;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#29702;&#35770;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20351;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#30340;&#35266;&#27979;&#36164;&#26009;&#65292;&#26469;&#35299;&#37322;&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#12290;&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.08195</link><description>&lt;p&gt;
&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep-learning Model of Proton Auroras on Mars. (arXiv:2309.08195v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20351;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#30340;&#35266;&#27979;&#36164;&#26009;&#65292;&#26469;&#35299;&#37322;&#28779;&#26143;&#36136;&#23376;&#26497;&#20809;&#12290;&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#26143;&#30333;&#22825;&#20391;&#24191;&#27867;&#35266;&#23519;&#21040;&#36136;&#23376;&#26497;&#20809;&#65292;&#34987;&#35748;&#20026;&#26159;&#27682; Ly alpha (121.6 nm) &#36752;&#23556;&#22312;120&#33267;150&#20844;&#37324;&#39640;&#24230;&#20043;&#38388;&#30340;&#26174;&#33879;&#22686;&#24378;&#12290;&#22826;&#38451;&#39118;&#36136;&#23376;&#20316;&#20026;&#39640;&#33021;&#20013;&#24615;&#21407;&#23376;&#31359;&#36807;&#28779;&#26143;&#28909;&#23618;&#36827;&#20837;&#22823;&#27668;&#23618;&#65292;&#34987;&#35748;&#20026;&#26159;&#36136;&#23376;&#26497;&#20809;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36136;&#23376;&#26497;&#20809;&#23545;&#20110;&#25551;&#32472;&#22826;&#38451;&#39118;&#19982;&#28779;&#26143;&#22823;&#27668;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#35266;&#27979;&#21040;&#23616;&#37096;"&#26001;&#22359;&#29366;"&#36136;&#23376;&#26497;&#20809;&#65292;&#26263;&#31034;&#22312;&#19981;&#31283;&#23450;&#30340;&#22826;&#38451;&#39118;&#26465;&#20214;&#19979;&#65292;&#36136;&#23376;&#21487;&#33021;&#30452;&#25509;&#27785;&#31215;&#21040;&#28779;&#26143;&#22823;&#27668;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#28779;&#26143;&#22823;&#27668;&#21644;&#25381;&#21457;&#29289;&#28436;&#21270; (MAVEN) &#38750;&#29616;&#22330;&#35266;&#27979;&#21644;&#36793;&#32536;&#25195;&#25551;&#30340; Ly alpha &#36752;&#23556;&#36164;&#26009;&#24320;&#21457;&#20102;&#19968;&#20010;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26469;&#27169;&#25311;&#36136;&#23376;&#26497;&#20809;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20197;0.95&#30340;Pearson&#30456;&#20851;&#24615;&#37325;&#29616;&#27599;&#20010;Ly alpha&#36752;&#23556;&#30340;&#24378;&#24230;&#65292;&#24182;&#23545;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#24544;&#23454;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proton auroras are widely observed on the day side of Mars, identified as a significant intensity enhancement in the hydrogen Ly alpha (121.6 nm) emission between 120 and 150~km altitudes. Solar wind protons penetrating as energetic neutral atoms into the Martian thermosphere are thought to be responsible for these auroras. Understanding proton auroras is therefore important for characterizing the solar wind interaction with the atmosphere of Mars. Recent observations of spatially localized "patchy" proton auroras suggest a possible direct deposition of protons into the atmosphere of Mars during unstable solar wind conditions. Here, we develop a purely data-driven model of proton auroras using Mars Atmosphere and Volatile EvolutioN (MAVEN) in situ observations and limb scans of Ly alpha emissions between 2014 and 2022. We train an artificial neural network that reproduces individual Ly alpha intensities with a Pearson correlation of 0.95 along with a faithful reconstruction of the obse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#23398;&#20064;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#22312;&#22810;&#27425;&#24212;&#29992;&#26102;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07899</link><description>&lt;p&gt;
&#25913;&#36827;&#20855;&#26377;&#30828;&#32422;&#26463;&#30340;&#29289;&#29702;&#20449;&#24687;DeepONets
&lt;/p&gt;
&lt;p&gt;
Improving physics-informed DeepONets with hard constraints. (arXiv:2309.07899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#23398;&#20064;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#22312;&#22810;&#27425;&#24212;&#29992;&#26102;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;&#26631;&#20934;&#25110;&#25805;&#20316;&#31526;&#65289;&#20173;&#28982;&#20381;&#36182;&#20110;&#20934;&#30830;&#22320;&#23398;&#20064;&#25152;&#35299;&#20915;&#31995;&#32479;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#36825;&#20123;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#28436;&#21270;&#36825;&#20123;&#21021;&#22987;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25913;&#36827;&#24403;&#21069;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#24471;&#19981;&#38656;&#35201;&#23398;&#20064;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#19988;&#23558;&#20854;&#20934;&#30830;&#22320;&#34920;&#31034;&#22312;&#39044;&#27979;&#30340;&#35299;&#20013;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#24403;&#23558;DeepONet&#22810;&#27425;&#24212;&#29992;&#20110;&#26102;&#38388;&#27493;&#38271;&#35299;&#19978;&#26102;&#65292;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current physics-informed (standard or operator) neural networks still rely on accurately learning the initial conditions of the system they are solving. In contrast, standard numerical methods evolve such initial conditions without needing to learn these. In this study, we propose to improve current physics-informed deep learning strategies such that initial conditions do not need to be learned and are represented exactly in the predicted solution. Moreover, this method guarantees that when a DeepONet is applied multiple times to time step a solution, the resulting function is continuous.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.15640</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22797;&#26434;&#36229;&#24377;&#24615;&#22266;&#20307;&#30340;&#32452;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#29983;&#29289;&#26448;&#26009;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#21644;&#26426;&#26800;&#34892;&#20026;&#30340;&#26448;&#26009;&#20013;&#65292;&#35782;&#21035;&#32452;&#20998;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#36890;&#24120;&#20165;&#38480;&#20110;&#22522;&#26412;&#30340;&#32452;&#20998;&#23450;&#24459;&#65292;&#24182;&#22312;&#19982;&#23454;&#39564;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#36935;&#21040;&#23454;&#38469;&#32422;&#26463;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PINN&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#36719;&#26448;&#26009;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#21576;&#29616;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#12290;&#35813;&#27169;&#22411;&#24378;&#35843;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;PINN&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#22330;&#21464;&#24418;&#21644;&#21152;&#36733;&#21382;&#21490;&#65292;&#20197;&#30830;&#20445;&#31639;&#27861;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#20173;&#28982;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13279</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Random Forests. (arXiv:2308.13279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13279
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#23558;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30001;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#20998;&#23618;&#32467;&#26500;&#65288;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#65289;&#32780;&#25104;&#20026;&#34920;&#31034;&#25968;&#25454;&#30340;&#27969;&#34892;&#36873;&#25321;&#12290;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#38656;&#35201;&#33021;&#22815;&#22312;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#35299;&#20915;&#20998;&#31867;&#31561;&#22522;&#26412;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#26377;&#22810;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#20998;&#31867;&#22120;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20998;&#23618;&#25968;&#25454;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#26862;&#26519;&#25512;&#24191;&#21040;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#27700;&#24179;&#29699;&#37325;&#26032;&#23450;&#20041;&#20102;&#20998;&#21106;&#30340;&#27010;&#24565;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#30001;&#20110;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#20998;&#21106;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22823;&#36793;&#30028;&#20998;&#31867;&#22120;&#25214;&#21040;&#20505;&#36873;&#30340;&#27700;&#24179;&#29699;&#12290;&#20026;&#20102;&#20351;&#38750;&#27431;&#20960;&#37324;&#24471;&#38543;&#26426;&#26862;&#26519;&#36866;&#29992;&#20110;&#22810;&#31867;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#23454;&#39564;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#23427;&#20204;&#30340;&#26368;&#20302;&#20844;&#20849;&#31062;&#20808;&#21644;&#31867;&#24179;&#34913;&#30340;&#31867;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic space is becoming a popular choice for representing data due to the hierarchical structure - whether implicit or explicit - of many real-world datasets. Along with it comes a need for algorithms capable of solving fundamental tasks, such as classification, in hyperbolic space. Recently, multiple papers have investigated hyperbolic alternatives to hyperplane-based classifiers, such as logistic regression and SVMs. While effective, these approaches struggle with more complex hierarchical data. We, therefore, propose to generalize the well-known random forests to hyperbolic space. We do this by redefining the notion of a split using horospheres. Since finding the globally optimal split is computationally intractable, we find candidate horospheres through a large-margin classifier. To make hyperbolic random forests work on multi-class data and imbalanced experiments, we furthermore outline a new method for combining classes based on their lowest common ancestor and a class-balan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12059</link><description>&lt;p&gt;
&#25805;&#20316;&#31283;&#23450;&#25193;&#25955;&#25552;&#31034;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Manipulating Embeddings of Stable Diffusion Prompts. (arXiv:2308.12059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#32454;&#31890;&#24230;&#21644;&#30446;&#26631;&#21270;&#30340;&#25511;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#29983;&#25104;&#27169;&#22411;&#35270;&#20026;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#12290;&#25913;&#21464;&#25552;&#31034;&#20173;&#28982;&#26159;&#29992;&#25143;&#24819;&#35201;&#25913;&#21464;&#29983;&#25104;&#22270;&#20687;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#24605;&#25552;&#31034;&#26469;&#25913;&#21464;&#22270;&#20687;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#35797;&#38169;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#25552;&#31034;&#24037;&#31243;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#30452;&#25509;&#25913;&#21464;&#25552;&#31034;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#25552;&#31034;&#25991;&#26412;&#12290;&#23427;&#20801;&#35768;&#26356;&#31934;&#32454;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#25511;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#36830;&#32493;&#20989;&#25968;&#65292;&#24182;&#22312;&#22270;&#20687;&#31354;&#38388;&#21644;&#25552;&#31034;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#20256;&#36882;&#26799;&#24230;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#30340;&#29992;&#25143;&#20132;&#20114;&#38382;&#39064;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#24212;&#29992;&#36825;&#20010;&#24819;&#27861;&#65306;&#65288;1&#65289;&#20248;&#21270;&#22270;&#20687;&#31354;&#38388;&#20013;&#23450;&#20041;&#30340;&#24230;&#37327;&#65292;&#21487;&#20197;&#27979;&#37327;&#22270;&#20687;&#39118;&#26684;&#31561;&#12290;&#65288;2&#65289;&#24110;&#21161;&#29992;&#25143;&#36827;&#34892;&#21019;&#36896;&#24615;&#30340;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative text-to-image models such as Stable Diffusion allow users to generate images based on a textual description, the prompt. Changing the prompt is still the primary means for the user to change a generated image as desired. However, changing the image by reformulating the prompt remains a difficult process of trial and error, which has led to the emergence of prompt engineering as a new field of research. We propose and analyze methods to change the embedding of a prompt directly instead of the prompt text. It allows for more fine-grained and targeted control that takes into account user intentions. Our approach treats the generative text-to-image model as a continuous function and passes gradients between the image space and the prompt embedding space. By addressing different user interaction problems, we can apply this idea in three scenarios: (1) Optimization of a metric defined in image space that could measure, for example, image style. (2) Assistance of users in creative 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#19990;&#30028;&#20013;&#28040;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.12797</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#23454;&#29616;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Fair Machine Learning via Rank-Preserving Interventional Distributions. (arXiv:2307.12797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12797
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#29702;&#24819;&#30340;&#19990;&#30028;&#20013;&#28040;&#38500;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#30340;&#22240;&#26524;&#24433;&#21709;&#26469;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#30456;&#21516;&#30340;&#20010;&#20307;&#24471;&#21040;&#30456;&#21516;&#30340;&#23545;&#24453;&#65292;&#32780;&#19981;&#21516;&#30340;&#20010;&#20307;&#24471;&#21040;&#19981;&#21516;&#30340;&#23545;&#24453;&#65292;&#37027;&#20040;&#19968;&#20010;&#20915;&#31574;&#34987;&#23450;&#20041;&#20026;&#20844;&#24179;&#30340;&#12290;&#26681;&#25454;&#36825;&#20010;&#23450;&#20041;&#65292;&#22312;&#35774;&#35745;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#20943;&#23569;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19981;&#20844;&#24179;&#26102;&#65292;&#24517;&#39035;&#24341;&#20837;&#22240;&#26524;&#24605;&#32771;&#26469;&#24341;&#20837;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#23558;&#20010;&#20307;&#23450;&#20041;&#20026;&#22312;&#19968;&#20010;&#20551;&#35774;&#30340;&#12289;&#29702;&#24819;&#30340;&#65288;FiND&#65289;&#19990;&#30028;&#20013;&#26159;&#35268;&#33539;&#19978;&#30456;&#31561;&#30340;&#65292;&#36825;&#20010;&#19990;&#30028;&#20013;&#21463;&#20445;&#25252;&#23646;&#24615;&#23545;&#30446;&#26631;&#27809;&#26377;&#65288;&#30452;&#25509;&#25110;&#38388;&#25509;&#65289;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20445;&#25345;&#25490;&#24207;&#30340;&#24178;&#39044;&#20998;&#24067;&#26469;&#23450;&#20041;&#36825;&#20010;FiND&#19990;&#30028;&#30340;&#20272;&#35745;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#35777;&#25968;&#25454;&#30340;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20215;&#26631;&#20934;&#12290;&#36890;&#36807;&#36825;&#20123;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#26368;&#21463;&#27495;&#35270;&#30340;&#20010;&#20307;&#24182;&#20943;&#23569;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes. Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attribute has no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define an estimand of this FiND world and a warping method for estimation. Evaluation criteria for both the method and resulting model are presented and validated through simulations and empirical data. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.05284</link><description>&lt;p&gt;
&#20851;&#20110;&#38656;&#35201;&#25551;&#36848;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65306;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#25512;&#21160;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#31934;&#32454;&#35821;&#35328;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;WhyShift&#23454;&#39564;&#24179;&#21488;&#65292;&#24182;&#35752;&#35770;&#20102;$Y|X$-&#20559;&#31227;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#24067;&#20559;&#31227;&#38656;&#35201;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#25805;&#20316;&#24178;&#39044;&#12290;&#26041;&#27861;&#30740;&#31350;&#24517;&#39035;&#20197;&#20854;&#25152;&#28041;&#21450;&#30340;&#20855;&#20307;&#20559;&#31227;&#20026;&#22522;&#30784;&#12290;&#23613;&#31649;&#26032;&#20852;&#30340;&#22522;&#20934;&#25968;&#25454;&#20026;&#23454;&#35777;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#38544;&#21547;&#22320;&#20851;&#27880;&#21327;&#21464;&#37327;&#20559;&#31227;&#65292;&#24182;&#19988;&#23454;&#35777;&#21457;&#29616;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20559;&#31227;&#31867;&#22411;&#65292;&#20363;&#22914;&#65292;&#24403;$Y|X$&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20043;&#21069;&#20851;&#20110;&#31639;&#27861;&#24615;&#33021;&#30340;&#35266;&#23519;&#21487;&#33021;&#26080;&#25928;&#12290;&#25105;&#20204;&#23545;5&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#33258;&#28982;&#20559;&#31227;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;86,000&#20010;&#27169;&#22411;&#37197;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;$Y|X$-&#20559;&#31227;&#26368;&#20026;&#26222;&#36941;&#12290;&#20026;&#20102;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19968;&#31181;&#31934;&#32454;&#30340;&#25551;&#36848;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#35821;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;WhyShift&#65292;&#19968;&#20010;&#30001;&#31574;&#21010;&#30340;&#30495;&#23454;&#19990;&#30028;&#20559;&#31227;&#27979;&#35797;&#24179;&#21488;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#25105;&#20204;&#22522;&#20934;&#24615;&#33021;&#30340;&#20559;&#31227;&#31867;&#22411;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#30001;&#20110;$Y|X$-&#20559;&#31227;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#24456;&#24120;&#35265;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21463;&#21040;&#26368;&#22823;$Y|X$-&#20559;&#31227;&#24433;&#21709;&#30340;&#21327;&#21464;&#37327;&#21306;&#22495;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different distribution shifts require different algorithmic and operational interventions. Methodological research must be grounded by the specific shifts they address. Although nascent benchmarks provide a promising empirical foundation, they implicitly focus on covariate shifts, and the validity of empirical findings depends on the type of shift, e.g., previous observations on algorithmic performance can fail to be valid when the $Y|X$ distribution changes. We conduct a thorough investigation of natural shifts in 5 tabular datasets over 86,000 model configurations, and find that $Y|X$-shifts are most prevalent. To encourage researchers to develop a refined language for distribution shifts, we build WhyShift, an empirical testbed of curated real-world shifts where we characterize the type of shift we benchmark performance over. Since $Y|X$-shifts are prevalent in tabular settings, we identify covariate regions that suffer the biggest $Y|X$-shifts and discuss implications for algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;</title><link>http://arxiv.org/abs/2307.01753</link><description>&lt;p&gt;
&#26469;&#33258;DESI&#20142;&#32418;&#26143;&#31995;&#22823;&#23610;&#24230;&#32858;&#31867;&#30340;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies. (arXiv:2307.01753v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Dark Energy Spectroscopic Instrument (DESI)&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#26469;&#38480;&#21046;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#21253;&#25324;&#36229;&#36807;1200&#19975;&#20010;&#30446;&#26631;&#65292;&#35206;&#30422;&#20102;14000&#24179;&#26041;&#24230;&#30340;&#22825;&#31354;&#21306;&#22495;&#65292;&#32418;&#31227;&#33539;&#22260;&#20026;0.2 &lt; z &lt; 1.35&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38134;&#27827;&#28040;&#20809;&#12289;&#35843;&#26597;&#28145;&#24230;&#21644;&#35266;&#27979;&#26465;&#20214;&#26159;&#31995;&#32479;&#35823;&#24046;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#24182;&#37319;&#29992;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#22823;&#23610;&#24230;&#19978;&#30340;&#38750;&#23431;&#23449;&#23398;&#36807;&#24230;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32463;&#36807;&#20102;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;fNL&#21644;&#31995;&#32479;&#35823;&#24046;&#30340;&#23545;&#25968;&#27491;&#24577;&#27169;&#25311;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22312;&#20943;&#23567;&#21097;&#20313;&#31995;&#32479;&#35823;&#24046;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;fNL&#30340;68\%&#65288;95\%&#65289;&#32622;&#20449;&#21306;&#38388;&#20026;fNL = 47^{+14(+29)}_{-11(-22)}&#12290;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25152;&#26377;&#25104;&#20687;&#22270;&#38598;&#36827;&#34892;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;
We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2&lt; z &lt; 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.12361</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340; sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#19982;&#38750;&#32447;&#24615;&#26410;&#30693;&#36755;&#20837;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sigma-point Kalman Filter with Nonlinear Unknown Input Estimation via Optimization and Data-driven Approach for Dynamic Systems. (arXiv:2306.12361v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20551;&#35774;&#26410;&#30693;&#36755;&#20837;&#20026;&#32447;&#24615;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#26410;&#30693;&#36755;&#20837;&#30340;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#23558;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#20272;&#35745;&#20013;&#65292;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#20851;&#20110;&#29366;&#24577;&#21644;&#26410;&#30693;&#36755;&#20837;(UI)&#20272;&#35745;&#30340;&#25991;&#29486;&#37117;&#35201;&#27714;UI&#26159;&#32447;&#24615;&#30340;&#65292;&#36825;&#20010;&#38480;&#21046;&#21487;&#33021;&#22826;&#20005;&#26684;&#20102;&#65292;&#22240;&#20026;&#23427;&#24182;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#26234;&#33021;&#33258;&#20027;&#31995;&#32479;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23548;&#25968;&#26410;&#30693;&#36755;&#20837; Sigma-point &#21345;&#23572;&#26364;&#28388;&#27874;&#22120;(SPKE-nUI)&#65292;&#20854;&#20013; SPKF &#19982;&#26222;&#36890;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#36890;&#36807;&#38750;&#32447;&#24615;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23454;&#29616;&#12290;&#38750;&#32447;&#24615; UI &#20272;&#35745;&#22120;&#20351;&#29992;&#21518;&#39564;&#29366;&#24577;&#20272;&#35745;&#65292;&#36825;&#23545;&#29366;&#24577;&#39044;&#27979;&#35823;&#24046;&#19981;&#22826;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32852;&#21512; sigma-point &#21464;&#25442;&#26041;&#26696;&#65292;&#23558;&#29366;&#24577;&#21644; UI &#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837; SPKF-nUI &#30340;&#20272;&#35745;&#20013;&#12290;&#28145;&#20837;&#30340;&#38543;&#26426;&#31283;&#23450;&#24615;&#20998;&#26512;&#35777;&#26126;&#20102;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340; SPKF-nUI &#21487;&#20197;&#20135;&#29983;&#25351;&#25968;&#32423;&#25910;&#25947;&#30340;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22522;&#20110;&#27169;&#25311;&#30340;&#36335;&#38754;&#36710;&#36742;&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most works on joint state and unknown input (UI) estimation require the assumption that the UIs are linear; this is potentially restrictive as it does not hold in many intelligent autonomous systems. To overcome this restriction and circumvent the need to linearize the system, we propose a derivative-free Unknown Input Sigma-point Kalman Filter (SPKF-nUI) where the SPKF is interconnected with a general nonlinear UI estimator that can be implemented via nonlinear optimization and data-driven approaches. The nonlinear UI estimator uses the posterior state estimate which is less susceptible to state prediction error. In addition, we introduce a joint sigma-point transformation scheme to incorporate both the state and UI uncertainties in the estimation of SPKF-nUI. An in-depth stochastic stability analysis proves that the proposed SPKF-nUI yields exponentially converging estimation error bounds under reasonable assumptions. Finally, two case studies are carried out on a simulation-based ri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#20998;&#31867;&#21644;&#27169;&#25311;&#32676;&#20307;&#32769;&#40736;&#34892;&#20026;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#36328;&#31548;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#21305;&#37197;&#32769;&#40736;&#36523;&#20221;&#65292;&#22312;&#23478;&#40736;&#29615;&#22659;&#19979;&#30740;&#31350;&#32769;&#40736;&#21487;&#20197;&#25429;&#25417;&#21040;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#65292;&#32780;&#19988;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;</title><link>http://arxiv.org/abs/2306.03066</link><description>&lt;p&gt;
&#12298;&#40736;&#31867;&#19982;&#37197;&#20598;&#65306;&#21333;&#19968;&#27169;&#22411;&#33258;&#21160;&#23545;&#32676;&#20307;&#20013;&#30340;&#32769;&#40736;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#21644;&#24314;&#27169;&#36328;&#31548;&#12299;
&lt;/p&gt;
&lt;p&gt;
Of Mice and Mates: Automated Classification and Modelling of Mouse Behaviour in Groups using a Single Model across Cages. (arXiv:2306.03066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#33258;&#21160;&#20998;&#31867;&#21644;&#27169;&#25311;&#32676;&#20307;&#32769;&#40736;&#34892;&#20026;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#36328;&#31548;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#21305;&#37197;&#32769;&#40736;&#36523;&#20221;&#65292;&#22312;&#23478;&#40736;&#29615;&#22659;&#19979;&#30740;&#31350;&#32769;&#40736;&#21487;&#20197;&#25429;&#25417;&#21040;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#65292;&#32780;&#19988;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#23454;&#39564;&#36890;&#24120;&#22312;&#19987;&#38376;&#30340;&#31454;&#25216;&#22330;&#20013;&#36827;&#34892;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#28151;&#28102;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#30740;&#31350;&#23478;&#40736;&#29615;&#22659;&#20013;&#30340;&#32769;&#40736;&#65292;&#20026;&#29983;&#29289;&#23398;&#23478;&#25552;&#20379;&#20102;&#25429;&#25417;&#20010;&#20307;&#34892;&#20026;&#30340;&#26102;&#38388;&#22240;&#32032;&#21644;&#27169;&#25311;&#26368;&#23567;&#20154;&#20026;&#24178;&#39044;&#19979;&#31548;&#21451;&#20043;&#38388;&#20114;&#21160;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#8220;&#27963;&#21160;&#26631;&#31614;&#27169;&#22359;&#8221;&#65288;ALM&#65289;&#26469;&#33258;&#21160;&#23545;&#32769;&#40736;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#32676;&#20307;&#34892;&#20026;&#27169;&#22411;&#8221;&#65288;GBM&#65289;&#26469;&#27010;&#25324;&#20182;&#20204;&#22312;&#31548;&#23376;&#20013;&#30340;&#32852;&#21512;&#34892;&#20026;&#65292;&#20351;&#29992;&#32622;&#25442;&#30697;&#38453;&#23558;&#27599;&#20010;&#31548;&#23376;&#20013;&#30340;&#32769;&#40736;&#36523;&#20221;&#19982;&#27169;&#22411;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#34892;&#20026;&#20998;&#31867;&#22120;&#65288;ABODe&#65289;&#21644;&#34892;&#20026;&#24314;&#27169;&#65288;IMADGE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural experiments often happen in specialised arenas, but this may confound the analysis. To address this issue, we provide tools to study mice in the homecage environment, equipping biologists with the possibility to capture the temporal aspect of the individual's behaviour and model the interaction and interdependence between cage-mates with minimal human intervention. We develop the Activity Labelling Module (ALM) to automatically classify mouse behaviour from video, and a novel Group Behaviour Model (GBM) for summarising their joint behaviour across cages, using a permutation matrix to match the mouse identities in each cage to the model. We also release two datasets, ABODe for training behaviour classifiers and IMADGE for modelling behaviour.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.16610</link><description>&lt;p&gt;
&#23398;&#20064;&#21333;&#35843;&#21338;&#24328;&#30340;&#25237;&#30707;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Slingshot Approach to Learning in Monotone Games. (arXiv:2305.16610v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;, &#36890;&#36807;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#21644;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#37117;&#33021;&#22815;&#23454;&#29616;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#21333;&#35843;&#21338;&#24328;&#20013;&#35745;&#31639;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36981;&#24490;&#27491;&#21017;&#21270;&#39046;&#23548;&#32773;&#31639;&#27861;&#21363;&#20351;&#22312;&#21452;&#20154;&#38646;&#21644;&#28216;&#25103;&#20013;&#20063;&#26080;&#27861;&#25910;&#25947;&#21040;&#22343;&#34913;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20048;&#35266;&#29256;&#26412;&#24182;&#20855;&#26377;&#26368;&#21518;&#36845;&#20195;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#26080;&#22122;&#22768;&#30340;&#26799;&#24230;&#21453;&#39304;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#25200;&#21160;&#25110;&#27491;&#21017;&#21270;&#28216;&#25103;&#30340;&#25903;&#20184;&#25110;&#25928;&#29992;&#12290;&#36825;&#31181;&#25200;&#21160;&#26377;&#21161;&#20110;&#23558;&#24403;&#21069;&#31574;&#30053;&#25289;&#21521;&#19968;&#20010;&#38170;&#23450;&#31574;&#30053;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25237;&#30707;&#32034;&#8221;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20174;&#32780;&#33719;&#24471;&#38752;&#36817;&#22343;&#34913;&#28857;&#30340;&#31283;&#23450;&#28857;&#65292;&#26080;&#35770;&#26159;&#21542;&#23384;&#22312;&#22122;&#22768;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23450;&#26399;&#26356;&#26032;&#25237;&#30707;&#32034;&#31574;&#30053;&#21644;&#24403;&#21069;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#35299;&#37322;&#20026;&#36817;&#31471;p
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the problem of computing equilibria in monotone games. The traditional Follow the Regularized Leader algorithms fail to converge to an equilibrium even in two-player zero-sum games. Although optimistic versions of these algorithms have been proposed with last-iterate convergence guarantees, they require noiseless gradient feedback. To overcome this limitation, we present a novel framework that achieves last-iterate convergence even in the presence of noise. Our key idea involves perturbing or regularizing the payoffs or utilities of the games. This perturbation serves to pull the current strategy to an anchored strategy, which we refer to as a {\it slingshot} strategy. First, we establish the convergence rates of our framework to a stationary point near an equilibrium, regardless of the presence or absence of noise. Next, we introduce an approach to periodically update the slingshot strategy with the current strategy. We interpret this approach as a proximal p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;</title><link>http://arxiv.org/abs/2305.12131</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Online Convex Optimization with Arbitrary Delays. (arXiv:2305.12131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#26799;&#24230;&#25110;&#20854;&#20182;&#20989;&#25968;&#20449;&#24687;&#21487;&#20197;&#20219;&#24847;&#24310;&#36831;&#20026;&#29305;&#28857;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20043;&#21069;&#30740;&#31350;&#31283;&#24577;&#29615;&#22659;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#24310;&#36831;OCO&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#27604;&#36739;&#22120;&#24207;&#21015;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;DOGD&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#20854;&#21040;&#36798;&#39034;&#24207;&#20026;&#27599;&#20010;&#24310;&#36831;&#26799;&#24230;&#25191;&#34892;&#28176;&#21464;&#19979;&#38477;&#27493;&#39588;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#22411;&#20998;&#26512;&#34920;&#26126;&#65292;DOGD&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#26368;&#22823;&#24310;&#36831;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$P_T$&#26159;&#27604;&#36739;&#22120;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#24310;&#36831;&#19981;&#25913;&#21464;&#28176;&#21464;&#30340;&#21040;&#36798;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#65292;&#20854;&#20013;$S$&#26159;&#24310;&#36831;&#20043;&#21644;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;DOGD&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;DOGD&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) with arbitrary delays, in which gradients or other information of functions could be arbitrarily delayed, has received increasing attention recently. Different from previous studies that focus on stationary environments, this paper investigates the delayed OCO in non-stationary environments, and aims to minimize the dynamic regret with respect to any sequence of comparators. To this end, we first propose a simple algorithm, namely DOGD, which performs a gradient descent step for each delayed gradient according to their arrival order. Despite its simplicity, our novel analysis shows that DOGD can attain an $O(\sqrt{dT}(P_T+1)$ dynamic regret bound in the worst case, where $d$ is the maximum delay, $T$ is the time horizon, and $P_T$ is the path length of comparators. More importantly, in case delays do not change the arrival order of gradients, it can automatically reduce the dynamic regret to $O(\sqrt{S}(1+P_T))$, where $S$ is the sum of delays. Furtherm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10267</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
State Representation Learning Using an Unbalanced Atlas. (arXiv:2305.10267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#26041;&#27861;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35828;&#35748;&#20026;&#65292;&#39640;&#32500;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#36739;&#20302;&#32500;&#30340;&#27969;&#24418;&#19978;&#65292;&#24182;&#19988;&#21033;&#29992;&#35813;&#27969;&#24418;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#24418;&#30340;&#25216;&#26415;&#29992;&#20110;&#38477;&#32500;&#65292;&#20294;&#23427;&#20204;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#12290;&#26368;&#36817;&#30340;MSIMCLR&#26041;&#27861;&#23558;&#27969;&#24418;&#32534;&#30721;&#19982;SimCLR&#30456;&#32467;&#21512;&#65292;&#20294;&#38656;&#35201;&#26497;&#20302;&#30340;&#30446;&#26631;&#32534;&#30721;&#32500;&#24230;&#25165;&#33021;&#32988;&#36807;SimCLR&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#24179;&#34913;&#22270;&#20876;&#65288;UA&#65289;&#30340;&#26032;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#35843;&#25972;&#26102;&#31354;DeepInfomax&#65288;ST-DIM&#65289;&#26694;&#26550;&#20197;&#19982;&#25105;&#20204;&#25552;&#35758;&#30340;UA&#27169;&#24335;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#37319;&#29992;&#20005;&#35880;&#30340;&#31185;&#23398;&#26041;&#27861;&#26469;&#31934;&#24515;&#30740;&#31350;&#21644;&#35774;&#35745;&#20102;&#20351;&#29992;UA&#30340;DeepInfomax&#65288;DIM-UA&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis posits that high-dimensional data often lies on a lower-dimensional manifold and that utilizing this manifold as the target space yields more efficient representations. While numerous traditional manifold-based techniques exist for dimensionality reduction, their application in self-supervised learning has witnessed slow progress. The recent MSIMCLR method combines manifold encoding with SimCLR but requires extremely low target encoding dimensions to outperform SimCLR, limiting its applicability. This paper introduces a novel learning paradigm using an unbalanced atlas (UA), capable of surpassing state-of-the-art self-supervised learning approaches. We meticulously investigated and engineered the DeepInfomax with an unbalanced atlas (DIM-UA) method by systematically adapting the Spatiotemporal DeepInfomax (ST-DIM) framework to align with our proposed UA paradigm, employing rigorous scientific methodologies throughout the process. The efficacy of DIM-UA is demons
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2305.08295</link><description>&lt;p&gt;
CLCIFAR&#65306;&#24102;&#20154;&#31867;&#26631;&#27880;&#20114;&#34917;&#26631;&#31614;&#30340;CIFAR&#27966;&#29983;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CLCIFAR: CIFAR-Derived Benchmark Datasets with Human Annotated Complementary Labels. (arXiv:2305.08295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#30001;&#20154;&#31867;&#26631;&#27880;&#30340;&#20114;&#34917;&#26631;&#31614;&#65292;&#21019;&#36896;&#20102;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#29616;&#23454;&#34920;&#29616;&#19979;CLL&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#23454;&#38469;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#65288;CLL&#65289;&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20165;&#20351;&#29992;&#20114;&#34917;&#26631;&#31614;&#65288;&#26631;&#31034;&#23454;&#20363;&#19981;&#23646;&#20110;&#21738;&#20123;&#31867;&#21035;&#65289;&#26469;&#35757;&#32451;&#22810;&#31867;&#20998;&#31867;&#22120;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;CLL&#31639;&#27861;&#65292;&#20294;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#34920;&#29616;&#20173;&#19981;&#28165;&#26970;&#12290;&#39318;&#20808;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#20114;&#34917;&#26631;&#31614;&#29983;&#25104;&#30340;&#20551;&#35774;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#20165;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#33719;&#21462;&#26377;&#20851;CLL&#31639;&#27861;&#30340;&#30495;&#23454;&#19990;&#30028;&#34920;&#29616;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21327;&#35758;&#26469;&#25910;&#38598;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#12290;&#36825;&#19968;&#21162;&#21147;&#23548;&#33268;&#21019;&#24314;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CLCIFAR10&#21644;CLCIFAR20&#65292;&#20998;&#21035;&#30001;CIFAR10&#21644;CIFAR100&#27966;&#29983;&#32780;&#26469;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;https://github.com/ntucllab/complementary_cifar&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#36739;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24403;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#20114;&#34917;&#26631;&#31614;&#26102;&#65292;&#24615;&#33021;&#26377;&#26126;&#26174;&#19979;&#38477;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;CLL&#25968;&#25454;&#38598;&#20351;&#24471;&#22312;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#26465;&#20214;&#19979;&#35780;&#20272;&#31639;&#27861;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#26356;&#30495;&#23454;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical performance remains unclear for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels annotated by human annotators. This effort resulted in the creation of two datasets, CLCIFAR10 and CLCIFAR20, derived from CIFAR10 and CIFAR100, respectively. These datasets, publicly released at https://github.com/ntucllab/complementary_cifar, represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decline in performance when 
&lt;/p&gt;</description></item><item><title>FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01658</link><description>&lt;p&gt;
FlightBERT++&#65306;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlightBERT++: A Non-autoregressive Multi-Horizon Flight Trajectory Prediction Framework. (arXiv:2305.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01658
&lt;/p&gt;
&lt;p&gt;
FlightBERT++&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#35299;&#20915;&#20102;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#26159;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#24110;&#21161;&#31354;&#31649;&#21592;&#26356;&#23433;&#20840;&#39640;&#25928;&#22320;&#31649;&#29702;&#31354;&#22495;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#33258;&#22238;&#24402;&#26041;&#24335;&#25191;&#34892;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#23481;&#26131;&#20986;&#29616;&#35823;&#24046;&#32047;&#31215;&#21644;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;FlightBERT++&#65292;&#20197;i&#65289;&#30452;&#25509;&#20197;&#38750;&#33258;&#22238;&#24402;&#26041;&#24335;&#39044;&#27979;&#22810;&#26102;&#22495;&#39134;&#34892;&#36712;&#36857;&#65292;&#21644;ii&#65289;&#25913;&#21892;FlightBERT&#26694;&#26550;&#20013;&#20108;&#36827;&#21046;&#32534;&#30721;&#65288;BE&#65289;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36890;&#36807;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23454;&#29616;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#20174;&#21382;&#21490;&#35266;&#27979;&#20013;&#23398;&#20064;&#26102;&#31354;&#27169;&#24335;&#65292;&#32780;&#35299;&#30721;&#22120;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#39134;&#34892;&#29366;&#24577;&#12290;&#19982;&#20256;&#32479;&#26550;&#26500;&#30456;&#27604;&#65292;&#39069;&#22806;&#30340;&#26102;&#22495;&#24863;&#30693;&#19978;&#19979;&#25991;&#29983;&#25104;&#22120;&#65288;HACG&#65289;&#19987;&#38376;&#35774;&#35745;&#32771;&#34385;&#20808;&#21069;&#30340;&#26102;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flight Trajectory Prediction (FTP) is an essential task in Air Traffic Control (ATC), which can assist air traffic controllers to manage airspace more safely and efficiently. Existing approaches generally perform multi-horizon FTP tasks in an autoregressive manner, which is prone to suffer from error accumulation and low-efficiency problems. In this paper, a novel framework, called FlightBERT++, is proposed to i) forecast multi-horizon flight trajectories directly in a non-autoregressive way, and ii) improved the limitation of the binary encoding (BE) representation in the FlightBERT framework. Specifically, the proposed framework is implemented by a generalized Encoder-Decoder architecture, in which the encoder learns the temporal-spatial patterns from historical observations and the decoder predicts the flight status for the future time steps. Compared to conventional architecture, an extra horizon-aware contexts generator (HACG) is dedicatedly designed to consider the prior horizon 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#21517;&#20026;Jacobian-Scaled K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#19981;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;&#21363;&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#12290;&#26412;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.01539</link><description>&lt;p&gt;
Jacobian-Scaled K-means&#32858;&#31867;&#29992;&#20110;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#21453;&#24212;&#27969;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Jacobian-Scaled K-means Clustering for Physics-Informed Segmentation of Reacting Flows. (arXiv:2305.01539v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#21517;&#20026;Jacobian-Scaled K-means&#32858;&#31867;&#26041;&#27861;&#65292;&#33021;&#22815;&#19981;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;&#21363;&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#12290;&#26412;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jacobian-Scaled K-means (JSK-means)&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;K-means&#26694;&#26550;&#30340;&#21463;&#29289;&#29702;&#23398;&#24433;&#21709;&#30340;&#32858;&#31867;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36317;&#31163;&#20989;&#25968;&#30340;&#20462;&#25913;&#23558;&#28508;&#22312;&#30340;&#29289;&#29702;&#30693;&#35782;&#27880;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65306;JSK-means&#32858;&#31867;&#36807;&#31243;&#19981;&#20351;&#29992;&#20256;&#32479;&#30340;&#27431;&#27663;&#36317;&#31163;&#21521;&#37327;&#65292;&#32780;&#26159;&#20351;&#29992;&#20174;&#38598;&#32676;&#36136;&#24515;&#22788;&#27714;&#24471;&#30340;&#21160;&#24577;&#31995;&#32479;Jacobian&#30697;&#38453;&#32553;&#25918;&#30340;&#36317;&#31163;&#21521;&#37327;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;JSK-means&#31639;&#27861;--&#32780;&#19981;&#26159;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#38598;--&#29983;&#25104;&#33021;&#22815;&#25429;&#33719;&#21160;&#21147;&#23398;&#30456;&#20284;&#24615;&#21306;&#22495;&#30340;&#32858;&#31867;&#65292;&#21363;&#32858;&#31867;&#26159;&#21521;&#39640;&#25935;&#24863;&#21306;&#22495;&#22312;&#30456;&#31354;&#38388;&#20013;&#37325;&#26032;&#20998;&#24067;&#24182;&#30001;&#26679;&#26412;&#30340;&#28304;&#39033;&#30340;&#30456;&#20284;&#24615;&#25551;&#36848;&#32780;&#38750;&#26679;&#26412;&#26412;&#36523;&#12290;&#35813;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#21453;&#24212;&#27969;&#27169;&#25311;&#25968;&#25454;&#38598;(&#36890;&#36947;&#29190;&#36720;&#37197;&#32622;)&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#20854;&#20013;&#21160;&#24577;&#24615;&#36136;&#26159;
&lt;/p&gt;
&lt;p&gt;
This work introduces Jacobian-scaled K-means (JSK-means) clustering, which is a physics-informed clustering strategy centered on the K-means framework. The method allows for the injection of underlying physical knowledge into the clustering procedure through a distance function modification: instead of leveraging conventional Euclidean distance vectors, the JSK-means procedure operates on distance vectors scaled by matrices obtained from dynamical system Jacobians evaluated at the cluster centroids. The goal of this work is to show how the JSK-means algorithm -- without modifying the input dataset -- produces clusters that capture regions of dynamical similarity, in that the clusters are redistributed towards high-sensitivity regions in phase space and are described by similarity in the source terms of samples instead of the samples themselves. The algorithm is demonstrated on a complex reacting flow simulation dataset (a channel detonation configuration), where the dynamics in the the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;</title><link>http://arxiv.org/abs/2304.08658</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#22312;DED&#25171;&#21360;SS316L&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22768;&#21457;&#23556;&#65288;AE&#65289;&#31561;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#26102;&#39057;&#27169;&#24335;&#19982;DED&#36807;&#31243;&#20013;&#30340;&#23380;&#38553;&#29575;&#24418;&#25104;&#36827;&#34892;&#39640;&#31354;&#38388;&#65288;0.5mm&#65289;&#21644;&#26102;&#38388;&#65288;&lt;1ms&#65289;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#20013;&#30340;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#38750;&#29305;&#23450;&#24615;&#35299;&#37322;&#65289;&#65292;&#23558;AE&#20013;&#30340;&#26576;&#20123;&#39640;&#39057;&#27874;&#24418;&#29305;&#24449;&#24402;&#22240;&#20110;DED&#36807;&#31243;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#65306;&#39134;&#28293;&#20107;&#20214;&#21644;&#20302;&#28909;&#37327;&#36755;&#20837;&#19979;&#30456;&#37051;&#25171;&#21360;&#36712;&#36857;&#30340;&#19981;&#20805;&#20998;&#29076;&#21512;&#12290;&#35813;&#26041;&#27861;&#20026;&#23454;&#26102;&#39044;&#27979;&#27599;&#20010;&#27779;&#20811;&#22622;&#23572;&#65288;0.5mm&#65289;&#20013;&#30340;&#27668;&#23380;&#23384;&#22312;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#26159;&#19982;&#20808;&#21069;&#21162;&#21147;&#30456;&#27604;&#30340;&#19968;&#20010;&#37325;&#22823;&#39134;&#36291;&#12290;&#22312;&#25171;&#21360;&#24182;&#38543;&#21518;&#21152;&#24037;SS316L&#26448;&#26009;&#26679;&#21697;&#26102;&#65292;&#21516;&#27493;&#37319;&#38598;&#20102;&#21253;&#25324;&#21147;&#65292;AE&#65292;&#25391;&#21160;&#21644;&#28201;&#24230;&#22312;&#20869;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#35782;&#21035;&#20004;&#31181;&#23380;&#38553;&#24418;&#25104;&#36884;&#24452;&#30340;AE&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#36827;&#19968;&#27493;&#20998;&#26512;&#36825;&#20123;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#21644;AI&#26041;&#27861;&#39044;&#27979;DED&#25171;&#21360;&#37096;&#20214;&#20013;&#30340;&#21407;&#20301;&#34920;&#38754;&#23380;&#38553;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (&lt; 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)</title><link>http://arxiv.org/abs/2303.00890</link><description>&lt;p&gt;
BBOB&#19978;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB. (arXiv:2303.00890v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;BBOB&#19978;&#20116;&#31181;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#20197;&#21450;CMA-ES&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;... (&#26681;&#25454;&#35770;&#25991;&#30340;&#20855;&#20307;&#20869;&#23481;&#36827;&#34892;&#24635;&#32467;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#22522;&#20110;&#40657;&#30418;&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#35780;&#20272;&#25104;&#26412;&#39640;&#12289;&#21482;&#33021;&#25317;&#26377;&#26377;&#38480;&#30340;&#35780;&#20272;&#39044;&#31639;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#35299;&#20915;&#24037;&#19994;&#30028;&#30340;&#25968;&#20540;&#20248;&#21270;&#38382;&#39064;&#20013;&#23588;&#20026;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#32791;&#26102;&#30340;&#27169;&#25311;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24037;&#19994;&#38382;&#39064;&#28041;&#21450;&#22823;&#37327;&#21442;&#25968;&#65292;&#36825;&#32473;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#20854;&#24615;&#33021;&#22312;&#32500;&#24230;&#36229;&#36807;15&#20010;&#21464;&#37327;&#26102;&#24120;&#24120;&#19979;&#38477;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26032;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#31181;&#31639;&#27861;&#22312;&#21738;&#31181;&#20248;&#21270;&#22330;&#26223;&#20013;&#34920;&#29616;&#26368;&#22909;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;5&#31181;&#26368;&#26032;&#30340;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#19982;&#20256;&#32479;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;CMA-ES&#31639;&#27861;&#22312;COCA&#29615;&#22659;&#19979;24&#20010;BBOB&#20989;&#25968;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#32500;&#24230;&#20174;10&#21040;60&#20010;&#21464;&#37327;&#19981;&#26029;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a class of black-box, surrogate-based heuristics that can efficiently optimize problems that are expensive to evaluate, and hence admit only small evaluation budgets. BO is particularly popular for solving numerical optimization problems in industry, where the evaluation of objective functions often relies on time-consuming simulations or physical experiments. However, many industrial problems depend on a large number of parameters. This poses a challenge for BO algorithms, whose performance is often reported to suffer when the dimension grows beyond 15 variables. Although many new algorithms have been proposed to address this problem, it is not well understood which one is the best for which optimization scenario.  In this work, we compare five state-of-the-art high-dimensional BO algorithms, with vanilla BO and CMA-ES on the 24 BBOB functions of the COCO environment at increasing dimensionality, ranging from 10 to 60 variables. Our results confirm the su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15269</link><description>&lt;p&gt;
Swin Transformer &#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Swin Transformers. (arXiv:2206.15269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696; Swin DQN&#65292;&#36890;&#36807;&#23558;&#32452;&#21512;&#30340;&#22270;&#20687;&#20687;&#32032;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#24182;&#22312;&#23616;&#37096;&#24212;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312; Atari &#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36234;&#29616;&#26377;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22810;&#23618;&#33258;&#25105;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;&#26377;&#20154;&#21162;&#21147;&#23558; Transformer &#36866;&#24212;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#21253;&#25324; Vision Transformer &#21644; Swin Transformer&#12290;&#34429;&#28982;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#23558; Vision Transformer &#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#20173;&#20572;&#30041;&#22312;&#23567;&#35268;&#27169;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#24517;&#39035;&#20381;&#36182;&#20110;&#25216;&#26415;&#26469;&#20943;&#23569; Vision Transformer &#30340;&#25104;&#26412;&#65292;&#36825;&#20063;&#20250;&#20135;&#29983;&#36739;&#24046;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110; Swin Transformer &#30340;&#31532;&#19968;&#20010;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65306;Swin DQN&#12290;Swin Transformer &#21487;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20027;&#24178;&#39592;&#24178;&#65292;&#23558;&#22270;&#20687;&#20687;&#32032;&#30340;&#32452;&#21512;&#20998;&#25104;&#23567;&#30340;&#34917;&#19969;&#65292;&#24182;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#31383;&#21475;&#20869;&#24212;&#29992;&#23616;&#37096;&#33258;&#25105;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#23427;&#20204;&#22312; ImageNet &#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#21331;&#36234;&#12290;Swin DQN &#22312; Atari &#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110; CNN &#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are neural network models that utilize multiple layers of self-attention heads and have exhibited enormous potential in natural language processing tasks. Meanwhile, there have been efforts to adapt transformers to visual tasks of machine learning, including Vision Transformers and Swin Transformers. Although some researchers use Vision Transformers for reinforcement learning tasks, their experiments remain at a small scale due to the high computational cost. Experiments conducted at a large scale, on the other hand, have to rely on techniques to cut the costs of Vision Transformers, which also yield inferior results.  To address this challenge, this article presents the first online reinforcement learning scheme that is based on Swin Transformers: Swin DQN. Swin Transformers are promising as a backbone in neural networks by splitting groups of image pixels into small patches and applying local self-attention operations inside the (shifted) windows of fixed sizes. They hav
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2202.11046</link><description>&lt;p&gt;
&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;on-policy&#21644;off-policy&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#26102;&#38388;&#27573;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#32047;&#31215;&#25240;&#25187;&#22870;&#21169;&#30340;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#26469;&#24314;&#27169;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#26495;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20998;&#21035;&#22312;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#20248;&#21270;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#38750;&#28176;&#36827;&#24615;&#30028;&#65292;&#37327;&#21270;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#30340;&#36895;&#29575;&#12290;&#20316;&#20026;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#21035;&#24212;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Adversarial Robustness&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2111.01996</link><description>&lt;p&gt;
Pareto&#23545;&#25239;&#40065;&#26834;&#24615;&#65306;&#24179;&#34913;&#31354;&#38388;&#40065;&#26834;&#24615;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (arXiv:2111.01996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Adversarial Robustness&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#20027;&#35201;&#21253;&#25324;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#29616;&#40065;&#26834;&#27867;&#21270;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#23454;&#29616;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#30340;&#31354;&#38388;&#40065;&#26834;&#24615;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#38388;&#30340;&#33030;&#24369;&#24615;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#31354;&#38388;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20840;&#38754;&#20851;&#31995;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#20851;&#38190;&#26159;&#65292;&#20026;&#20102;&#23558;&#21508;&#31181;&#40065;&#26834;&#24615;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#32435;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23558;&#24085;&#32047;&#25176;&#20934;&#21017;&#24341;&#20837;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#20998;&#26512;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Adversarial Robustness&#30340;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness, which primarily comprises sensitivity-based robustness and spatial robustness, plays an integral part in achieving robust generalization. In this paper, we endeavor to design strategies to achieve universal adversarial robustness. To achieve this, we first investigate the relatively less-explored realm of spatial robustness. Then, we integrate the existing spatial robustness methods by incorporating both local and global spatial vulnerability into a unified spatial attack and adversarial training approach. Furthermore, we present a comprehensive relationship between natural accuracy, sensitivity-based robustness, and spatial robustness, supported by strong evidence from the perspective of robust representation. Crucially, to reconcile the interplay between the mutual impacts of various robustness components into one unified framework, we incorporate the \textit{Pareto criterion} into the adversarial robustness analysis, yielding a novel strategy called Pareto Ad
&lt;/p&gt;</description></item></channel></rss>