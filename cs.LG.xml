<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2311.13870</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-intention Inverse Q-learning for Interpretable Behavior Representation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#26041;&#38754;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#22312;&#37325;&#26500;&#21160;&#29289;&#22797;&#26434;&#34892;&#20026;&#20013;&#30340;&#22810;&#20010;&#24847;&#22270;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#37492;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#24847;&#22270;IRL&#26694;&#26550;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;IRL&#25512;&#26029;&#31163;&#25955;&#30340;&#26102;&#21464;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#65288;&#39532;&#23572;&#31185;&#22827;&#65289;&#21464;&#37327;&#36870;Q&#23398;&#20064;&#65288;L(M)V-IQL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#36866;&#24212;&#31163;&#25955;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#30340;IRL&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#24182;&#20026;&#27599;&#20010;&#24847;&#22270;&#29420;&#31435;&#35299;&#20915;IRL&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23545;&#19981;&#21516;&#30495;&#23454;&#40736;&#31867;&#34892;&#20026;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#26395;&#25171;&#24320;&#25512;&#21160;&#31185;&#23398;&#19982;&#24037;&#31243;&#24212;&#29992;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advancing the understanding of decision-making processes, Inverse Reinforcement Learning (IRL) have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To tackle the challenge, we introduce Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a novel class of IRL algorthms tailored for accommodating discrete intrinsic reward functions. Leveraging an Expectation-Maximization approach, we cluster observed expert trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00785</link><description>&lt;p&gt;
&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#20043;&#35868;&#65306;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30740;&#31350;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Disentangling Hippocampal Shape Variations: A Study of Neurological Disorders Using Graph Variational Autoencoder with Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#35299;&#24320;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20013;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#30340;&#20851;&#38190;&#28508;&#21464;&#37327;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#22312;&#35299;&#24320;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#30740;&#31350;&#65292;&#19987;&#27880;&#20110;&#22312;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#32972;&#26223;&#19979;&#20174;&#25193;&#25955;&#24352;&#37327;&#25104;&#20687;&#65288;DTI&#65289;&#25968;&#25454;&#38598;&#20013;&#35299;&#24320;&#28023;&#39532;&#24418;&#29366;&#21464;&#24322;&#12290;&#20511;&#21161;&#22686;&#24378;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21306;&#20998;&#20195;&#34920;&#24180;&#40836;&#21644;&#26159;&#21542;&#24739;&#30149;&#30340;&#20004;&#20010;&#19981;&#21516;&#28508;&#21464;&#37327;&#26469;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31995;&#21015;VAE&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22686;&#24378;&#30340;&#35299;&#24320;&#33021;&#21147;&#12290;&#36825;&#20010;&#35780;&#20272;&#20351;&#29992;&#20102;&#26469;&#33258;DTI&#28023;&#39532;&#25968;&#25454;&#38598;&#30340;&#21512;&#25104;3D&#29615;&#24418;&#32593;&#26684;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;3D&#28023;&#39532;&#32593;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#35299;&#24320;&#27169;&#22411;&#22312;&#35299;&#24320;&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#23646;&#24615;&#21644;&#24341;&#23548;VAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21306;&#20998;&#19981;&#21516;&#24180;&#40836;&#32452;&#21644;&#30142;&#30149;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00785v1 Announce Type: cross  Abstract: This paper presents a comprehensive study focused on disentangling hippocampal shape variations from diffusion tensor imaging (DTI) datasets within the context of neurological disorders. Leveraging a Graph Variational Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach aims to improve interpretability by disentangling two distinct latent variables corresponding to age and the presence of diseases. In our ablation study, we investigate a range of VAE architectures and contrastive loss functions, showcasing the enhanced disentanglement capabilities of our approach. This evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh datasets derived from the DTI hippocampal dataset. Our supervised disentanglement model outperforms several state-of-the-art (SOTA) methods like attribute and guided VAEs in terms of disentanglement scores. Our model distinguishes between age groups and disease status in pa
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.10940</link><description>&lt;p&gt;
ViSaRL&#65306;&#21463;&#20154;&#31867;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ViSaRL: Visual Reinforcement Learning Guided by Human Saliency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10940
&lt;/p&gt;
&lt;p&gt;
ViSaRL&#25552;&#20986;&#20102;Visual Saliency-Guided Reinforcement Learning&#65288;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26469;&#26174;&#33879;&#25552;&#39640;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#39640;&#32500;&#20687;&#32032;&#36755;&#20837;&#22521;&#35757;&#26426;&#22120;&#20154;&#25191;&#34892;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#35266;&#23519;&#20027;&#35201;&#30001;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#32452;&#25104;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#33021;&#22815;&#22312;&#35270;&#35273;&#19978;&#20851;&#27880;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#35270;&#35273;&#26174;&#33879;&#24615;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;ViSaRL&#65289;&#12290;&#20351;&#29992;ViSaRL&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26174;&#30528;&#25552;&#39640;&#20102;RL&#20195;&#29702;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;DeepMind&#25511;&#21046;&#22522;&#20934;&#12289;&#20223;&#30495;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#19978;&#30340;&#25104;&#21151;&#29575;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26174;&#33879;&#24615;&#25972;&#21512;&#21040;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#32534;&#30721;&#22120;&#20013;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20351;&#29992;ViSaRL&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#23545;&#21508;&#31181;&#35270;&#35273;&#25200;&#21160;&#65292;&#21253;&#25324;&#24863;&#30693;&#22122;&#22768;&#21644;&#22330;&#26223;&#21464;&#21270;&#65292;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;ViSaRL&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#25104;&#21151;&#29575;&#20960;&#20046;&#32763;&#20102;&#19968;&#30058;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10940v1 Announce Type: cross  Abstract: Training robots to perform complex control tasks from high-dimensional pixel input using reinforcement learning (RL) is sample-inefficient, because image observations are comprised primarily of task-irrelevant information. By contrast, humans are able to visually attend to task-relevant objects and areas. Based on this insight, we introduce Visual Saliency-Guided Reinforcement Learning (ViSaRL). Using ViSaRL to learn visual representations significantly improves the success rate, sample efficiency, and generalization of an RL agent on diverse tasks including DeepMind Control benchmark, robot manipulation in simulation and on a real robot. We present approaches for incorporating saliency into both CNN and Transformer-based encoders. We show that visual representations learned using ViSaRL are robust to various sources of visual perturbations including perceptual noise and scene variations. ViSaRL nearly doubles success rate on the real-
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02672</link><description>&lt;p&gt;
&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65306;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22914;&#26524;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#21487;&#20197;&#38598;&#20013;&#65292;&#21487;&#20197;&#23545;CATEs&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#65292;&#21017;&#24456;&#38590;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;CATE&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#12290;&#21322;&#21442;&#25968;&#25110;&#38750;&#21442;&#25968;&#30340;CATE&#27169;&#22411;&#33021;&#22815;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#31283;&#20581;&#22320;&#36827;&#34892;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#23545;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#25552;&#20986;&#26377;&#25928;&#30340;&#36890;&#20449;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02636</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#23398;&#20064;&#29420;&#31435;&#30340;&#22240;&#26524;&#26426;&#21046;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Learn Independent Causal Mechanisms?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#19981;&#24120;&#35265;&#30340;&#29615;&#22659;&#35774;&#32622;&#25110;&#20998;&#24067;&#21464;&#21270;&#30340;&#20219;&#21153;&#20013;&#65292;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#30446;&#21069;&#36890;&#24120;&#36890;&#36807;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26159;&#33030;&#24369;&#30340;&#65292;&#22240;&#20026;&#20219;&#21153;&#30340;&#33539;&#22260;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#25110;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#37027;&#20123;&#23398;&#20064;&#25277;&#35937;&#21464;&#37327;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#31995;&#32479;&#65292;&#22914;&#22240;&#26524;&#27169;&#22411;&#65292;&#21487;&#20197;&#34920;&#29616;&#20986;&#23545;&#20998;&#24067;&#21464;&#21270;&#30340;&#26356;&#24378;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#23384;&#22312;&#24182;&#20351;&#29992;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65288;ICMs&#65289;&#65292;&#34920;&#31034;&#21482;&#31232;&#30095;&#20132;&#20114;&#30340;&#39640;&#23618;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#20004;&#20010;&#27010;&#24565;&#65292;&#22312;LLMs&#20013;&#23398;&#20064;ICMs&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#31232;&#30095;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#30340;&#26032;LLM&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite impressive performance on language modelling and complex reasoning tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon settings or with distribution shifts, exhibiting some lack of generalisation ability. This issue has usually been alleviated by feeding more training data into the LLM. However, this method is brittle, as the scope of tasks may not be readily predictable or may evolve, and updating the model with new data generally requires extensive additional training. By contrast, systems, such as causal models, that learn abstract variables and causal relationships can demonstrate increased robustness against changes in the distribution. One reason for this success is the existence and use of Independent Causal Mechanisms (ICMs) representing high-level concepts that only sparsely interact. In this work, we apply two concepts from causality to learn ICMs within LLMs. We develop a new LLM architecture composed of multiple sparsely interacting language
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00515</link><description>&lt;p&gt;
&#24320;&#21457;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#25237;&#36164;&#32452;&#21512;&#39118;&#38505;&#31649;&#29702;&#12290;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#65292;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#30340;&#25237;&#36164;&#31574;&#30053;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#29992;&#20316;&#21453;&#24212;&#24615;&#26234;&#33021;&#20307;&#20197;&#22312;&#39640;&#24230;&#21160;&#33633;&#30340;&#37329;&#34701;&#24066;&#22330;&#29615;&#22659;&#19979;&#24555;&#36895;&#23398;&#20064;&#24182;&#21709;&#24212;&#26032;&#30340;&#25237;&#36164;&#31574;&#30053;&#65292;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#37329;&#34701;&#34892;&#19994;&#20043;&#38388;&#23384;&#22312;&#38750;&#24120;&#22797;&#26434;&#30340;&#20851;&#32852;&#24615;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#36235;&#21183;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#22522;&#20110;&#30340;&#26234;&#33021;&#20307;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#26368;&#22823;&#21270;&#26032;&#21046;&#23450;&#30340;&#25237;&#36164;&#32452;&#21512;&#30340;&#24635;&#22238;&#25253;&#65292;&#32780;&#24573;&#35270;&#20854;&#22312;&#20840;&#29699;&#25110;&#21306;&#22495;&#37096;&#38376;&#30340;&#21508;&#31181;&#24066;&#22330;&#26465;&#20214;&#21160;&#33633;&#19979;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MASA&#30340;&#22810;&#26234;&#33021;&#20307;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#21327;&#21516;&#21644;&#21453;&#24212;&#30340;&#26234;&#33021;&#20307;&#37319;&#29992;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#20180;&#32454;&#21160;&#24577;&#24179;&#34913;&#25972;&#20307;&#25237;&#36164;&#32452;&#21512;&#22238;&#25253;&#21644;&#28508;&#22312;&#39118;&#38505;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10483</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#23545;&#25286;&#20998;&#23398;&#20064;&#36827;&#34892;&#34989;&#20987;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Passive Inference Attacks on Split Learning via Adversarial Regularization. (arXiv:2310.10483v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25286;&#20998;&#23398;&#20064;&#30340;&#34987;&#21160;&#25512;&#29702;&#25915;&#20987;&#26694;&#26550;SDAR&#65292;&#36890;&#36807;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#25512;&#26029;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#21644;&#26631;&#31614;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25286;&#20998;&#23398;&#20064;(SL)&#24050;&#25104;&#20026;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#34429;&#28982;&#20197;&#21069;&#25915;&#20987;SL&#30340;&#23581;&#35797;&#24448;&#24448;&#20381;&#36182;&#20110;&#36807;&#20110;&#24378;&#30828;&#30340;&#20551;&#35774;&#25110;&#32773;&#38024;&#23545;&#26131;&#21463;&#25915;&#20987;&#30340;&#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#35797;&#22270;&#24320;&#21457;&#26356;&#21152;&#23454;&#29992;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SDAR&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#25317;&#26377;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#30340;SL&#30340;&#26032;&#25915;&#20987;&#26694;&#26550;&#12290;SDAR&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#21644;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#23458;&#25143;&#31471;&#31169;&#26377;&#27169;&#22411;&#30340;&#21487;&#35299;&#30721;&#27169;&#25311;&#22120;&#65292;&#22312;&#22522;&#26412;SL&#19979;&#21487;&#20197;&#26377;&#25928;&#22320;&#25512;&#26029;&#20986;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#29305;&#24449;&#65292;&#24182;&#22312;U&#22411;SL&#19979;&#25512;&#26029;&#20986;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#22330;&#26223;&#20013;&#65292;&#29616;&#26377;&#30340;&#34987;&#21160;&#25915;&#20987;&#38590;&#20197;&#26377;&#25928;&#22320;&#37325;&#24314;&#23458;&#25143;&#31471;&#30340;&#31169;&#26377;&#25968;&#25454;&#26102;&#65292;SDAR&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#20027;&#21160;&#25915;&#20987;&#30456;&#24403;&#30340;&#25915;&#20987;&#24615;&#33021;&#12290;&#22312;CIFAR-10&#19978;&#65292;&#22312;&#28145;&#24230;&#25286;&#20998;&#27700;&#24179;&#20026;7&#30340;&#24773;&#20917;&#19979;&#65292;SDAR&#36798;&#21040;&#20102;&#25915;&#20987;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) has emerged as a practical and efficient alternative to traditional federated learning. While previous attempts to attack SL have often relied on overly strong assumptions or targeted easily exploitable models, we seek to develop more practical attacks. We introduce SDAR, a novel attack framework against SL with an honest-but-curious server. SDAR leverages auxiliary data and adversarial regularization to learn a decodable simulator of the client's private model, which can effectively infer the client's private features under the vanilla SL, and both features and labels under the U-shaped SL. We perform extensive experiments in both configurations to validate the effectiveness of our proposed attacks. Notably, in challenging but practical scenarios where existing passive attacks struggle to reconstruct the client's private data effectively, SDAR consistently achieves attack performance comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR achi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2310.07511</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#24046;&#20851;&#31995;&#23398;&#20064;&#23454;&#29616;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Remote Sensing Anomaly Detector Across Modalities and Scenes via Deviation Relationship Learning. (arXiv:2310.07511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07511
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20559;&#24046;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#24322;&#24120;&#26816;&#27979;&#22120;&#21487;&#20197;&#25214;&#21040;&#19982;&#32972;&#26223;&#19981;&#31526;&#30340;&#30446;&#26631;&#20316;&#20026;&#28508;&#22312;&#30446;&#26631;&#12290;&#37492;&#20110;&#22320;&#29699;&#24322;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#36328;&#27169;&#24577;&#21644;&#22330;&#26223;&#30340;&#32479;&#19968;&#24322;&#24120;&#26816;&#27979;&#22120;&#24212;&#35813;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#65292;&#24182;&#19988;&#23545;&#20110;&#26032;&#30340;&#22320;&#29699;&#35266;&#27979;&#28304;&#21644;&#24322;&#24120;&#31867;&#22411;&#20855;&#26377;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#20165;&#38480;&#20110;&#21333;&#19968;&#27169;&#24577;&#21644;&#21333;&#19968;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#26088;&#22312;&#23398;&#20064;&#19981;&#26029;&#21464;&#21270;&#30340;&#32972;&#26223;&#20998;&#24067;&#12290;&#22312;&#26222;&#36941;&#30340;&#24322;&#24120;&#20559;&#24046;&#27169;&#24335;&#30340;&#28608;&#21457;&#19979;&#65292;&#21363;&#24322;&#24120;&#23637;&#29616;&#20986;&#19982;&#20854;&#23616;&#37096;&#29615;&#22659;&#30340;&#20559;&#24046;&#29305;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29305;&#24449;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#22522;&#20110;&#20559;&#24046;&#20851;&#31995;&#30340;&#26080;&#21521;&#21452;&#23618;&#22270;&#65292;&#20854;&#20013;&#24322;&#24120;&#35780;&#20998;&#34987;&#24314;&#27169;&#20026;&#22312;&#32972;&#26223;&#21644;&#27491;&#24120;&#23545;&#35937;&#30340;&#27169;&#24335;&#32473;&#23450;&#19979;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#30446;&#26631;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#26465;&#20214;&#27010;&#29575;&#25490;&#24207;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#20363;&#21270;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing anomaly detector can find the objects deviating from the background as potential targets. Given the diversity in earth anomaly types, a unified anomaly detector across modalities and scenes should be cost-effective and flexible to new earth observation sources and anomaly types. However, the current anomaly detectors are limited to a single modality and single scene, since they aim to learn the varying background distribution. Motivated by the universal anomaly deviation pattern, in that anomalies exhibit deviations from their local context, we exploit this characteristic to build a unified anomaly detector. Firstly, we reformulate the anomaly detection task as an undirected bilayer graph based on the deviation relationship, where the anomaly score is modeled as the conditional probability, given the pattern of the background and normal objects. The learning objective is then expressed as a conditional probability ranking problem. Furthermore, we design an instantiation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06488</link><description>&lt;p&gt;
SpikeCLIP&#65306;&#19968;&#31181;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#25193;&#23637;&#65292;&#24182;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#24050;&#32463;&#35777;&#26126;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#33021;&#25928;&#25552;&#39640;&#21644;&#31526;&#21512;&#29983;&#29289;&#21512;&#29702;&#24615;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#21333;&#27169;&#24577;&#30340;SNNs&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#30340;&#24773;&#26223;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#21463;&#21040;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SpikeCLIP&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#23545;&#40784;&#39044;&#35757;&#32451;+&#21452;&#25439;&#22833;&#24494;&#35843;&#8221;&#30340;&#20004;&#27493;&#39588;&#37197;&#26041;&#65292;&#26469;&#35299;&#20915;&#33033;&#20914;&#35745;&#31639;&#32972;&#26223;&#19979;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24120;&#29992;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#35780;&#20272;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;SNNs&#21462;&#24471;&#20102;&#19982;&#20854;DNNs&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;SpikeCLIP&#22312;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#20445;&#25345;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) have demonstrated the capability to achieve comparable performance to deep neural networks (DNNs) in both visual and linguistic domains while offering the advantages of improved energy efficiency and adherence to biological plausibility. However, the extension of such single-modality SNNs into the realm of multimodal scenarios remains an unexplored territory. Drawing inspiration from the concept of contrastive language-image pre-training (CLIP), we introduce a novel framework, named SpikeCLIP, to address the gap between two modalities within the context of spike-based computing through a two-step recipe involving ``Alignment Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that SNNs achieve comparable results to their DNN counterparts while significantly reducing energy consumption across a variety of datasets commonly used for multimodal model evaluation. Furthermore, SpikeCLIP maintains robust performance in image classification 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#35268;&#24459;&#12290;&#36890;&#36807;&#23545;&#31890;&#23376;&#36712;&#36857;&#30340;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20102;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#23384;&#22312;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.05273</link><description>&lt;p&gt;
&#22312;&#22810;&#20307;&#31995;&#32479;&#20013;&#23398;&#20064;&#21147;&#23398;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Learning force laws in many-body systems. (arXiv:2310.05273v1 [physics.plasm-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05273
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#35268;&#24459;&#12290;&#36890;&#36807;&#23545;&#31890;&#23376;&#36712;&#36857;&#30340;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20102;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#23384;&#22312;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#33258;&#28982;&#31995;&#32479;&#30340;&#31185;&#23398;&#35268;&#24459;&#21487;&#33021;&#27604;&#25105;&#20204;&#30340;&#30452;&#35273;&#26356;&#22797;&#26434;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#29616;&#35268;&#24459;&#30340;&#26041;&#27861;&#24517;&#39035;&#25913;&#21464;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#20854;&#32467;&#26500;&#24212;&#35813;&#31526;&#21512;&#22522;&#26412;&#30340;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#29289;&#29702;&#30452;&#35273;&#30340;ML&#26041;&#27861;&#65292;&#20197;&#25512;&#26029;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#23454;&#39564;&#20013;&#30340;&#21147;&#23398;&#27861;&#21017;&#12290;&#36890;&#36807;&#23545;3D&#31890;&#23376;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#21644;&#38750;&#30456;&#21516;&#31890;&#23376;&#20043;&#38388;&#30340;&#26377;&#25928;&#38750;&#20114;&#36870;&#21147;&#65292;&#24182;&#25552;&#21462;&#20986;&#27599;&#20010;&#31890;&#23376;&#30340;&#36136;&#37327;&#21644;&#30005;&#33655;&#12290;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65288;R^2 &gt; 0.99&#65289;&#25351;&#31034;&#20986;&#23576;&#22467;&#31561;&#31163;&#23376;&#20307;&#20013;&#36229;&#20986;&#24403;&#21069;&#29702;&#35770;&#20998;&#36776;&#29575;&#30340;&#26032;&#29289;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26041;&#27861;&#22914;&#20309;&#24341;&#23548;&#22810;&#20307;&#31995;&#32479;&#20013;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific laws describing natural systems may be more complex than our intuition can handle, and thus how we discover laws must change. Machine learning (ML) models can analyze large quantities of data, but their structure should match the underlying physical constraints to provide useful insight. Here we demonstrate a ML approach that incorporates such physical intuition to infer force laws in dusty plasma experiments. Trained on 3D particle trajectories, the model accounts for inherent symmetries and non-identical particles, accurately learns the effective non-reciprocal forces between particles, and extracts each particle's mass and charge. The model's accuracy (R^2 &gt; 0.99) points to new physics in dusty plasma beyond the resolution of current theories and demonstrates how ML-powered approaches can guide new routes of scientific discovery in many-body systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00158</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#20998;&#31867;&#20013;&#30340;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21453;&#39304;&#24341;&#23548;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#65292;&#23558;&#38745;&#24577;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#21253;&#21547;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29616;&#29366;&#26159;&#20351;&#29992;&#26469;&#33258;&#38271;&#23614;&#20998;&#24067;&#30340;&#30495;&#23454;&#22270;&#20687;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#29992;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#36825;&#20123;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#25253;&#21578;&#20102;&#36866;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#39304;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#36825;&#23558;&#20419;&#36827;&#29983;&#25104;&#26679;&#26412;&#30340;&#26377;&#29992;&#24615;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#26377;&#29992;&#30340;&#21512;&#25104;&#26679;&#26412;&#22686;&#24378;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20174;&#20998;&#31867;&#22120;&#21040;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#21453;&#39304;&#26469;&#39537;&#21160;&#37319;&#26679;&#12290;&#20026;&#20102;&#20351;&#35813;&#26694;&#26550;&#26377;&#25928;&#65292;&#25105;&#20204;&#21457;&#29616;&#26679;&#26412;&#24517;&#39035;&#25509;&#36817;&#25163;&#22836;&#20219;&#21153;&#30340;&#30495;&#23454;&#25968;&#25454;&#25903;&#25345;&#65292;&#24182;&#19988;&#20855;&#26377;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38271;&#23614;&#25968;&#25454;&#38598;&#65288;ImageNe...&#19978;&#39564;&#35777;&#20102;&#19977;&#20010;&#21453;&#39304;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current status quo in machine learning is to use static datasets of real images for training, which often come from long-tailed distributions. With the recent advances in generative models, researchers have started augmenting these static datasets with synthetic data, reporting moderate performance improvements on classification tasks. We hypothesize that these performance gains are limited by the lack of feedback from the classifier to the generative model, which would promote the usefulness of the generated samples to improve the classifier's performance. In this work, we introduce a framework for augmenting static datasets with useful synthetic samples, which leverages one-shot feedback from the classifier to drive the sampling of the generative model. In order for the framework to be effective, we find that the samples must be close to the support of the real data of the task at hand, and be sufficiently diverse. We validate three feedback criteria on a long-tailed dataset (ImageNe
&lt;/p&gt;</description></item><item><title>NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.07761</link><description>&lt;p&gt;
NeFL: &#38024;&#23545;&#24322;&#26500;&#23458;&#25143;&#31471;&#30340;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NeFL: Nested Federated Learning for Heterogeneous Clients. (arXiv:2308.07761v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07761
&lt;/p&gt;
&lt;p&gt;
NeFL&#26159;&#19968;&#20010;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#21010;&#20998;&#20026;&#23376;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#23548;&#33268;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#21644;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#25345;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#24930;&#25110;&#33021;&#21147;&#26377;&#38480;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;&#38459;&#22622;&#32773;&#65289;&#20250;&#20943;&#24930;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#65292;&#21253;&#25324;&#24322;&#26500;&#35745;&#31639;&#21644;&#32593;&#32476;&#24102;&#23485;&#65292;&#24050;&#32463;&#34987;&#29992;&#26469;&#20943;&#36731;&#38459;&#22622;&#32773;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27169;&#22411;&#20998;&#21106;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#27169;&#22411;&#26550;&#26500;&#26041;&#38754;&#30340;&#33258;&#30001;&#24230;&#36739;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23884;&#22871;&#32852;&#37030;&#23398;&#20064;&#65288;NeFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#21644;&#23485;&#24230;&#32553;&#25918;&#23558;&#27169;&#22411;&#26377;&#25928;&#22320;&#20998;&#25104;&#23376;&#27169;&#22411;&#12290;NeFL&#36890;&#36807;&#23558;&#27169;&#22411;&#35299;&#37322;&#20026;&#35299;&#20915;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#27493;&#38271;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#20010;&#23376;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#35299;&#32806;&#20102;&#19968;&#20123;&#21442;&#25968;&#12290;NeFL&#20351;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#27969;&#31243;&#65292;&#24182;&#20351;&#27169;&#22411;&#33021;&#22815;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising approach in distributed learning keeping privacy. However, during the training pipeline of FL, slow or incapable clients (i.e., stragglers) slow down the total training time and degrade performance. System heterogeneity, including heterogeneous computing and network bandwidth, has been addressed to mitigate the impact of stragglers. Previous studies split models to tackle the issue, but with less degree-of-freedom in terms of model architecture. We propose nested federated learning (NeFL), a generalized framework that efficiently divides a model into submodels using both depthwise and widthwise scaling. NeFL is implemented by interpreting models as solving ordinary differential equations (ODEs) with adaptive step sizes. To address the inconsistency that arises when training multiple submodels with different architecture, we decouple a few parameters. NeFL enables resource-constrained clients to effectively join the FL pipeline and the model to be 
&lt;/p&gt;</description></item><item><title>INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08131</link><description>&lt;p&gt;
INFLECT-DGNN: &#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24433;&#21709;&#32773;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN: Influencer Prediction with Dynamic Graph Neural Networks. (arXiv:2307.08131v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08131
&lt;/p&gt;
&lt;p&gt;
INFLECT-DGNN&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12289;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#21644;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#29992;&#20110;&#24433;&#21709;&#32773;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#21644;GNN&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#32593;&#32476;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#12290;&#22312;&#25512;&#33616;&#21644;&#23450;&#21521;&#33829;&#38144;&#39046;&#22495;&#20013;&#65292;&#24433;&#21709;&#32773;&#26816;&#27979;&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#21160;&#24577;&#32593;&#32476;&#34920;&#31034;&#22823;&#22823;&#21463;&#30410;&#30340;&#39046;&#22495;&#65292;&#21407;&#22240;&#26159;&#19981;&#26029;&#21457;&#23637;&#30340;&#23458;&#25143;-&#21697;&#29260;&#20851;&#31995;&#12290;&#20026;&#20102;&#38416;&#36848;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;INFLECT-DGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38024;&#23545;&#22270;&#25968;&#25454;&#36866;&#24212;&#30340;&#21512;&#25104;&#23569;&#25968;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#20197;&#21450;&#31934;&#24515;&#35774;&#35745;&#30340;&#28378;&#21160;&#31383;&#21475;&#31574;&#30053;&#30340;&#26032;&#26694;&#26550;&#12290;&#20026;&#20102;&#35780;&#20272;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#22478;&#24066;&#32593;&#32476;&#30340;&#29420;&#29305;&#20225;&#19994;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20197;&#21033;&#28070;&#20026;&#39537;&#21160;&#30340;&#24433;&#21709;&#32773;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;RNN&#26469;&#32534;&#30721;&#26102;&#38388;&#23646;&#24615;&#20197;&#21450;GNN&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging network information for predictive modeling has become widespread in many domains. Within the realm of referral and targeted marketing, influencer detection stands out as an area that could greatly benefit from the incorporation of dynamic network representation due to the ongoing development of customer-brand relationships. To elaborate this idea, we introduce INFLECT-DGNN, a new framework for INFLuencer prEdiCTion with Dynamic Graph Neural Networks that combines Graph Neural Networks (GNN) and Recurrent Neural Networks (RNN) with weighted loss functions, the Synthetic Minority Oversampling TEchnique (SMOTE) adapted for graph data, and a carefully crafted rolling-window strategy. To evaluate predictive performance, we utilize a unique corporate data set with networks of three cities and derive a profit-driven evaluation methodology for influencer prediction. Our results show how using RNN to encode temporal attributes alongside GNNs significantly improves predictive perform
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.07975</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;: &#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21512;&#29702;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07975
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#20803;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#21644;&#29289;&#29702;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#23398;&#20064;&#21487;&#21464;&#24418;&#29289;&#20307;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#36890;&#36807;&#27491;&#36816;&#21160;&#23398;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#27169;&#25311;&#21487;&#21464;&#24418;&#32447;&#24615;&#29289;&#20307;&#65288;DLO&#65289;&#30340;&#21160;&#21147;&#23398;&#22312;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#34987;&#20154;&#35299;&#35835;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#27169;&#22411;&#24182;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21018;&#24615;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;R-FEM&#65289;&#65292;&#23558;DLO&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#21018;&#20307;&#38142;&#65292;&#20854;&#20869;&#37096;&#29366;&#24577;&#36890;&#36807;&#21160;&#21147;&#23398;&#32593;&#32476;&#20197;&#26102;&#38388;&#23637;&#24320;&#12290;&#30001;&#20110;&#35813;&#29366;&#24577;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#65292;&#21160;&#21147;&#23398;&#32593;&#32476;&#19982;&#19968;&#20010;&#29289;&#29702;&#24863;&#30693;&#30340;&#32534;&#30721;&#22120;&#20849;&#21516;&#35757;&#32451;&#65292;&#23558;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21464;&#37327;&#26144;&#23556;&#21040;&#21018;&#20307;&#38142;&#30340;&#29366;&#24577;&#12290;&#20026;&#20102;&#20419;&#20351;&#29366;&#24577;&#33719;&#24471;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#21033;&#29992;&#24213;&#23618;R-FEM&#27169;&#22411;&#30340;&#27491;&#36816;&#21160;&#23398;&#65288;FK&#65289;&#20316;&#20026;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#34987;&#31216;&#20026;&#8220;&#26377;&#38480;&#20803;&#21551;&#21457;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26159;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;DLO&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#37096;&#20998;&#35266;&#23519;&#20013;&#24471;&#20986;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate simulation of deformable linear object (DLO) dynamics is challenging if the task at hand requires a human-interpretable and data-efficient model that also yields fast predictions. To arrive at such model, we draw inspiration from the rigid finite element method (R-FEM) and model a DLO as a serial chain of rigid bodies whose internal state is unrolled through time by a dynamics network. As this state is not observed directly, the dynamics network is trained jointly with a physics-informed encoder mapping observed motion variables to the body chain's state. To encourage that the state acquires a physically meaningful representation, we leverage the forward kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a robot experiment that this architecture - being termed "Finite element inspired network" - forms an easy to handle, yet capable DLO dynamics model yielding physically interpretable predictions from partial observations.  The project code is ava
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15056</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#33021;&#22815;&#30830;&#20445;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#27844;&#28431;&#31169;&#23494;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24046;&#20998;&#38544;&#31169;&#30340;&#20195;&#20215;&#26159;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#21487;&#20197;&#35775;&#38382;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#36741;&#21161;&#20844;&#20849;&#25968;&#25454;&#12290;&#36825;&#20419;&#20351;&#20102;&#26368;&#36817;&#30740;&#31350;&#20844;&#20849;&#25968;&#25454;&#22312;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#26377;&#19968;&#23450;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20197;&#19979;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#65306;1.&#22312;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#22522;&#20110;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#26368;&#20248;&#65288;&#26368;&#22351;&#24773;&#20917;&#65289;&#35823;&#24046;&#26159;&#22810;&#23569;&#65311;&#21738;&#20123;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65311;2.&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#23454;&#36341;&#20013;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#35757;&#32451;&#65311;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#21644;&#20013;&#24515;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#19979;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#26368;&#20248;&#35823;&#24046;&#29575;&#30340;&#32039;&#23494;&#65288;&#26368;&#39640;&#24120;&#25968;&#22240;&#23376;&#65289;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#36825;&#19977;&#20010;&#38382;&#39064;&#26159;&#65306;&#22343;&#20540;&#20272;&#35745;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20984;&#22855;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.02128</link><description>&lt;p&gt;
&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65306;&#22312;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#24230;&#37327;&#34892;&#20026;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
System Neural Diversity: Measuring Behavioral Heterogeneity in Multi-Agent Learning. (arXiv:2305.02128v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24230;&#37327;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#25506;&#35752;&#20102;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31185;&#23398;&#25552;&#20379;&#20102;&#22810;&#26679;&#24615;&#20855;&#26377;&#38887;&#24615;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#24378;&#21046;&#35201;&#27714;&#21516;&#36136;&#24615;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25928;&#29575;&#12290;&#24403;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#19981;&#21463;&#21516;&#36136;&#31574;&#30053;&#30340;&#38480;&#21046;&#26102;&#65292;&#20010;&#20307;&#20195;&#29702;&#21487;&#33021;&#20250;&#21457;&#23637;&#20986;&#19981;&#21516;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#20135;&#29983;&#26377;&#21033;&#20110;&#31995;&#32479;&#30340;&#26032;&#20852;&#20114;&#34917;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#32570;&#20047;&#34913;&#37327;&#23398;&#20064;&#20195;&#29702;&#31995;&#32479;&#20013;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#24037;&#20855;&#24847;&#21619;&#30528;&#25105;&#20204;&#26080;&#27861;&#28145;&#20837;&#20102;&#35299;&#22810;&#26679;&#24615;&#23545;&#38598;&#20307;&#24377;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31995;&#32479;&#31070;&#32463;&#22810;&#26679;&#24615;&#65288;SND&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#31574;&#30053;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#34892;&#20026;&#24322;&#36136;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#25506;&#35752;&#24182;&#35777;&#26126;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#24182;&#23558;&#20854;&#19982;&#36328;&#23398;&#31185;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#26368;&#26032;&#34892;&#20026;&#22810;&#26679;&#24615;&#25351;&#26631;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary science provides evidence that diversity confers resilience. Yet, traditional multi-agent reinforcement learning techniques commonly enforce homogeneity to increase training sample efficiency. When a system of learning agents is not constrained to homogeneous policies, individual agents may develop diverse behaviors, resulting in emergent complementarity that benefits the system. Despite this feat, there is a surprising lack of tools that measure behavioral diversity in systems of learning agents. Such techniques would pave the way towards understanding the impact of diversity in collective resilience and performance. In this paper, we introduce System Neural Diversity (SND): a measure of behavioral heterogeneity for multi-agent systems where agents have stochastic policies. %over a continuous state space. We discuss and prove its theoretical properties, and compare it with alternate, state-of-the-art behavioral diversity metrics used in cross-disciplinary domains. Through
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01457</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#65306;&#25105;&#20204;&#35745;&#31639;&#24471;&#23545;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Memory of recurrent networks: Do we compute it right?. (arXiv:2305.01457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#35745;&#31639;&#38382;&#39064;&#12290;&#36890;&#36807;&#21457;&#29616;&#25968;&#20540;&#35780;&#20272;&#30340;&#19981;&#20934;&#30830;&#24615;&#20027;&#35201;&#28304;&#20110;&#25968;&#20540;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#30697;&#38453;MC&#30456;&#23545;&#20110;&#20013;&#31435;&#24615;&#30340;&#31283;&#20581;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25968;&#20540;&#35780;&#20272;&#20013;&#30340;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#23481;&#37327;&#65288;MC&#65289;&#30340;&#25968;&#20540;&#35780;&#20272;&#24120;&#24120;&#19982;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#30028;&#38480;&#30456;&#30683;&#30462;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#24773;&#20917;&#65292;&#23545;&#24212;&#30340;Kalman&#21487;&#25511;&#30697;&#38453;&#30340;&#31209;&#24050;&#34987;&#35777;&#26126;&#31561;&#20110;&#24635;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#20110;&#35760;&#24518;&#19981;&#20934;&#30830;&#30340;&#25968;&#20540;&#35780;&#20272;&#30340;&#21508;&#31181;&#21407;&#22240;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#38382;&#39064;&#26159;&#32431;&#31929;&#25968;&#20540;&#26041;&#38754;&#19978;&#30340;&#65292;&#24448;&#24448;&#22312;&#36817;&#26399;&#25991;&#29486;&#20013;&#34987;&#24573;&#35270;&#12290;&#26356;&#26126;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32447;&#24615;MC&#30340;Krylov&#32467;&#26500;&#34987;&#24573;&#30053;&#26102;&#65292;&#29702;&#35770;MC&#21644;&#23427;&#30340;&#32463;&#39564;&#20540;&#20043;&#38388;&#20250;&#23384;&#22312;&#24046;&#36317;&#12290;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#65292;&#21033;&#29992;MC&#30456;&#23545;&#20110;&#36755;&#20837;&#25513;&#30721;&#30697;&#38453;&#30340;&#20013;&#31435;&#24615;&#65292;&#24320;&#21457;&#20986;&#31283;&#20581;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24471;&#21040;&#30340;&#35760;&#24518;&#26354;&#32447;&#19982;&#29702;&#35770;&#23436;&#20840;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical evaluations of the memory capacity (MC) of recurrent neural networks reported in the literature often contradict well-established theoretical bounds. In this paper, we study the case of linear echo state networks, for which the total memory capacity has been proven to be equal to the rank of the corresponding Kalman controllability matrix. We shed light on various reasons for the inaccurate numerical estimations of the memory, and we show that these issues, often overlooked in the recent literature, are of an exclusively numerical nature. More explicitly, we prove that when the Krylov structure of the linear MC is ignored, a gap between the theoretical MC and its empirical counterpart is introduced. As a solution, we develop robust numerical approaches by exploiting a result of MC neutrality with respect to the input mask matrix. Simulations show that the memory curves that are recovered using the proposed methods fully agree with the theory.
&lt;/p&gt;</description></item></channel></rss>