<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01105</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey for Foundation Models in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#22238;&#39038;&#20102;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#22312;&#35268;&#21010;&#12289;&#20223;&#30495;&#21644;&#20851;&#38190;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#35201;&#36129;&#29486;&#65292;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#21644;&#32763;&#35793;&#33021;&#21147;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#29289;&#20307;&#26816;&#27979;&#21644;&#39550;&#39542;&#22330;&#26223;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;40&#22810;&#31687;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#23637;&#31034;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#20316;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#35268;&#21010;&#21644;&#20223;&#30495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20854;&#22312;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#20851;&#38190;&#20219;&#21153;&#20013;&#24471;&#21040;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#21644;&#36319;&#36394;&#65292;&#20197;&#21450;&#20026;&#20223;&#30495;&#21644;&#27979;&#35797;&#21019;&#24314;&#36924;&#30495;&#30340;&#39550;&#39542;&#22330;&#26223;&#12290;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#22810;&#26679;&#30340;&#36755;&#20837;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#29702;&#35299;&#21644;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#20998;&#31867;&#65292;&#26681;&#25454;&#27169;&#24577;&#21644;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#30340;&#21151;&#33021;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the meth
&lt;/p&gt;</description></item><item><title>ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09724</link><description>&lt;p&gt;
ClaimVer&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09724
&lt;/p&gt;
&lt;p&gt;
ClaimVer&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#22768;&#26126;&#32423;&#39564;&#35777;&#21644;&#35777;&#25454;&#24402;&#22240;&#65292;&#33268;&#21147;&#20110;&#25552;&#39640;&#29992;&#25143;&#23545;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#30340;&#20449;&#20219;&#24182;&#24378;&#35843;&#32454;&#31890;&#24230;&#35777;&#25454;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20256;&#25773;&#30340;&#20449;&#24687;&#35823;&#23548;&#21644;&#31038;&#20132;&#23186;&#20307;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#28608;&#22686;&#20013;&#65292;&#39564;&#35777;&#21644;&#20449;&#20219;&#25152;&#36935;&#21040;&#30340;&#20449;&#24687;&#21464;&#24471;&#26085;&#30410;&#22256;&#38590;&#12290;&#35768;&#22810;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21644;&#24037;&#20855;&#24050;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#36866;&#24403;&#30340;&#21487;&#35299;&#37322;&#24615;&#25110;&#32454;&#31890;&#24230;&#65292;&#26080;&#27861;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#12289;&#21487;&#35775;&#38382;&#19988;&#33021;&#22815;&#25191;&#34892;&#32454;&#31890;&#24230;&#35777;&#25454;&#24402;&#22240;&#30340;&#25991;&#26412;&#39564;&#35777;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24314;&#31435;&#29992;&#25143;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20449;&#20219;&#38656;&#35201;&#21576;&#29616;&#27599;&#20010;&#39044;&#27979;&#32972;&#21518;&#30340;&#29702;&#30001;&#65292;&#22240;&#20026;&#30740;&#31350;&#34920;&#26126;&#36825;&#26174;&#33879;&#24433;&#21709;&#20154;&#20204;&#23545;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#23558;&#29992;&#25143;&#20851;&#27880;&#37325;&#28857;&#25918;&#22312;&#20855;&#20307;&#30340;&#38382;&#39064;&#20869;&#23481;&#19978;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#31616;&#21333;&#30340;&#31548;&#32479;&#26631;&#31614;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{ClaimVer&#65292;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;}$&#65292;&#26088;&#22312;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09724v1 Announce Type: new  Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. It is also paramount to localize and bring users' attention to the specific problematic content, instead of providing simple blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric framework}$ tailored to meet users' info
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20843;&#20010;&#31454;&#36187;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04693</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#20013;&#31995;&#32479;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Systems' Performance in Natural Language Processing Competitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#32467;&#26524;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20843;&#20010;&#31454;&#36187;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#20854;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#31454;&#36187;&#22312;&#31185;&#23398;&#25216;&#26415;&#39046;&#22495;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#31454;&#36187;&#28041;&#21450;&#23450;&#20041;&#20219;&#21153;&#65292;&#36873;&#25321;&#35780;&#20272;&#20998;&#25968;&#21644;&#35774;&#35745;&#32467;&#26524;&#39564;&#35777;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#36890;&#24120;&#20250;&#25910;&#21040;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24182;&#34987;&#35201;&#27714;&#20026;&#20027;&#21150;&#26041;&#20445;&#30041;&#30340;&#19968;&#20010;&#26410;&#20844;&#24320;&#25968;&#25454;&#38598;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#31454;&#36187;&#32467;&#26524;&#21644;&#31454;&#20105;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#35774;&#35745;&#25104;&#36890;&#29992;&#30340;&#65292;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#20102;&#20843;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31454;&#36187;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04693v1 Announce Type: new  Abstract: Collaborative competitions have gained popularity in the scientific and technological fields. These competitions involve defining tasks, selecting evaluation scores, and devising result verification methods. In the standard scenario, participants receive a training set and are expected to provide a solution for a held-out dataset kept by organizers. An essential challenge for organizers arises when comparing algorithms' performance, assessing multiple participants, and ranking them. Statistical tools are often used for this purpose; however, traditional statistical methods often fail to capture decisive differences between systems' performance. This manuscript describes an evaluation methodology for statistically analyzing competition results and competition. The methodology is designed to be universally applicable; however, it is illustrated using eight natural language competitions as case studies involving classification and regressio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01942</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;&#20943;&#36731;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#38899;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise on Graph via Topological Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31934;&#24515;&#27880;&#37322;&#30340;&#22522;&#20934;&#27979;&#35797;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#21463;&#21040;&#30456;&#24403;&#22823;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#22312;&#26679;&#26412;&#36873;&#25321;&#26041;&#38754;&#30340;&#25506;&#32034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24212;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#20391;&#37325;&#20110;i.i.d&#25968;&#25454;&#65292;&#24403;&#36716;&#21521;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#21644;GNNs&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#25361;&#25112;&#65306;(1) &#20301;&#20110;&#25299;&#25169;&#31867;&#36793;&#30028;&#38468;&#36817;&#30340;&#33410;&#28857;&#23545;&#20998;&#31867;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#26080;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#26679;&#26412;&#36873;&#25321;&#25104;&#21151;&#21306;&#20998;&#12290;(2) &#27809;&#26377;&#21487;&#29992;&#30340;&#34913;&#37327;&#26631;&#20934;&#32771;&#34385;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#20197;&#20419;&#36827;&#22270;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#65288;TSS&#65289;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#25552;&#21319;&#22270;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01942v1 Announce Type: new  Abstract: Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11404</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Stability of Deep Learning Latent Feature Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11404
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#31283;&#23450;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#21487;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#31283;&#23450;&#24615;&#35780;&#20272;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#20102;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#21644;&#19968;&#22871;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#38598;&#22312;&#21508;&#20010;&#23398;&#31185;&#30340;&#32479;&#35745;&#24314;&#27169;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#38477;&#32500;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20197;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#25552;&#28860;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#32780;&#33879;&#31216;&#65292;&#36890;&#36807;&#38477;&#32500;&#30340;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#23454;&#29616;&#24314;&#27169;&#12289;&#21487;&#35270;&#21270;&#21644;&#21387;&#32553;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#21040;&#22320;&#29699;&#31185;&#23398;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#28508;&#22312;&#31354;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#30830;&#20445;&#21518;&#32493;&#20998;&#26512;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#31283;&#23450;&#24615;&#34987;&#23450;&#20041;&#20026;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#24494;&#23567;&#25968;&#25454;&#12289;&#35757;&#32451;&#23454;&#29616;&#21644;&#21442;&#25968;&#25200;&#21160;&#30340;&#19981;&#21464;&#24615;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#21364;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#30028;&#23450;&#20102;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19977;&#31181;&#31283;&#23450;&#24615;&#31867;&#22411;&#65292;&#26679;&#26412;&#31283;&#23450;&#24615;&#12289;&#32467;&#26500;&#31283;&#23450;&#24615;&#21644;&#25512;&#26029;&#31283;&#23450;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11404v1 Announce Type: new  Abstract: High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked.   Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03655</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23884;&#22871;&#20302;&#31209;&#36817;&#20284;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Operator SVD with Neural Networks via Nested Low-Rank Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#65292;&#35745;&#31639;&#32473;&#23450;&#32447;&#24615;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#65288;EVD&#65289;&#25110;&#25214;&#21040;&#20854;&#20027;&#35201;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#29305;&#24449;&#20989;&#25968;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#22522;&#20110;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36817;&#20284;&#34920;&#24449;&#65292;&#24182;&#20276;&#38543;&#30528;&#31216;&#20026;&#23884;&#22871;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#27491;&#30830;&#30340;&#39034;&#24207;&#23398;&#20064;&#21069;L&#20010;&#22855;&#24322;&#20540;&#21644;&#22855;&#24322;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20419;&#36827;&#20102;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#65292;&#36825;&#20010;&#20844;&#24335;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#27714;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#22312;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02047</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Quality and Trust in LLM-generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#38169;&#12290;&#29992;&#25143;&#38656;&#35201;&#21487;&#38752;&#30340;&#25351;&#31034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#65292;&#20174;&#32780;&#21487;&#20197;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#26159;&#21542;&#20351;&#29992;&#35813;&#36755;&#20986;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#23558;&#36755;&#20986;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#32852;&#65307;&#22914;&#26524;&#32622;&#20449;&#24230;&#19982;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#24378;&#30456;&#20851;&#65292;&#21017;&#31216;&#35813;&#27169;&#22411;&#20026;&#33391;&#22909;&#26657;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#23433;&#20840;&#25509;&#21463;&#65292;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#25298;&#32477;&#12290;&#26657;&#20934;&#36804;&#20170;&#20027;&#35201;&#22312;&#38750;&#29983;&#25104;&#24615;&#65288;&#20363;&#22914;&#20998;&#31867;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#24456;&#23481;&#26131;&#20986;&#38169;&#65306;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#30452;&#25509;&#20351;&#29992;&#12289;&#32463;&#36807;&#20180;&#32454;&#23457;&#26597;&#21518;&#20351;&#29992;&#25110;&#20002;&#24323;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;&#26657;&#20934;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#27492;&#26657;&#20934;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2311.12163</link><description>&lt;p&gt;
&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Quantum Inception Score
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12163
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#26032;&#25351;&#26631;&#65292;&#35777;&#26126;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#25581;&#31034;&#20102;&#20854;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#32463;&#20856;&#29983;&#25104;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21462;&#24471;&#24040;&#22823;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24320;&#22987;&#20102;&#23545;&#23427;&#20204;&#37327;&#23376;&#29256;&#26412;&#30340;&#28909;&#20999;&#25506;&#32034;&#12290;&#20026;&#20102;&#24320;&#22987;&#36825;&#19968;&#25506;&#32034;&#20043;&#26053;&#65292;&#24320;&#21457;&#19968;&#20010;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#24456;&#37325;&#35201;&#30340;&#65307;&#22312;&#32463;&#20856;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#36825;&#26679;&#30340;&#20363;&#23376;&#20415;&#26159;&#21551;&#33945;&#20998;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#65292;&#23427;&#23558;&#36136;&#37327;&#19982;&#29992;&#20110;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#30340;&#37327;&#23376;&#36890;&#36947;&#30340;Holevo&#20449;&#24687;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#65292;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#27604;&#23427;&#20204;&#30340;&#32463;&#20856;&#23545;&#24212;&#29289;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#30528;&#30001;&#19981;&#23545;&#31216;&#24615;&#30340;&#36164;&#28304;&#29702;&#35770;&#21644;&#32416;&#32544;&#25152;&#34920;&#24449;&#30340;&#37327;&#23376;&#30456;&#24178;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#23376;&#27874;&#21160;&#23450;&#29702;&#26469;&#34920;&#24449;&#38480;&#21046;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#37327;&#23376;&#21551;&#33945;&#20998;&#25968;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12163v2 Announce Type: replace-cross  Abstract: Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such example is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the Holevo information of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence, characterized by the resource theory of asymmetry, and entanglement. Furthermore, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models. Finally, we apply the quantum inception score 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;3D&#24418;&#29366;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#36824;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20165;&#20351;&#29992;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#24773;&#20917;&#30340;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2310.06644</link><description>&lt;p&gt;
&#31070;&#32463;&#36317;&#31163;&#22330;&#30340;&#38646;&#27700;&#24179;&#38598;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Zero-Level-Set Encoder for Neural Distance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23884;&#20837;3D&#24418;&#29366;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#21644;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#65292;&#19981;&#20165;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#36824;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20165;&#20351;&#29992;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#38024;&#23545;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#24773;&#20917;&#30340;&#25439;&#22833;&#20989;&#25968;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#29366;&#34920;&#31034;&#36890;&#24120;&#25351;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;3D&#20960;&#20309;&#65292;&#20363;&#22914;&#65292;&#22312;&#29305;&#23450;&#31354;&#38388;&#20301;&#32622;&#35745;&#31639;&#26377;&#31526;&#21495;&#36317;&#31163;&#25110;&#21344;&#25454;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23884;&#20837;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22522;&#20110;&#22810;&#23610;&#24230;&#28151;&#21512;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#22270;&#24418;&#21644;&#22522;&#20110;&#20307;&#32032;&#30340;&#32452;&#20214;&#65292;&#20197;&#21450;&#36830;&#32493;&#21487;&#24494;&#30340;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#20197;&#35299;&#20915;Eikonal&#26041;&#31243;&#65292;&#20165;&#38656;&#35201;&#38646;&#27700;&#24179;&#38598;&#30340;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#36755;&#20986;&#26377;&#25928;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#38750;&#38646;&#36317;&#31163;&#20540;&#25110;&#24418;&#29366;&#21344;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20462;&#25913;&#65292;&#20197;&#35299;&#20915;&#26354;&#38754;&#27861;&#32447;&#19981;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#65292;&#38750;&#23553;&#38381;&#26354;&#38754;&#21644;&#38750;&#27969;&#24418;&#20960;&#20309;&#30340;&#19978;&#19979;&#25991;&#12290;&#24635;&#20307;&#19978;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#24517;&#35201;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural shape representation generally refers to representing 3D geometry using neural networks, e.g., to compute a signed distance or occupancy value at a specific spatial position. In this paper, we present a novel encoder-decoder neural network for embedding 3D shapes in a single forward pass. Our architecture is based on a multi-scale hybrid system incorporating graph-based and voxel-based components, as well as a continuously differentiable decoder. Furthermore, the network is trained to solve the Eikonal equation and only requires knowledge of the zero-level set for training and inference. This means that in contrast to most previous work, our network is able to output valid signed distance fields without explicit prior knowledge of non-zero distance values or shape occupancy. We further propose a modification of the loss function in case that surface normals are not well defined, e.g., in the context of non-watertight surfaces and non-manifold geometry. Overall, this can help red
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10685</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#20551;&#20266;&#36317;&#20462;&#27491;&#23454;&#29616;&#31471;&#21040;&#31471;GPS&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-End GPS Localization with Neural Pseudorange Correction. (arXiv:2401.10685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;PrNet&#26469;&#36827;&#34892;&#20266;&#36317;&#20462;&#27491;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#36317;&#35823;&#24046;&#26159;GPS&#23450;&#20301;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20197;&#24448;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#20013;&#38388;&#26631;&#31614;&#36827;&#34892;&#20266;&#36317;&#35823;&#24046;&#22238;&#24402;&#21644;&#28040;&#38500;&#12290;&#19982;&#20043;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;GPS&#23450;&#20301;&#26694;&#26550;E2E-PrNet&#65292;&#36890;&#36807;&#20351;&#29992;GPS&#25509;&#25910;&#26426;&#29366;&#24577;&#30340;&#30495;&#23454;&#20540;&#35745;&#31639;&#26368;&#32456;&#20219;&#21153;&#25439;&#22833;&#65292;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#29992;&#20110;&#20266;&#36317;&#20462;&#27491;&#30340;&#31070;&#32463;&#32593;&#32476;PrNet&#12290;&#25439;&#22833;&#23545;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#36890;&#36807;&#21487;&#24494;&#38750;&#32447;&#24615;&#26368;&#23567;&#20108;&#20056;&#20248;&#21270;&#22120;&#21453;&#21521;&#20256;&#25773;&#21040;PrNet&#12290;&#36890;&#36807;&#20351;&#29992;Android&#25163;&#26426;&#25910;&#38598;&#30340;GPS&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#65292;&#32467;&#26524;&#26174;&#31034;E2E-PrNet&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31471;&#21040;&#31471;GPS&#23450;&#20301;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudorange errors are the root cause of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a differentiable nonlinear least squares optimizer to PrNet. The feasibility is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the state-of-the-art end-to-end GPS localization methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06715</link><description>&lt;p&gt;
S4Sleep: &#35299;&#26512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models. (arXiv:2310.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#36890;&#36947;&#30561;&#30496;&#33041;&#30005;&#22270;&#35760;&#24405;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#25171;&#20998;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#35780;&#20998;&#20154;&#21592;&#20043;&#38388;&#24046;&#24322;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24102;&#26469;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#34429;&#28982;&#24050;&#32463;&#20026;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20294;&#26576;&#20123;&#20851;&#38190;&#30340;&#26550;&#26500;&#20915;&#31574;&#24182;&#26410;&#24471;&#21040;&#31995;&#32479;&#24615;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35843;&#26597;&#20102;&#24191;&#27867;&#30340;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#26550;&#26500;&#33539;&#30068;&#20869;&#30340;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#22768;&#35889;&#22270;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#24191;&#27867;&#30340;SHHS&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#25913;&#36827;&#36890;&#36807;&#32479;&#35745;&#21644;&#31995;&#32479;&#35823;&#24046;&#20272;&#35745;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#20174;&#26412;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#26550;&#26500;&#27934;&#23519;&#19981;&#20165;&#23545;&#26410;&#26469;&#30340;&#30561;&#30496;&#20998;&#26399;&#30740;&#31350;&#26377;&#20215;&#20540;&#65292;&#32780;&#19988;&#23545;&#25972;&#20307;&#30561;&#30496;&#30740;&#31350;&#37117;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hol
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.09104</link><description>&lt;p&gt;
&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32553;&#20943;&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comprehensive study of spike and slab shrinkage priors for structurally sparse Bayesian neural networks. (arXiv:2308.09104v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;Lasso&#21644;Horseshoe&#20004;&#31181;&#32553;&#20943;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#32467;&#26500;&#31232;&#30095;&#65292;&#36890;&#36807;&#25552;&#20986;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso&#21644;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe&#20808;&#39564;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#22797;&#26434;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#31232;&#30095;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#20943;&#23569;&#36807;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#24674;&#22797;&#24213;&#23618;&#30446;&#26631;&#20989;&#25968;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32467;&#26500;&#31232;&#30095;&#65288;&#22914;&#33410;&#28857;&#31232;&#30095;&#65289;&#21387;&#32553;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#25552;&#20379;&#20102;&#20302;&#24310;&#36831;&#25512;&#29702;&#12289;&#26356;&#39640;&#30340;&#25968;&#25454;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32553;&#20943;&#25216;&#26415;&#65292;Lasso&#21644;Horseshoe&#65292;&#22312;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;&#31232;&#30095;Lasso (SS-GL)&#21644;&#22522;&#20110;&#23574;&#23792;&#19982;&#22359;&#32452;Horseshoe (SS-GHS)&#20808;&#39564;&#30340;&#32467;&#26500;&#31232;&#30095;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24320;&#21457;&#20102;&#21487;&#35745;&#31639;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#21253;&#25324;&#23545;&#20271;&#21162;&#21033;&#21464;&#37327;&#30340;&#36830;&#32493;&#26494;&#24347;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21464;&#20998;&#25512;&#26029;&#30340;&#25910;&#32553;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network complexity and computational efficiency have become increasingly significant aspects of deep learning. Sparse deep learning addresses these challenges by recovering a sparse representation of the underlying target function by reducing heavily over-parameterized deep neural networks. Specifically, deep neural architectures compressed via structured sparsity (e.g. node sparsity) provide low latency inference, higher data throughput, and reduced energy consumption. In this paper, we explore two well-established shrinkage techniques, Lasso and Horseshoe, for model compression in Bayesian neural networks. To this end, we propose structurally sparse Bayesian neural networks which systematically prune excessive nodes with (i) Spike-and-Slab Group Lasso (SS-GL), and (ii) Spike-and-Slab Group Horseshoe (SS-GHS) priors, and develop computationally tractable variational inference including continuous relaxation of Bernoulli variables. We establish the contraction rates of the variational 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2307.03690</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Suppressing unknown disturbances to dynamical systems using machine learning. (arXiv:2307.03690v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#35266;&#27979;&#65292;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#36825;&#39033;&#26041;&#27861;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#25233;&#21046;&#21160;&#21147;&#31995;&#32479;&#20013;&#30340;&#26410;&#30693;&#24178;&#25200;&#26159;&#19968;&#20010;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20165;&#22522;&#20110;&#31995;&#32479;&#22312;&#24050;&#30693;&#24378;&#36843;&#20989;&#25968;&#24433;&#21709;&#19979;&#30340;&#20808;&#21069;&#35266;&#27979;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#35757;&#32451;&#20989;&#25968;&#26377;&#38750;&#24120;&#28201;&#21644;&#30340;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#31283;&#20581;&#22320;&#35782;&#21035;&#21644;&#25233;&#21046;&#22823;&#31867;&#21035;&#30340;&#26410;&#30693;&#24178;&#25200;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#65292;&#20854;&#20013;&#35782;&#21035;&#21644;&#25233;&#21046;&#20102; Lorenz &#31995;&#32479;&#30340;&#28151;&#27788;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying and suppressing unknown disturbances to dynamical systems is a problem with applications in many different fields. In this Letter, we present a model-free method to identify and suppress an unknown disturbance to an unknown system based only on previous observations of the system under the influence of a known forcing function. We find that, under very mild restrictions on the training function, our method is able to robustly identify and suppress a large class of unknown disturbances. We illustrate our scheme with an example where a chaotic disturbance to the Lorenz system is identified and suppressed.
&lt;/p&gt;</description></item><item><title>&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.16156</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Optimal Transport for Machine Learning. (arXiv:2306.16156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16156
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21253;&#25324;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561;&#39046;&#22495;&#65292;&#24182;&#19988;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#21457;&#23637;&#20063;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26368;&#20248;&#36755;&#36816;&#34987;&#25552;&#20986;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#27604;&#36739;&#21644;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#30340;&#27010;&#29575;&#26694;&#26550;&#12290;&#36825;&#20010;&#26694;&#26550;&#28304;&#20110;&#20854;&#20016;&#23500;&#30340;&#21382;&#21490;&#21644;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#29983;&#25104;&#24314;&#27169;&#21644;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;2012&#24180;&#33267;2022&#24180;&#26399;&#38388;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#36129;&#29486;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#30340;&#22235;&#20010;&#23376;&#39046;&#22495;&#65306;&#26377;&#30417;&#30563;&#23398;&#20064;&#12289;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#31361;&#20986;&#20102;&#35745;&#31639;&#26368;&#20248;&#36755;&#36816;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Optimal Transport has been proposed as a probabilistic framework in Machine Learning for comparing and manipulating probability distributions. This is rooted in its rich history and theory, and has offered new solutions to different problems in machine learning, such as generative modeling and transfer learning. In this survey we explore contributions of Optimal Transport for Machine Learning over the period 2012 -- 2022, focusing on four sub-fields of Machine Learning: supervised, unsupervised, transfer and reinforcement learning. We further highlight the recent development in computational Optimal Transport, and its interplay with Machine Learning practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; PathMLP&#65292;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479; GNNs &#20013;&#22312;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12289;&#39640;&#38454;&#20449;&#24687;&#26410;&#20805;&#20998;&#21033;&#29992;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#39640;&#38454;&#20449;&#24687;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.13532</link><description>&lt;p&gt;
PathMLP&#65306;&#39640;&#38454;&#21516;&#36136;&#24615;&#30340;&#24179;&#28369;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
PathMLP: Smooth Path Towards High-order Homophily. (arXiv:2306.13532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; PathMLP&#65292;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479; GNNs &#20013;&#22312;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#32570;&#38519;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12289;&#39640;&#38454;&#20449;&#24687;&#26410;&#20805;&#20998;&#21033;&#29992;&#20197;&#21450;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#39640;&#38454;&#20449;&#24687;&#36827;&#34892;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#22270;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22810;&#30340;&#24322;&#36136;&#24615;&#65292;&#33410;&#28857;&#19981;&#20877;&#20542;&#21521;&#20110;&#36830;&#25509;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#65292;&#25361;&#25112;&#20102;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#24182;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26576;&#20123;&#24322;&#36136;&#25968;&#25454;&#30340;&#39640;&#38454;&#20449;&#24687;&#34920;&#29616;&#20986;&#39640;&#21516;&#36136;&#24615;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#20013;&#28041;&#21450;&#39640;&#38454;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;GNNs&#20013;&#33719;&#24471;&#39640;&#38454;&#20449;&#24687;&#30340;&#24120;&#35265;&#20570;&#27861;&#20027;&#35201;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#21644;&#25913;&#21464;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#34429;&#28982;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19977;&#20010;&#32570;&#28857;&#65306;1&#65289;&#30001;&#20110;&#36807;&#24230;&#30340;&#27169;&#22411;&#28145;&#24230;&#21644;&#20256;&#25773;&#26102;&#38388;&#32780;&#36807;&#24230;&#24179;&#28369;; 2&#65289;&#39640;&#38454;&#20449;&#24687;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;; 3&#65289;&#35745;&#31639;&#25928;&#29575;&#20302;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#36335;&#24452;&#37319;&#26679;&#31574;&#30053;&#65292;&#29992;&#20110;&#25429;&#33719;&#21253;&#21547;&#39640;&#38454;&#21516;&#36136;&#24615;&#30340;&#24179;&#28369;&#36335;&#24452;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#22120;(Multi-layer Perceptrons, MLP)&#30340;&#36731;&#37327;&#21270;&#27169;&#22411;&#65292;&#31216;&#20043;&#20026;PathMLP&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world graphs exhibit increasing heterophily, where nodes no longer tend to be connected to nodes with the same label, challenging the homophily assumption of classical graph neural networks (GNNs) and impeding their performance. Intriguingly, we observe that certain high-order information on heterophilous data exhibits high homophily, which motivates us to involve high-order information in node representation learning. However, common practices in GNNs to acquire high-order information mainly through increasing model depth and altering message-passing mechanisms, which, albeit effective to a certain extent, suffer from three shortcomings: 1) over-smoothing due to excessive model depth and propagation times; 2) high-order information is not fully utilized; 3) low computational efficiency. In this regard, we design a similarity-based path sampling strategy to capture smooth paths containing high-order homophily. Then we propose a lightweight model based on multi-layer perceptrons (M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2306.09237</link><description>&lt;p&gt;
SCALE: &#25552;&#21319;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
SCALE: Scaling up the Complexity for Advanced Language Model Evaluation. (arXiv:2306.09237v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#25361;&#25112;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#21270;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#36827;&#34892;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#24050;&#32463;&#39281;&#21644;&#20102;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#21253;&#25324;&#19987;&#19994;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#65289;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26469;&#27491;&#30830;&#35780;&#20272;LLM&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#23545;&#24403;&#21069;LLM&#30340;&#22235;&#20010;&#20851;&#38190;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65306;&#22788;&#29702;&#38271;&#25991;&#26723;&#65288;&#22810;&#36798;50K&#20010;&#26631;&#35760;&#65289;&#12289;&#21033;&#29992;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65288;&#20307;&#29616;&#22312;&#27861;&#24459;&#25991;&#26412;&#20013;&#65289;&#12289;&#22810;&#35821;&#35328;&#29702;&#35299;&#65288;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#65289;&#21644;&#22810;&#20219;&#21153;&#22788;&#29702;&#65288;&#21253;&#25324;&#27861;&#24459;&#25991;&#20214;&#21040;&#25991;&#20214;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24237;&#35270;&#22270;&#29983;&#25104;&#12289;&#37325;&#35201;&#20915;&#31574;&#25688;&#35201;&#12289;&#24341;&#29992;&#25552;&#21462;&#21644;&#20843;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#29790;&#22763;&#27861;&#24459;&#31995;&#32479;&#30340;&#22810;&#26679;&#30340;&#27861;&#24459;NLP&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23545;&#24213;&#23618;&#38750;&#33521;&#35821;&#12289;&#22266;&#26377;&#22810;&#35821;&#35328;&#30340;&#32852;&#37030;&#27861;&#24459;&#31995;&#32479;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#24378;&#28872;&#30340;&#23457;&#26597;/&#20998;&#26512;&#20219;&#21153;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26723;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent strides in Large Language Models (LLMs) have saturated many NLP benchmarks (even professional domain-specific ones), emphasizing the need for novel, more challenging novel ones to properly assess LLM capabilities. In this paper, we introduce a novel NLP benchmark that poses challenges to current LLMs across four key dimensions: processing long documents (up to 50K tokens), utilizing domain specific knowledge (embodied in legal texts), multilingual understanding (covering five languages), and multitasking (comprising legal document to document Information Retrieval, Court View Generation, Leading Decision Summarization, Citation Extraction, and eight challenging Text Classification tasks). Our benchmark comprises diverse legal NLP datasets from the Swiss legal system, allowing for a comprehensive study of the underlying Non-English, inherently multilingual, federal legal system. Despite recent advances, efficiently processing long documents for intense review/analysis tasks remai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.05102</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#35793;&#21518;&#30340;&#36719;&#20214;&#20197;&#21487;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#24418;&#24335;&#20132;&#20184;&#12290;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#28304;&#20195;&#30721;&#26469;&#34920;&#36798;&#36719;&#20214;&#30340;&#35821;&#20041;&#65292;&#20294;&#32534;&#35793;&#22120;&#23558;&#20854;&#36716;&#25442;&#20026;CPU&#21487;&#20197;&#30452;&#25509;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#20998;&#26512;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#31561;&#27809;&#26377;&#28304;&#20195;&#30721;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#21253;&#21547;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#28304;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;AI&#27169;&#22411;&#36741;&#21161;&#28304;&#20195;&#30721;&#20998;&#26512;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#20108;&#36827;&#21046;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;COMBO&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;COMBO&#20013;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#29992;&#20110;&#20919;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#20027;&#35201;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#65288;2&#65289;&#29992;&#20110;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#25554;&#20837;&#21040;&#20108;&#36827;&#21046;&#20195;&#30721;&#20013;&#30340;&#21333;&#32431;&#25554;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compiled software is delivered as executable binary code. Developers write source code to express the software semantics, but the compiler converts it to a binary format that the CPU can directly execute. Therefore, binary code analysis is critical to applications in reverse engineering and computer security tasks where source code is not available. However, unlike source code and natural language that contain rich semantic information, binary code is typically difficult for human engineers to understand and analyze. While existing work uses AI models to assist source code analysis, few studies have considered binary code. In this paper, we propose a COntrastive learning Model for Binary cOde Analysis, or COMBO, that incorporates source code and comment information into binary code during representation learning. Specifically, we present three components in COMBO: (1) a primary contrastive learning method for cold-start pre-training, (2) a simplex interpolation method to incorporate so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#30340;&#31561;&#20960;&#29575;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#27599;&#20010;&#26063;&#32676;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#65292;&#20197;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2107.08310</link><description>&lt;p&gt;
FairBalance&#65306;&#22914;&#20309;&#36890;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#23454;&#29616;&#31561;&#20960;&#29575;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
FairBalance: How to Achieve Equalized Odds With Data Pre-processing. (arXiv:2107.08310v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#30340;&#31561;&#20960;&#29575;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24179;&#34913;&#27599;&#20010;&#26063;&#32676;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#65292;&#20197;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#20013;&#23454;&#29616;&#24179;&#31561;&#26426;&#20250;&#24179;&#31561;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#22312;&#39640;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20844;&#24179;&#24615;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#25152;&#26377;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#20013;&#65292;&#26412;&#25991;&#29305;&#21035;&#38024;&#23545;&#8220;&#31561;&#20960;&#29575;&#24615;&#8221;&#65292;&#22240;&#20026;&#23427;&#22312;&#22987;&#32456;&#20801;&#35768;&#23436;&#32654;&#20998;&#31867;&#22120;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#31561;&#20960;&#29575;&#35201;&#27714;&#27599;&#20010;&#26063;&#32676;&#30340;&#25104;&#21592;&#37117;&#19981;&#20250;&#21463;&#21040;&#19981;&#21516;&#30340;&#24453;&#36935;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20248;&#21270;&#19982;&#31561;&#20960;&#29575;&#26377;&#20851;&#30340;&#25351;&#26631;&#65292;&#22914;&#40657;&#30418;&#65292;&#35201;&#20040;&#36981;&#24490;&#19968;&#20123;&#30452;&#35273;&#26469;&#25805;&#32437;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36829;&#21453;&#31561;&#20960;&#29575;&#30340;&#26681;&#26412;&#21407;&#22240;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26679;&#26412;&#26435;&#37325;&#24179;&#34913;&#27599;&#20010;&#26063;&#32676;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#26159;&#23454;&#29616;&#31561;&#20960;&#29575;&#25152;&#24517;&#38656;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research seeks to benefit the software engineering society by providing a simple yet effective pre-processing approach to achieve equalized odds fairness in machine learning software. Fairness issues have attracted increasing attention since machine learning software is increasingly used for high-stakes and high-risk decisions. Amongst all the existing fairness notions, this work specifically targets "equalized odds" given its advantage in always allowing perfect classifiers. Equalized odds requires that members of every demographic group do not receive disparate mistreatment. Prior works either optimize for an equalized odds related metric during the learning process like a black-box, or manipulate the training data following some intuition. This work studies the root cause of the violation of equalized odds and how to tackle it. We found that equalizing the class distribution in each demographic group with sample weights is a necessary condition for achieving equalized odds with
&lt;/p&gt;</description></item></channel></rss>