<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;</title><link>https://rss.arxiv.org/abs/2401.15963</link><description>&lt;p&gt;
NoFunEval: &#26377;&#36259;&#30340;&#26159;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#19978;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional Correctness
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; NoFunEval&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#36825;&#20123;&#35201;&#27714;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#30340;&#35780;&#20272;&#22522;&#20934;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;LMs&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#21151;&#33021;&#27491;&#30830;&#30340;&#20195;&#30721;&#19978;&#12290;&#22312;&#23454;&#38469;&#30340;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#24320;&#21457;&#20154;&#21592;&#20250;&#32771;&#34385;&#36229;&#20986;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;&#20182;&#20204;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#23454;&#29616;&#21151;&#33021;&#26377;&#30528;&#23545;&#25972;&#20307;&#31995;&#32479;&#35774;&#35745;&#30446;&#26631;&#65288;&#22914;&#25928;&#29575;&#12289;&#23433;&#20840;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#65289;&#30340;&#35201;&#27714;&#12290;&#22914;&#26524;LMs&#33021;&#22815;&#23637;&#31034;&#23545;&#35201;&#27714;&#21644;&#20195;&#30721;&#35821;&#20041;&#30340;&#24378;&#22823;&#29702;&#35299;&#33021;&#21147;&#65292;&#20182;&#20204;&#20063;&#20250;&#26356;&#21152;&#20449;&#20219;&#36825;&#20123;LMs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;NoFunEval&#26469;&#35780;&#20272;&#20195;&#30721;LMs&#22312;&#38750;&#21151;&#33021;&#24615;&#35201;&#27714;&#21644;&#31616;&#21333;&#20998;&#31867;&#23454;&#20363;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#26041;&#27861;Coding Concepts (CoCo)&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#20154;&#21592;&#21521;LMs&#20256;&#36798;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;22&#20010;&#20195;&#30721;LMs&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26222;&#36941;&#34920;&#29616;&#19981;&#20339;&#65292;&#26263;&#31034;&#30528;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#26102;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing evaluation benchmarks of language models of code (code LMs) focus almost exclusively on whether the LMs can generate functionally-correct code. In real-world software engineering, developers think beyond functional correctness. They have requirements on "how" a functionality should be implemented to meet overall system design objectives like efficiency, security, and maintainability. They would also trust the code LMs more if the LMs demonstrate robust understanding of requirements and code semantics.   We propose a new benchmark NoFunEval to evaluate code LMs on non-functional requirements and simple classification instances for both functional and non-functional requirements. We propose a prompting method, Coding Concepts (CoCo), as a way for a developer to communicate the domain knowledge to the LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is that they generally falter when tested on our benchmark, hinting at fundamental blindspots in their tr
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02289</link><description>&lt;p&gt;
&#34892;&#26143;&#25506;&#27979;&#30340;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Agent Mapping for Planetary Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02289
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#65292;&#31649;&#29702;&#21644;&#26377;&#25928;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26144;&#23556;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#21327;&#20316;&#23398;&#20064;&#20013;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;FL&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#21270;&#25110;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#24102;&#23485;&#21644;&#23384;&#20648;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#65292;&#23558;&#22320;&#22270;&#34920;&#31034;&#20026;&#30001;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#20415;&#23454;&#29616;&#32039;&#20945;&#21644;&#36866;&#24212;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20803;&#21021;&#22987;&#21270;&#26469;&#22686;&#24378;&#36825;&#19968;&#26041;&#27861;&#65292;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#22320;&#22270;&#32467;&#26500;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#35832;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#31561;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#36825;&#19968;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02289v1 Announce Type: cross  Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiven
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.18035</link><description>&lt;p&gt;
&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#24212;&#20110;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;PF ODE&#65289;&#31227;&#21160;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;DMs&#36824;&#21487;&#20197;&#36890;&#36807;&#27839;&#30528;PF ODE&#21521;&#21518;&#31227;&#21160;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25554;&#20540;&#21644;&#22270;&#20687;&#32534;&#36753;&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30340;&#36845;&#20195;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#36895;&#24230;&#65292;&#38459;&#30861;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#36817;&#20284;PF ODE&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26174;&#24335;ODE&#27714;&#35299;&#22120;&#20351;&#24471;&#21453;&#28436;&#36807;&#31243;&#22797;&#26434;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#27839;&#30528;PF ODE&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#29983;&#25104;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#24066;&#22330;&#20013;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#26356;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.13893</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35774;&#35745;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#24066;&#22330;&#30340;&#25968;&#25454;&#37319;&#38598;
&lt;/p&gt;
&lt;p&gt;
Data Acquisition via Experimental Design for Decentralized Data Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13893
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#24066;&#22330;&#20013;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#26356;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#25968;&#25454;&#24066;&#22330;&#36890;&#36807;&#28608;&#21169;&#28508;&#22312;&#30340;&#25968;&#25454;&#21334;&#23478;&#21152;&#20837;&#24066;&#22330;&#30340;&#26041;&#24335;&#26469;&#22686;&#21152;&#25968;&#25454;&#20379;&#24212;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;&#23454;&#39564;&#35774;&#35745;&#21551;&#21457;&#30340;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#36873;&#25321;&#38382;&#39064;&#30340;&#32852;&#37030;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#39564;&#35777;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#24555;&#36895;&#21644;&#32852;&#37030;&#30340;&#36807;&#31243;&#20013;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#19968;&#31181;&#30452;&#25509;&#20272;&#35745;&#33719;&#21462;&#25968;&#25454;&#23545;&#20110;&#27979;&#35797;&#38598;&#39044;&#27979;&#30340;&#22909;&#22788;&#30340;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#21435;&#20013;&#24515;&#21270;&#24066;&#22330;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13893v1 Announce Type: new  Abstract: Acquiring high-quality training data is essential for current machine learning models. Data markets provide a way to increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data sellers to join the market. A major challenge for a data buyer in such a market is selecting the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data selection problem that is inspired by linear experimental design. Our proposed data selection method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#39318;&#20010;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20248;&#21270;&#19988;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#22312;Condorcet&#21644;Borda&#20043;&#38388;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.12950</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#38750;&#24179;&#31283;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#22312;&#24191;&#20041;&#27874;&#36798;&#20934;&#21017;&#19979;
&lt;/p&gt;
&lt;p&gt;
Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized Borda Criterion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#39318;&#20010;&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#20248;&#21270;&#19988;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#25581;&#31034;&#20102;&#22312;Condorcet&#21644;Borda&#20043;&#38388;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#20013;&#65292;&#23398;&#20064;&#32773;&#25509;&#25910;&#33218;&#20043;&#38388;&#30340;&#20559;&#22909;&#21453;&#39304;&#65292;&#24182;&#23558;&#26576;&#20010;&#33218;&#30340;&#36951;&#25022;&#23450;&#20041;&#20026;&#20854;&#30456;&#23545;&#20110;&#20248;&#32988;&#33218;&#30340;&#27425;&#20248;&#24615;&#12290;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#36341;&#21160;&#26426;&#30340;&#38750;&#24179;&#31283;&#23545;&#25239;&#24615;&#22810;&#33218;&#32769;&#34382;&#26426;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#21464;&#20307;&#20013;&#65292;&#20559;&#22909;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#36817;&#26399;&#22810;&#39033;&#24037;&#20316;&#30340;&#28966;&#28857;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#31639;&#27861;&#65292;&#32780;&#26080;&#38656;&#25552;&#21069;&#20102;&#35299;&#21464;&#21270;&#37327;&#12290;&#24050;&#30693;&#32467;&#26524;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20102;&#23380;&#22810;&#22622;&#20248;&#32988;&#32773;&#35774;&#32622;&#65292;&#20854;&#20013;&#20248;&#20808;&#20110;&#20854;&#20182;&#20219;&#20309;&#33218;&#30340;&#33218;&#22312;&#20219;&#20309;&#26102;&#20505;&#37117;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20248;&#32988;&#32773;&#21487;&#33021;&#24182;&#19981;&#23384;&#22312;&#65292;&#20026;&#20102;&#23545;&#27604;&#65292;&#27492;&#38382;&#39064;&#30340;&#27874;&#36798;&#29256;&#26412;&#65288;&#22987;&#32456;&#26377;&#26126;&#30830;&#23450;&#20041;&#65289;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#26368;&#20248;&#21644;&#33258;&#36866;&#24212;&#30340;&#27874;&#36798;&#21160;&#24577;&#36951;&#25022;&#19978;&#30028;&#65292;&#31361;&#26174;&#20102;&#22312;&#23380;&#22810;&#22622;&#21644;&#27874;&#36798;&#20043;&#38388;&#30340;&#20005;&#37325;&#38750;&#24179;&#31283;&#24615;&#21487;&#23398;&#20064;&#24615;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12950v1 Announce Type: new  Abstract: In dueling bandits, the learner receives preference feedback between arms, and the regret of an arm is defined in terms of its suboptimality to a winner arm. The more challenging and practically motivated non-stationary variant of dueling bandits, where preferences change over time, has been the focus of several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and Agarwal, 2023). The goal is to design algorithms without foreknowledge of the amount of change.   The bulk of known results here studies the Condorcet winner setting, where an arm preferred over any other exists at all times. Yet, such a winner may not exist and, to contrast, the Borda version of this problem (which is always well-defined) has received little attention. In this work, we establish the first optimal and adaptive Borda dynamic regret upper bound, which highlights fundamental differences in the learnability of severe non-stationarity between Condorce
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.12529</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#20449;&#24687;&#25552;&#21319;&#20102;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextualized Messages Boost Graph Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22788;&#29702;&#20197;&#22270;&#34920;&#31034;&#30340;&#20219;&#24847;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GNN&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#26412;&#22320;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#20351;&#29992;&#22270;&#35835;&#20986;&#20989;&#25968;&#21019;&#24314;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32858;&#21512;&#21644;&#32452;&#21512;&#31574;&#30053;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;GNN&#65292;&#24120;&#24120;&#21463;&#21551;&#21457;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20174;&#22522;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#29702;&#35770;&#35282;&#24230;&#25506;&#32034;GNN&#65292;&#35813;&#38382;&#39064;&#22266;&#26377;&#22320;&#20551;&#35774;&#21487;&#25968;&#30340;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#24037;&#20316;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21487;&#25968;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;GNN&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;GNN&#22312;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12529v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-l
&lt;/p&gt;</description></item><item><title>LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2403.11735</link><description>&lt;p&gt;
LSKNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#30340;&#36731;&#37327;&#32423;&#22522;&#30784;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
LSKNet: A Foundation Lightweight Backbone for Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11735
&lt;/p&gt;
&lt;p&gt;
LSKNet&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#39592;&#24178;&#65292;&#33021;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#31243;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#23545;&#19979;&#28216;&#20219;&#21153;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#36965;&#24863;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#24573;&#35270;&#20102;&#23884;&#20837;&#22312;&#36965;&#24863;&#22330;&#26223;&#20013;&#30340;&#23453;&#36149;&#20808;&#39564;&#30693;&#35782;&#12290;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#21442;&#32771;&#36275;&#22815;&#38271;&#31243;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#36965;&#24863;&#23545;&#35937;&#21487;&#33021;&#20250;&#34987;&#38169;&#35823;&#35782;&#21035;&#65292;&#32780;&#36825;&#21487;&#20197;&#22240;&#19981;&#21516;&#23545;&#35937;&#32780;&#24322;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22823;&#22411;&#36873;&#25321;&#26680;&#32593;&#32476;&#65288;LSKNet&#65289;&#39592;&#24178;&#32593;&#32476;&#12290;LSKNet&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36739;&#22823;&#30340;&#31354;&#38388;&#24863;&#21463;&#37326;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#36965;&#24863;&#22330;&#26223;&#20013;&#21508;&#31181;&#23545;&#35937;&#30340;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20808;&#21069;&#23578;&#26410;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#25506;&#32034;&#36807;&#22823;&#22411;&#21644;&#36873;&#25321;&#24615;&#26680;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#27809;&#26377;&#22826;&#22810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11735v1 Announce Type: cross  Abstract: Remote sensing images pose distinct challenges for downstream tasks due to their inherent complexity. While a considerable amount of research has been dedicated to remote sensing classification, object detection and semantic segmentation, most of these studies have overlooked the valuable prior knowledge embedded within remote sensing scenarios. Such prior knowledge can be useful because remote sensing objects may be mistakenly recognized without referencing a sufficiently long-range context, which can vary for different objects. This paper considers these priors and proposes a lightweight Large Selective Kernel Network (LSKNet) backbone. LSKNet can dynamically adjust its large spatial receptive field to better model the ranging context of various objects in remote sensing scenarios. To our knowledge, large and selective kernel mechanisms have not been previously explored in remote sensing images. Without bells and whistles, our lightw
&lt;/p&gt;</description></item><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.04990</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21943;&#27880;&#21028;&#21035;
&lt;/p&gt;
&lt;p&gt;
Jet Discrimination with Quantum Complete Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04990
&lt;/p&gt;
&lt;p&gt;
QCGNN&#36890;&#36807;&#37327;&#23376;&#24182;&#34892;&#24615;&#23454;&#29616;&#20102;&#23545;&#21943;&#27880;&#21028;&#21035;&#30340;&#22810;&#39033;&#24335;&#21152;&#36895;&#65292;&#20026;&#21943;&#27880;&#21028;&#21035;&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#24050;&#25193;&#23637;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#65292;&#21363;&#37327;&#23376;&#23436;&#20840;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;QCGNN&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#23436;&#20840;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;QCGNN&#30001;&#20110;&#37327;&#23376;&#24182;&#34892;&#24615;&#30340;&#29305;&#24615;&#65292;&#22312;&#36895;&#24230;&#19978;&#23545;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#20855;&#26377;&#22810;&#39033;&#24335;&#21152;&#36895;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;QCGNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21943;&#27880;&#21028;&#21035;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#21943;&#27880;&#29992;&#23436;&#20840;&#22270;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#19982;&#32463;&#20856;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#24314;&#31435;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04990v1 Announce Type: cross  Abstract: Machine learning, particularly deep neural networks, has been widely utilized in high energy physics and has shown remarkable results in various applications. Moreover, the concept of machine learning has been extended to quantum computers, giving rise to a new research area known as quantum machine learning. In this paper, we propose a novel variational quantum circuit model, Quantum Complete Graph Neural Network (QCGNN), designed for learning complete graphs. We argue that QCGNN has a polynomial speedup against its classical counterpart, due to the property of quantum parallelism. In this paper, we study the application of QCGNN through the challenging jet discrimination, where the jets are represented with complete graphs. Subsequently, we conduct a comparative analysis with classical graph neural networks to establish a benchmark.
&lt;/p&gt;</description></item><item><title>WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.04808</link><description>&lt;p&gt;
WaterMax: &#25171;&#30772;LLM&#27700;&#21360;&#21487;&#26816;&#27979;&#24615;-&#31283;&#20581;&#24615;-&#36136;&#37327;&#30340;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04808
&lt;/p&gt;
&lt;p&gt;
WaterMax&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#25171;&#30772;&#20102;&#27700;&#21360;&#25216;&#26415;&#20013;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#20256;&#32479;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#26159;&#38459;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#30340;&#25216;&#26415;&#25163;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;WaterMax&#30340;&#26032;&#39062;&#27700;&#21360;&#26041;&#26696;&#65292;&#20855;&#26377;&#39640;&#26816;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20854;&#26032;&#35774;&#35745;&#19981;&#20250;&#23545;LLM&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#65288;&#19981;&#35843;&#25972;&#26435;&#37325;&#12289;&#23545;&#25968;&#12289;&#28201;&#24230;&#25110;&#37319;&#26679;&#25216;&#26415;&#65289;&#12290;WaterMax&#24179;&#34913;&#20102;&#31283;&#20581;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#19982;&#25991;&#29486;&#20013;&#30340;&#27700;&#21360;&#25216;&#26415;&#30456;&#21453;&#65292;&#20174;&#26681;&#26412;&#19978;&#24341;&#21457;&#20102;&#36136;&#37327;&#21644;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#20854;&#24615;&#33021;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#26126;&#24182;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;&#22312;&#26368;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19979;&#65292;&#23427;&#32988;&#36807;&#25152;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04808v1 Announce Type: cross  Abstract: Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03849</link><description>&lt;p&gt;
MedMamba: &#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba
&lt;/p&gt;
&lt;p&gt;
MedMamba: Vision Mamba for Medical Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03849
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Vision Mamba&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#32467;&#21512;&#20102;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#38750;&#24120;&#22522;&#30784;&#21644;&#20851;&#38190;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;CNN&#21644;Transformer&#30340;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#31867;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;CNN&#22312;&#38271;&#36317;&#31163;&#24314;&#27169;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#26080;&#27861;&#26377;&#25928;&#25552;&#21462;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#32780;Transformers&#21463;&#21040;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;Mamba&#34920;&#31034;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#21487;&#20197;&#39640;&#25928;&#22320;&#24314;&#27169;&#38271;&#36317;&#31163;&#20132;&#20114;&#20316;&#29992;&#21516;&#26102;&#20445;&#25345;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;Vision Mamba&#65288;MedMamba&#65289;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;Conv-SSM&#27169;&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#23637;&#31034;MedMamba&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03849v1 Announce Type: cross  Abstract: Medical image classification is a very fundamental and crucial task in the field of computer vision. These years, CNN-based and Transformer-based models are widely used in classifying various medical images. Unfortunately, The limitation of CNNs in long-range modeling capabilities prevent them from effectively extracting fine-grained features in medical images , while Transformers are hampered by their quadratic computational complexity. Recent research has shown that the state space model (SSM) represented by Mamba can efficiently model long-range interactions while maintaining linear computational complexity. Inspired by this, we propose Vision Mamba for medical image classification (MedMamba). More specifically, we introduce a novel Conv-SSM module, which combines the local feature extraction ability of convolutional layers with the ability of SSM to capture long-range dependency. To demonstrate the potential of MedMamba, we conduct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.18612</link><description>&lt;p&gt;
&#29702;&#35299;&#38543;&#26426;&#26862;&#26519;&#21644;&#36807;&#25311;&#21512;&#65306;&#19968;&#39033;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding random forests and overfitting: a visualization and simulation study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18612
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#27169;&#25311;&#30740;&#31350;&#25506;&#35752;&#20102;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#38598;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#24314;&#27169;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#22312;&#19968;&#39033;&#20851;&#20110;&#39044;&#27979;&#21365;&#24034;&#24694;&#24615;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#38598;&#19978;&#30340;c-&#32479;&#35745;&#20540;&#25509;&#36817;1&#12290;&#23613;&#31649;&#36825;&#34920;&#26126;&#23384;&#22312;&#36807;&#25311;&#21512;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#65288;1&#65289;&#22312;&#19977;&#20010;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#20013;&#21487;&#35270;&#21270;&#25968;&#25454;&#31354;&#38388;&#21644;&#65288;2&#65289;&#36827;&#34892;&#27169;&#25311;&#30740;&#31350;&#26469;&#29702;&#35299;&#38543;&#26426;&#26862;&#26519;&#30340;&#34892;&#20026;&#12290;&#22312;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#28909;&#21147;&#22270;&#22312;&#20108;&#32500;&#23376;&#31354;&#38388;&#20013;&#21487;&#35270;&#21270;&#39118;&#38505;&#20272;&#35745;&#12290;&#27169;&#25311;&#30740;&#31350;&#21253;&#25324;48&#20010;&#36923;&#36753;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65288;DGM&#65289;&#65292;&#21464;&#21270;&#39044;&#27979;&#21464;&#37327;&#20998;&#24067;&#12289;&#39044;&#27979;&#21464;&#37327;&#25968;&#37327;&#12289;&#39044;&#27979;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12289;&#30495;&#23454;c-&#32479;&#35745;&#20540;&#21644;&#30495;&#23454;&#39044;&#27979;&#21464;&#37327;&#30340;&#24378;&#24230;&#12290;&#23545;&#20110;&#27599;&#20010;DGM&#65292;&#27169;&#25311;&#29983;&#25104;&#22823;&#23567;&#20026;200&#25110;4000&#30340;1000&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;ranger&#21253;&#35757;&#32451;&#26368;&#23567;&#33410;&#28857;&#22823;&#23567;&#20026;2&#25110;20&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;192&#20010;&#22330;&#26223;&#12290;&#21487;&#35270;&#21270;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18612v1 Announce Type: cross  Abstract: Random forests have become popular for clinical risk prediction modelling. In a case study on predicting ovarian malignancy, we observed training c-statistics close to 1. Although this suggests overfitting, performance was competitive on test data. We aimed to understand the behaviour of random forests by (1) visualizing data space in three real world case studies and (2) a simulation study. For the case studies, risk estimates were visualised using heatmaps in a 2-dimensional subspace. The simulation study included 48 logistic data generating mechanisms (DGM), varying the predictor distribution, the number of predictors, the correlation between predictors, the true c-statistic and the strength of true predictors. For each DGM, 1000 training datasets of size 200 or 4000 were simulated and RF models trained with minimum node size 2 or 20 using ranger package, resulting in 192 scenarios in total. The visualizations suggested that the mod
&lt;/p&gt;</description></item><item><title>&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.12550</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65306;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12550
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#38024;&#23545;&#35270;&#35273;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#19987;&#23478;&#29305;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#21644;&#36807;&#39640;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#33539;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#23558;&#38590;&#20197;&#29702;&#35299;&#30340;&#23494;&#38598;&#23618;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#27169;&#22359;&#21270;&#30340;&#35745;&#31639;&#65292;&#36890;&#24120;&#26356;&#26131;&#20110;&#20154;&#31867;&#35299;&#37322;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25193;&#23637;&#19987;&#23478;&#25968;&#37327;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20197;&#23454;&#29616;&#36275;&#22815;&#31934;&#32454;&#30340;&#19987;&#19994;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#32447;&#24615;&#19987;&#23478;&#28151;&#21512;&#65288;MMoE&#65289;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#25918;&#22312;&#35270;&#35273;&#27169;&#22411;&#19978;&#12290;MMoE&#23618;&#23436;&#20840;&#20197;&#22240;&#24335;&#21270;&#24418;&#24335;&#23545;&#24222;&#22823;&#30340;&#26435;&#37325;&#24352;&#37327;&#36827;&#34892;&#38544;&#24335;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;MMoEs&#26082;&#36991;&#20813;&#20102;&#22312;&#27969;&#34892;&#30340;&#8220;&#31232;&#30095;&#8221;MoE&#27169;&#22411;&#20013;&#31163;&#25955;&#19987;&#23478;&#36335;&#30001;&#25152;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#21448;&#19981;&#20250;&#24341;&#36215;&#8220;&#36719;&#8221;MoE&#26367;&#20195;&#26041;&#26696;&#20013;&#36807;&#39640;&#30340;&#25512;&#29702;&#26102;&#38388;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#21453;&#20107;&#23454;&#24178;&#39044;&#65292;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#25193;&#23637;MMoE&#23618;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12550v1 Announce Type: cross  Abstract: The Mixture of Experts (MoE) paradigm provides a powerful way to decompose inscrutable dense layers into smaller, modular computations often more amenable to human interpretation, debugging, and editability. A major problem however lies in the computational cost of scaling the number of experts to achieve sufficiently fine-grained specialization. In this paper, we propose the Multilinear Mixutre of Experts (MMoE) layer to address this, focusing on vision models. MMoE layers perform an implicit computation on prohibitively large weight tensors entirely in factorized form. Consequently, MMoEs both (1) avoid the issues incurred through the discrete expert routing in the popular 'sparse' MoE models, yet (2) do not incur the restrictively high inference-time costs of 'soft' MoE alternatives. We present both qualitative and quantitative evidence (through visualization and counterfactual interventions respectively) that scaling MMoE layers wh
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09299</link><description>&lt;p&gt;
&#26410;&#32463;&#26412;&#20154;&#21516;&#24847;&#30340;&#35757;&#32451;&#65306;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#35745;&#36890;&#36807;&#39564;&#35777;&#24320;&#21457;&#30340;&#20195;&#30721;&#26159;&#21542;&#31526;&#21512;&#26631;&#20934;&#12289;&#27861;&#35268;&#21644;&#29256;&#26435;&#20445;&#25252;&#65292;&#30830;&#20445;&#20854;&#19981;&#21253;&#21547;&#26469;&#33258;&#21463;&#20445;&#25252;&#26469;&#28304;&#30340;&#20195;&#30721;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#20986;&#29616;&#32473;&#20195;&#30721;&#23457;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#12290;&#36825;&#24341;&#21457;&#20102;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#20026;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#24050;&#21253;&#21547;&#22312;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#24320;&#21457;&#30340;&#20195;&#30721;&#23457;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;LLM&#26159;&#21542;&#24050;&#32463;&#22312;&#29305;&#23450;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#37492;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#23494;&#24615;&#65292;&#20256;&#32479;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#31561;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#29256;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04163</link><description>&lt;p&gt;
Tempered Calculus for ML: &#24212;&#29992;&#20110;&#21452;&#26354;&#27169;&#22411;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tempered Calculus for ML: Application to Hyperbolic Model Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26412;&#36136;&#19978;&#37117;&#26159;&#31215;&#20998;&#30340;&#65306;$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#36825;&#20123;&#25197;&#26354;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#30340;&#40654;&#26364;&#31215;&#20998;&#24320;&#22987;&#65292;&#36825;&#31181;&#31215;&#20998;&#36824;&#21253;&#25324;&#19981;&#20005;&#26684;&#21487;&#21152;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#26356;&#19968;&#33324;&#22320;&#26159;$t$-&#21487;&#21152;&#30340;&#65292;&#23601;&#20687;&#38750;&#26497;&#38480;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#24773;&#20917;&#19968;&#26679;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;Volterra&#30340;&#20056;&#31215;&#31215;&#20998;&#20316;&#20026;&#29305;&#20363;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#27431;&#20960;&#37324;&#24471;&#65289;&#23548;&#25968;&#30340;&#25193;&#23637;&#26469;&#25512;&#24191;&#22522;&#26412;&#23450;&#29702;&#12290;&#36825;&#20123;&#25512;&#24191;&#20197;&#21450;&#19968;&#31995;&#21015;&#26356;&#20855;&#20307;&#30340;&#23450;&#29702;&#20026;&#32467;&#26524;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#19987;&#38376;&#35774;&#35745;&#12289;&#25913;&#21464;&#25110;&#25913;&#21464;&#25197;&#26354;&#24230;&#37327;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20934;&#30830;&#22320;&#27979;&#37327;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03698</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#20272;&#35745;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating the Local Learning Coefficient at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20934;&#30830;&#22320;&#27979;&#37327;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#26159;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#26368;&#21021;&#26159;&#22312;&#36125;&#21494;&#26031;&#32479;&#35745;&#20013;&#20351;&#29992;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;(SLT)&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#24050;&#30693;&#26377;&#20960;&#31181;&#25968;&#20540;&#20272;&#35745;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#26041;&#27861;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#25193;&#23637;&#21040;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;&#36890;&#36807;&#22312;arXiv:2308.12108 [stat.ML]&#20013;&#24320;&#21457;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#21644;&#33258;&#27965;&#22320;&#27979;&#37327;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;(DLN)&#20013;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#29702;&#35770;&#25968;&#37327;&#25152;&#20855;&#22791;&#30340;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02442</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;ReLU&#30340;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#20013;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20154;&#20204;&#23545;&#38750;&#32447;&#24615;&#30697;&#38453;&#20998;&#35299;&#65288;NMD&#65289;&#30340;&#25506;&#32034;&#26085;&#30410;&#22686;&#22810;&#12290;NMD&#26088;&#22312;&#20174;&#31232;&#30095;&#30340;&#38750;&#36127;&#30697;&#38453;&#20013;&#25214;&#21040;&#19968;&#20010;&#20302;&#31209;&#30697;&#38453;&#65292;&#20854;&#20013;&#27599;&#20010;&#20803;&#32032;&#37117;&#32463;&#36807;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#20856;&#22411;&#30340;&#36873;&#25321;&#26159;ReLU&#28608;&#27963;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#30340;ReLU-NMD&#27169;&#22411;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Tikhonov&#27491;&#21017;&#21270;&#30340;ReLU-NMD&#27169;&#22411;&#65292;&#31216;&#20026;ReLU-NMD-T&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;ReLU-NMD-T&#27169;&#22411;&#30340;&#21160;&#37327;&#21152;&#36895;&#31639;&#27861;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21516;&#26102;&#32467;&#21512;&#20102;&#27491;&#21160;&#37327;&#21644;&#36127;&#21160;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#20195;&#30721;&#21487;&#22312;https://github.com/nothing2wang/NMD-TM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a growing interest in the exploration of Nonlinear Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims to find a low-rank matrix from a sparse nonnegative matrix with a per-element nonlinear function. A typical choice is the Rectified Linear Unit (ReLU) activation function. To address over-fitting in the existing ReLU-based NMD model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for handling the ReLU-NMD-T model. A distinctive feature, setting our work apart from most existing studies, is the incorporation of both positive and negative momentum parameters in our algorithm. Our numerical experiments on real-world datasets show the effectiveness of the proposed model and algorithm. Moreover, the code is available at https://github.com/nothing2wang/NMD-TM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17177</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Discovery of PDEs via the Adjoint Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20197;&#19968;&#33324;&#24418;&#24335;&#32771;&#34385;&#21442;&#25968;&#21270;&#30340;PDE&#65292;&#24182;&#21046;&#23450;&#26368;&#23567;&#21270;PDE&#35299;&#19982;&#25968;&#25454;&#35823;&#24046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#21464;&#20998;&#35745;&#31639;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#20276;&#38543;&#26041;&#31243;&#65289;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#20986;&#19982;PDE&#21442;&#25968;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#26063;&#21442;&#25968;&#21270;&#21644;&#38750;&#32447;&#24615;PDEs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20276;&#38543;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20276;&#38543;&#26041;&#27861;&#21487;&#20197;&#20197;&#26426;&#22120;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20276;&#38543;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#33879;&#21517;&#30340;PDE-FIND&#65288;Rudy et al., 2017&#65289;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
&lt;/p&gt;</description></item><item><title>CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;</title><link>https://arxiv.org/abs/2301.05872</link><description>&lt;p&gt;
CEDAS&#65306;&#19968;&#31181;&#20855;&#26377;&#25913;&#36827;&#25910;&#25947;&#24615;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#27861;
&lt;/p&gt;
&lt;p&gt;
CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.05872
&lt;/p&gt;
&lt;p&gt;
CEDAS&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#20855;&#26377;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65292;&#23545;&#20809;&#28369;&#24378;&#20984;&#21644;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#37117;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#36890;&#20449;&#21463;&#38480;&#29615;&#22659;&#19979;&#35299;&#20915;&#22810;&#20195;&#29702;&#32593;&#32476;&#19978;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20855;&#26377;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;&#21387;&#32553;&#31934;&#30830;&#25193;&#25955;&#65288;CEDAS&#65289;&#8221;&#30340;&#21387;&#32553;&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26080;&#20559;&#21387;&#32553;&#36816;&#31639;&#31526;&#19979;&#28176;&#36817;&#22320;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30456;&#24403;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36866;&#29992;&#20110;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;CEDAS&#36804;&#20170;&#20026;&#27490;&#20197;&#20854;&#26368;&#30701;&#30340;&#30636;&#24577;&#26102;&#38388;&#65288;&#20851;&#20110;&#22270;&#30340;&#29305;&#24615;&#65289;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;SGD&#30456;&#21516;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#22312;&#20809;&#28369;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$&#65292;&#22312;&#20809;&#28369;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#19979;&#34920;&#29616;&#20026;$\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$&#65292;&#20854;&#20013;$(1-\lambda_2)$&#34920;&#31034;&#35889;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.05872v2 Announce Type: replace-cross  Abstract: In this paper, we consider solving the distributed optimization problem over a multi-agent network under the communication restricted setting. We study a compressed decentralized stochastic gradient method, termed ``compressed exact diffusion with adaptive stepsizes (CEDAS)", and show the method asymptotically achieves comparable convergence rate as centralized { stochastic gradient descent (SGD)} for both smooth strongly convex objective functions and smooth nonconvex objective functions under unbiased compression operators. In particular, to our knowledge, CEDAS enjoys so far the shortest transient time (with respect to the graph specifics) for achieving the convergence rate of centralized SGD, which behaves as $\mathcal{O}(n{C^3}/(1-\lambda_2)^{2})$ under smooth strongly convex objective functions, and $\mathcal{O}(n^3{C^6}/(1-\lambda_2)^4)$ under smooth nonconvex objective functions, where $(1-\lambda_2)$ denotes the spectr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2210.15182</link><description>&lt;p&gt;
Text2Model:&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#24402;&#32435;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text2Model: Text-based Model Induction for Zero-shot Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15182
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20165;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#26500;&#24314;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#22120;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#12289;3D&#28857;&#20113;&#20998;&#31867;&#20197;&#21450;&#20174;&#22330;&#26223;&#20013;&#35782;&#21035;&#21160;&#20316;&#12290;&#19982;&#23398;&#20064;&#22266;&#23450;&#36755;&#20986;&#31867;&#21035;&#34920;&#31034;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#25512;&#26029;&#26102;&#29983;&#25104;&#38024;&#23545;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#23450;&#21046;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#29983;&#25104;&#22522;&#20110;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#36229;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#25509;&#25910;&#31867;&#25551;&#36848;&#24182;&#36755;&#20986;&#19968;&#20010;&#22810;&#31867;&#27169;&#22411;&#12290;&#36229;&#32593;&#32476;&#35774;&#35745;&#20026;&#23545;&#25551;&#36848;&#38598;&#21512;&#21644;&#20998;&#31867;&#23618;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#22240;&#27492;&#31526;&#21512;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#38750;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#21160;&#20316;&#35782;&#21035;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15182v2 Announce Type: replace-cross  Abstract: We address the challenge of building task-agnostic classifiers using only text descriptions, demonstrating a unified approach to image classification, 3D point cloud classification, and action recognition from scenes. Unlike approaches that learn a fixed representation of the output classes, we generate at inference time a model tailored to a query classification task. To generate task-based zero-shot classifiers, we train a hypernetwork that receives class descriptions and outputs a multi-class model. The hypernetwork is designed to be equivariant with respect to the set of descriptions and the classification layer, thus obeying the symmetries of the problem and improving generalization. Our approach generates non-linear classifiers and can handle rich textual descriptions. We evaluate this approach in a series of zero-shot classification tasks, for image, point-cloud, and action recognition, using a range of text descriptions
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.12609</link><description>&lt;p&gt;
&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#35299;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#28151;&#21512;&#30340;&#24555;&#36895;&#21322;&#30417;&#30563;&#38750;&#20984;&#20248;&#21270;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#21644;&#20016;&#24230;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20808;&#39564;&#30340;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#30417;&#30563;/&#22522;&#20110;&#24211;&#30340;&#35299;&#28151;&#21512;&#35774;&#35745;&#30340;&#26032;&#22411;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#24211;&#19981;&#21305;&#37197;&#65292;&#24182;&#33021;&#22815;&#23454;&#26045;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#12290;&#19982;&#20256;&#32479;&#30340;&#31232;&#30095;&#35299;&#28151;&#21512;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#38750;&#20984;&#20248;&#21270;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20132;&#26367;&#20056;&#27861;&#22120;&#26041;&#27861;&#65288;ADMM&#65289;&#22312;&#24490;&#29615;&#27714;&#35299;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21322;&#30417;&#30563;&#35299;&#28151;&#21512;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#24212;&#29992;&#20110;&#26032;&#27169;&#22411;&#30340;&#19981;&#21516;&#20808;&#39564;&#20197;&#21450;&#20016;&#24230;&#21644;&#31561;&#20110;&#19968;&#30340;&#32422;&#26463;&#65306;&#31232;&#30095;&#20808;&#39564;&#21644;&#20984;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23454;&#26045;&#20984;&#24615;&#32422;&#26463;&#20248;&#20110;&#31232;&#30095;&#20808;&#39564;&#23545;&#20110;&#31471;&#20803;&#24211;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#22312;&#19977;&#20010;&#27169;&#25311;&#25968;&#25454;&#38598;&#65288;&#32771;&#34385;&#20102;&#20809;&#35889;&#21464;&#21270;&#21644;&#19981;&#21516;&#20687;&#32032;&#32431;&#24230;&#27700;&#24179;&#65289;&#21644;Cuprite&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel linear model tailored for semisupervised/library-based unmixing. Our model incorporates considerations for library mismatch while enabling the enforcement of the abundance sum-to-one constraint (ASC). Unlike conventional sparse unmixing methods, this model involves nonconvex optimization, presenting significant computational challenges. We demonstrate the efficacy of Alternating Methods of Multipliers (ADMM) in cyclically solving these intricate problems. We propose two semisupervised unmixing approaches, each relying on distinct priors applied to the new model in addition to the ASC: sparsity prior and convexity constraint. Our experimental results validate that enforcing the convexity constraint outperforms the sparsity prior for the endmember library. These results are corroborated across three simulated datasets (accounting for spectral variability and varying pixel purity levels) and the Cuprite dataset. Additionally, our comparison with convent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.11143</link><description>&lt;p&gt;
&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#25152;&#38656;&#30340;&#65306;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20581;&#22766;&#19978;&#19979;&#25991;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Gaussian Adaptive Attention is All You Need: Robust Contextual Representations Across Multiple Modalities. (arXiv:2401.11143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GAAM&#30340;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#29992;&#20110;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#32435;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;GAAM&#33021;&#22815;&#21160;&#24577;&#22320;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#24378;&#19988;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65292;&#20855;&#26377;&#25913;&#36827;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#22836;&#39640;&#26031;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GAAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#27880;&#24847;&#21147;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#26031;&#33258;&#36866;&#24212;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#36328;&#22810;&#20010;&#27169;&#24577;&#65288;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35270;&#35273;&#65289;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;GAAM&#23558;&#21487;&#23398;&#20064;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#34701;&#20837;&#20854;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#37319;&#29992;&#22810;&#22836;&#26694;&#26550;&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#38598;&#20307;&#24314;&#27169;&#20219;&#20309;&#27010;&#29575;&#20998;&#24067;&#65292;&#20197;&#21160;&#24577;&#37325;&#26032;&#35843;&#25972;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#24230;&#38750;&#24179;&#31283;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#36890;&#36807;&#35782;&#21035;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#25216;&#26415;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#29366;&#24577;&#65288;&#31934;&#24230;&#22686;&#21152;&#32422;20%&#65289;&#12290;GAAM&#19982;&#22522;&#20110;&#28857;&#31215;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#20855;&#26377;&#30456;&#23545;&#36739;&#20302;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#21644;&#25552;&#21319;&#29616;&#26377;&#27880;&#24847;&#21147;&#26694;&#26550;&#30340;&#28508;&#21147;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;GAAM&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a novel probabilistic attention framework, and the Gaussian Adaptive Transformer (GAT), designed to enhance information aggregation across multiple modalities, including Speech, Text and Vision. GAAM integrates learnable mean and variance into its attention mechanism, implemented in a Multi-Headed framework enabling it to collectively model any Probability Distribution for dynamic recalibration of feature significance. This method demonstrates significant improvements, especially with highly non-stationary data, surpassing the state-of-the-art attention techniques in model performance (up to approximately +20% in accuracy) by identifying key elements within the feature space. GAAM's compatibility with dot-product-based attention models and relatively low number of parameters showcases its adaptability and potential to boost existing attention frameworks. Empirically, GAAM exhibits superior adaptability and efficacy
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.09769</link><description>&lt;p&gt;
&#36208;&#21521;&#24322;&#36136;&#22270;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#21512;&#27010;&#36848;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#26681;&#25454;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#31561;&#26041;&#38754;&#23545;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#24322;&#36136;&#22270;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#25110;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#35768;&#22810;&#24212;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#20063;&#22312;&#19981;&#26029;&#21162;&#21147;&#25512;&#36827;&#20174;&#24322;&#36136;&#22270;&#20013;&#23398;&#20064;&#12290;&#34429;&#28982;&#26377;&#20851;&#35813;&#20027;&#39064;&#30340;&#35843;&#26597;&#23384;&#22312;&#65292;&#20294;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#32780;&#24573;&#30053;&#20102;&#24322;&#36136;&#22270;&#23398;&#20064;&#30340;&#20854;&#20182;&#23376;&#20027;&#39064;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#20174;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#22270;&#20013;&#23398;&#20064;&#30340;&#29616;&#26377;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;180&#22810;&#31687;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23618;&#27425;&#20998;&#31867;&#27861;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#21253;&#25324;&#23398;&#20064;&#31574;&#30053;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are structured data that models complex relations between real-world entities. Heterophilous graphs, where linked nodes are prone to be with different labels or dissimilar features, have recently attracted significant attention and found many applications. Meanwhile, increasing efforts have been made to advance learning from heterophilous graphs. Although there exist surveys on the relevant topic, they focus on heterophilous GNNs, which are only sub-topics of heterophilous graph learning. In this survey, we comprehensively overview existing works on learning from graphs with heterophily.First, we collect over 180 publications and introduce the development of this field. Then, we systematically categorize existing methods based on a hierarchical taxonomy including learning strategies, model architectures and practical applications. Finally, we discuss the primary challenges of existing studies and highlight promising avenues for future research.More publication details and corres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03653</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#33258;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An exploratory study on automatic identification of assumptions in the development of deep learning frameworks. (arXiv:2401.03653v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#26041;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#32463;&#24120;&#20570;&#20986;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#28041;&#21450;&#21508;&#31181;&#36719;&#20214;&#26500;&#20214;&#65288;&#20363;&#22914;&#38656;&#27714;&#12289;&#35774;&#35745;&#20915;&#31574;&#21644;&#25216;&#26415;&#20538;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#34987;&#35777;&#26126;&#26080;&#25928;&#65292;&#20174;&#32780;&#23548;&#33268;&#31995;&#32479;&#25925;&#38556;&#12290;&#29616;&#26377;&#30340;&#20551;&#35774;&#31649;&#29702;&#26041;&#27861;&#21644;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#20998;&#25955;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#30340;&#21508;&#31181;&#28304;&#22836;&#65288;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#12289;&#25552;&#20132;&#12289;&#25289;&#21462;&#35831;&#27714;&#21644;&#38382;&#39064;&#65289;&#20013;&#65292;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#25104;&#26412;&#36739;&#39640;&#65288;&#20363;&#22914;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#24182;&#19988;&#26368;&#22823;&#30340;&#20551;&#35774;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;AssuEval&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;GitHub&#19978;&#30340;TensorFlow&#21644;Keras&#20195;&#30721;&#24211;&#65307;&#25105;&#20204;&#25506;&#35752;&#20102;&#19971;&#20010;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#65289;&#21644;&#19968;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01537</link><description>&lt;p&gt;
&#27450;&#39575;&#30340;&#33402;&#26415;&#65306;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#30340;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01537
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#35302;&#21457;&#22120;&#36827;&#34892;&#24378;&#20581;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#65292;&#20351;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#34892;&#19994;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#22686;&#38271;&#30340;&#23454;&#26045;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;AI&#38450;&#24481;&#26426;&#21046;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#19981;&#23436;&#20840;&#21487;&#20449;&#30340;&#31532;&#19977;&#26041;&#25552;&#20379;&#21830;&#30340;&#28508;&#22312;&#38544;&#34109;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21548;&#35273;&#21518;&#38376;&#21487;&#33021;&#20351;&#29992;&#26576;&#20123;&#20462;&#25913;&#20316;&#20026;&#20854;&#21551;&#21160;&#26426;&#21046;&#12290;DynamicTrigger&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#29992;&#20110;&#36827;&#34892;&#20351;&#29992;&#24039;&#22937;&#35774;&#35745;&#30340;&#35843;&#25972;&#26469;&#30830;&#20445;&#25439;&#22351;&#30340;&#26679;&#26412;&#19982;&#24178;&#20928;&#30340;&#26679;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#21160;&#24577;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#27874;&#21160;&#30340;&#20449;&#21495;&#37319;&#26679;&#29575;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#22768;&#38899;&#35302;&#21457;&#22120;&#65288;&#27604;&#22914;&#25293;&#25163;&#22768;&#65289;&#23545;&#35828;&#35805;&#32773;&#36523;&#20221;&#36827;&#34892;&#25513;&#30422;&#65292;&#21487;&#20197;&#27450;&#39575;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#34920;&#26126;&#65292;DynamicTrigger&#22312;&#38544;&#34109;&#25915;&#20987;&#20013;&#26082;&#26377;&#25928;&#21448;&#38544;&#34109;&#65292;&#24182;&#22312;&#25915;&#20987;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.16713</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#26465;&#20214;&#25554;&#34917;&#26041;&#27861;&#65292;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26465;&#20214;&#33258;&#27880;&#24847;&#21147;&#25554;&#34917;&#65288;CSAI&#65289;&#27169;&#22411;&#20197;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#20026;&#22522;&#30784;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32454;&#33410;&#30340;&#26465;&#20214;&#38544;&#34255;&#29366;&#24577;&#21021;&#22987;&#21270;&#26041;&#24335;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25554;&#34917;&#25216;&#26415;&#19981;&#21516;&#65292;&#23427;&#29305;&#21035;&#38024;&#23545;&#21307;&#30103;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#36890;&#36807;&#25972;&#21512;&#20808;&#36827;&#30340;&#30693;&#35782;&#23884;&#20837;&#21644;&#38750;&#22343;&#21248;&#25513;&#34109;&#31574;&#30053;&#65292;CSAI&#33021;&#22815;&#28789;&#27963;&#36866;&#24212;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#32570;&#22833;&#25968;&#25454;&#30340;&#19981;&#21516;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to addressing the challenge of missing data in multivariate time series, with a particular focus on the complexities of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model, grounded in a transformer-based framework, introduces a conditional hidden state initialization tailored to the intricacies of medical time series data. This methodology diverges from traditional imputation techniques by specifically targeting the imbalance in missing data distribution, a crucial aspect often overlooked in healthcare datasets. By integrating advanced knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to the distinct patterns of missing data in Electronic Health Records (EHRs).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#29289;&#32852;&#32593;&#26102;&#20195;&#30340;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#12290;USTD&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;&#26102;&#31354;&#27169;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17360</link><description>&lt;p&gt;
&#12298;&#38754;&#21521;&#27010;&#29575;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#32479;&#19968;&#21270;&#12299;
&lt;/p&gt;
&lt;p&gt;
Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning. (arXiv:2310.17360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#29289;&#32852;&#32593;&#26102;&#20195;&#30340;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#12290;USTD&#32508;&#21512;&#32771;&#34385;&#20102;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;&#26102;&#31354;&#27169;&#24335;&#36827;&#34892;&#39044;&#27979;&#12290;&#27492;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#26102;&#31354;&#22270;&#23398;&#20064;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#26234;&#33021;&#22478;&#24066;&#12289;&#20154;&#31867;&#31227;&#21160;&#24615;&#21644;&#27668;&#20505;&#20998;&#26512;&#31561;&#20247;&#22810;&#32593;&#32476;&#24212;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#22320;&#35299;&#20915;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#26681;&#25454;&#20219;&#21153;&#29305;&#28857;&#35843;&#25972;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#27169;&#25311;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#27169;&#22411;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#26222;&#36866;&#26102;&#31354;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#32479;&#19968;&#35270;&#35282;&#24314;&#27169;&#23398;&#20064;&#20219;&#21153;&#65292;&#23558;&#20854;&#35270;&#20026;&#22522;&#20110;&#20849;&#20139;&#26102;&#31354;&#27169;&#24335;&#30340;&#26465;&#20214;&#20449;&#24687;&#39044;&#27979;&#12290;&#22522;&#20110;&#36825;&#19968;&#24314;&#35758;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#26102;&#31354;&#25193;&#25955;&#27169;&#22411;&#65288;USTD&#65289;&#65292;&#22312;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#25193;&#25955;&#26694;&#26550;&#19979;&#32479;&#19968;&#22788;&#29702;&#20219;&#21153;&#12290;USTD&#30340;&#35774;&#35745;&#26159;&#20840;&#38754;&#30340;&#65292;&#21253;&#25324;&#19968;&#20010;&#20849;&#20139;&#26102;&#31354;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21435;&#22122;&#32593;&#32476;&#65292;&#38024;&#23545;&#19981;&#21516;&#20219;&#21153;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph learning is a fundamental problem in the Web of Things era, which enables a plethora of Web applications such as smart cities, human mobility and climate analysis. Existing approaches tackle different learning tasks independently, tailoring their models to unique task characteristics. These methods, however, fall short of modeling intrinsic uncertainties in the spatio-temporal data. Meanwhile, their specialized designs limit their universality as general spatio-temporal learning solutions. In this paper, we propose to model the learning tasks in a unified perspective, viewing them as predictions based on conditional information with shared spatio-temporal patterns. Based on this proposal, we introduce Unified Spatio-Temporal Diffusion Models (USTD) to address the tasks uniformly within the uncertainty-aware diffusion framework. USTD is holistically designed, comprising a shared spatio-temporal encoder and attention-based denoising networks that are task-specific. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11256</link><description>&lt;p&gt;
&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#31867;&#20284;&#20110;Gromov-Wassertein&#30340;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#20013;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#65292;&#20998;&#21035;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#21644;&#25512;&#23548;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#38598;&#21512;&#19978;&#30340;Gromov-Wasserstein&#31867;&#22411;&#36317;&#31163;&#12290;&#31532;&#19968;&#31181;&#36317;&#31163;&#26159;&#22312;&#39640;&#26031;&#27979;&#24230;&#31354;&#38388;&#19978;&#20004;&#20010;&#31163;&#25955;&#20998;&#24067;&#30340;Gromov-Wasserstein&#36317;&#31163;&#12290;&#35813;&#36317;&#31163;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#26367;&#20195;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19981;&#33021;&#30452;&#25509;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#36816;&#36755;&#26041;&#26696;&#12290;&#20026;&#20102;&#35774;&#35745;&#20986;&#36825;&#26679;&#30340;&#36816;&#36755;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21478;&#19968;&#31181;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#36317;&#31163;&#19982;Gromov-Wasserstein&#23494;&#20999;&#30456;&#20851;&#12290;&#24403;&#23558;&#20801;&#35768;&#30340;&#36816;&#36755;&#32806;&#21512;&#38480;&#21046;&#20026;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26102;&#65292;&#36825;&#23450;&#20041;&#20102;&#21478;&#19968;&#31181;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21487;&#20197;&#20316;&#20026;Gromov-Wasserstein&#30340;&#21478;&#19968;&#31181;&#26367;&#20195;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#28857;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#36890;&#20449;&#25104;&#26412;&#30340;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11015</link><description>&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pure Exploration in Asynchronous Federated Bandits. (arXiv:2310.11015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24322;&#27493;&#32852;&#37030;&#36172;&#21338;&#26426;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#36890;&#20449;&#25104;&#26412;&#30340;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32852;&#37030;&#32431;&#25506;&#32034;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#65292;&#20854;&#20013;M&#20010;&#20195;&#29702;&#36890;&#36807;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#21512;&#20316;&#22320;&#30830;&#23450;&#26368;&#20339;&#25277;&#33218;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#24310;&#36831;&#21644;&#20195;&#29702;&#19981;&#21487;&#29992;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32852;&#37030;&#24322;&#27493;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#32447;&#24615;&#36172;&#21338;&#26426;&#32431;&#25506;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#22266;&#23450;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#23436;&#20840;&#24322;&#27493;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#20174;&#32463;&#39564;&#19978;&#38416;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#21147;&#21644;&#36890;&#20449;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.07355</link><description>&lt;p&gt;
IMITATE: &#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training. (arXiv:2310.07355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07355
&lt;/p&gt;
&lt;p&gt;
IMITATE&#26159;&#19968;&#31181;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#21307;&#23398;&#25253;&#21578;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20174;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#39046;&#22495;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#20174;&#20020;&#24202;&#25253;&#21578;&#21644;&#30456;&#20851;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#21033;&#29992;&#20020;&#24202;&#25253;&#21578;&#22266;&#26377;&#30340;&#23618;&#32423;&#32467;&#26500;&#30340;&#26426;&#20250;&#65292;&#36825;&#20123;&#25253;&#21578;&#36890;&#24120;&#34987;&#20998;&#20026;&#25551;&#36848;&#24615;&#20869;&#23481;&#30340;&#8220;&#21457;&#29616;&#8221;&#21644;&#32467;&#35770;&#24615;&#35266;&#23519;&#30340;&#8220;&#21360;&#35937;&#8221;&#12290;&#24403;&#21069;&#30340;&#21307;&#23398;VLP&#26041;&#27861;&#24448;&#24448;&#23558;&#25253;&#21578;&#31616;&#21270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23454;&#20307;&#25110;&#20998;&#25955;&#30340;&#26631;&#35760;&#65292;&#32780;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#20016;&#23500;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#26684;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20020;&#24202;&#20808;&#39564;&#25351;&#23548;&#30340;VLP&#26694;&#26550;&#65292;&#21517;&#20026;IMITATE&#65292;&#29992;&#20110;&#20174;&#21307;&#23398;&#25253;&#21578;&#20013;&#23398;&#20064;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#20998;&#23618;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20174;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#22270;&#20687;&#20013;&#25552;&#21462;&#22810;&#32423;&#35270;&#35273;&#29305;&#24449;&#65292;&#24182;&#23558;&#36825;&#20123;&#29305;&#24449;&#19982;&#20998;&#23618;&#21307;&#23398;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#24615;&#21644;&#32467;&#35770;&#24615;&#25991;&#26412;&#20998;&#21035;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical Vision-Language Pre-training (VLP), significant efforts have been devoted to deriving text and image features from both clinical reports and associated medical images. However, most existing methods may have overlooked the opportunity in leveraging the inherent hierarchical structure of clinical reports, which are generally split into `findings' for descriptive content and `impressions' for conclusive observation. Instead of utilizing this rich, structured format, current medical VLP approaches often simplify the report into either a unified entity or fragmented tokens. In this work, we propose a novel clinical prior guided VLP framework named IMITATE to learn the structure information from medical reports with hierarchical vision-language alignment. The framework derives multi-level visual features from the chest X-ray (CXR) images and separately aligns these features with the descriptive and the conclusive text encoded in the hierarchical medical report. Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06286</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#34892;&#20026;&#25233;&#21046;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Suppressing Overestimation in Q-Learning through Adversarial Behaviors. (arXiv:2310.06286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#26377;&#25928;&#35843;&#33410;&#20102;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#34394;&#25311;&#23545;&#25239;&#24615;&#29609;&#23478;&#65292;&#31216;&#20026;&#34394;&#25311;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65288;DAQ&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#35843;&#33410;&#26631;&#20934;Q&#23398;&#20064;&#20013;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;&#36890;&#36807;&#34394;&#25311;&#29609;&#23478;&#65292;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#19968;&#20010;&#21452;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25152;&#25552;&#20986;&#30340;DAQ&#23558;&#20960;&#31181;Q&#23398;&#20064;&#30340;&#21464;&#20307;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#25511;&#21046;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#20363;&#22914;maxmin Q&#23398;&#20064;&#21644;minmax Q&#23398;&#20064;&#65288;&#26412;&#25991;&#25552;&#20986;&#65289;&#12290;&#36890;&#36807;&#34394;&#25311;&#23545;&#25239;&#24615;&#34892;&#20026;&#65292;&#25152;&#25552;&#20986;&#30340;DAQ&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#23545;&#25239;&#24615;Q&#23398;&#20064;&#65292;&#20174;&#32508;&#21512;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;DAQ&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19979;&#65292;&#23454;&#35777;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;DAQ&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to propose a new Q-learning algorithm with a dummy adversarial player, which is called dummy adversarial Q-learning (DAQ), that can effectively regulate the overestimation bias in standard Q-learning. With the dummy player, the learning can be formulated as a two-player zero-sum game. The proposed DAQ unifies several Q-learning variations to control overestimation biases, such as maxmin Q-learning and minmax Q-learning (proposed in this paper) in a single framework. The proposed DAQ is a simple but effective way to suppress the overestimation bias thourgh dummy adversarial behaviors and can be easily applied to off-the-shelf reinforcement learning algorithms to improve the performances. A finite-time convergence of DAQ is analyzed from an integrated perspective by adapting an adversarial Q-learning. The performance of the suggested DAQ is empirically demonstrated under various benchmark environments.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03946</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#27169;&#22411;&#25913;&#36827;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20934;&#30830;&#31579;&#36873;&#20505;&#36873;&#33647;&#29289;&#37197;&#20307;&#19982;&#38774;&#34507;&#30333;&#30340;&#32467;&#21512;&#26159;&#33647;&#29289;&#24320;&#21457;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#31579;&#36873;&#28508;&#22312;&#20505;&#36873;&#29289;&#33021;&#22815;&#33410;&#30465;&#25214;&#33647;&#29289;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#12290;&#36825;&#31181;&#34394;&#25311;&#31579;&#36873;&#37096;&#20998;&#20381;&#36182;&#20110;&#39044;&#27979;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24050;&#21457;&#34920;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#32452;&#21512;&#30340;&#20010;&#21035;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#24211;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#20803;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#35768;&#22810;&#20803;&#27169;&#22411;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20010;&#21035;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#20803;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#32431;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.08017</link><description>&lt;p&gt;
Stackelberg&#36712;&#36857;&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Inverse Learning in Stackelberg Trajectory Games. (arXiv:2308.08017v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08017
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Stackelberg&#21338;&#24328;&#20013;&#30340;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27963;&#36291;&#22320;&#26368;&#22823;&#21270;&#36319;&#38543;&#32773;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#30340;&#36712;&#36857;&#24046;&#24322;&#26469;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#30340;&#36870;&#21521;&#23398;&#20064;&#26159;&#20174;&#29609;&#23478;&#30340;&#34892;&#20026;&#20013;&#25512;&#26029;&#20986;&#20182;&#20204;&#30340;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;Stackelberg&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#27599;&#20010;&#29609;&#23478;&#30340;&#21160;&#24577;&#31995;&#32479;&#36712;&#36857;&#26469;&#23450;&#20041;&#19968;&#20010;&#36870;&#21521;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#39046;&#23548;&#32773;&#21644;&#19968;&#20010;&#36319;&#38543;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#36870;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#35753;&#39046;&#23548;&#32773;&#25512;&#26029;&#20986;&#19968;&#20010;&#26377;&#38480;&#20505;&#36873;&#38598;&#20013;&#25551;&#36848;&#36319;&#38543;&#32773;&#30446;&#26631;&#20989;&#25968;&#30340;&#20551;&#35774;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#34987;&#21160;&#35266;&#23519;&#21040;&#30340;&#36712;&#36857;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20027;&#21160;&#22320;&#26368;&#22823;&#21270;&#19981;&#21516;&#20551;&#35774;&#19979;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#24046;&#24322;&#65292;&#21152;&#36895;&#39046;&#23548;&#32773;&#30340;&#25512;&#26029;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#36882;&#36827;&#30340;&#37325;&#22797;&#36712;&#36857;&#21338;&#24328;&#20013;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#19982;&#22343;&#21248;&#38543;&#26426;&#36755;&#20837;&#30456;&#27604;&#65292;&#25152;&#25552;&#20379;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#27010;&#29575;&#25910;&#25947;&#21040;&#26465;&#20214;&#20110;&#36319;&#38543;&#32773;&#36712;&#36857;&#30340;&#19981;&#21516;&#20551;&#35774;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game-theoretic inverse learning is the problem of inferring the players' objectives from their actions. We formulate an inverse learning problem in a Stackelberg game between a leader and a follower, where each player's action is the trajectory of a dynamical system. We propose an active inverse learning method for the leader to infer which hypothesis among a finite set of candidates describes the follower's objective function. Instead of using passively observed trajectories like existing methods, the proposed method actively maximizes the differences in the follower's trajectories under different hypotheses to accelerate the leader's inference. We demonstrate the proposed method in a receding-horizon repeated trajectory game. Compared with uniformly random inputs, the leader inputs provided by the proposed method accelerate the convergence of the probability of different hypotheses conditioned on the follower's trajectory by orders of magnitude.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00071</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#26041;&#27861;&#29992;&#20110;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25512;&#29702;&#26041;&#27861;&#65292;&#22312;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#24182;&#21457;&#29616;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#27169;&#22411;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20351;&#29992;&#20102;&#21253;&#21547;&#22266;&#26377;&#20559;&#35265;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#25345;&#32493;&#31995;&#32479;&#24615;&#27495;&#35270;&#65292;&#22240;&#27492;&#65292;&#23457;&#26597;&#21644;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23558;&#20844;&#24179;&#24615;&#25972;&#21512;&#21040;&#23427;&#20204;&#30340;&#21457;&#23637;&#20013;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#20844;&#27491;&#21644;&#26080;&#20559;&#30340;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Vicuna-13B-v1.3&#30340;&#38646;&#23556;&#20987;&#21051;&#26495;&#21360;&#35937;&#35782;&#21035;&#20013;&#25512;&#29702;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;13B&#21040;33B&#30340;&#35268;&#27169;&#25193;&#23637;&#20250;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#25512;&#29702;&#30340;&#24615;&#33021;&#22686;&#30410;&#36828;&#36828;&#36229;&#36807;&#35268;&#27169;&#25193;&#23637;&#30340;&#22686;&#30410;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#29702;&#21487;&#33021;&#26159;&#20351;LLMs&#22312;&#21051;&#26495;&#21360;&#35937;&#31561;&#39046;&#22495;&#20219;&#21153;&#19978;&#36229;&#36234;&#35268;&#27169;&#23450;&#24459;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#36873;&#23450;&#30340;&#25512;&#29702;&#36861;&#36394;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#31361;&#20986;&#26174;&#31034;&#20102;&#25512;&#29702;&#19981;&#20165;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#36824;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18779</link><description>&lt;p&gt;
&#20174;&#20960;&#20309;&#35282;&#24230;&#30475;&#24453;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#36793;&#30028;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
It begins with a boundary: A geometric view on probabilistically robust learning. (arXiv:2305.18779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#30340;&#26032;&#39062;&#35270;&#35282;&#65292;&#20171;&#32461;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#20248;&#21270;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#30340;&#21407;&#22987;&#34920;&#36848;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#36229;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#23545;&#20110;&#23545;&#25239;&#29983;&#25104;&#30340;&#31034;&#20363;&#32570;&#20047;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#23558;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#37325;&#26500;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#30340;&#26694;&#26550;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#28857;&#24050;&#32463;&#36716;&#21521;&#20102;&#20171;&#20110;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#21644;ERM&#25552;&#20379;&#30340;&#26356;&#39640;&#24178;&#20928;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#35757;&#32451;&#26102;&#38388;&#20043;&#38388;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20174;&#20960;&#20309;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#40065;&#26834;&#23398;&#20064;&#65288;PRL&#65289;&#65288;Robey&#31561;&#20154;&#65292;ICML&#65292;2022&#65289;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#30340;&#25506;&#35752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#26694;&#26550;&#26469;&#29702;&#35299;PRL&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#20854;&#21407;&#22987;&#34920;&#36848;&#20013;&#30340;&#24494;&#22937;&#32570;&#38519;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#26063;&#27010;&#29575;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#39062;&#30340;&#26494;&#24347;&#26041;&#27861;&#35777;&#26126;&#20102;&#35299;&#30340;&#23384;&#22312;&#65292;&#24182;&#30740;&#31350;&#20102;&#24341;&#20837;&#30340;&#38750;&#23616;&#37096;&#21608;&#38271;&#20989;&#25968;&#30340;&#29305;&#24615;&#20197;&#21450;&#23616;&#37096;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural networks have achieved super-human performance on many classification tasks, they often exhibit a worrying lack of robustness towards adversarially generated examples. Thus, considerable effort has been invested into reformulating Empirical Risk Minimization (ERM) into an adversarially robust framework. Recently, attention has shifted towards approaches which interpolate between the robustness offered by adversarial training and the higher clean accuracy and faster training times of ERM. In this paper, we take a fresh and geometric view on one such method -- Probabilistically Robust Learning (PRL) (Robey et al., ICML, 2022). We propose a geometric framework for understanding PRL, which allows us to identify a subtle flaw in its original formulation and to introduce a family of probabilistic nonlocal perimeter functionals to address this. We prove existence of solutions using novel relaxation methods and study properties as well as local limits of the introduced per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13289</link><description>&lt;p&gt;
&#23454;&#29616;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26497;&#23567;&#26497;&#22823;&#26679;&#26412;&#22797;&#26434;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;DRO&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#26377;&#38480;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23547;&#25214;&#22312;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#20027;&#21160;&#25506;&#32034;&#12290;&#35813;&#38382;&#39064;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#20998;&#24067;&#36716;&#31227;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#24754;&#35266;&#30340;&#24577;&#24230;&#23545;&#24453;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24809;&#32602;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#29366;&#24577;-&#34892;&#20026;&#23545;&#30340;&#22870;&#21169;&#26469;&#20445;&#23432;&#20272;&#35745;&#20540;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#22522;&#20110;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#19988;&#26159;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30452;&#25509;&#24314;&#27169;&#36716;&#31227;&#26680;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32479;&#35745;&#21512;&#29702;&#30340;&#36716;&#31227;&#26680;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;&#28982;&#21518;&#25105;&#20204;&#23547;&#25214;&#22312;&#35813;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#19978;&#26368;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#24230;&#37327;&#30340;&#38669;&#22827;&#19969;&#39118;&#26684;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36825;&#26679;&#30495;&#23454;&#30340;&#36716;&#31227;&#26680;&#20197;&#39640;&#27010;&#29575;&#20301;&#20110;&#35813;&#38598;&#21512;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#23454;&#29616;$\epsilon$&#30340;&#27425;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mat&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10740</link><description>&lt;p&gt;
&#19968;&#31181;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#30340;&#22522;&#20934;&#65292;&#20351;&#29992;&#21160;&#29289;&#25658;&#24102;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
A benchmark for computational analysis of animal behavior, using animal-borne tags. (arXiv:2305.10740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#25658;&#24102;&#30340;&#20256;&#24863;&#22120;&#65288;&#8220;&#29983;&#29289;&#35760;&#24405;&#22120;&#8221;&#65289;&#21487;&#20197;&#35760;&#24405;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#25968;&#25454;&#65292;&#25581;&#31034;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#24182;&#25913;&#21892;&#20445;&#25252;&#24037;&#20316;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#35299;&#37322;&#29983;&#29289;&#35760;&#24405;&#22120;&#35760;&#24405;&#30340;&#22823;&#37327;&#25968;&#25454;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#27809;&#26377;&#26631;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bio-logger Ethogram Benchmark&#65288;BEBE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#34892;&#20026;&#27880;&#37322;&#65292;&#26631;&#20934;&#21270;&#24314;&#27169;&#20219;&#21153;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;BEBE&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#36825;&#31181;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#20061;&#20010;&#20998;&#31867;&#21333;&#20803;&#20013;149&#20010;&#20010;&#20307;&#25910;&#38598;&#30340;1654&#23567;&#26102;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;BEBE&#19978;&#35780;&#20272;&#20102;&#21313;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/earthspecies/BEBE&#65292;&#20197;&#20415;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animal-borne sensors ('bio-loggers') can record a suite of kinematic and environmental data, which can elucidate animal ecophysiology and improve conservation efforts. Machine learning techniques are useful for interpreting the large amounts of data recorded by bio-loggers, but there exists no standard for comparing the different machine learning techniques in this domain. To address this, we present the Bio-logger Ethogram Benchmark (BEBE), a collection of datasets with behavioral annotations, standardized modeling tasks, and evaluation metrics. BEBE is to date the largest, most taxonomically diverse, publicly available benchmark of this type, and includes 1654 hours of data collected from 149 individuals across nine taxa. We evaluate the performance of ten different machine learning methods on BEBE, and identify key challenges to be addressed in future work. Datasets, models, and evaluation code are made publicly available at https://github.com/earthspecies/BEBE, to enable community 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;&#35745;&#31639;&#20986;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#26159;&#30446;&#21069;&#31532;&#19968;&#20010;&#23454;&#29616;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#37327;&#23376;&#32447;&#24615;&#32534;&#31243;&#27714;&#35299;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14197</link><description>&lt;p&gt;
&#38646;&#21644;&#28216;&#25103;&#30340;&#23545;&#25968;&#36951;&#25022;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Logarithmic-Regret Quantum Learning Algorithms for Zero-Sum Games. (arXiv:2304.14197v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14197
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;&#35745;&#31639;&#20986;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#26159;&#30446;&#21069;&#31532;&#19968;&#20010;&#23454;&#29616;&#22522;&#20110;&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#37327;&#23376;&#32447;&#24615;&#32534;&#31243;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32447;&#38646;&#21644;&#28216;&#25103;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#22312;&#28216;&#25103;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102; $\tilde O(1)$ &#30340;&#36951;&#25022;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#37327;&#23376;&#31639;&#27861;&#21487;&#20197;&#22312;&#37327;&#23376;&#26102;&#38388;$\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$&#20869;&#35745;&#31639;$m\times n$&#30697;&#38453;&#38646;&#21644;&#28216;&#25103;&#30340;$\varepsilon$-&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#31639;&#27861;&#22312;$m, n$&#26041;&#38754;&#23454;&#29616;&#20102;&#20108;&#27425;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#26631;&#20934;&#30340;&#37327;&#23376;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#31616;&#26126;&#30340;&#25551;&#36848;&#24615;&#30340;&#32463;&#20856;&#36755;&#20986;&#65292;&#26041;&#20415;&#36827;&#34892;&#31471;&#21040;&#31471;&#24212;&#29992;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;&#37327;&#23376;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#30340;&#22312;&#32447;&#37327;&#23376;&#31639;&#27861;&#22522;&#20110;&#20048;&#35266;&#30340;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#26041;&#27861;&#8220;&#37327;&#23376;&#21270;&#8221;&#20102;&#32463;&#20856;&#31639;&#27861;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#24555;&#36895;&#37327;&#23376;&#22810;&#37325;&#37319;&#26679;&#36807;&#31243;&#65292;&#29992;&#20110;Gibbs&#37319;&#26679;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first online quantum algorithm for zero-sum games with $\tilde O(1)$ regret under the game setting. Moreover, our quantum algorithm computes an $\varepsilon$-approximate Nash equilibrium of an $m \times n$ matrix zero-sum game in quantum time $\tilde O(\sqrt{m+n}/\varepsilon^{2.5})$, yielding a quadratic improvement over classical algorithms in terms of $m, n$. Our algorithm uses standard quantum inputs and generates classical outputs with succinct descriptions, facilitating end-to-end applications. As an application, we obtain a fast quantum linear programming solver. Technically, our online quantum algorithm "quantizes" classical algorithms based on the optimistic multiplicative weight update method. At the heart of our algorithm is a fast quantum multi-sampling procedure for the Gibbs sampling problem, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.10640</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#36127;&#36131;&#20154;&#25171;&#31639;&#22312;&#19968;&#32452;&#20855;&#26377;&#19968;&#20123;&#26041;&#31243;&#32452;&#23376;&#38598;&#30340;&#26426;&#22120;&#30340;&#20998;&#24067;&#24335;/&#32852;&#21512;&#24110;&#21161;&#19979;&#35299;&#20915;&#35813;&#31995;&#32479;&#30340;&#35774;&#32622;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#23545;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20005;&#26684;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#36825;&#20004;&#31867;&#31639;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#26368;&#36817;&#25552;&#20986;&#30340;&#21152;&#36895;&#25237;&#24433;&#19968;&#33268;&#24615;(APC)&#21644;&#20998;&#24067;&#24335;&#37325;&#29699;&#26041;&#27861;(D-HBM)&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31216;&#20026;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#26222;&#36941;&#24615;&#12290;&#20351;&#29992;&#35813;&#27010;&#24565;&#65292;&#25105;&#20204;&#32422;&#26463;&#24182;&#27604;&#36739;&#25152;&#30740;&#31350;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25429;&#25417;&#20004;&#31181;&#26041;&#27861;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2302.10763</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19982;&#23646;&#24615;&#20851;&#32852;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10763
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#20307;&#21576;&#29616;&#65292;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36890;&#24120;&#20250;&#32473;&#20986;&#19968;&#20010;&#31616;&#27905;&#30340;&#26631;&#31614;&#12290;&#32780;&#20154;&#31867;&#22312;&#31867;&#20284;&#30340;&#21576;&#29616;&#19979;&#65292;&#38500;&#20102;&#32473;&#20986;&#19968;&#20010;&#26631;&#31614;&#22806;&#65292;&#36824;&#20250;&#34987;&#22823;&#37327;&#30340;&#20851;&#32852;&#20449;&#24687;&#25152;&#28153;&#27809;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21576;&#29616;&#29289;&#20307;&#30340;&#23646;&#24615;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#22522;&#20110;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#26412;&#30740;&#31350;&#25512;&#27979;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#21576;&#29616;&#29289;&#20307;&#30340;&#36523;&#20221;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#20854;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#30340;&#36523;&#20221;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#36755;&#20986;&#34920;&#31034;&#19981;&#20165;&#23545;&#20110;&#21576;&#29616;&#29289;&#20307;&#30340;&#20998;&#31867;&#26377;&#20215;&#20540;&#65292;&#36824;&#23545;&#20110;&#20219;&#20309;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#26377;&#20215;&#20540;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;</title><link>http://arxiv.org/abs/2301.13392</link><description>&lt;p&gt;
&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Causal Bandits without Graph Skeleton. (arXiv:2301.13392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#19968;&#36718;&#36873;&#25321;&#19968;&#32452;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#25910;&#38598;&#35266;&#27979;&#21464;&#37327;&#30340;&#21453;&#39304;&#20197;&#26368;&#23567;&#21270;&#26399;&#26395;&#36951;&#25022;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;&#20108;&#20540;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;BGLMs&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#37117;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#26500;&#24314;&#22240;&#26524;&#20851;&#31995;&#22270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#25351;&#25968;&#19979;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#22270;&#39592;&#26550;&#26469;&#23454;&#29616;BGLMs&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#34920;&#26126;&#23427;&#20173;&#28982;&#36798;&#21040;$O(\sqrt{T}\ln T)$&#30340;&#26399;&#26395;&#36951;&#25022;&#12290;&#36825;&#20010;&#28176;&#36827;&#30340;&#36951;&#25022;&#29575;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In combinatorial causal bandits (CCB), the learning agent chooses a subset of variables in each round to intervene and collects feedback from the observed variables to minimize expected regret or sample complexity. Previous works study this problem in both general causal models and binary generalized linear models (BGLMs). However, all of them require prior knowledge of causal graph structure. This paper studies the CCB problem without the graph structure on binary general causal models and BGLMs. We first provide an exponential lower bound of cumulative regrets for the CCB problem on general causal models. To overcome the exponentially large space of parameters, we then consider the CCB problem on BGLMs. We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$ expected regret. This asymptotic regret is the same as the state-of-art algorithms relying on the graph structure. Moreover, we sacrifice the regret t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11405</link><description>&lt;p&gt;
&#21028;&#21035;&#29109;&#32858;&#31867;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Discriminative Entropy Clustering and its Relation to K-means and SVM. (arXiv:2301.11405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21028;&#21035;&#29109;&#32858;&#31867;&#30340;&#30456;&#20851;&#29702;&#35770;&#21450;&#20854;&#19982;K-means&#21644;SVM&#30340;&#21306;&#21035;&#21644;&#30456;&#20284;&#20043;&#22788;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#36827;&#28145;&#24230;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21028;&#21035;&#27169;&#22411;&#20013;&#65292;&#26368;&#22823;&#21270;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#24418;&#24335;&#19978;&#19982; softmax &#39044;&#27979;&#30340;&#8220;&#20915;&#31574;&#24615;&#8221;&#21644;&#8220;&#20844;&#24179;&#24615;&#8221;&#26377;&#20851;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#22522;&#20110;&#29109;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#25105;&#26631;&#35760;&#26041;&#27861;&#20195;&#34920;&#20102;&#28145;&#24230;&#32858;&#31867;&#30340;&#26368;&#26032;&#30740;&#31350;&#26041;&#21521;&#12290; &#39318;&#20808;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#29109;&#32858;&#31867;&#26041;&#27861;&#30340;&#35768;&#22810;&#36890;&#29992;&#23646;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#19982; K-means &#21644;&#26080;&#30417;&#30563; SVM &#25216;&#26415;&#30340;&#20851;&#31995;&#12290; &#25105;&#20204;&#35777;&#26126;&#19982; K-&#22343;&#20540;&#26377;&#30528;&#26681;&#26412;&#30340;&#21306;&#21035;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#19982;&#22522;&#20110; SVM &#30340;&#32858;&#31867;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26174;&#24335;&#30340;&#36793;&#38469;&#26368;&#22823;&#21270;&#19982;&#29109;&#32858;&#31867;&#32852;&#31995;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20132;&#21449;&#29109;&#30340;&#24120;&#35265;&#24418;&#24335;&#23545;&#20110;&#20266;&#26631;&#31614;&#38169;&#35823;&#19981;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340; EM &#31639;&#27861;&#65292;&#22312;&#35768;&#22810;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#25913;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximization of mutual information between the model's input and output is formally related to "decisiveness" and "fairness" of the softmax predictions, motivating such unsupervised entropy-based losses for discriminative models. Recent self-labeling methods based on such losses represent the state of the art in deep clustering. First, we discuss a number of general properties of such entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques. Disproving some earlier published claims, we point out fundamental differences with K-means. On the other hand, we show similarity with SVM-based clustering allowing us to link explicit margin maximization to entropy clustering. Finally, we observe that the common form of cross-entropy is not robust to pseudo-label errors. Our new loss addresses the problem and leads to a new EM algorithm improving the state of the art on many standard benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16715</link><description>&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;RL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;</title><link>http://arxiv.org/abs/2211.05408</link><description>&lt;p&gt;
&#29992;&#26680;&#26031;&#22374;&#31163;&#24046;&#25511;&#21046;&#30697;
&lt;/p&gt;
&lt;p&gt;
Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#29992;&#20110;&#34913;&#37327;&#20998;&#24067;&#36924;&#36817;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#30446;&#26631;&#23494;&#24230;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#26102;&#35745;&#31639;&#12290;&#26174;&#33879;&#30340;&#24212;&#29992;&#21253;&#25324;&#35786;&#26029;&#36817;&#20284;MCMC&#37319;&#26679;&#22120;&#21644;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#36866;&#37197;&#24230;&#26816;&#39564;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;KSD&#30340;&#25910;&#25947;&#25511;&#21046;&#24615;&#36136;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#29992;&#20110;&#24369;&#25910;&#25947;&#25511;&#21046;&#30340;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#19979;&#28216;&#25193;&#25955;KSD&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#23545;&#20110;&#27599;&#20010;$q&gt;0$&#65292;&#31532;&#19968;&#32452;&#24050;&#30693;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;&#20132;&#21449;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;BHRMs&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2011.14238</link><description>&lt;p&gt;
Bayesian&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Approximate Cross-validated Mean Estimates for Bayesian Hierarchical Regression Models. (arXiv:2011.14238v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14238
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;&#30340;&#36817;&#20284;&#20132;&#21449;&#39564;&#35777;&#22343;&#20540;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;&#20132;&#21449;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#31616;&#21333;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;BHRMs&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#21462;&#36125;&#21494;&#26031;&#23618;&#27425;&#22238;&#24402;&#27169;&#22411;(BHRMs)&#30340;&#20132;&#21449;&#39564;&#35777;&#39044;&#27979;&#20272;&#35745;&#12290;&#36125;&#21494;&#26031;&#23618;&#27425;&#27169;&#22411;&#20197;&#20854;&#33021;&#22815;&#24314;&#27169;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#24182;&#25552;&#20379;&#27010;&#29575;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#36816;&#34892;&#30340;&#35745;&#31639;&#24320;&#38144;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#20132;&#21449;&#39564;&#35777;(CV)&#19981;&#26159;&#35780;&#20272;BHRMs&#39044;&#27979;&#24615;&#33021;&#30340;&#24120;&#35265;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#20026;&#27599;&#20010;&#20132;&#21449;&#39564;&#35777;&#25240;&#21472;&#37325;&#26032;&#36816;&#34892;&#35745;&#31639;&#24320;&#38144;&#26114;&#36149;&#30340;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#65292;&#20351;CV&#22312;&#22823;&#22411;BHRMs&#20013;&#26356;&#21487;&#34892;&#12290;&#36890;&#36807;&#22312;&#26041;&#24046;-&#21327;&#26041;&#24046;&#21442;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#65292;&#23558;CV&#38382;&#39064;&#20174;&#22522;&#20110;&#27010;&#29575;&#30340;&#25277;&#26679;&#36716;&#21270;&#20026;&#31616;&#21333;&#29087;&#24713;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#20135;&#29983;&#30340;&#20272;&#35745;&#19982;&#23436;&#25972;&#30340;CV&#31561;&#25928;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#21644;&#27169;&#25311;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel procedure for obtaining cross-validated predictive estimates for Bayesian hierarchical regression models (BHRMs). Bayesian hierarchical models are popular for their ability to model complex dependence structures and provide probabilistic uncertainty estimates, but can be computationally expensive to run. Cross-validation (CV) is therefore not a common practice to evaluate the predictive performance of BHRMs. Our method circumvents the need to re-run computationally costly estimation methods for each cross-validation fold and makes CV more feasible for large BHRMs. By conditioning on the variance-covariance parameters, we shift the CV problem from probability-based sampling to a simple and familiar optimization problem. In many cases, this produces estimates which are equivalent to full CV. We provide theoretical results and demonstrate its efficacy on publicly available data and in simulations.
&lt;/p&gt;</description></item></channel></rss>