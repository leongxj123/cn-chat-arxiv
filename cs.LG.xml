<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18209</link><description>&lt;p&gt;
&#38271;&#30701;&#26399;&#32422;&#26463;&#39537;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Long and Short-Term Constraints Driven Safe Reinforcement Learning for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#20013;&#21487;&#20197;&#21516;&#26102;&#20445;&#35777;&#36710;&#36742;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#26080;&#27861;&#20445;&#35777;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#26399;&#21644;&#30701;&#26399;&#32422;&#26463;&#65288;LSTC&#65289;&#30340;&#26032;&#31639;&#27861;&#29992;&#20110;&#23433;&#20840;RL&#12290;&#30701;&#26399;&#32422;&#26463;&#26088;&#22312;&#30830;&#20445;&#36710;&#36742;&#25506;&#27979;&#21040;&#30340;&#30701;&#26399;&#29366;&#24577;&#23433;&#20840;&#65292;&#32780;&#38271;&#26399;&#32422;&#26463;&#21017;&#30830;&#20445;&#25972;&#20307;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18209v1 Announce Type: cross  Abstract: Reinforcement learning (RL) has been widely used in decision-making tasks, but it cannot guarantee the agent's safety in the training process due to the requirements of interaction with the environment, which seriously limits its industrial applications such as autonomous driving. Safe RL methods are developed to handle this issue by constraining the expected safety violation costs as a training objective, but they still permit unsafe state occurrence, which is unacceptable in autonomous driving tasks. Moreover, these methods are difficult to achieve a balance between the cost and return expectations, which leads to learning performance degradation for the algorithms. In this paper, we propose a novel algorithm based on the long and short-term constraints (LSTC) for safe RL. The short-term constraint aims to guarantee the short-term state safety that the vehicle explores, while the long-term constraint ensures the overall safety of the
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.00564</link><description>&lt;p&gt;
&#39640;&#25928;Zero V2&#65306;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#25484;&#25569;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2: Mastering Discrete and Continuous Control with Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00564
&lt;/p&gt;
&lt;p&gt;
EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#36890;&#29992;&#31639;&#27861;DreamerV3&#26377;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#31639;&#27861;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EfficientZero V2&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#39640;&#25928;RL&#31639;&#27861;&#35774;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;EfficientZero&#30340;&#24615;&#33021;&#25193;&#23637;&#21040;&#22810;&#20010;&#39046;&#22495;&#65292;&#28085;&#30422;&#36830;&#32493;&#21644;&#31163;&#25955;&#34892;&#21160;&#65292;&#20197;&#21450;&#35270;&#35273;&#21644;&#20302;&#32500;&#36755;&#20837;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#65292;EfficientZero V2&#22312;&#26377;&#38480;&#25968;&#25454;&#35774;&#32622;&#19979;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22823;&#24133;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#65288;SOTA&#65289;&#12290;EfficientZero V2&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36827;&#27493;&#65292;&#27604;&#22914;Atari 100k&#65292;Proprio Control&#31561;&#20013;&#65292;&#22312;66&#20010;&#35780;&#20272;&#20219;&#21153;&#20013;&#26377;50&#20010;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00564v1 Announce Type: cross  Abstract: Sample efficiency remains a crucial challenge in applying Reinforcement Learning (RL) to real-world tasks. While recent algorithms have made significant strides in improving sample efficiency, none have achieved consistently superior performance across diverse domains. In this paper, we introduce EfficientZero V2, a general framework designed for sample-efficient RL algorithms. We have expanded the performance of EfficientZero to multiple domains, encompassing both continuous and discrete actions, as well as visual and low-dimensional inputs. With a series of improvements we propose, EfficientZero V2 outperforms the current state-of-the-art (SOTA) by a significant margin in diverse tasks under the limited data setting. EfficientZero V2 exhibits a notable advancement over the prevailing general algorithm, DreamerV3, achieving superior outcomes in 50 of 66 evaluated tasks across diverse benchmarks, such as Atari 100k, Proprio Control, an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.00381</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#21453;&#27493;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Structured Deep Neural Networks-Based Backstepping Trajectory Tracking Control for Lagrangian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00381
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#20248;&#21270;&#21442;&#25968;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#23398;&#20064;&#25511;&#21046;&#22120;&#65292;&#22240;&#20026;&#20854;&#20986;&#33394;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#38381;&#29615;&#31283;&#23450;&#24615;&#20445;&#35777;&#21644;&#24615;&#33021;&#20998;&#26512;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;DNN&#30340;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#37319;&#29992;&#21453;&#25512;&#25216;&#26415;&#23454;&#29616;&#25289;&#26684;&#26391;&#26085;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;&#12290;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#30830;&#20445;&#20219;&#20309;&#20860;&#23481;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36319;&#36394;&#35823;&#24046;&#30340;&#26126;&#30830;&#19978;&#38480;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#25511;&#21046;&#21442;&#25968;&#26469;&#23454;&#29616;&#25152;&#38656;&#30340;&#36319;&#36394;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#24403;&#31995;&#32479;&#27169;&#22411;&#26410;&#30693;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00381v1 Announce Type: cross  Abstract: Deep neural networks (DNN) are increasingly being used to learn controllers due to their excellent approximation capabilities. However, their black-box nature poses significant challenges to closed-loop stability guarantees and performance analysis. In this paper, we introduce a structured DNN-based controller for the trajectory tracking control of Lagrangian systems using backing techniques. By properly designing neural network structures, the proposed controller can ensure closed-loop stability for any compatible neural network parameters. In addition, improved control performance can be achieved by further optimizing neural network parameters. Besides, we provide explicit upper bounds on tracking errors in terms of controller parameters, which allows us to achieve the desired tracking performance by properly selecting the controller parameters. Furthermore, when system models are unknown, we propose an improved Lagrangian neural net
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2211.10777</link><description>&lt;p&gt;
&#26080;&#30456;&#24178;&#31354;&#20013;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Non-Coherent Over-the-Air Decentralized Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35832;&#22914;&#36828;&#31243;&#24863;&#30693;&#12289;&#20998;&#24067;&#24335;&#25512;&#26029;&#12289;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#21508;&#31181;&#39046;&#22495;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#21463;&#21040;&#22122;&#22768;&#12289;&#34928;&#33853;&#21644;&#24102;&#23485;&#21463;&#38480;&#30340;&#26080;&#32447;&#31995;&#32479;&#19978;&#25191;&#34892;DGD&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#24230;&#20256;&#36755;&#20197;&#20943;&#36731;&#24178;&#25200;&#65292;&#24182;&#33719;&#21462;&#25299;&#25169;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36825;&#22312;&#26080;&#32447;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#26080;&#32447;&#31995;&#32479;&#23450;&#21046;&#30340;DGD&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#26080;&#38656;&#36827;&#34892;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#26080;&#30456;&#24178;&#31354;&#20013;&#65288;NCOTA&#65289;&#20849;&#35782;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#22122;&#22768;&#33021;&#37327;&#21472;&#21152;&#29305;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#21270;&#20256;&#36755;&#31574;&#30053;&#26469;&#36866;&#24212;&#21322;&#21452;&#24037;&#25805;&#20316;&#65292;&#21457;&#23556;&#26426;&#23558;&#20301;&#32622;&#26144;&#23556;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10777v2 Announce Type: replace-cross  Abstract: Decentralized Gradient Descent (DGD) is a popular algorithm used to solve decentralized optimization problems in diverse domains such as remote sensing, distributed inference, multi-agent coordination, and federated learning. Yet, executing DGD over wireless systems affected by noise, fading and limited bandwidth presents challenges, requiring scheduling of transmissions to mitigate interference and the acquisition of topology and channel state information -- complex tasks in wireless decentralized systems. This paper proposes a DGD algorithm tailored to wireless systems. Unlike existing approaches, it operates without inter-agent coordination, topology information, or channel state information. Its core is a Non-Coherent Over-The-Air (NCOTA) consensus scheme, exploiting a noisy energy superposition property of wireless channels. With a randomized transmission strategy to accommodate half-duplex operation, transmitters map loca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13751</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#24314;&#27169;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#38024;&#23545;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#24615;&#24314;&#27169;&#12290;&#30740;&#31350;&#21457;&#29616;&#38544;&#34255;&#23618;&#25968;&#37327;&#23545;&#27169;&#22411;&#30340;&#25512;&#24191;&#24615;&#33021;&#26377;&#24433;&#21709;&#65292;&#21516;&#26102;&#36824;&#27979;&#35797;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#31561;&#21442;&#25968;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#20351;&#29992;&#20855;&#26377;&#36234;&#26469;&#36234;&#22810;&#21487;&#35843;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#27169;&#22411;&#25439;&#22833;&#25110;&#21019;&#24314;&#26356;&#20855;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#36890;&#24120;&#30456;&#20114;&#30683;&#30462;&#12290;&#29305;&#21035;&#26159;&#65292;&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#26356;&#22823;&#27169;&#22411;&#33021;&#21542;&#25512;&#24191;&#21040;&#21463;&#25511;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20043;&#22806;&#30340;&#25968;&#25454;&#30340;&#30097;&#38382;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ResNet&#27169;&#22411;&#20013;&#38544;&#34255;&#23618;&#30340;&#25968;&#37327;&#22312;MNIST&#12289;CIFAR10&#21644;CIFAR100&#25968;&#25454;&#38598;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21508;&#31181;&#21442;&#25968;&#65292;&#21253;&#25324;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#28014;&#28857;&#31934;&#24230;&#65292;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#36755;&#20986;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#25913;&#36827;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35825;&#21457;&#25925;&#38556;&#26469;&#24314;&#27169;&#25925;&#38556;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks have shown to be widely applicable to a large number of fields when large amounts of labelled data are available. The recent trend has been to use models with increasingly larger sets of tunable parameters to increase model accuracy, reduce model loss, or create more adversarially robust models -- goals that are often at odds with one another. In particular, recent theoretical work raises questions about the ability for even larger models to generalize to data outside of the controlled train and test sets. As such, we examine the role of the number of hidden layers in the ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a variety of parameters including the size of the model, the floating point precision, and the noise level of both the training data and the model output. To encapsulate the model's predictive power and computational cost, we provide a method that uses induced failures to model the probability of failure as a fun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.02398</link><description>&lt;p&gt;
&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data for neural operators. (arXiv:2401.02398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33719;&#21462;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#36229;&#20986;&#20102;&#24403;&#21069;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#37117;&#23384;&#22312;&#21516;&#26679;&#30340;&#38382;&#39064;&#65306;&#35757;&#32451;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#22914;&#26377;&#38480;&#24046;&#20998;&#25110;&#26377;&#38480;&#20803;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#20989;&#25968;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65306;&#25105;&#20204;&#20174;&#24050;&#30693;&#35299;&#20301;&#20110;&#30340;&#32463;&#20856;&#29702;&#35770;&#35299;&#31354;&#38388;&#65288;&#20363;&#22914;$H_0^1(\Omega)$&#65289;&#20013;&#25277;&#21462;&#22823;&#37327;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#8220;&#38543;&#26426;&#20989;&#25968;&#8221;$u_j$&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#38543;&#26426;&#35299;&#26041;&#26696;&#20195;&#20837;&#26041;&#31243;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#21491;&#20391;&#20989;&#25968;$f_j$&#65292;&#23558;$(f_j, u_j)_{j=1}^N$&#20316;&#20026;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically. The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.09157</link><description>&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Finding Stationary Points in Non-Convex Optimization. (arXiv:2310.09157v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20294;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;$f$&#22312;&#26080;&#38480;&#21046;&#30340;$d$&#32500;&#22495;&#19978;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21363;&#26799;&#24230;&#36817;&#20284;&#20026;&#38646;&#30340;&#28857;&#65292;&#26159;&#32463;&#20856;&#38750;&#20984;&#20248;&#21270;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#38382;&#39064;&#30340;&#32500;&#24230;$d$&#19982;&#36817;&#20284;&#35823;&#24046;&#29420;&#31435;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#20173;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#32467;&#26524;&#65306;1.&#22312;&#26080;&#38480;&#21046;&#30340;&#22495;&#20013;&#23547;&#25214;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#38382;&#39064;&#26159;PLS&#23436;&#20840;&#38382;&#39064;&#12290;2.&#23545;&#20110;$d=2$&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#26368;&#22810;$O(1/\varepsilon)$&#27425;&#20989;&#25968;&#20540;&#26597;&#35810;&#12290;3.&#25105;&#20204;&#35777;&#26126;&#24403;$d=2$&#26102;&#65292;&#20219;&#20309;&#31639;&#27861;&#33267;&#23569;&#38656;&#35201;$\Omega(1/\varepsilon)$&#27425;&#23545;&#30446;&#26631;&#20989;&#25968;&#21644;/&#25110;&#26799;&#24230;&#30340;&#26597;&#35810;&#26469;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions $f$ over unrestricted $d$-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension $d$ of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:  1. The problem of finding approximate stationary points over unrestricted domains is PLS-complete.  2. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-approximate stationary points that requires at most $O(1/\varepsilon)$ value queries to the objective function.  3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries to the objective function and/or its gradient to find $\varepsilon$-approximate stationary points when $d=2$.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03146</link><description>&lt;p&gt;
&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#22312;&#31751;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#25968;&#25454;&#19978;&#25913;&#21892;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data. (arXiv:2310.03146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#22312;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#25200;&#12290;&#39318;&#20808;&#65292;&#23427;&#20551;&#35774;&#35757;&#32451;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#23558;&#26679;&#26412;&#25353;&#20849;&#20139;&#30340;&#27979;&#37327;&#20540;&#36827;&#34892;&#20998;&#32452;&#65288;&#20363;&#22914;&#65292;&#30740;&#31350;&#21442;&#19982;&#32773;&#25110;&#32454;&#32990;&#65289;&#65292;&#36829;&#21453;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#26174;&#31034;&#20986;&#24615;&#33021;&#19979;&#38477;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#21644;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#30528;&#31751;&#28151;&#28102;&#24341;&#36215;&#30340;&#31532;&#19968;&#22411;&#21644;&#31532;&#20108;&#22411;&#38169;&#35823;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#36890;&#24120;&#34987;&#35757;&#32451;&#20197;&#23454;&#29616;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#34987;&#20302;&#20272;&#30340;&#32676;&#20307;&#65292;&#22312;&#36151;&#27454;&#25209;&#20934;&#25110;&#30830;&#23450;&#20581;&#24247;&#20445;&#38505;&#36153;&#29575;&#31561;&#20851;&#38190;&#39046;&#22495;&#24341;&#20837;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#12290;MEDL&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#20869;&#23481;&#20998;&#21035;&#37327;&#21270;&#31751;&#19981;&#21464;&#30340;&#22266;&#23450;&#25928;&#24212;&#21644;&#31751;&#29305;&#23450;&#30340;&#38543;&#26426;&#25928;&#24212;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#19968;&#20010;&#31751;&#23545;&#25163;&#65292;&#40723;&#21169;&#31751;&#38388;&#24046;&#24322;&#30340;&#26368;&#23567;&#21270;&#65307;
&lt;/p&gt;
&lt;p&gt;
Traditional deep learning (DL) suffers from two core problems. Firstly, it assumes training samples are independent and identically distributed. However, numerous real-world datasets group samples by shared measurements (e.g., study participants or cells), violating this assumption. In these scenarios, DL can show compromised performance, limited generalization, and interpretability issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly, models are typically trained for overall accuracy, often neglecting underrepresented groups and introducing biases in crucial areas like loan approvals or determining health insurance rates, such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourage
&lt;/p&gt;</description></item><item><title>fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;</title><link>http://arxiv.org/abs/2310.02008</link><description>&lt;p&gt;
fmeffects: &#19968;&#20010;&#29992;&#20110;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#30340;R&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
fmeffects: An R Package for Forward Marginal Effects. (arXiv:2310.02008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02008
&lt;/p&gt;
&lt;p&gt;
fmeffects&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#30340;R&#36719;&#20214;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#36793;&#38469;&#25928;&#24212;&#65288;FMEs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#26377;&#25928;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#35299;&#37322;&#26041;&#27861;&#26368;&#36817;&#34987;&#24341;&#20837;&#12290;&#23427;&#20204;&#20197;&#8220;&#22914;&#26524;&#25105;&#20204;&#23558;$x$&#25913;&#21464;$h$&#65292;&#37027;&#20040;&#39044;&#27979;&#32467;&#26524;$\widehat{y}$&#20250;&#21457;&#29983;&#20160;&#20040;&#21464;&#21270;&#65311;&#8221;&#30340;&#24418;&#24335;&#25552;&#20379;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#25805;&#20316;&#30340;&#27169;&#22411;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;fmeffects&#36719;&#20214;&#21253;&#65292;&#36825;&#26159;FMEs&#30340;&#31532;&#19968;&#20010;&#36719;&#20214;&#23454;&#29616;&#12290;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#29702;&#35770;&#32972;&#26223;&#12289;&#36719;&#20214;&#21253;&#21151;&#33021;&#21644;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#21450;&#36719;&#20214;&#35774;&#35745;&#21644;&#26410;&#26469;&#25193;&#23637;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forward marginal effects (FMEs) have recently been introduced as a versatile and effective model-agnostic interpretation method. They provide comprehensible and actionable model explanations in the form of: If we change $x$ by an amount $h$, what is the change in predicted outcome $\widehat{y}$? We present the R package fmeffects, the first software implementation of FMEs. The relevant theoretical background, package functionality and handling, as well as the software design and options for future extensions are discussed in this paper.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00183</link><description>&lt;p&gt;
&#22270;&#21367;&#31215;&#21644;Mixup&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Equivalence of Graph Convolution and Mixup. (arXiv:2310.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00183
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#21644;Mixup&#25216;&#26415;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22270;&#21367;&#31215;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#26159;&#36890;&#36807;&#32858;&#21512;&#37051;&#23621;&#26679;&#26412;&#30340;&#29305;&#24449;&#26469;&#23398;&#20064;&#29305;&#23450;&#33410;&#28857;&#25110;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;&#32780;Mixup&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#29420;&#28909;&#26631;&#31614;&#36827;&#34892;&#24179;&#22343;&#26469;&#29983;&#25104;&#26032;&#30340;&#31034;&#20363;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#20043;&#38388;&#30340;&#19968;&#20010;&#20849;&#21516;&#20043;&#22788;&#26159;&#23427;&#20204;&#21033;&#29992;&#20102;&#26469;&#33258;&#22810;&#20010;&#26679;&#26412;&#30340;&#20449;&#24687;&#26469;&#24471;&#20986;&#29305;&#24449;&#34920;&#31034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;&#20004;&#20010;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#21367;&#31215;&#21487;&#20197;&#34987;&#35270;&#20026;Mixup&#30340;&#19968;&#31181;&#29305;&#27530;&#24418;&#24335;&#65292;&#23427;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#37117;&#34987;&#24212;&#29992;&#12290;&#36825;&#20004;&#20010;&#26465;&#20214;&#26159;&#65306;1&#65289;\textit{&#21516;&#36136;&#25913;&#26631;} - &#23558;&#30446;&#26631;&#33410;&#28857;&#30340;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#25152;&#26377;&#37051;&#23621;&#65292;&#20197;&#21450;2&#65289;\textit{&#27979;&#35797;&#26102;Mixup} - &#22312;&#27979;&#35797;&#26102;&#23545;&#29305;&#24449;&#36827;&#34892;Mixup&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20004;&#20010;&#26465;&#20214;&#30340;&#25968;&#23398;&#34920;&#36798;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20010;&#31561;&#20215;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textit{Homophily Relabel} - assigning the target node's label to all its neighbors, and 2) \textit{Test-Time Mixup} - Mixup the feature during the test time. We establis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#27531;&#24046;&#20462;&#27491;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#24182;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07140</link><description>&lt;p&gt;
&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Short-term power load forecasting method based on CNN-SAEDN-Res. (arXiv:2309.07140v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#27531;&#24046;&#20462;&#27491;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#24182;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#38590;&#20197;&#36890;&#36807;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;SAEDN&#65289;&#21644;&#27531;&#24046;&#20462;&#27491;&#65288;Res&#65289;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#30001;&#19968;&#20010;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#29992;&#20110;&#25366;&#25496;&#25968;&#25454;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#24182;&#33719;&#21462;&#39640;&#32500;&#25968;&#25454;&#29305;&#24449;&#12290;&#21021;&#22987;&#36127;&#33655;&#39044;&#27979;&#27169;&#22359;&#30001;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#19968;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#32452;&#25104;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#23545;&#39640;&#32500;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20010;&#25805;&#20316;&#21487;&#20197;&#33719;&#21462;&#25968;&#25454;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22522;&#20110;&#28151;&#21512;&#20102;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#30340;&#32806;&#21512;&#20851;&#31995;&#20445;&#30041;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, the load data with non-temporal factors are difficult to process by sequence models. This problem results in insufficient precision of the prediction. Therefore, a short-term load forecasting method based on convolutional neural network (CNN), self-attention encoder-decoder network (SAEDN) and residual-refinement (Res) is proposed. In this method, feature extraction module is composed of a two-dimensional convolutional neural network, which is used to mine the local correlation between data and obtain high-dimensional data features. The initial load fore-casting module consists of a self-attention encoder-decoder network and a feedforward neural network (FFN). The module utilizes self-attention mechanisms to encode high-dimensional features. This operation can obtain the global correlation between data. Therefore, the model is able to retain important information based on the coupling relationship between the data in data mixed with non-time series factors. Then, self
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;API&#27969;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#12289;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#12289;&#24322;&#26500;&#27169;&#22411;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.00023</link><description>&lt;p&gt;
&#20174;API&#27969;&#20013;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning From a Stream of APIs. (arXiv:2309.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;API&#27969;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#12289;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#12289;&#24322;&#26500;&#27169;&#22411;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#21407;&#22987;&#25968;&#25454;&#65292;&#30001;&#20110;&#29256;&#26435;&#32771;&#34385;&#21644;&#38544;&#31169;&#39118;&#38505;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#21033;&#30410;&#30456;&#20851;&#32773;&#36890;&#24120;&#36890;&#36807;API&#37322;&#25918;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;API&#35775;&#38382;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#31181;&#23454;&#29992;&#20294;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;DECL-APIs&#65289;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;DFCL-APIs&#65289;&#65292;&#36890;&#36807;&#37096;&#20998;&#25110;&#26080;&#21407;&#22987;&#25968;&#25454;&#20174;API&#27969;&#20013;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#26032;&#35774;&#32622;&#19979;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#65306;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20219;&#24847;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#24322;&#26500;&#27169;&#22411;&#20197;&#21450;&#23545;&#20197;&#21069;API&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25968;&#25454;&#21512;&#20316;&#25345;&#32493;&#33976;&#39311;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn new tasks without forgetting previous tasks. However, existing CL methods require a large amount of raw data, which is often unavailable due to copyright considerations and privacy risks. Instead, stakeholders usually release pre-trained machine learning models as a service (MLaaS), which users can access via APIs. This paper considers two practical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL (DFCL-APIs), which achieve CL from a stream of APIs with partial or no raw data. Performing CL under these two new settings faces several challenges: unavailable full raw data, unknown model parameters, heterogeneous models of arbitrary architecture and scale, and catastrophic forgetting of previous APIs. To overcome these issues, we propose a novel data-free cooperative continual distillation learning framework that distills knowledge from a stream of APIs into a CL model by generating pseudo data, just by querying APIs. Specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.10614</link><description>&lt;p&gt;
&#24102;&#26377;&#22024;&#26434;&#27835;&#30103;&#21644;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#30340;&#21487;&#35782;&#21035;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Identifiable causal inference with noisy treatment and no side information. (arXiv:2306.10614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#20391;&#38754;&#20449;&#24687;&#21644;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32416;&#27491;&#22240;&#27835;&#30103;&#21464;&#37327;&#19981;&#20934;&#30830;&#27979;&#37327;&#24341;&#36215;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#20559;&#24046;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#21644;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#22240;&#26524;&#25512;&#26029;&#22330;&#26223;&#20013;&#65292;&#27835;&#30103;&#65288;&#21363;&#21407;&#22240;&#65289;&#21464;&#37327;&#30340;&#27979;&#37327;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#20363;&#22914;&#22312;&#27969;&#34892;&#30149;&#23398;&#25110;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#12290;&#26410;&#33021;&#32416;&#27491;&#27979;&#37327;&#35823;&#24046;&#30340;&#24433;&#21709;&#21487;&#33021;&#23548;&#33268;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#20174;&#22240;&#26524;&#35270;&#35282;&#30740;&#31350;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#24182;&#19988;&#19981;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20391;&#38754;&#20449;&#24687;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#22330;&#26223;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23427;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#27835;&#30103;&#21464;&#37327;&#65292;&#35813;&#21464;&#37327;&#27979;&#37327;&#19981;&#20934;&#30830;&#12290;&#24314;&#31435;&#22312;&#29616;&#26377;&#27979;&#37327;&#35823;&#24046;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#21363;&#20351;&#27809;&#26377;&#27979;&#37327;&#35823;&#24046;&#26041;&#24046;&#25110;&#20854;&#20182;&#20391;&#38754;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65292;&#20854;&#20013;&#39640;&#26031;&#26465;&#20214;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#25674;&#26435;&#37325;&#21464;&#20998;&#23458;&#35266;&#20989;&#25968;&#26469;&#35757;&#32451;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In some causal inference scenarios, the treatment (i.e. cause) variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such as scenario, this paper proposes a model that assumes a continuous treatment variable which is inaccurately measured. Building on existing results for measurement error models, we prove that our model's causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model where Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06338</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#32570;&#22833;&#20540;&#25554;&#34917;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Missing Values Imputation in Categorical Datasets. (arXiv:2306.06338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#21644;&#25554;&#34917;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#29992;ECOC&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#20010;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#65292;&#20294;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30528;&#37325;&#30740;&#31350;&#20102;&#20351;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;SVM&#21644;KNN&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#20197;&#21450;&#32467;&#21512;&#20102;SVM&#12289;KNN&#21644;MLP&#27169;&#22411;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31639;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#25968;&#25454;&#38598;: CPU&#25968;&#25454;&#38598;&#12289;&#30002;&#29366;&#33146;&#21151;&#33021;&#20943;&#36864;&#25968;&#25454;&#38598;&#21644;&#20083;&#33146;&#30284;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#22312;&#39044;&#27979;&#21644;&#25554;&#34917;&#32570;&#22833;&#20540;&#26041;&#38754;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20855;&#20307;&#32467;&#26524;&#22240;&#25968;&#25454;&#38598;&#21644;&#32570;&#22833;&#20540;&#27169;&#24335;&#32780;&#24322;&#12290;&#37319;&#29992;&#35823;&#24046;&#32416;&#27491;&#36755;&#20986;&#30721;(ECOC)&#26694;&#26550;&#30340;&#38598;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#21333;&#20010;&#27169;&#22411;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#32570;&#22833;&#20540;&#25554;&#34917;&#20063;&#23384;&#22312;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explored the use of machine learning algorithms for predicting and imputing missing values in categorical datasets. We focused on ensemble models that use the error correction output codes (ECOC) framework, including SVM-based and KNN-based ensemble models, as well as an ensemble classifier that combines SVM, KNN, and MLP models. We applied these algorithms to three datasets: the CPU dataset, the hypothyroid dataset, and the Breast Cancer dataset. Our experiments showed that the machine learning algorithms were able to achieve good performance in predicting and imputing the missing values, with some variations depending on the specific dataset and missing value pattern. The ensemble models using the error correction output codes (ECOC) framework were particularly effective in improving the accuracy and robustness of the predictions, compared to individual models. However, there are also challenges and limitations to using deep learning for missing value imputation, including
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07272</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#22810;&#37325;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is the state of the art? Accounting for multiplicity in machine learning benchmark performance. (arXiv:2303.07272v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07272
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#24615;&#33021;&#35780;&#20272;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#30340;&#20272;&#35745;&#20540;&#36807;&#20110;&#20048;&#35266;&#65292;&#23481;&#26131;&#23548;&#33268;&#26041;&#27861;&#30340;&#24573;&#35270;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26469;&#36827;&#34892;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;&#36825;&#20801;&#35768;&#22810;&#31181;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#24182;&#36328;&#36234;&#26102;&#38388;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#38382;&#39064;&#20013;&#25490;&#21517;&#26368;&#39640;&#30340;&#24615;&#33021;&#34987;&#31216;&#20026;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#29992;&#20316;&#26032;&#26041;&#27861;&#20986;&#29256;&#30340;&#21442;&#32771;&#28857;&#12290;&#20294;&#20351;&#29992;&#26368;&#39640;&#25490;&#21517;&#30340;&#24615;&#33021;&#20316;&#20026;SOTA&#30340;&#20272;&#35745;&#20540;&#26159;&#19968;&#31181;&#26377;&#20559;&#30340;&#20272;&#35745;&#22120;&#65292;&#20250;&#32473;&#20986;&#36807;&#20110;&#20048;&#35266;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#22810;&#37325;&#24615;&#30340;&#26426;&#21046;&#26159;&#22810;&#37325;&#27604;&#36739;&#21644;&#22810;&#37325;&#26816;&#39564;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#20294;&#22312;&#20851;&#20110;SOTA&#20272;&#35745;&#30340;&#35752;&#35770;&#20013;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#25552;&#21450;&#12290;&#36807;&#20110;&#20048;&#35266;&#30340;&#26368;&#20808;&#36827;&#20272;&#35745;&#20540;&#34987;&#29992;&#20316;&#35780;&#20272;&#26032;&#26041;&#27861;&#30340;&#26631;&#20934;&#65292;&#32780;&#20855;&#26377;&#26126;&#26174;&#21155;&#21183;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#22810;&#37325;&#24615;&#20559;&#24046;&#24182;&#27604;&#36739;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability 
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2302.10763</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19982;&#23646;&#24615;&#20851;&#32852;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning and the Emergence of Attributes Associations. (arXiv:2302.10763v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10763
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#36890;&#36807;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#36523;&#20221;&#20445;&#25345;&#30340;&#21464;&#25442;&#65292;&#19981;&#20165;&#26377;&#21161;&#20110;&#29289;&#20307;&#30340;&#20998;&#31867;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29289;&#20307;&#21576;&#29616;&#65292;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36890;&#24120;&#20250;&#32473;&#20986;&#19968;&#20010;&#31616;&#27905;&#30340;&#26631;&#31614;&#12290;&#32780;&#20154;&#31867;&#22312;&#31867;&#20284;&#30340;&#21576;&#29616;&#19979;&#65292;&#38500;&#20102;&#32473;&#20986;&#19968;&#20010;&#26631;&#31614;&#22806;&#65292;&#36824;&#20250;&#34987;&#22823;&#37327;&#30340;&#20851;&#32852;&#20449;&#24687;&#25152;&#28153;&#27809;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#21576;&#29616;&#29289;&#20307;&#30340;&#23646;&#24615;&#12290;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#65292;&#22522;&#20110;&#23545;&#29289;&#20307;&#36755;&#20837;&#34920;&#31034;&#36827;&#34892;&#20445;&#25345;&#36523;&#20221;&#30340;&#21464;&#25442;&#12290;&#26412;&#30740;&#31350;&#25512;&#27979;&#65292;&#36825;&#20123;&#21464;&#25442;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#21576;&#29616;&#29289;&#20307;&#30340;&#36523;&#20221;&#65292;&#36824;&#21487;&#20197;&#20445;&#25345;&#20854;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#30340;&#36523;&#20221;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#30340;&#36755;&#20986;&#34920;&#31034;&#19981;&#20165;&#23545;&#20110;&#21576;&#29616;&#29289;&#20307;&#30340;&#20998;&#31867;&#26377;&#20215;&#20540;&#65292;&#36824;&#23545;&#20110;&#20219;&#20309;&#24863;&#20852;&#36259;&#23646;&#24615;&#30340;&#26377;&#26080;&#20915;&#31574;&#26377;&#20215;&#20540;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#35266;&#28857;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to an object presentation, supervised learning schemes generally respond with a parsimonious label. Upon a similar presentation we humans respond again with a label, but are flooded, in addition, by a myriad of associations. A significant portion of these consist of the presented object attributes. Contrastive learning is a semi-supervised learning scheme based on the application of identity preserving transformations on the object input representations. It is conjectured in this work that these same applied transformations preserve, in addition to the identity of the presented object, also the identity of its semantically meaningful attributes. The corollary of this is that the output representations of such a contrastive learning scheme contain valuable information not only for the classification of the presented object, but also for the presence or absence decision of any attribute of interest. Simulation results which demonstrate this idea and the feasibility of this co
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.11777</link><description>&lt;p&gt;
&#20855;&#26377;&#23450;&#20301;&#24212;&#29992;&#30340;&#36335;&#24452;&#25439;&#32791;&#21644;&#21040;&#36798;&#26102;&#38388;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dataset of Pathloss and ToA Radio Maps With Localization Application. (arXiv:2212.11777v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#33021;&#22815;&#29992;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#21644;&#26080;&#32447;&#23450;&#20301;&#65292;&#36890;&#36807;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#21040;RSS&#21644;ToA&#22320;&#22270;&#65292;&#21487;&#20197;&#20844;&#24179;&#27604;&#36739;&#20004;&#31181;&#23450;&#20301;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#31264;&#23494;&#22478;&#24066;&#29615;&#22659;&#20013;&#29983;&#25104;&#24182;&#20844;&#24320;&#25552;&#20379;&#30340;&#19968;&#32452;&#26080;&#32447;&#22320;&#22270;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#27169;&#25311;&#30340;&#36335;&#24452;&#25439;&#32791;/&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65288;RSS&#65289;&#21644;&#21040;&#36798;&#26102;&#38388;&#65288;ToA&#65289;&#26080;&#32447;&#22320;&#22270;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;&#30495;&#23454;&#22478;&#24066;&#22320;&#22270;&#30340;&#31264;&#23494;&#22478;&#24066;&#35774;&#32622;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20004;&#20010;&#20027;&#35201;&#24212;&#29992;&#26159;1&#65289;&#20174;&#36755;&#20837;&#30340;&#22478;&#24066;&#22320;&#22270;&#39044;&#27979;&#36335;&#24452;&#25439;&#32791;&#30340;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#25311;&#65289;&#65292;&#20197;&#21450;2&#65289;&#26080;&#32447;&#23450;&#20301;&#12290;RSS&#21644;ToA&#22320;&#22270;&#36890;&#36807;&#30456;&#21516;&#30340;&#27169;&#25311;&#22312;&#30456;&#21516;&#30340;&#22478;&#24066;&#22320;&#22270;&#19978;&#35745;&#31639;&#24471;&#20986;&#65292;&#21487;&#20197;&#23545;&#22522;&#20110;RSS&#21644;ToA&#30340;&#23450;&#20301;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we present a collection of radio map datasets in dense urban setting, which we generated and made publicly available. The datasets include simulated pathloss/received signal strength (RSS) and time of arrival (ToA) radio maps over a large collection of realistic dense urban setting in real city maps. The two main applications of the presented dataset are 1) learning methods that predict the pathloss from input city maps (namely, deep learning-based simulations), and, 2) wireless localization. The fact that the RSS and ToA maps are computed by the same simulations over the same city maps allows for a fair comparison of the RSS and ToA-based localization methods.
&lt;/p&gt;</description></item></channel></rss>