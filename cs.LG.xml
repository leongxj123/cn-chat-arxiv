<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2404.02424</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#36328;&#27169;&#24577;&#36866;&#24212;&#20462;&#22797;&#31232;&#30095;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;VLM&#20462;&#21098;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#25552;&#20986;&#31232;&#30095;&#27604;&#29575;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#20462;&#22797;&#31232;&#30095;VLMs&#24615;&#33021;&#25152;&#38656;&#30340;&#19987;&#38376;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#25972;&#21512;&#20102;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#19981;&#21516;&#20449;&#24687;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#37096;&#32626;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#30340;VLMs&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#23613;&#31649;&#20462;&#21098;&#21518;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#20445;&#25345;&#26356;&#23567;&#27169;&#22411;&#22823;&#23567;&#24615;&#33021;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20854;&#22312;VLMs&#20013;&#30340;&#24212;&#29992;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#65292;&#36825;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#19981;&#21516;&#27169;&#24577;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20462;&#22797;&#34987;&#20462;&#21098;&#31232;&#30095;&#30340;VLMs&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20851;&#20110;VLM&#20462;&#21098;&#30340;&#21021;&#27493;&#30740;&#31350;&#65292;&#21457;&#29616;&#20351;&#29992;&#30456;&#21516;&#31232;&#30095;&#27604;&#29575;&#20462;&#21098;&#35270;&#35273;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#23454;&#29616;&#25509;&#36817;&#26368;&#20339;&#24615;&#33021;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#19982;&#24494;&#35843;&#21333;&#27169;&#31232;&#30095;&#27169;&#22411;&#19981;&#21516;&#65292;&#31232;&#30095;VLMs&#28041;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02424v1 Announce Type: new  Abstract: Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.15022</link><description>&lt;p&gt;
&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Insights into the Lottery Ticket Hypothesis and the Iterative Magnitude Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15022
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#20013;&#19981;&#21516;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35797;&#22270;&#27934;&#23519;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#21644;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#24378;&#35843;&#20102;&#37325;&#26032;&#35757;&#32451;&#21033;&#29992;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#33719;&#24471;&#30340;&#26356;&#31232;&#30095;&#32593;&#32476;&#26102;&#25152;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#33267;&#20170;&#23578;&#32570;&#20047;&#20851;&#20110;&#36208;&#21183;&#24425;&#31080;&#20551;&#35774;&#20013;&#25552;&#20986;&#30340;&#29305;&#23450;&#21021;&#22987;&#21270;&#20026;&#20309;&#26356;&#26377;&#21033;&#20110;&#27867;&#21270;&#65288;&#21644;&#35757;&#32451;&#65289;&#24615;&#33021;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#20013;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#22914;&#21098;&#26525;&#36739;&#23567;&#24133;&#24230;&#26435;&#37325;&#21644;&#36845;&#20195;&#36807;&#31243;&#30340;&#20316;&#29992;&#65292;&#23578;&#32570;&#20047;&#23436;&#20840;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#23545;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#33719;&#24471;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20307;&#31215;/&#20960;&#20309;&#21644;&#25439;&#22833;&#26223;&#35266;&#29305;&#24449;&#36827;&#34892;&#32463;&#39564;&#30740;&#31350;&#65292;&#20197;&#27934;&#23519;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15022v1 Announce Type: new  Abstract: Lottery ticket hypothesis for deep neural networks emphasizes the importance of initialization used to re-train the sparser networks obtained using the iterative magnitude pruning process. An explanation for why the specific initialization proposed by the lottery ticket hypothesis tends to work better in terms of generalization (and training) performance has been lacking. Moreover, the underlying principles in iterative magnitude pruning, like the pruning of smaller magnitude weights and the role of the iterative process, lack full understanding and explanation. In this work, we attempt to provide insights into these phenomena by empirically studying the volume/geometry and loss landscape characteristics of the solutions obtained at various stages of the iterative magnitude pruning process.
&lt;/p&gt;</description></item><item><title>ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11795</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Cost Privacy-Aware Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11795
&lt;/p&gt;
&lt;p&gt;
ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;ZIP-DL&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#12290;&#36825;&#31181;&#25216;&#26415;&#30830;&#20445;&#20102;&#30001;&#20110;&#20854;&#30456;&#20851;&#24615;&#65292;&#22312;&#32858;&#21512;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#20960;&#20046;&#30456;&#20114;&#25269;&#28040;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;ZIP-DL&#19981;&#38656;&#35201;&#22810;&#27425;&#36890;&#20449;&#36718;&#36827;&#34892;&#22122;&#22768;&#25269;&#28040;&#65292;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#20449;&#24320;&#38144;&#20043;&#38388;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#20351;ZIP-DL&#21487;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;ZIP-DL&#22312;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#26435;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#32447;DL&#30456;&#27604;&#65292;ZIP-DL&#65288;i&#65289;&#23558;&#21487;&#36861;&#36394;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#38477;&#20302;&#20102;&#22810;&#36798;52&#20010;&#28857;&#65292;&#65288;ii&#65289;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;37&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.07095</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#24179;&#28369;&#20811;&#26381;&#35748;&#35777;&#22521;&#35757;&#30340;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Paradox of Certified Training with Gaussian Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07095
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#35748;&#35777;&#20934;&#30830;&#24230;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#23613;&#31649;&#35748;&#35777;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#36827;&#34892;&#30028;&#35745;&#31639;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#36739;&#26494;&#30340;&#26494;&#24347;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#36825;&#26159;&#30001;&#36825;&#20123;&#26356;&#32039;&#30340;&#26494;&#24347;&#23548;&#33268;&#30340;&#25439;&#22833;&#34920;&#38754;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#25200;&#21160;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;PGPE&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#35745;&#31639;&#24179;&#28369;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20984;&#25918;&#23485;&#26469;&#30830;&#35748;&#36825;&#19968;&#28857;&#12290;&#22312;&#20351;&#29992;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#30830;&#23454;&#23548;&#33268;&#26356;&#22909;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#30456;&#21516;&#32593;&#32476;&#19978;&#32988;&#36807;&#21516;&#31867;&#25216;&#26415;&#12290;&#23613;&#31649;&#25193;&#23637;&#22522;&#20110;PGPE&#30340;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07095v1 Announce Type: new  Abstract: Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challengin
&lt;/p&gt;</description></item><item><title>&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04666</link><description>&lt;p&gt;
&#30005;&#20449;&#35821;&#35328;&#27169;&#22411;&#65306;&#23427;&#20204;&#24517;&#39035;&#24222;&#22823;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Telecom Language Models: Must They Be Large?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04666
&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#23637;&#31034;&#20986;&#19982;&#22823;&#22411;&#23545;&#24212;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#25552;&#21319;&#20102;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#20449;&#37096;&#38376;&#23545;&#24222;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#20851;&#27880;&#20984;&#26174;&#20102;&#23427;&#20204;&#22312;&#25913;&#21464;&#36816;&#33829;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#20854;&#24040;&#22823;&#20307;&#31215;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#24433;&#21709;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#21487;&#34892;&#24615;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#20986;&#29616;&#20102;&#19968;&#25209;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#19982;&#20854;&#36739;&#22823;&#23545;&#24212;&#29289;&#30456;&#24403;&#65292;&#27604;&#22914;&#32534;&#30721;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;Phi-2&#26159;&#19968;&#31181;&#32039;&#20945;&#20294;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#20307;&#29616;&#20102;&#36825;&#19968;&#31995;&#21015;&#39640;&#25928;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#28010;&#28526;&#12290;&#26412;&#25991;&#23545;Phi-2&#22312;&#30005;&#20449;&#39046;&#22495;&#20869;&#22312;&#26412;&#36136;&#19978;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#37492;&#20110;&#35268;&#27169;&#30456;&#20851;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#65292;&#31934;&#24515;&#22686;&#24378;&#20102;Phi-2&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04666v1 Announce Type: new  Abstract: The increasing interest in Large Language Models (LLMs) within the telecommunications sector underscores their potential to revolutionize operational efficiency. However, the deployment of these sophisticated models is often hampered by their substantial size and computational demands, raising concerns about their viability in resource-constrained environments. Addressing this challenge, recent advancements have seen the emergence of small language models that surprisingly exhibit performance comparable to their larger counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a compact yet powerful model, exemplifies this new wave of efficient small language models. This paper conducts a comprehensive evaluation of Phi-2's intrinsic understanding of the telecommunications domain. Recognizing the scale-related limitations, we enhance Phi-2's capabilities through a Retrieval-Augmented Generation approach, meticulously i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.15422</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#19988;&#39640;&#36136;&#37327;&#30340;&#30149;&#20154;&#24635;&#32467;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#21644;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#33021;&#26377;&#25928;&#20943;&#23569;&#24187;&#35273;&#30340;&#29983;&#25104;&#65292;&#24182;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#32463;&#24120;&#38754;&#20020;&#38590;&#20197;&#29702;&#35299;&#20854;&#20303;&#38498;&#24773;&#20917;&#30340;&#22256;&#38590;&#65292;&#32780;&#21307;&#25252;&#20154;&#21592;&#36164;&#28304;&#26377;&#38480;&#20197;&#25552;&#20379;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#21307;&#29983;&#31508;&#35760;&#29983;&#25104;&#24739;&#32773;&#24635;&#32467;&#30340;&#28508;&#21147;&#65292;&#24182;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#29983;&#25104;&#24635;&#32467;&#30340;&#24544;&#23454;&#24615;&#21644;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20005;&#26684;&#30340;&#26631;&#35760;&#21327;&#35758;&#29992;&#20110;&#24187;&#35273;&#65292;&#35753;&#20004;&#20301;&#21307;&#23398;&#19987;&#23478;&#26631;&#35760;&#20102;100&#20010;&#30495;&#23454;&#24635;&#32467;&#21644;100&#20010;&#29983;&#25104;&#30340;&#24635;&#32467;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#24187;&#35273;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;Llama 2&#27599;&#20010;&#24635;&#32467;&#30340;&#24187;&#35273;&#20174;2.60&#38477;&#20302;&#21040;1.55&#65292;&#21516;&#26102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#12290;&#34429;&#28982;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#65292;&#20294;&#24403;&#20351;&#29992;&#20116;&#20010;&#20363;&#23376;&#25552;&#31034;GPT-4&#26102;&#65292;&#35813;&#25928;&#26524;&#35201;&#23567;&#24471;&#22810;&#65288;0.70&#38477;&#33267;0.40&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;&#26080;&#24187;&#35273;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;&#21363;&#20351;&#22312;&#24187;&#35273;&#33258;&#30001;&#25968;&#25454;&#19979;&#65292;GPT-4&#20063;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15422v1 Announce Type: cross  Abstract: Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we develop a rigorous labeling protocol for hallucinations, and have two medical experts annotate 100 real-world summaries and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. Although the effect is still present, it is much smaller for GPT-4 when prompted with five examples (0.70 to 0.40). We also conduct a qualitative evaluation using hallucination-free and improved training data. GPT-4 shows very good results even in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14169</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27880;&#24847;&#21147;&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
A Temporal Bias Correction using a Machine Learning Attention model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35270;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27010;&#29575;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#36866;&#37197;&#20559;&#24046;&#26657;&#27491;&#20219;&#21153;&#65292;&#21487;&#20934;&#30830;&#26657;&#27491;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65292;&#25552;&#39640;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#27169;&#22411;&#22312;&#19982;&#30495;&#23454;&#19990;&#30028;&#35266;&#27979;&#25968;&#25454;&#30456;&#27604;&#23384;&#22312;&#20559;&#24046;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#24433;&#21709;&#30740;&#31350;&#20043;&#21069;&#36827;&#34892;&#26657;&#20934;&#12290;&#20351;&#26657;&#20934;&#25104;&#20026;&#21487;&#33021;&#30340;&#32479;&#35745;&#26041;&#27861;&#38598;&#21512;&#34987;&#31216;&#20026;&#20559;&#24046;&#26657;&#27491;&#65288;BC&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;BC&#26041;&#27861;&#22312;&#35843;&#25972;&#26102;&#38388;&#20559;&#24046;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#38271;&#26399;&#26102;&#38388;&#23646;&#24615;&#30340;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#65288;&#22914;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#29575;&#65289;&#26080;&#27861;&#20934;&#30830;&#26657;&#27491;&#65292;&#36825;&#20351;&#24471;&#22312;&#36825;&#20123;&#27668;&#20505;&#32479;&#35745;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#38752;&#24433;&#21709;&#30740;&#31350;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BC&#26041;&#27861;&#26469;&#26657;&#27491;&#26102;&#38388;&#20559;&#24046;&#12290;&#36825;&#24471;&#30410;&#20110;&#23558;BC&#37325;&#26032;&#26500;&#24819;&#20026;&#27010;&#29575;&#27169;&#22411;&#32780;&#19981;&#26159;&#31639;&#27861;&#27969;&#31243;&#65292;&#24182;&#23558;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27010;&#29575;&#20851;&#27880;&#27169;&#22411;&#35843;&#25972;&#21040;BC&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#23612;&#26085;&#21033;&#20122;&#38463;&#24067;&#36158;&#30340;&#28909;&#28010;&#25345;&#32493;&#26102;&#38388;&#32479;&#35745;&#26696;&#20363;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14169v1 Announce Type: new  Abstract: Climate models are biased with respect to real world observations and usually need to be calibrated prior to impact studies. The suite of statistical methods that enable such calibrations is called bias correction (BC). However, current BC methods struggle to adjust for temporal biases, because they disregard the dependence between consecutive time-points. As a result, climate statistics with long-range temporal properties, such as heatwave duration and frequency, cannot be corrected accurately, making it more difficult to produce reliable impact studies on such climate statistics. In this paper, we offer a novel BC methodology to correct for temporal biases. This is made possible by i) re-thinking BC as a probability model rather than an algorithmic procedure, and ii) adapting state-of-the-art machine-learning (ML) probabilistic attention models to fit the BC task. With a case study of heatwave duration statistics in Abuja, Nigeria, and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.14103</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Computational-Statistical Gaps for Improper Learning in Sparse Linear Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#38382;&#39064;&#65292;&#20026;&#20102;&#39640;&#25928;&#22320;&#25214;&#21040;&#21487;&#20197;&#22312;&#26679;&#26412;&#19978;&#23454;&#29616;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#65292;&#38656;&#35201;&#33267;&#23569; $\Omega(k \log (d/k))$ &#20010;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#20013;&#19981;&#24403;&#23398;&#20064;&#30340;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#26469;&#33258;&#32500;&#24230;&#20026; $d$ &#30340; $k$-&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#30340; $n$ &#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35810;&#38382;&#20102;&#22312;&#26102;&#38388;&#22810;&#39033;&#24335;&#20013;&#30340;&#26368;&#23567;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25214;&#21040;&#19968;&#20010;&#23545;&#36825; $n$ &#20010;&#26679;&#26412;&#36798;&#21040;&#38750;&#24179;&#20961;&#39044;&#27979;&#35823;&#24046;&#30340;&#28508;&#22312;&#23494;&#38598;&#20272;&#35745;&#30340;&#22238;&#24402;&#21521;&#37327;&#12290;&#20449;&#24687;&#29702;&#35770;&#19978;&#65292;&#36825;&#21487;&#20197;&#29992; $\Theta(k \log (d/k))$ &#20010;&#26679;&#26412;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#24456;&#26174;&#33879;&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#22312;&#19981;&#38468;&#21152;&#23545;&#27169;&#22411;&#30340;&#20854;&#20182;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#23569;&#20110; $\Theta(d)$ &#20010;&#26679;&#26412;&#36798;&#21040;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#31867;&#20284;&#22320;&#65292;&#29616;&#26377;&#30340;&#22256;&#38590;&#32467;&#26524;&#35201;&#20040;&#20165;&#38480;&#20110;&#36866;&#24403;&#35774;&#32622;&#65292;&#22312;&#35813;&#35774;&#32622;&#20013;&#20272;&#35745;&#20540;&#20063;&#24517;&#39035;&#26159;&#31232;&#30095;&#30340;&#65292;&#35201;&#20040;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14103v1 Announce Type: new  Abstract: We study computational-statistical gaps for improper learning in sparse linear regression. More specifically, given $n$ samples from a $k$-sparse linear model in dimension $d$, we ask what is the minimum sample complexity to efficiently (in time polynomial in $d$, $k$, and $n$) find a potentially dense estimate for the regression vector that achieves non-trivial prediction error on the $n$ samples. Information-theoretically this can be achieved using $\Theta(k \log (d/k))$ samples. Yet, despite its prominence in the literature, there is no polynomial-time algorithm known to achieve the same guarantees using less than $\Theta(d)$ samples without additional restrictions on the model. Similarly, existing hardness results are either restricted to the proper setting, in which the estimate must be sparse as well, or only apply to specific algorithms.   We give evidence that efficient algorithms for this task require at least (roughly) $\Omega(
&lt;/p&gt;</description></item><item><title>EvolMPNN&#36890;&#36807;&#36827;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#25429;&#25417;&#34507;&#30333;&#36136;&#31361;&#21464;&#23545;&#20110;&#38170;&#23450;&#34507;&#30333;&#36136;&#30340;&#24433;&#21709;&#65292;&#24182;&#26368;&#32456;&#29983;&#25104;&#32508;&#21512;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.13418</link><description>&lt;p&gt;
EvolMPNN&#65306;&#36890;&#36807;&#36827;&#21270;&#32534;&#30721;&#39044;&#27979;&#21516;&#28304;&#34507;&#30333;&#36136;&#30340;&#31361;&#21464;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13418
&lt;/p&gt;
&lt;p&gt;
EvolMPNN&#36890;&#36807;&#36827;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#25429;&#25417;&#34507;&#30333;&#36136;&#31361;&#21464;&#23545;&#20110;&#38170;&#23450;&#34507;&#30333;&#36136;&#30340;&#24433;&#21709;&#65292;&#24182;&#26368;&#32456;&#29983;&#25104;&#32508;&#21512;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34507;&#30333;&#36136;&#23646;&#24615;&#23545;&#29983;&#29289;&#21644;&#21307;&#23398;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#34507;&#30333;&#24037;&#31243;&#36890;&#36807;&#23545;&#20856;&#22411;&#34507;&#30333;&#36136;&#65288;&#31216;&#20026;&#37326;&#29983;&#22411;&#65289;&#36827;&#34892;&#31361;&#21464;&#65292;&#26500;&#24314;&#21516;&#28304;&#34507;&#30333;&#36136;&#23478;&#26063;&#24182;&#30740;&#31350;&#20854;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#23481;&#26131;&#24573;&#30053;&#32454;&#24494;&#30340;&#31361;&#21464;&#65292;&#26080;&#27861;&#25429;&#25417;&#34507;&#30333;&#36136;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvolMPNN&#65292;&#19968;&#31181;&#20855;&#26377;&#36827;&#21270;&#24863;&#30693;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#36827;&#21270;&#24863;&#30693;&#30340;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13418v1 Announce Type: new  Abstract: Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than sta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.13414</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#20107;&#21518;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models as Post-hoc Correctors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24182;&#38656;&#27714;&#26356;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#20851;&#30340;&#36153;&#29992;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#25104;&#23601;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#20197;&#26497;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#25913;&#21892;ML&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#20449;&#24687;&#21644;ML&#27169;&#22411;&#23545;&#39564;&#35777;&#38598;&#30340;&#39044;&#27979;&#26469;&#24418;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#24635;&#32467;ML&#27169;&#22411;&#29359;&#38169;&#35823;&#30340;&#23454;&#20363;&#20197;&#21450;&#20027;&#35201;&#39044;&#27979;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;LLM&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13414v1 Announce Type: cross  Abstract: As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can tr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;</title><link>https://arxiv.org/abs/2402.12479</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20462;&#21098;&#32593;&#32476;&#26159;&#19968;&#20010;&#22909;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
In deep reinforcement learning, a pruned network is a good network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#28176;&#21098;&#26525;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#65292;&#20174;&#32780;&#20135;&#29983;&#27604;&#20256;&#32479;&#32593;&#32476;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#26377;&#25928;&#21033;&#29992;&#20854;&#32593;&#32476;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#31232;&#30095;&#35757;&#32451;&#25216;&#26415;&#20248;&#21183;&#30340;&#20808;&#21069;&#35265;&#35299;&#65292;&#24182;&#35777;&#26126;&#36880;&#28176;&#21098;&#26525;&#20351;&#20195;&#29702;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#21442;&#25968;&#25928;&#33021;&#12290;&#36825;&#23548;&#33268;&#32593;&#32476;&#27604;&#20256;&#32479;&#32593;&#32476;&#20135;&#29983;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#34920;&#29616;&#20986;&#19968;&#31181;&#8220;&#32553;&#25918;&#23450;&#24459;&#8221;&#65292;&#20165;&#20351;&#29992;&#23436;&#25972;&#32593;&#32476;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12479v1 Announce Type: cross  Abstract: Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of "scaling law", using only a small fraction of the full network parameters.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#36793;&#30028;&#27010;&#24565;&#21644;&#21033;&#29992;STL&#23646;&#24615;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12397</link><description>&lt;p&gt;
&#22810;&#31867;&#21035;&#26102;&#38388;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-class Temporal Logic Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#65292;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#24341;&#20837;&#36793;&#30028;&#27010;&#24565;&#21644;&#21033;&#29992;STL&#23646;&#24615;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21487;&#20197;&#20195;&#34920;&#26080;&#20154;&#31995;&#32479;&#65288;&#22914;&#26080;&#20154;&#26426;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65289;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20998;&#31867;&#25968;&#25454;&#30340;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#22312;&#20174;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#23450;&#26102;&#34892;&#20026;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25152;&#26377;&#36825;&#20123;&#20803;&#32032;&#32467;&#21512;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#34920;&#31034;STL&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;1&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#36793;&#30028;&#27010;&#24565;&#65292;2&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;STL&#30340;&#23646;&#24615;&#26469;&#22686;&#24378;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12397v1 Announce Type: cross  Abstract: Time-series data can represent the behaviors of autonomous systems, such as drones and self-driving cars. The problem of binary and multi-class classification has received a lot of attention in this field. Neural networks represent a popular approach to classifying data; However, they lack interpretability, which poses a significant challenge in extracting meaningful information from them. Signal Temporal Logic (STL) is a formalism to describe the properties of timed behaviors. We propose a method that combines all of the above: neural networks that represent STL specifications for multi-class classification of time-series data. We offer two key contributions: 1) We introduce a notion of margin for multi-class classification, and 2) we introduce the use of STL-based attributes for enhancing the interpretability of the results. We evaluate our method on two datasets and compare with state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;</title><link>https://arxiv.org/abs/2402.10963</link><description>&lt;p&gt;
GLoRe: &#20309;&#26102;&#12289;&#20309;&#22320;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#25913;&#36827;&#26469;&#25552;&#39640;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#31185;&#23398;&#25110;&#32534;&#30721;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#25913;&#36827;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#24456;&#38590;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#25913;&#36827;&#12290;&#22522;&#20110;&#32467;&#26524;&#30340;&#22870;&#21169;&#27169;&#22411;(ORMs)&#65292;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25351;&#31034;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#65292;&#20026;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#20415;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#22870;&#21169;&#27169;&#22411;(PRMs)&#21463;&#36807;&#35757;&#32451;&#65292;&#29992;&#20197;&#39044;&#27979;&#20013;&#38388;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#25351;&#31034;&#20309;&#22788;&#36827;&#34892;&#25913;&#36827;&#12290;&#20294;&#23427;&#20204;&#24456;&#26114;&#36149;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;ORMs(SORMs)&#65292;&#23427;&#20204;&#21482;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#25110;$V^{\star}$&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;SORMs&#21463;&#35757;&#32451;&#26469;&#39044;&#27979;&#24403;&#21462;&#26679;&#26102;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
&lt;/p&gt;</description></item><item><title>DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.09910</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#26816;&#27979;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65306;DE-COP
&lt;/p&gt;
&lt;p&gt;
DE-COP: Detecting Copyrighted Content in Language Models Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09910
&lt;/p&gt;
&lt;p&gt;
DE-COP&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#29256;&#26435;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25506;&#27979;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27169;&#22411;&#35757;&#32451;&#25991;&#26412;&#20013;&#21487;&#33021;&#21253;&#21547;&#30340;&#29256;&#26435;&#20869;&#23481;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;9.6%&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;72%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32771;&#34385;&#21040;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#20445;&#23494;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22914;&#20309;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#20351;&#29992;&#20102;&#29256;&#26435;&#20869;&#23481;&#65311;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#22522;&#20110;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#24456;&#21487;&#33021;&#33021;&#22815;&#35782;&#21035;&#20986;&#20854;&#35757;&#32451;&#25991;&#26412;&#20013;&#30340;&#29420;&#25991;&#25688;&#24405;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DE-COP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#22312;&#35757;&#32451;&#20013;&#21253;&#21547;&#20102;&#19968;&#27573;&#29256;&#26435;&#20869;&#23481;&#12290;DE-COP&#30340;&#26680;&#24515;&#26041;&#27861;&#26159;&#36890;&#36807;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25506;&#27979;&#65292;&#36873;&#25321;&#39033;&#21253;&#25324;&#29420;&#25991;&#26412;&#21644;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;BookTection&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22312;&#27169;&#22411;&#35757;&#32451;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#21644;&#20043;&#21518;&#20986;&#29256;&#30340;165&#26412;&#20070;&#30340;&#25688;&#24405;&#20197;&#21450;&#23427;&#20204;&#30340;&#37322;&#20041;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DE-COP&#22312;&#27169;&#22411;&#30340;&#36923;&#36753;&#21487;&#29992;&#26102;&#65292;&#26816;&#27979;&#24615;&#33021;&#65288;AUC&#65289;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20339;&#26041;&#27861;9.6%&#12290;&#27492;&#22806;&#65292;DE-COP&#22312;&#23436;&#20840;&#40657;&#30418;&#27169;&#22411;&#19978;&#26816;&#27979;&#21487;&#30097;&#20070;&#31821;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#36798;&#21040;72%&#65292;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#21482;&#26377;$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09910v1 Announce Type: new  Abstract: How can we detect if copyrighted content was used in the training process of a language model, considering that the training data is typically undisclosed? We are motivated by the premise that a language model is likely to identify verbatim excerpts from its training text. We propose DE-COP, a method to determine whether a piece of copyrighted content was included in training. DE-COP's core approach is to probe an LLM with multiple-choice questions, whose options include both verbatim text and their paraphrases. We construct BookTection, a benchmark with excerpts from 165 books published prior and subsequent to a model's training cutoff, along with their paraphrases. Our experiments show that DE-COP surpasses the prior best method by 9.6% in detection performance (AUC) on models with logits available. Moreover, DE-COP also achieves an average accuracy of 72% for detecting suspect books on fully black-box models where prior methods give $
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02111</link><description>&lt;p&gt;
&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#65306;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#23601;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21152;&#36895;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#21069;&#30651;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#22312;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#33945;&#29305;&#21345;&#27931;(MLMC)&#26469;&#25552;&#39640;&#28041;&#21450;&#23884;&#22871;&#26399;&#26395;&#21644;&#26368;&#22823;&#21270;&#30340;&#22810;&#27493;&#21069;&#30651;&#36125;&#21494;&#26031;&#20248;&#21270;(BO)&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26222;&#36890;&#33945;&#29305;&#21345;&#27931;&#30340;&#22797;&#26434;&#24230;&#22312;&#23884;&#22871;&#25805;&#20316;&#20013;&#20250;&#38477;&#20302;&#65292;&#32780;MLMC&#33021;&#22815;&#20197;&#35268;&#33539;&#33945;&#29305;&#21345;&#27931;&#25910;&#25947;&#36895;&#24230;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;&#21644;&#24179;&#28369;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#19968;&#27493;&#21644;&#20004;&#27493;&#21069;&#30651;&#37319;&#38598;&#20989;&#25968;&#30340;&#36817;&#20284;&#25913;&#36827;&#65292;&#20294;&#27491;&#22914;&#25105;&#20204;&#25152;&#35752;&#35770;&#30340;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#31181;&#26041;&#38754;&#26159;&#21487;&#25512;&#24191;&#30340;&#65292;&#21253;&#25324;&#36229;&#36234;BO&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;MLMC&#22312;BO&#20013;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#22312;&#36825;&#37324;&#33719;&#21462;&#65306;https://github.com/Shangda-Yang/MLMCBO&#12290;
&lt;/p&gt;
&lt;p&gt;
We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. The complexity rate of naive Monte Carlo degrades for nested operations, whereas MLMC is capable of achieving the canonical Monte Carlo convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for one- and two-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available here https://github.com/Shangda-Yang/MLMCBO.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#21644;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#65292;&#24182;&#32771;&#34385;&#20102;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.00299</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00299
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21160;&#24577;&#22810;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#21644;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#65292;&#24182;&#32771;&#34385;&#20102;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20449;&#29992;&#35780;&#20998;&#20542;&#21521;&#20110;&#20165;&#20351;&#29992;&#20010;&#20307;&#20511;&#27454;&#20154;&#25110;&#36151;&#27454;&#32423;&#21035;&#30340;&#39044;&#27979;&#22240;&#32032;&#65292;&#28982;&#32780;&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#20511;&#27454;&#20154;&#20043;&#38388;&#30340;&#20851;&#32852;&#21487;&#33021;&#20250;&#23548;&#33268;&#39118;&#38505;&#22312;&#32593;&#32476;&#19978;&#30340;&#20256;&#25773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30001;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#21160;&#24577;&#22810;&#23618;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#21453;&#26144;&#20102;&#19981;&#21516;&#26469;&#28304;&#30340;&#32593;&#32476;&#36830;&#25509;&#12290;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#25269;&#25276;&#36151;&#27454;&#20844;&#21496;Freddie Mac&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#34892;&#20026;&#20449;&#29992;&#35780;&#20998;&#30340;&#32972;&#26223;&#19979;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19981;&#21516;&#31867;&#22411;&#30340;&#36830;&#25509;&#28304;&#33258;&#20511;&#27454;&#20154;&#30340;&#22320;&#29702;&#20301;&#32622;&#21644;&#20182;&#20204;&#36873;&#25321;&#30340;&#25269;&#25276;&#36151;&#27454;&#25552;&#20379;&#21830;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#20004;&#31181;&#36830;&#25509;&#20197;&#21450;&#36825;&#20123;&#36830;&#25509;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#33258;&#23450;&#20041;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#37325;&#35201;&#24615;&#23545;&#19981;&#21516;&#30340;&#26102;&#38388;&#24555;&#29031;&#36827;&#34892;&#21152;&#26435;&#12290;&#32463;&#36807;&#22810;&#27425;&#37197;&#32622;&#27979;&#35797;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2401.17802</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#23545;&#27604;&#23398;&#20064;&#30340;&#33976;&#39311;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distillation Enhanced Time Series Forecasting Network with Momentum Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#35774;&#35745;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#21644;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#33719;&#24471;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#22122;&#22768;&#12289;&#19981;&#23436;&#25972;&#24615;&#20197;&#21450;&#30417;&#30563;&#20449;&#21495;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#36890;&#24120;&#32858;&#28966;&#20110;&#26102;&#38388;&#20869;&#37096;&#29305;&#24449;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DE-TSMCL&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#29992;&#20110;&#38271;&#24207;&#21015;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33976;&#39311;&#22686;&#24378;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26159;&#21542;&#23631;&#34109;&#26102;&#38388;&#25139;&#20197;&#33719;&#24471;&#20248;&#21270;&#30340;&#23376;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21160;&#37327;&#26356;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#25506;&#32034;&#26102;&#38388;&#24207;&#21015;&#30340;&#26679;&#26412;&#38388;&#21644;&#26102;&#38388;&#20869;&#37096;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23398;&#20064;&#26410;&#26631;&#35760;&#26102;&#38388;&#24207;&#21015;&#30340;&#28508;&#22312;&#32467;&#26500;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30417;&#30563;&#20219;&#21153;&#65292;&#20197;&#23398;&#20064;&#26356;&#40065;&#26834;&#30340;&#34920;&#31034;&#24182;&#20419;&#36827;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#19978;&#36848;&#20004;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive representation learning is crucial in time series analysis as it alleviates the issue of data noise and incompleteness as well as sparsity of supervision signal. However, existing constrastive learning frameworks usually focus on intral-temporal features, which fails to fully exploit the intricate nature of time series data. To address this issue, we propose DE-TSMCL, an innovative distillation enhanced framework for long sequence time series forecasting. Specifically, we design a learnable data augmentation mechanism which adaptively learns whether to mask a timestamp to obtain optimized sub-sequences. Then, we propose a contrastive learning task with momentum update to explore inter-sample and intra-temporal correlations of time series to learn the underlying structure feature on the unlabeled time series. Meanwhile, we design a supervised task to learn more robust representations and facilitate the contrastive learning process. Finally, we jointly optimize the above two 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;Random Forest&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16843</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating ML-Based Anomaly Detection Across Datasets of Varied Integrity: A Case Study. (arXiv:2401.16843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;Random Forest&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#24322;&#24120;&#26816;&#27979;&#20013;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#32463;&#36807;&#25913;&#36827;&#30340;CICIDS-2017&#25968;&#25454;&#38598;&#30340;&#29256;&#26412;&#65292;NFS-2023-nTE&#21644;NFS-2023-TE&#65292;&#21033;&#29992;NFStream&#36827;&#34892;&#26041;&#27861;&#19978;&#21512;&#29702;&#30340;&#27969;&#37327;&#21040;&#26399;&#21644;&#26631;&#35760;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#27604;&#20102;Random Forest&#65288;RF&#65289;&#31639;&#27861;&#22312;&#21407;&#22987;&#30340;CICIDS-2017&#25968;&#25454;&#38598;&#12289;&#20854;&#25913;&#36827;&#29256;&#26412;WTMC-2021&#21644;CRiSIS-2022&#20197;&#21450;&#25105;&#20204;&#22522;&#20110;NFStream&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;RF&#27169;&#22411;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#31283;&#20581;&#24615;&#65292;&#26080;&#35770;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#22914;&#20309;&#65292;&#22312;&#24615;&#33021;&#25351;&#26631;&#19978;&#37117;&#33021;&#20445;&#25345;&#19968;&#33268;&#30340;&#39640;&#27700;&#24179;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23454;&#38469;&#24433;&#21709;&#30340;&#37325;&#35201;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14555</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revisiting Active Learning in the Era of Vision Foundation Models. (arXiv:2401.14555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#23545;&#26377;&#25928;&#20027;&#21160;&#23398;&#20064;&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#24179;&#34913;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#35270;&#35273;&#25110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25110;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#23398;&#20064;&#21040;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26631;&#27880;&#25110;&#23569;&#26631;&#27880;&#24615;&#33021;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#37492;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;&#23427;&#20204;&#26159;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#26088;&#22312;&#23454;&#29616;&#26631;&#35760;&#25928;&#29575;&#30340;&#26368;&#22823;&#21270;&#65292;&#20294;&#22312;&#20302;&#39044;&#31639;&#26465;&#20214;&#19979;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#20840;&#37096;&#28508;&#21147;&#22312;AL&#29615;&#22659;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#30784;&#27169;&#22411;&#23545;&#26377;&#25928;AL&#30340;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#65292;&#21363;1&#65289;&#21021;&#22987;&#26631;&#35760;&#26679;&#26412;&#27744;&#30340;&#36873;&#25321;&#65292;2&#65289;&#30830;&#20445;&#22810;&#26679;&#24615;&#25277;&#26679;&#65292;&#20197;&#21450;3&#65289;&#20195;&#34920;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25277;&#26679;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#65288;DINOv2&#12289;OpenCLIP&#65289;&#30340;&#40065;&#26834;&#34920;&#31034;&#22914;&#20309;&#25361;&#25112;&#24050;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#20026;&#19968;&#20010;&#26032;&#30340;&#31616;&#21333;&#20248;&#38597;&#30340;AL&#31574;&#30053;&#30340;&#26377;&#21407;&#21017;&#26500;&#24314;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;dropout&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation vision or vision-language models are trained on large unlabeled or noisy data and learn robust representations that can achieve impressive zeroor few-shot performance on diverse tasks. Given these properties, they are a natural fit for active learning (AL), which aims to maximize labeling efficiency, but the full potential of foundation models has not been explored in the context of AL, specifically in the low-budget regime. In this work, we evaluate how foundation models influence three critical components of effective AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling, and 3) the trade-off between representative and uncertainty sampling. We systematically study how the robust representations of foundation models (DINOv2, OpenCLIP) challenge existing findings in active learning. Our observations inform the principled construction of a new simple and elegant AL strategy that balances uncertainty estimated via dropout with sample diversity. We exten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.10748</link><description>&lt;p&gt;
&#38024;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast gradient-free activation maximization for neurons in spiking neural networks. (arXiv:2401.10748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#26080;&#26799;&#24230;&#28608;&#27963;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#29305;&#21270;&#12290;&#22312;&#19968;&#20010;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#19978;&#25104;&#21151;&#27979;&#35797;&#20102;&#36825;&#20010;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#65292;&#37117;&#26159;&#30001;&#31070;&#32463;&#20803;&#26500;&#25104;&#30340;&#22797;&#26434;&#31995;&#32479;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#37117;&#26377;&#33258;&#24049;&#30340;&#19987;&#19994;&#21270;&#12290;&#25581;&#31034;&#36825;&#20123;&#19987;&#19994;&#21270;&#23545;&#20110;&#29702;&#35299;NNs&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#19968;&#20010;&#29983;&#29289;&#31995;&#32479;&#65292;&#20854;&#23545;&#21050;&#28608;&#30340;&#31070;&#32463;&#21709;&#24212;&#19981;&#26159;&#24050;&#30693;&#30340;&#65288;&#26356;&#19981;&#29992;&#35828;&#26159;&#21487;&#24494;&#20998;&#30340;&#20989;&#25968;&#65289;&#65292;&#21807;&#19968;&#30340;&#26041;&#24335;&#26159;&#24314;&#31435;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#23558;&#20854;&#26292;&#38706;&#20110;&#21050;&#28608;&#20043;&#20013;&#65292;&#20854;&#24615;&#36136;&#21487;&#20197;&#36845;&#20195;&#22320;&#21464;&#21270;&#65292;&#20197;&#23547;&#27714;&#26368;&#22823;&#21709;&#24212;&#30340;&#26041;&#21521;&#12290;&#20026;&#20102;&#22312;&#19968;&#20010;&#29983;&#29289;&#32593;&#32476;&#19978;&#27979;&#35797;&#36825;&#26679;&#30340;&#24490;&#29615;&#65292;&#39318;&#20808;&#38656;&#35201;&#23398;&#20250;&#24555;&#36895;&#21644;&#39640;&#25928;&#22320;&#36816;&#34892;&#23427;&#65292;&#20197;&#22312;&#26368;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#36798;&#21040;&#26368;&#26377;&#25928;&#30340;&#21050;&#28608;&#65288;&#26368;&#22823;&#21270;&#26576;&#20123;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#26377;&#25928;&#35774;&#35745;&#30340;&#26694;&#26550;&#65292;&#25104;&#21151;&#22320;&#22312;&#20154;&#24037;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65292;&#27169;&#25311;&#29983;&#29289;&#22823;&#33041;NNs&#34892;&#20026;&#30340;&#27169;&#22411;&#65289;&#19978;&#27979;&#35797;&#20102;&#23427;&#12290;&#25105;&#20204;&#29992;&#20110;&#28608;&#27963;&#26368;&#22823;&#21270;&#65288;AM&#65289;&#30340;&#20248;&#21270;&#26041;&#27861;&#26159;&#22522;&#20110;&#24555;&#26799;&#24230;&#26041;&#27861;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs), both living and artificial, work due to being complex systems of neurons, each having its own specialization. Revealing these specializations is important for understanding NNs inner working mechanisms. The only way to do this for a living system, the neural response of which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop of exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction of maximal response. To test such a loop on a living network, one should first learn how to run it quickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons activation) in least possible number of iterations. We present a framework with an effective design of such a loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the behaviour of NNs in living brains). Our optimization method used for activation maximization (AM) was ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10710</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification with neural networks with quadratic decision functions. (arXiv:2401.10710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20855;&#26377;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20351;&#29992;&#20108;&#27425;&#20915;&#31574;&#20989;&#25968;&#20316;&#20026;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#21697;&#26469;&#23454;&#29616;&#19968;&#31181;&#20248;&#21183;&#65292;&#24403;&#38656;&#35201;&#35782;&#21035;&#30340;&#23545;&#35937;&#20855;&#26377;&#32039;&#20945;&#22522;&#26412;&#20960;&#20309;&#24418;&#29366;&#65288;&#22914;&#22278;&#24418;&#12289;&#26925;&#22278;&#24418;&#31561;&#65289;&#26102;&#65292;&#36825;&#31181;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#31867;&#38382;&#39064;&#19978;&#20351;&#29992;&#36825;&#31181;&#20551;&#35774;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#21644;&#27604;&#36739;&#20102;&#35813;&#31639;&#27861;&#22312;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#20122;&#31181;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;Tensorflow&#21644;Keras&#36719;&#20214;&#20013;&#21487;&#20197;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear one. They are advantageous when the objects to be identified are of compact basic geometries like circles, ellipsis etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.
&lt;/p&gt;</description></item><item><title>MambaTab&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#29305;&#24615;&#65292;&#22312;&#23569;&#37327;&#21442;&#25968;&#21644;&#26368;&#23567;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08867</link><description>&lt;p&gt;
MambaTab&#65306;&#19968;&#31181;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MambaTab: A Simple Yet Effective Approach for Handling Tabular Data. (arXiv:2401.08867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08867
&lt;/p&gt;
&lt;p&gt;
MambaTab&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#29305;&#24615;&#65292;&#22312;&#23569;&#37327;&#21442;&#25968;&#21644;&#26368;&#23567;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;&#26368;&#20808;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#34920;&#26684;&#25968;&#25454;&#20173;&#28982;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;transformers&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#35843;&#20248;&#21644;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;MambaTab&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#12290;SSM&#23545;&#20110;&#20174;&#20855;&#26377;&#38271;&#31243;&#20381;&#36182;&#30340;&#25968;&#25454;&#20013;&#39640;&#25928;&#25552;&#21462;&#26377;&#25928;&#34920;&#31034;&#20855;&#26377;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;MambaTab&#21033;&#29992;&#20102;Mamba&#65292;&#19968;&#31181;&#26032;&#20852;&#30340;SSM&#21464;&#20307;&#65292;&#22312;&#34920;&#26684;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;MambaTab&#22312;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#39564;&#35777;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#36739;&#23569;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;MambaTab&#30340;&#25928;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#12289;&#21363;&#24320;&#21363;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data remains ubiquitous across domains despite growing use of images and texts for machine learning. While deep learning models like convolutional neural networks and transformers achieve strong performance on tabular data, they require extensive data preprocessing, tuning, and resources, limiting accessibility and scalability. This work develops an innovative approach based on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have strong capabilities for efficiently extracting effective representations from data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM variant, for end-to-end supervised learning on tables. Compared to state-of-the-art baselines, MambaTab delivers superior performance while requiring significantly fewer parameters and minimal preprocessing, as empirically validated on diverse benchmark datasets. MambaTab's efficiency, scalability, generalizability, and predictive gains signify it as a lightweight, "out-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04364</link><description>&lt;p&gt;
SoK&#65306;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#36805;&#36895;&#25104;&#20026;&#23545;&#31038;&#20250;&#26500;&#25104;&#28145;&#36828;&#21644;&#20005;&#37325;&#23041;&#32961;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#26131;&#20110;&#21046;&#20316;&#21644;&#20256;&#25773;&#12290;&#36825;&#31181;&#24773;&#20917;&#21152;&#36895;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#39564;&#35777;&#26102; heavily &#20381;&#36182;&#23454;&#39564;&#23460;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35753;&#23427;&#20204;&#24212;&#23545;&#26032;&#39062;&#12289;&#26032;&#20852;&#21644;&#23454;&#38469;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#24191;&#27867;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#26681;&#25454;&#20960;&#20010;&#20851;&#38190;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#26631;&#20934;&#23558;&#36825;&#20123;&#26816;&#27979;&#22120;&#20998;&#20026; 4 &#20010;&#39640;&#32423;&#32452;&#21035;&#21644; 13 &#20010;&#32454;&#31890;&#24230;&#23376;&#32452;&#21035;&#65292;&#37117;&#36981;&#24490;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#27010;&#24565;&#26694;&#26550;&#12290;&#36825;&#31181;&#20998;&#31867;&#21644;&#26694;&#26550;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#26816;&#27979;&#22120;&#21151;&#25928;&#30340;&#22240;&#32032;&#30340;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#23545; 16 &#20010;&#20027;&#35201;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.02736</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#24179;&#28369;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65306;MaxPool&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the numerical reliability of nonsmooth autodiff: a MaxPool case study. (arXiv:2401.02736v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#21487;&#38752;&#24615;&#65292;&#24182;&#21457;&#29616;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;AD&#20960;&#20046;&#22312;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65292;&#38656;&#35201;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#36873;&#25321;&#30340;&#38750;&#24179;&#28369;MaxPool&#38597;&#21487;&#27604;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20998;&#27495;&#21306;&#21644;&#34917;&#20607;&#21306;&#20004;&#20010;&#21487;&#33021;&#23548;&#33268;AD&#25968;&#20540;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28041;&#21450;&#38750;&#24179;&#28369;MaxPool&#25805;&#20316;&#30340;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#24494;&#20998;&#65288;AD&#65289;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#31934;&#24230;&#32423;&#21035;&#65288;16&#20301;&#12289;32&#20301;&#12289;64&#20301;&#65289;&#21644;&#21367;&#31215;&#26550;&#26500;&#65288;LeNet&#12289;VGG&#21644;ResNet&#65289;&#20197;&#21450;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;MNIST&#12289;CIFAR10&#12289;SVHN&#21644;ImageNet&#65289;&#19978;&#30340;AD&#34892;&#20026;&#12290;&#23613;&#31649;AD&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#22312;&#20960;&#20046;&#27599;&#20010;&#22320;&#26041;&#37117;&#19982;&#23548;&#25968;&#30456;&#31526;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#38750;&#24179;&#28369;&#25805;&#20316;&#65288;&#22914;MaxPool&#21644;ReLU&#65289;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;AD&#20351;&#29992;&#30340;&#26159;&#28014;&#28857;&#25968;&#65288;&#32780;&#19981;&#26159;&#23454;&#25968;&#65289;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;AD&#21487;&#33021;&#22312;&#25968;&#20540;&#19978;&#19981;&#27491;&#30830;&#30340;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#21253;&#25324;&#20998;&#27495;&#21306;&#65288;AD&#22312;&#23454;&#25968;&#19978;&#19981;&#27491;&#30830;&#65289;&#21644;&#34917;&#20607;&#21306;&#65288;AD&#22312;&#28014;&#28857;&#25968;&#19978;&#19981;&#27491;&#30830;&#20294;&#22312;&#23454;&#25968;&#19978;&#27491;&#30830;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;SGD&#36827;&#34892;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#30740;&#31350;&#20102;MaxPool&#38750;&#24179;&#28369;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#19981;&#21516;&#36873;&#25321;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.02721</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;ODE&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Cost-Efficient FPGA Implementation of Tiny Transformer Model using Neural ODE. (arXiv:2401.02721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;ODE&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#30340;&#39640;&#24615;&#20215;&#27604;FPGA&#23454;&#29616;&#24494;&#22411;Transformer&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#19988;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#36866;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#20855;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#26032;&#20852;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;CNN&#21644;RNN&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#34429;&#28982;&#27880;&#24847;&#26426;&#21046;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#32452;&#20214;&#65292;&#20294;&#26159;&#35768;&#22810;Transformer&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#30456;&#27604;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#24182;&#23558;&#37096;&#20998;&#21367;&#31215;&#23618;&#26367;&#25442;&#20026;MHSA&#65288;&#22810;&#22836;&#33258;&#27880;&#24847;&#65289;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;ODE&#65288;&#24120;&#24494;&#20998;&#26041;&#31243;&#65289;&#32780;&#19981;&#26159;ResNet&#20316;&#20026;&#39592;&#24178;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#21442;&#25968;&#22823;&#23567;&#12290;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#30456;&#27604;&#20110;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#23558;&#21442;&#25968;&#22823;&#23567;&#20943;&#23569;&#20102;94.6%&#65292;&#32780;&#19988;&#27809;&#26377;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37096;&#32626;&#22312;&#19968;&#21488;&#36866;&#24230;&#35268;&#27169;&#30340;FPGA&#35774;&#22791;&#19978;&#36827;&#34892;&#36793;&#32536;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer is an emerging neural network model with attention mechanism. It has been adopted to various tasks and achieved a favorable accuracy compared to CNNs and RNNs. While the attention mechanism is recognized as a general-purpose component, many of the Transformer models require a significant number of parameters compared to the CNN-based ones. To mitigate the computational complexity, recently, a hybrid approach has been proposed, which uses ResNet as a backbone architecture and replaces a part of its convolution layers with an MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly reduce the parameter size of such models by using Neural ODE (Ordinary Differential Equation) as a backbone architecture instead of ResNet. The proposed hybrid model reduces the parameter size by 94.6% compared to the CNN-based ones without degrading the accuracy. We then deploy the proposed model on a modest-sized FPGA device for edge computing. To further reduce FPGA resource u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.02283</link><description>&lt;p&gt;
DEM: &#33322;&#31354;&#33322;&#22825;&#20013;&#29992;&#20110;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36755;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace. (arXiv:2401.02283v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#26469;&#35748;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26631;&#35760;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#20154;&#24037;&#19987;&#23478;&#26816;&#26597;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21333;&#20010;&#36755;&#20986;&#32780;&#19981;&#26159;&#25972;&#20010;DNN&#30340;&#35748;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#35201;&#27714;&#36981;&#24490;&#20005;&#26684;&#12289;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#21830;&#29992;&#36719;&#20214;&#30340;&#30417;&#31649;&#25351;&#21335;&#65288;&#20363;&#22914;ARP-4754&#21644;DO-178&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#21335;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#32452;&#20214;&#30340;&#36719;&#20214;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20351;&#33322;&#31354;&#33322;&#22825;&#31995;&#32479;&#21463;&#30410;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38761;&#21629;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#20197;&#36755;&#20986;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#29992;&#20110;DNN&#30340;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32479;&#35745;&#39564;&#35777;&#25216;&#26415;&#65292;&#24182;&#20855;&#26377;&#33021;&#22815;&#26631;&#35760;DNN&#36755;&#20986;&#21487;&#33021;&#19981;&#21487;&#38752;&#30340;&#29305;&#23450;&#36755;&#20837;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#20197;&#20415;&#21518;&#32493;&#30001;&#19987;&#23478;&#26816;&#26597;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;DNN&#23545;&#20854;&#20182;&#38468;&#36817;&#36755;&#20837;&#30340;&#39044;&#27979;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#21453;&#65292;&#21518;&#32773;&#36890;&#24120;&#35797;&#22270;&#23545;&#25972;&#20010;DNN&#36827;&#34892;&#35748;&#35777;&#65292;&#32780;&#38750;&#21333;&#20010;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2311.00787</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;
&lt;/p&gt;
&lt;p&gt;
Accelerating Electronic Stopping Power Predictions by 10 Million Times with a Combination of Time-Dependent Density Functional Theory and Machine Learning. (arXiv:2311.00787v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00787
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#30005;&#23376;&#38459;&#27490;&#33021;&#21147;&#39044;&#27979;&#21152;&#24555;&#20102;1000&#19975;&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#31890;&#23376;&#36752;&#23556;&#22312;&#26448;&#26009;&#20013;&#37322;&#25918;&#33021;&#37327;&#30340;&#36895;&#29575;&#65292;&#20063;&#23601;&#26159;&#38459;&#27490;&#33021;&#21147;&#65292;&#23545;&#20110;&#35774;&#35745;&#26680;&#21453;&#24212;&#22534;&#12289;&#21307;&#30103;&#27835;&#30103;&#12289;&#21322;&#23548;&#20307;&#21644;&#37327;&#23376;&#26448;&#26009;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#25216;&#26415;&#37117;&#26159;&#20851;&#38190;&#12290;&#34429;&#28982;&#20851;&#20110;&#38459;&#27490;&#33021;&#21147;&#30340;&#26680;&#36129;&#29486;&#65292;&#21363;&#21407;&#23376;&#20043;&#38388;&#30340;&#24377;&#24615;&#25955;&#23556;&#65292;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#20294;&#26159;&#33719;&#21462;&#30005;&#23376;&#36129;&#29486;&#25968;&#25454;&#30340;&#36884;&#24452;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#20195;&#20215;&#39640;&#26114;&#19988;&#20381;&#36182;&#35768;&#22810;&#31616;&#21270;&#20551;&#35774;&#65292;&#21253;&#25324;&#26448;&#26009;&#26159;&#21508;&#21521;&#21516;&#24615;&#30340;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30456;&#20851;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;TDDFT&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23558;&#35780;&#20272;&#26032;&#26448;&#26009;&#30340;&#26102;&#38388;&#32553;&#30701;&#21040;&#20165;&#38656;&#20960;&#20010;&#23567;&#26102;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#21407;&#23376;&#32454;&#33410;&#22914;&#20309;&#24433;&#21709;&#30005;&#23376;&#38459;&#27490;&#30340;&#23453;&#36149;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;TDDFT&#26469;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#30005;&#23376;&#38459;&#27490;&#23545;&#38459;&#27490;&#33021;&#21147;&#30340;&#36129;&#29486;&#65292;&#24182;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25554;&#20540;&#21040;&#20854;&#20182;&#26041;&#21521;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;1000&#19975;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing the rate at which particle radiation releases energy in a material, the stopping power, is key to designing nuclear reactors, medical treatments, semiconductor and quantum materials, and many other technologies. While the nuclear contribution to stopping power, i.e., elastic scattering between atoms, is well understood in the literature, the route for gathering data on the electronic contribution has for decades remained costly and reliant on many simplifying assumptions, including that materials are isotropic. We establish a method that combines time-dependent density functional theory (TDDFT) and machine learning to reduce the time to assess new materials to mere hours on a supercomputer and provides valuable data on how atomic details influence electronic stopping. Our approach uses TDDFT to compute the electronic stopping contributions to stopping power from first principles in several directions and then machine learning to interpolate to other directions at rates 10 milli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.19809</link><description>&lt;p&gt;
MgNO:&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
MgNO: Efficient Parameterization of Linear Operators via Multigrid. (arXiv:2310.19809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#36890;&#36807;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#26377;&#25928;&#21442;&#25968;&#21270;&#32447;&#24615;&#31639;&#23376;&#65292;&#23454;&#29616;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#25968;&#23398;&#20005;&#23494;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#26469;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#23558;&#20854;&#19982;&#20256;&#32479;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31867;&#27604;&#65292;&#23558;&#31070;&#32463;&#31639;&#23376;&#23450;&#20041;&#20026;&#38750;&#32447;&#24615;&#31639;&#23376;&#23618;&#20013;&#31532;$i$&#20010;&#31070;&#32463;&#20803;&#30340;&#36755;&#20986;&#65292;&#35760;&#20316;$\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$&#12290;&#20854;&#20013;&#65292;$\mathcal W_{ij}$&#34920;&#31034;&#36830;&#25509;&#31532;$j$&#20010;&#36755;&#20837;&#31070;&#32463;&#20803;&#21644;&#31532;$i$&#20010;&#36755;&#20986;&#31070;&#32463;&#20803;&#30340;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;&#32780;&#20559;&#24046;$\mathcal B_{ij}$&#37319;&#29992;&#20989;&#25968;&#24418;&#24335;&#32780;&#38750;&#26631;&#37327;&#24418;&#24335;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#31070;&#32463;&#20803;&#65288;Banach&#31354;&#38388;&#65289;&#20043;&#38388;&#26377;&#25928;&#21442;&#25968;&#21270;&#26377;&#30028;&#32447;&#24615;&#31639;&#23376;&#65292;MgNO&#24341;&#20837;&#20102;&#22810;&#37325;&#32593;&#26684;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#26082;&#20855;&#22791;&#20102;&#25968;&#23398;&#20005;&#23494;&#24615;&#65292;&#21448;&#20855;&#22791;&#20102;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;MgNO&#28040;&#38500;&#20102;&#23545;&#20256;&#32479;&#30340;lifting&#21644;projecting&#25805;&#20316;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting ope
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#35757;&#32451;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#30340;&#32039;&#20945;&#29305;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#25104;&#27969;&#30340;&#36817;&#20284;&#65292;&#35813;&#36817;&#20284;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20854;&#34928;&#20943;&#36895;&#24230;&#20026;$\Theta_n(\frac{1}{n})$&#65292;&#36825;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.03575</link><description>&lt;p&gt;
&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#23398;&#20064;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of learning a flow-based generative model from limited sample complexity. (arXiv:2310.03575v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03575
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20174;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#20013;&#35757;&#32451;&#22522;&#20110;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#30340;&#32039;&#20945;&#29305;&#24615;&#65292;&#24182;&#25551;&#36848;&#20102;&#29983;&#25104;&#27969;&#30340;&#36817;&#20284;&#65292;&#35813;&#36817;&#20284;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20854;&#34928;&#20943;&#36895;&#24230;&#20026;$\Theta_n(\frac{1}{n})$&#65292;&#36825;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35757;&#32451;&#19968;&#20010;&#30001;&#20004;&#23618;&#33258;&#32534;&#30721;&#22120;&#21442;&#25968;&#21270;&#30340;&#27969;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#20174;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23574;&#38160;&#30340;&#31471;&#21040;&#31471;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32039;&#23494;&#30340;&#38381;&#24335;&#29305;&#24449;&#21270;&#23398;&#20064;&#21040;&#30340;&#36895;&#24230;&#22330;&#65292;&#24403;&#21442;&#25968;&#21270;&#20026;&#19968;&#20010;&#22312;&#30446;&#26631;&#20998;&#24067;&#19978;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#26679;&#26412;$ n $&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#27973;&#23618;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#26102;&#12290;&#22312;&#27492;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#29983;&#25104;&#27969;&#30340;&#23574;&#38160;&#25551;&#36848;&#65292;&#23558;&#22522;&#26412;&#39640;&#26031;&#23494;&#24230;&#25512;&#21521;&#30446;&#26631;&#23494;&#24230;&#30340;&#36817;&#20284;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#28151;&#21512;&#29289;&#30340;&#22343;&#20540;&#19982;&#30446;&#26631;&#28151;&#21512;&#29289;&#22343;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#38381;&#24335;&#20844;&#24335;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#36317;&#31163;&#20250;&#34928;&#20943;&#20026;$\Theta_n(\frac{1}{n})$&#12290;&#26368;&#21518;&#65292;&#36825;&#20010;&#36895;&#29575;&#34987;&#35777;&#26126;&#23454;&#38469;&#19978;&#26159;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number $n$ of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact Bayes-optimal.
&lt;/p&gt;</description></item><item><title>PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.</title><link>http://arxiv.org/abs/2310.01720</link><description>&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series&#65288;&#26102;&#38388;&#24207;&#21015;&#30340;&#24863;&#30693;-&#27880;&#24847;&#21147;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65289;
&lt;/p&gt;
&lt;p&gt;
PrACTiS: Perceiver-Attentional Copulas for Time Series. (arXiv:2310.01720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01720
&lt;/p&gt;
&lt;p&gt;
PrACTiS combines perceiver architecture with copula structure to enhance time-series forecasting and reduce computational demands. It incorporates midpoint inference and local attention mechanisms to effectively capture dependencies within imputed samples. The copula-based attention and output variance testing mechanism capture the joint distribution of missing data and mitigate error propagation during prediction.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24863;&#30693;&#22120;&#26550;&#26500;&#19982;&#32852;&#21512;&#20998;&#24067;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24863;&#30693;&#22120;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#22797;&#26434;&#30340;&#39640;&#32500;&#22810;&#27169;&#24577;&#25968;&#25454;&#36716;&#25442;&#20026;&#32039;&#20945;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#38656;&#27714;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20013;&#28857;&#25512;&#26029;&#21644;&#23616;&#37096;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25554;&#34917;&#26679;&#26412;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#30340;&#27880;&#24847;&#21147;&#21644;&#36755;&#20986;&#26041;&#24046;&#27979;&#35797;&#26426;&#21046;&#26469;&#25429;&#25417;&#32570;&#22833;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#21516;&#26102;&#20943;&#23569;&#39044;&#27979;&#36807;&#31243;&#20013;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers incorporating copula structures have demonstrated remarkable performance in time series prediction. However, their heavy reliance on self-attention mechanisms demands substantial computational resources, thus limiting their practical utility across a wide range of tasks. In this work, we present a model that combines the perceiver architecture with a copula structure to enhance time-series forecasting. By leveraging the perceiver as the encoder, we efficiently transform complex, high-dimensional, multimodal data into a compact latent space, thereby significantly reducing computational demands. To further reduce complexity, we introduce midpoint inference and local attention mechanisms, enabling the model to capture dependencies within imputed samples effectively. Subsequently, we deploy the copula-based attention and output variance testing mechanism to capture the joint distribution of missing data, while simultaneously mitigating error propagation during prediction. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#27491;&#21017;&#21270;&#22312;&#22810;&#31867;&#21035;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#19968;&#20123;&#29305;&#23450;&#24773;&#26223;&#19979;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#23545;&#19968;&#21253;&#21547;&#22270;(OIGs)&#23637;&#31034;&#20102;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#12289;&#26368;&#22823;&#29109;&#21407;&#21017;&#21644;&#36125;&#21494;&#26031;&#25512;&#29702;&#31561;&#31639;&#27861;&#21407;&#21017;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.13692</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#19982;&#26368;&#20248;&#22810;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularization and Optimal Multiclass Learning. (arXiv:2309.13692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#27491;&#21017;&#21270;&#22312;&#22810;&#31867;&#21035;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#22312;&#19968;&#20123;&#29305;&#23450;&#24773;&#26223;&#19979;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#23545;&#19968;&#21253;&#21547;&#22270;(OIGs)&#23637;&#31034;&#20102;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#12289;&#26368;&#22823;&#29109;&#21407;&#21017;&#21644;&#36125;&#21494;&#26031;&#25512;&#29702;&#31561;&#31639;&#27861;&#21407;&#21017;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#20026;&#20195;&#34920;&#30340;&#20856;&#22411;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#21457;&#29616;&#22312;&#19968;&#20123;&#23398;&#20064;&#38750;&#22343;&#21248;&#25910;&#25947;&#30340;&#24773;&#26223;&#20013;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#23384;&#22312;&#35768;&#22810;&#26356;&#20016;&#23500;&#30340;&#31639;&#27861;&#25216;&#26415;&#26469;&#25511;&#21046;&#27169;&#22411;&#23481;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#26356;&#19968;&#33324;&#30340;&#24773;&#22659;&#20013;&#65292;&#27809;&#26377;&#19968;&#31181;&#25216;&#26415;&#25110;&#21407;&#21017;&#33021;&#22815;&#33073;&#39062;&#32780;&#20986;&#26469;&#25551;&#36848;&#26368;&#20248;&#23398;&#20064;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#34920;&#24449;&#27491;&#21017;&#21270;&#22312;&#22810;&#31867;&#21035;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#36825;&#21487;&#33021;&#26159;ERM&#22833;&#36133;&#30340;&#26368;&#31616;&#21333;&#24773;&#26223;&#65292;&#32780;&#26631;&#31614;&#38598;&#26159;&#20219;&#24847;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#23545;&#19968;&#21253;&#21547;&#22270;&#65288;OIGs&#65289;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#31639;&#27861;&#21407;&#21017;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;&#22885;&#21345;&#22982;&#21059;&#20992;&#21407;&#21017;&#25152;&#20307;&#29616;&#30340;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;SRM&#65289;&#65292;&#26368;&#22823;&#29109;&#21407;&#21017;&#21644;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#19978;&#36827;&#34892;&#25918;&#26494;&#30340;&#26368;&#20248;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quintessential learning algorithm of empirical risk minimization (ERM) is known to fail in various settings for which uniform convergence does not characterize learning. It is therefore unsurprising that the practice of machine learning is rife with considerably richer algorithmic techniques for successfully controlling model capacity. Nevertheless, no such technique or principle has broken away from the pack to characterize optimal learning in these more general settings.  The purpose of this work is to characterize the role of regularization in perhaps the simplest setting for which ERM fails: multiclass learning with arbitrary label sets. Using one-inclusion graphs (OIGs), we exhibit optimal learning algorithms that dovetail with tried-and-true algorithmic principles: Occam's Razor as embodied by structural risk minimization (SRM), the principle of maximum entropy, and Bayesian reasoning. Most notably, we introduce an optimal learner which relaxes structural risk minimization on
&lt;/p&gt;</description></item><item><title>NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2309.05519</link><description>&lt;p&gt;
NExT-GPT: &#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NExT-GPT: Any-to-Any Multimodal LLM. (arXiv:2309.05519v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05519
&lt;/p&gt;
&lt;p&gt;
NExT-GPT&#26159;&#19968;&#20010;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#33021;&#22815;&#25509;&#21463;&#21644;&#29983;&#25104;&#20219;&#24847;&#32452;&#21512;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLM&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#25391;&#22859;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#21363;&#21482;&#33021;&#22312;&#36755;&#20837;&#31471;&#36827;&#34892;&#22810;&#27169;&#24577;&#29702;&#35299;&#65292;&#26080;&#27861;&#20197;&#22810;&#31181;&#27169;&#24335;&#29983;&#25104;&#20869;&#23481;&#12290;&#30001;&#20110;&#25105;&#20204;&#20154;&#31867;&#24635;&#26159;&#36890;&#36807;&#21508;&#31181;&#27169;&#24577;&#24863;&#30693;&#19990;&#30028;&#21644;&#19982;&#20154;&#20132;&#27969;&#65292;&#22240;&#27492;&#24320;&#21457;&#33021;&#22815;&#25509;&#21463;&#21644;&#20256;&#36882;&#20219;&#20309;&#27169;&#24577;&#20869;&#23481;&#30340;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;MM-LLM&#31995;&#32479;&#23545;&#20110;&#23454;&#29616;&#20154;&#32423;AI&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36890;&#29992;&#20219;&#20309;&#21040;&#20219;&#20309;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#65292;NExT-GPT&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#19968;&#20010;&#21547;&#26377;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#21644;&#19981;&#21516;&#25193;&#25955;&#35299;&#30721;&#22120;&#30340;LLM&#65292;&#20351;&#24471;NExT-GPT&#33021;&#22815;&#20197;&#20219;&#24847;&#30340;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#30340;&#32452;&#21512;&#36827;&#34892;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35757;&#32451;&#26377;&#32032;&#30340;&#39640;&#24615;&#33021;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;NExT-GPT&#20165;&#36890;&#36807;&#35843;&#25972;&#26576;&#20123;&#25237;&#24433;&#23618;&#30340;&#23569;&#37327;&#21442;&#25968;&#65288;1%&#65289;&#36827;&#34892;&#35843;&#20248;&#65292;&#36825;&#19981;&#20165;&#26377;&#21033;&#20110;&#20302;&#25104;&#26412;&#35757;&#32451;&#65292;&#36824;&#26377;&#21161;&#20110;&#26041;&#20415;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03145</link><description>&lt;p&gt;
&#26368;&#20339;&#33218;&#36530;&#36991;&#65306;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
The Best Arm Evades: Near-optimal Multi-pass Streaming Lower Bounds for Pure Exploration in Multi-armed Bandits. (arXiv:2309.03145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#36817;&#20046;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#30028;&#38480;&#65292;&#24182;&#22238;&#31572;&#20102;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#22810;&#36890;&#36947;&#27969;&#31639;&#27861;&#32473;&#20986;&#20102;&#32431;&#25506;&#32034;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MABs&#65289;&#30340;&#36817;&#20284;&#26368;&#20248;&#26679;&#26412;&#36890;&#36947;&#20132;&#25442;&#65306;&#20219;&#20309;&#20351;&#29992;&#23376;&#32447;&#24615;&#20869;&#23384;&#30340;&#27969;&#31639;&#27861;&#65292;&#20854;&#20351;&#29992; $O(\frac{n}{\Delta^2})$ &#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#38656;&#35201; $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ &#20010;&#36890;&#36947;&#12290;&#36825;&#37324;&#65292;$n$ &#26159;&#33218;&#30340;&#25968;&#37327;&#65292;$\Delta$ &#26159;&#26368;&#20339;&#33218;&#21644;&#27425;&#20339;&#33218;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;Jin&#31561;&#20154;[ICML'21]&#30340; $O(\log(\frac{1}{\Delta}))$ &#36890;&#36947;&#31639;&#27861;&#30456;&#21305;&#37197;&#65288;&#38500;&#20102;&#20302;&#38454;&#39033;&#65289;&#65292;&#35813;&#31639;&#27861;&#20165;&#20351;&#29992; $O(1)$ &#20869;&#23384;&#65292;&#24182;&#22238;&#31572;&#20102;Assadi&#21644;Wang[STOC'20]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a near-optimal sample-pass trade-off for pure exploration in multi-armed bandits (MABs) via multi-pass streaming algorithms: any streaming algorithm with sublinear memory that uses the optimal sample complexity of $O(\frac{n}{\Delta^2})$ requires $\Omega(\frac{\log{(1/\Delta)}}{\log\log{(1/\Delta)}})$ passes. Here, $n$ is the number of arms and $\Delta$ is the reward gap between the best and the second-best arms. Our result matches the $O(\log(\frac{1}{\Delta}))$-pass algorithm of Jin et al. [ICML'21] (up to lower order terms) that only uses $O(1)$ memory and answers an open question posed by Assadi and Wang [STOC'20].
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13049</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#36125;&#21494;&#26031;&#20195;&#29702;&#19981;&#20250;&#38754;&#20020;&#39057;&#29575;&#26041;&#27861;&#30340;&#25506;&#32034;/&#24320;&#21457;&#22256;&#22659;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#22312;&#29609;&#20855;&#39046;&#22495;&#20013;&#26159;&#21487;&#35745;&#31639;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#32780;&#19981;&#26159;&#22312;&#39640;&#32500;&#29366;&#24577;&#36716;&#31227;&#20998;&#24067;&#20013;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#35201;&#20040;&#19981;&#36890;&#36807;MDP&#20256;&#25773;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#22312;&#19968;&#32452;&#35821;&#22659;&#31574;&#30053;&#20013;&#20248;&#21270;&#32780;&#19981;&#26159;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20004;&#20010;&#36817;&#20284;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20219;&#24847;&#36125;&#21494;&#26031;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#65288;Bayesian exploration network&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.08634</link><description>&lt;p&gt;
FedPop: &#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
FedPop: Federated Population-based Hyperparameter Tuning. (arXiv:2308.08634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08634
&lt;/p&gt;
&lt;p&gt;
FedPop&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#20248;&#38382;&#39064;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#33539;&#24335;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#38598;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;ML&#27969;&#31243;&#31867;&#20284;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#26412;&#22320;&#20248;&#21270;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#36807;&#31243;&#23545;&#36229;&#21442;&#25968;&#65288;HP&#65289;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#23613;&#31649;&#22312;&#38598;&#20013;&#24335;ML&#20013;&#23545;&#35843;&#20248;HP&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;FL&#26102;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#8220;&#35843;&#20248;&#21518;&#35757;&#32451;&#8221;&#26694;&#26550;&#23545;&#20110;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#30340;FL&#19981;&#21512;&#36866;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#25552;&#20986;&#29992;&#20110;FL&#20013;&#30340;HP&#35843;&#20248;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20165;&#38480;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#26356;&#26032;&#30340;HP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#24335;&#22522;&#20110;&#20154;&#21475;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#65288;FedPop&#65289;&#30340;&#26032;&#22411;HP&#35843;&#20248;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;FedPop&#37319;&#29992;&#22522;&#20110;&#20154;&#21475;&#30340;&#36827;&#21270;&#31639;&#27861;&#26469;&#20248;&#21270;HP&#65292;&#27492;&#31639;&#27861;&#36866;&#29992;&#20110;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#21508;&#31181;HP&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning (ML) paradigm, in which multiple clients collaboratively train ML models without centralizing their local data. Similar to conventional ML pipelines, the client local optimization and server aggregation procedure in FL are sensitive to the hyperparameter (HP) selection. Despite extensive research on tuning HPs for centralized ML, these methods yield suboptimal results when employed in FL. This is mainly because their "training-after-tuning" framework is unsuitable for FL with limited client computation power. While some approaches have been proposed for HP-Tuning in FL, they are limited to the HPs for client local updates. In this work, we propose a novel HP-tuning algorithm, called Federated Population-based Hyperparameter Tuning (FedPop), to address this vital yet challenging problem. FedPop employs population-based evolutionary algorithms to optimize the HPs, which accommodates various HP types at both client and server sides
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;</title><link>http://arxiv.org/abs/2307.01753</link><description>&lt;p&gt;
&#26469;&#33258;DESI&#20142;&#32418;&#26143;&#31995;&#22823;&#23610;&#24230;&#32858;&#31867;&#30340;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#32763;&#35793;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
Local primordial non-Gaussianity from the large-scale clustering of photometric DESI luminous red galaxies. (arXiv:2307.01753v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;DESI&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#38480;&#21046;&#20102;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#65292;&#21457;&#29616;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;fNL&#20026;47^{+14(+29)}_{-11(-22)}&#65292;&#20351;&#29992;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#21518;&#65292;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Dark Energy Spectroscopic Instrument (DESI)&#25104;&#20687;&#35843;&#26597;&#30340;&#20142;&#32418;&#26143;&#31995;&#30340;&#35282;&#32858;&#31867;&#20449;&#24687;&#26469;&#38480;&#21046;&#23616;&#37096;&#21407;&#21021;&#38750;&#39640;&#26031;&#24615;&#21442;&#25968;fNL&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#21253;&#25324;&#36229;&#36807;1200&#19975;&#20010;&#30446;&#26631;&#65292;&#35206;&#30422;&#20102;14000&#24179;&#26041;&#24230;&#30340;&#22825;&#31354;&#21306;&#22495;&#65292;&#32418;&#31227;&#33539;&#22260;&#20026;0.2 &lt; z &lt; 1.35&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38134;&#27827;&#28040;&#20809;&#12289;&#35843;&#26597;&#28145;&#24230;&#21644;&#35266;&#27979;&#26465;&#20214;&#26159;&#31995;&#32479;&#35823;&#24046;&#30340;&#20027;&#35201;&#26469;&#28304;&#65292;&#24182;&#37319;&#29992;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#36731;&#22823;&#23610;&#24230;&#19978;&#30340;&#38750;&#23431;&#23449;&#23398;&#36807;&#24230;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32463;&#36807;&#20102;&#21253;&#21547;&#21644;&#19981;&#21253;&#21547;fNL&#21644;&#31995;&#32479;&#35823;&#24046;&#30340;&#23545;&#25968;&#27491;&#24577;&#27169;&#25311;&#30340;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22312;&#20943;&#23567;&#21097;&#20313;&#31995;&#32479;&#35823;&#24046;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#20551;&#35774;&#23431;&#23449;&#35268;&#24459;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;fNL&#30340;68\%&#65288;95\%&#65289;&#32622;&#20449;&#21306;&#38388;&#20026;fNL = 47^{+14(+29)}_{-11(-22)}&#12290;&#36890;&#36807;&#26356;&#31215;&#26497;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25152;&#26377;&#25104;&#20687;&#22270;&#38598;&#36827;&#34892;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#20540;&#30053;&#24494;&#20559;&#31163;fNL&#8776;5&#12290;
&lt;/p&gt;
&lt;p&gt;
We use angular clustering of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) imaging surveys to constrain the local primordial non-Gaussianity parameter fNL. Our sample comprises over 12 million targets, covering 14,000 square degrees of the sky, with redshifts in the range 0.2&lt; z &lt; 1.35. We identify Galactic extinction, survey depth, and astronomical seeing as the primary sources of systematic error, and employ linear regression and artificial neural networks to alleviate non-cosmological excess clustering on large scales. Our methods are tested against log-normal simulations with and without fNL and systematics, showing superior performance of the neural network treatment in reducing remaining systematics. Assuming the universality relation, we find fNL $= 47^{+14(+29)}_{-11(-22)}$ at 68\%(95\%) confidence. With a more aggressive treatment, including regression against the full set of imaging maps, our maximum likelihood value shifts slightly to fNL$ \sim 5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14094</link><description>&lt;p&gt;
&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Locally Differentially Private Distributed Online Learning with Guaranteed Optimality. (arXiv:2306.14094v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20445;&#35777;&#26368;&#20248;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#21516;&#26102;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#30001;&#20110;&#20854;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27969;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20010;&#20154;&#31169;&#23494;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#24120;&#24120;&#38754;&#20020;&#20026;&#20102;&#38544;&#31169;&#20445;&#25252;&#32780;&#29306;&#29298;&#23398;&#20064;&#20934;&#30830;&#24615;&#30340;&#22256;&#22659;&#12290;&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#24182;&#30830;&#20445;&#20998;&#24067;&#24335;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#21644;&#23398;&#20064;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#22312;&#30830;&#20445;&#39044;&#26399;&#30636;&#26102;&#36951;&#25022;&#31243;&#24230;&#36880;&#28176;&#20943;&#23567;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#20445;&#35777;&#26377;&#38480;&#30340;&#32047;&#31215;&#38544;&#31169;&#39044;&#31639;&#65292;&#21363;&#20351;&#22312;&#26080;&#38480;&#26102;&#38388;&#33539;&#22260;&#20869;&#12290;&#20026;&#20102;&#24212;&#23545;&#23436;&#20840;&#20998;&#24067;&#24335;&#29615;&#22659;&#65292;&#25105;&#20204;&#37319;&#29992;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#65292;&#36991;&#20813;&#20102;&#23545;&#20840;&#23616;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed online learning is gaining increased traction due to its unique ability to process large-scale datasets and streaming data. To address the growing public awareness and concern on privacy protection, plenty of private distributed online learning algorithms have been proposed, mostly based on differential privacy which has emerged as the ``gold standard" for privacy protection. However, these algorithms often face the dilemma of trading learning accuracy for privacy. By exploiting the unique characteristics of online learning, this paper proposes an approach that tackles the dilemma and ensures both differential privacy and learning accuracy in distributed online learning. More specifically, while ensuring a diminishing expected instantaneous regret, the approach can simultaneously ensure a finite cumulative privacy budget, even on the infinite time horizon. To cater for the fully distributed setting, we adopt the local differential-privacy framework which avoids the reliance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#25439;&#22833;&#20989;&#25968;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#38590;&#20813;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#36825;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#36866;&#24212;&#36825;&#20123;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#21482;&#26377;&#20351;&#29992;&#19981;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#40065;&#26834;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21019;&#24314;&#22122;&#22768;&#40065;&#26834;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25968;&#37327;&#20247;&#22810;&#65292;&#23427;&#20204;&#36890;&#24120;&#20276;&#38543;&#30528;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#21487;&#33021;&#23398;&#20064;&#36895;&#24230;&#27604;&#24191;&#27867;&#20351;&#29992;&#20294;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#35201;&#24930;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#32771;&#34385;&#21644;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#25439;&#22833;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#24102;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#65292;&#21363;&#30053;&#24494;&#22686;&#21152;&#19982;&#27491;&#30830;&#26631;&#31614;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#25216;&#26415;&#22312;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#24471;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.02568</link><description>&lt;p&gt;
Gumbel&#20256;&#25773;&#19979;&#30340;&#28508;&#22312;&#26368;&#20248;&#36335;&#24452;&#21464;&#20998;&#36125;&#21494;&#26031;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming. (arXiv:2306.02568v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#24471;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#65292;&#20174;&#32780;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;Gumbel&#20256;&#25773;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#33719;&#21462;&#32467;&#26500;&#21270;&#31232;&#30095;&#26368;&#20248;&#36335;&#24452;&#12290;&#25105;&#20204;&#36890;&#36807;&#27010;&#29575;&#36719;&#21270;&#35299;&#65292;&#21363;&#38543;&#26426;&#26368;&#20248;&#36335;&#24452;&#65292;&#26469;&#35299;&#20915;&#32463;&#20856;&#26368;&#20248;&#36335;&#24452;&#38382;&#39064;&#65292;&#24182;&#23558;&#24191;&#27867;&#30340;DP&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#65292;&#20854;&#20013;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#36981;&#24490;Gibbs&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;Gumbel&#20998;&#24067;&#30340;&#23646;&#24615;&#26174;&#31034;Gibbs&#20998;&#24067;&#19982;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#29702;&#25152;&#38656;&#30340;&#25152;&#26377;&#35201;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#21462;&#20102;&#28508;&#22312;&#26368;&#20248;&#36335;&#24452;&#65292;&#20351;&#29983;&#25104;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#65292;&#20854;&#20013;&#27169;&#22411;&#20381;&#36182;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#65306;&#25991;&#26412;&#36716;&#35821;&#38899;&#21644;&#27468;&#22768;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a unified approach to obtain structured sparse optimal paths in the latent space of a variational autoencoder (VAE) using dynamic programming and Gumbel propagation. We solve the classical optimal path problem by a probability softening solution, called the stochastic optimal path, and transform a wide range of DP problems into directed acyclic graphs in which all possible paths follow a Gibbs distribution. We show the equivalence of the Gibbs distribution to a message-passing algorithm by the properties of the Gumbel distribution and give all the ingredients required for variational Bayesian inference. Our approach obtaining latent optimal paths enables end-to-end training for generative tasks in which models rely on the information of unobserved structural features. We validate the behavior of our approach and showcase its applicability in two real-world applications: text-to-speech and singing voice synthesis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20803;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;gamma&#32593;&#32476;&#21644;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.15057</link><description>&lt;p&gt;
Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Meta-Calibration Regularized Neural Networks. (arXiv:2303.15057v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20803;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;gamma&#32593;&#32476;&#21644;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26657;&#20934;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35823;&#26657;&#20934;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#23384;&#22312;&#35823;&#26657;&#20934;&#38382;&#39064;&#65292;&#21363;&#39044;&#27979;&#27010;&#29575;&#19982;&#30495;&#23454;&#27491;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;&#26657;&#20934;&#27169;&#22411;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#21516;&#26102;&#20248;&#21270;&#26657;&#20934;&#35823;&#24046;&#30340;&#20195;&#29702;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#20803;&#26657;&#20934;&#65288;MC&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#26657;&#20934;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#25193;&#23637;&#20102;MC&#65306;&#65288;1&#65289;gamma&#32593;&#32476;&#65288;gamma-net&#65289;&#65292;&#19968;&#20010;&#20803;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#36830;&#32493;&#31354;&#38388;&#20026;&#35843;&#20248;&#39592;&#24178;&#32593;&#32476;&#30340;focal loss&#23398;&#20064;&#36880;&#26679;&#26412;gamma&#65307;&#65288;2&#65289;&#24179;&#28369;&#30340;&#39044;&#26399;&#26657;&#20934;&#35823;&#24046;&#65288;SECE&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#26680;&#30340;&#26080;&#20559;&#21644;&#21487;&#24494;&#30340;ECE&#65292;&#26088;&#22312;&#24179;&#28369;&#35843;&#20248;gamma-net&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#22320;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#65288;a&#65289;&#22312;&#36830;&#32493;&#31354;&#38388;&#23398;&#20064;&#36880;&#26679;&#26412;gamma&#21487;&#20197;&#26377;&#25928;&#22320;&#20248;&#21270;&#39592;&#24178;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Miscalibration-the mismatch between predicted probability and the true correctness likelihood-has been frequently identified in modern deep neural networks. Recent work in the field aims to address this problem by training calibrated models directly by optimizing a proxy of the calibration error alongside the conventional objective. Recently, Meta-Calibration (MC) showed the effectiveness of using meta-learning for learning better calibrated models. In this work, we extend MC with two main components: (1) gamma network (gamma-net), a meta network to learn a sample-wise gamma at a continuous space for focal loss for optimizing backbone network; (2) smooth expected calibration error (SECE), a Gaussian-kernel based unbiased and differentiable ECE which aims to smoothly optimizing gamma-net. The proposed method regularizes neural network towards better calibration meanwhile retain predictive performance. Our experiments show that (a) learning sample-wise gamma at continuous space can effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;</title><link>http://arxiv.org/abs/2303.09470</link><description>&lt;p&gt;
&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Combining Distance to Class Centroids and Outlier Discounting for Improved Learning with Noisy Labels. (arXiv:2303.09470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#31867;&#20013;&#24515;&#36317;&#31163;&#21644;&#24322;&#24120;&#20540;&#25240;&#25187;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615; &#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#29289;&#21697;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24039;&#22937;&#22320;&#20351;&#29992;&#36317;&#31163;&#31867;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20877;&#32467;&#21512;&#25240;&#25187;&#31574;&#30053;&#20197;&#20943;&#23569;&#36317;&#31163;&#25152;&#26377;&#31867;&#20013;&#24515;&#65288;&#21363;&#24322;&#24120;&#20540;&#65289;&#36828;&#30340;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#35299;&#20915;&#20102;&#22122;&#22768;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#36317;&#31163;&#21508;&#33258;&#31867;&#20013;&#24515;&#26356;&#36828;&#30340;&#26679;&#26412;&#26356;&#21487;&#33021;&#26159;&#22122;&#22768;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#39046;&#22495;&#30340;&#26368;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new approach for addressing the challenge of training machine learning models in the presence of noisy labels. By combining a clever usage of distance to class centroids in the items' latent space with a discounting strategy to reduce the importance of samples far away from all the class centroids (i.e., outliers), our method effectively addresses the issue of noisy labels. Our approach is based on the idea that samples farther away from their respective class centroid in the early stages of training are more likely to be noisy. We demonstrate the effectiveness of our method through extensive experiments on several popular benchmark datasets. Our results show that our approach outperforms the state-of-the-art in this area, achieving significant improvements in classification accuracy when the dataset contains noisy labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;</title><link>http://arxiv.org/abs/2211.05408</link><description>&lt;p&gt;
&#29992;&#26680;&#26031;&#22374;&#31163;&#24046;&#25511;&#21046;&#30697;
&lt;/p&gt;
&lt;p&gt;
Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#25511;&#21046;&#24615;&#36136;&#65292;&#21457;&#29616;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#65292;&#25552;&#20986;&#20102;&#21487;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#30340;&#19979;&#28216;&#25193;&#25955;KSD&#65292;&#24182;&#19988;&#21457;&#23637;&#20102;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26031;&#22374;&#31163;&#24046;&#65288;KSD&#65289;&#29992;&#20110;&#34913;&#37327;&#20998;&#24067;&#36924;&#36817;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#30446;&#26631;&#23494;&#24230;&#20855;&#26377;&#19981;&#21487;&#35745;&#31639;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#26102;&#35745;&#31639;&#12290;&#26174;&#33879;&#30340;&#24212;&#29992;&#21253;&#25324;&#35786;&#26029;&#36817;&#20284;MCMC&#37319;&#26679;&#22120;&#21644;&#38750;&#24402;&#19968;&#21270;&#32479;&#35745;&#27169;&#22411;&#30340;&#36866;&#37197;&#24230;&#26816;&#39564;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;KSD&#30340;&#25910;&#25947;&#25511;&#21046;&#24615;&#36136;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#29992;&#20110;&#24369;&#25910;&#25947;&#25511;&#21046;&#30340;&#26631;&#20934;KSD&#26080;&#27861;&#25511;&#21046;&#30697;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#19979;&#28216;&#25193;&#25955;KSD&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#30697;&#21644;&#24369;&#25910;&#25947;&#12290;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#23545;&#20110;&#27599;&#20010;$q&gt;0$&#65292;&#31532;&#19968;&#32452;&#24050;&#30693;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;$q$-Wasserstein&#25910;&#25947;&#30340;KSD&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23545;&#31867;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#23458;&#35266;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#20998;&#26512;&#20102;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26368;&#26032;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#20294;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.08101</link><description>&lt;p&gt;
&#20174;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#32034;&#26356;&#23458;&#35266;&#30340;&#35780;&#20215;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards More Objective Evaluation of Class Incremental Learning: Representation Learning Perspective. (arXiv:2206.08101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23545;&#31867;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#23458;&#35266;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#20998;&#26512;&#20102;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26368;&#26032;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#20294;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#25351;&#22312;&#19981;&#24536;&#35760;&#24050;&#32463;&#23398;&#20064;&#30340;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#26029;&#22320;&#20174;&#22686;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#30340;&#36807;&#31243;&#12290;&#34429;&#28982;&#35780;&#20272;CIL&#31639;&#27861;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22522;&#20110;&#25152;&#26377;&#24050;&#23398;&#20064;&#31867;&#21035;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#26368;&#22823;&#21270;&#20934;&#30830;&#29575;&#24182;&#19981;&#19968;&#23450;&#33021;&#23548;&#33268;&#26377;&#25928;&#30340;CIL&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20351;&#29992;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#23454;&#39564;&#20998;&#26512;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#19988;&#27809;&#26377;&#26174;&#33879;&#25913;&#21464;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#26377;&#26102;&#29978;&#33267;&#23398;&#20064;&#20102;&#27604;&#26420;&#32032;&#22522;&#32447;&#26356;&#24046;&#30340;&#34920;&#31034;&#12290;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26356;&#25509;&#36817;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#22522;&#27169;&#22411;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is the process of continually learning new object classes from incremental data while not forgetting past learned classes. While the common method for evaluating CIL algorithms is based on average test accuracy for all learned classes, we argue that maximizing accuracy alone does not necessarily lead to effective CIL algorithms. In this paper, we experimentally analyze neural network models trained by CIL algorithms using various evaluation protocols in representation learning and propose a new analysis method. Our experiments show that most state-of-the-art algorithms prioritize high stability and do not significantly change the learned representation, and sometimes even learn a representation of lower quality than a naive baseline. However, we observe that these algorithms can still achieve high test accuracy because they learn a classifier that is closer to the optimal classifier. We also found that the base model learned in the first task varies in 
&lt;/p&gt;</description></item></channel></rss>