<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15780</link><description>&lt;p&gt;
&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fairness-Oriented Reinforcement Learning Approach for the Operation and Control of Shared Micromobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#23454;&#29616;&#24615;&#33021;&#20248;&#21270;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#24179;&#34913;&#30340;&#21069;&#27839;&#35843;&#26597;&#65292;&#21033;&#29992;Q-Learning&#31639;&#27861;&#30830;&#20445;&#26041;&#27861;&#31283;&#20581;&#65292;&#33021;&#22815;&#23454;&#29616;&#21508;&#31181;&#31449;&#28857;&#31867;&#21035;&#20043;&#38388;&#30340;&#20844;&#24179;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#65292;&#21253;&#25324;&#37027;&#20123;&#30452;&#25509;&#28041;&#21450;&#20154;&#31867;&#30340;&#39046;&#22495;&#65292;&#24179;&#31561;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24517;&#35201;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#30028;&#24840;&#21457;&#31361;&#20986;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20849;&#20139;&#24494;&#31227;&#21160;&#31995;&#32479;&#30340;&#32972;&#26223;&#19979;&#65292;&#20844;&#24179;&#24615;&#23548;&#21521;&#26041;&#27861;&#30340;&#25506;&#32034;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#25506;&#35752;&#24615;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#20849;&#20139;&#24494;&#31227;&#21160;&#26381;&#21153;&#36816;&#33829;&#19982;&#25511;&#21046;&#20013;&#24615;&#33021;&#20248;&#21270;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Q-Learning&#31639;&#27861;&#65292;&#21033;&#29992;&#20854;&#25910;&#25947;&#20445;&#35777;&#26469;&#30830;&#20445;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31449;&#28857;&#31867;&#21035;&#65288;&#20013;&#24515;&#12289;&#36793;&#32536;&#21644;&#36828;&#31243;&#65289;&#20043;&#38388;&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#36890;&#36807;&#22522;&#23612;&#31995;&#25968;&#26469;&#34913;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15780v1 Announce Type: cross  Abstract: As Machine Learning systems become increasingly popular across diverse application domains, including those with direct human implications, the imperative of equity and algorithmic fairness has risen to prominence in the Artificial Intelligence community. On the other hand, in the context of Shared Micromobility Systems, the exploration of fairness-oriented approaches remains limited. Addressing this gap, we introduce a pioneering investigation into the balance between performance optimization and algorithmic fairness in the operation and control of Shared Micromobility Services. Our study leverages the Q-Learning algorithm in Reinforcement Learning, benefiting from its convergence guarantees to ensure the robustness of our proposed approach. Notably, our methodology stands out for its ability to achieve equitable outcomes, as measured by the Gini index, across different station categories--central, peripheral, and remote. Through stra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.14088</link><description>&lt;p&gt;
&#22522;&#20110;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;&#30340;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Protein Conformation Generation via Force-Guided SE(3) Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#65292;&#36890;&#36807;&#32467;&#21512;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#26223;&#35266;&#23545;&#20110;&#29702;&#35299;&#22797;&#26434;&#29983;&#29289;&#36807;&#31243;&#20013;&#30340;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#65292;&#23384;&#22312;&#31232;&#26377;&#20107;&#20214;&#37319;&#26679;&#21644;&#38271;&#26102;&#38388;&#24179;&#34913;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19968;&#33324;&#34507;&#30333;&#36136;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#29983;&#25104;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#26500;&#35937;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#32467;&#21512;&#37325;&#35201;&#30340;&#29289;&#29702;&#20808;&#39564;&#30693;&#35782;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#23548;&#33268;&#37319;&#26679;&#34507;&#30333;&#36136;&#26500;&#35937;&#19982;&#24179;&#34913;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#26500;&#35937;&#29983;&#25104;&#30340;&#21147;&#24341;&#23548;SE(3)&#25193;&#25955;&#27169;&#22411;ConfDiff&#65292;&#20197;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#36890;&#36807;&#23558;&#21147;&#24341;&#23548;&#32593;&#32476;&#19982;&#19968;&#31995;&#21015;&#22522;&#20110;&#25968;&#25454;&#30340;&#20998;&#25968;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;ConfDiff&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#26500;&#35937;&#30340;&#20934;&#30830;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14088v1 Announce Type: cross  Abstract: The conformational landscape of proteins is crucial to understanding their functionality in complex biological processes. Traditional physics-based computational methods, such as molecular dynamics (MD) simulations, suffer from rare event sampling and long equilibration time problems, hindering their applications in general protein systems. Recently, deep generative modeling techniques, especially diffusion models, have been employed to generate novel protein conformations. However, existing score-based diffusion methods cannot properly incorporate important physical prior knowledge to guide the generation process, causing large deviations in the sampled protein conformations from the equilibrium distribution. In this paper, to overcome these limitations, we propose a force-guided SE(3) diffusion model, ConfDiff, for protein conformation generation. By incorporating a force-guided network with a mixture of data-based score models, Conf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12486</link><description>&lt;p&gt;
&#22522;&#20110;NTK&#24341;&#23548;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NTK-Guided Few-Shot Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;NTK&#23545;FSCIL&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#33268;&#21147;&#20110;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#23454;&#29616;&#21331;&#36234;&#27867;&#21270;&#65292;&#36890;&#36807;&#20248;&#21270;NTK&#25910;&#25947;&#21644;&#38477;&#20302;&#27867;&#21270;&#35823;&#24046;&#26469;&#30830;&#20445;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21453;&#36951;&#24536;FSCIL&#23398;&#20064;&#32773;&#22312;&#22686;&#37327;&#20250;&#35805;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20182;&#20204;&#24448;&#24448;&#26356;&#27880;&#37325;&#20943;&#23569;&#30693;&#35782;&#27969;&#22833;&#65292;&#32780;&#24573;&#35270;&#20102;&#27169;&#22411;&#28508;&#22312;&#33719;&#21462;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#35270;&#35282;&#28145;&#20837;&#25506;&#35752;&#20102;FSCIL&#27169;&#22411;&#27867;&#21270;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#35774;&#35745;&#37325;&#28857;&#22312;&#20110;&#30830;&#20445;&#26368;&#20248;NTK&#25910;&#25947;&#21644;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#20316;&#20026;&#21331;&#36234;&#27867;&#21270;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#30340;NTK&#25910;&#25947;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26893;&#26681;&#20110;&#25968;&#23398;&#21407;&#29702;&#30340;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#25351;&#23548;&#25193;&#23637;&#32593;&#32476;&#20869;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;NTK&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#25105;&#20204;&#20174;&#22522;&#30784;&#23618;&#38754;&#24320;&#22987;&#65292;&#20248;&#21270;&#26500;&#25104;&#20854;&#27867;&#21270;&#25439;&#22833;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#22522;&#30784;&#20250;&#35805;&#19978;&#21551;&#21160;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26469;&#22609;&#36896;&#21021;&#22987;ne
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12486v1 Announce Type: cross  Abstract: While anti-amnesia FSCIL learners often excel in incremental sessions, they tend to prioritize mitigating knowledge attrition over harnessing the model's potential for knowledge acquisition. In this paper, we delve into the foundations of model generalization in FSCIL through the lens of the Neural Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal NTK convergence and NTK-related generalization error, serving as the theoretical bedrock for exceptional generalization. To attain globally optimal NTK convergence, we employ a meta-learning mechanism grounded in mathematical principles to guide the optimization process within an expanded network. Furthermore, to reduce the NTK-related generalization error, we commence from the foundational level, optimizing the relevant factors constituting its generalization loss. Specifically, we initiate self-supervised pre-training on the base session to shape the initial ne
&lt;/p&gt;</description></item><item><title>MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.08040</link><description>&lt;p&gt;
MicroT&#65306;&#29992;&#20110;MCUs&#30340;&#20302;&#33021;&#32791;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MicroT: Low-Energy and Adaptive Models for MCUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08040
&lt;/p&gt;
&lt;p&gt;
MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MicroT&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;MCUs&#30340;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#27169;&#22411;&#21010;&#20998;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#36890;&#36807;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#32852;&#21512;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#37096;&#20998;&#27169;&#22411;&#21644;&#23436;&#25972;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22312;MCUs&#19978;&#65292;&#22686;&#21152;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25191;&#34892;&#20851;&#33410;&#25512;&#29702;&#30340;&#38454;&#27573;&#20915;&#31574;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#37096;&#20998;&#27169;&#22411;&#26368;&#21021;&#22788;&#29702;&#26679;&#26412;&#65292;&#22914;&#26524;&#32622;&#20449;&#24230;&#24471;&#20998;&#20302;&#20110;&#35774;&#23450;&#30340;&#38408;&#20540;&#65292;&#23436;&#25972;&#27169;&#22411;&#23558;&#24674;&#22797;&#24182;&#32487;&#32493;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#12289;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;MCU&#26495;&#19978;&#35780;&#20272;&#20102;MicroT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#26412;&#22320;&#20219;&#21153;&#26102;&#65292;MicroT&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#33021;&#32791;&#12290;&#19982;&#26410;&#32463;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30456;&#27604;&#65292;MicroT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07262</link><description>&lt;p&gt;
&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advantage-Aware Policy Optimization for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07262
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#34892;&#20026;&#31574;&#30053;&#25910;&#38598;&#30340;&#32422;&#26463;&#20914;&#31361;&#38382;&#39064;&#65292;&#26377;&#25928;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33268;&#21147;&#20110;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#21046;&#23450;&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#32447;&#20132;&#20114;&#65292;&#36890;&#36807;&#22312;&#34892;&#20026;&#31574;&#30053;&#30340;&#25903;&#25345;&#19979;&#26045;&#21152;&#36866;&#24403;&#30340;&#20445;&#23432;&#32422;&#26463;&#26469;&#35299;&#20915;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#20248;&#21183;&#30340;&#31574;&#30053;&#20248;&#21270;&#65288;A2PO&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#26500;&#24314;&#38024;&#23545;&#28151;&#21512;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#31163;&#32447;&#23398;&#20064;&#20248;&#21183;&#24863;&#30693;&#31574;&#30053;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07262v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) endeavors to leverage offline datasets to craft effective agent policy without online interaction, which imposes proper conservative constraints with the support of behavior policies to tackle the Out-Of-Distribution (OOD) problem. However, existing works often suffer from the constraint conflict issue when offline datasets are collected from multiple behavior policies, i.e., different behavior policies may exhibit inconsistent actions with distinct returns across the state space. To remedy this issue, recent Advantage-Weighted (AW) methods prioritize samples with high advantage values for agent training while inevitably leading to overfitting on these samples. In this paper, we introduce a novel Advantage-Aware Policy Optimization (A2PO) method to explicitly construct advantage-aware policy constraints for offline learning under mixed-quality datasets. Specifically, A2PO employs a Conditional Variat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.05743</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#36816;&#33829;&#24212;&#29992;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Probabilistic Forecasting with Applications in Market Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#32534;&#30721;&#22120;&#21644;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#65292;&#36866;&#29992;&#20110;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#30340;&#39640;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28304;&#33258;&#20110;&#38750;&#21442;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;Wiener-Kallianpur&#21019;&#26032;&#34920;&#31034;&#12290;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#24335;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#26550;&#26500;&#21253;&#25324;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#38750;&#21442;&#25968;&#22810;&#21464;&#37327;&#38543;&#26426;&#36807;&#31243;&#36716;&#21270;&#20026;&#35268;&#33539;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#20174;&#20013;&#26681;&#25454;&#36807;&#21435;&#26679;&#26412;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#65292;&#26465;&#20214;&#26159;&#23427;&#20204;&#30340;&#27010;&#29575;&#20998;&#24067;&#21462;&#20915;&#20110;&#36807;&#21435;&#26679;&#26412;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#28508;&#22312;&#36807;&#31243;&#38480;&#21046;&#20026;&#20855;&#26377;&#21305;&#37197;&#33258;&#32534;&#30721;&#22120;&#36755;&#20837;-&#36755;&#20986;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#29420;&#31435;&#21516;&#20998;&#24067;&#24207;&#21015;&#12290;&#24314;&#31435;&#20102;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#24335;&#39044;&#27979;&#26041;&#27861;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#21644;&#32467;&#26500;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#26102;&#24066;&#22330;&#36816;&#33829;&#20013;&#28041;&#21450;&#39640;&#24230;&#21160;&#24577;&#21644;&#27874;&#21160;&#26102;&#38388;&#24207;&#21015;&#30340;&#19977;&#20010;&#24212;&#29992;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03175</link><description>&lt;p&gt;
The Matrix: &#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Matrix: A Bayesian learning model for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;LLM&#30340;&#20248;&#21270;&#25351;&#26631;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#20197;&#27492;&#21407;&#21017;&#20026;&#22522;&#30784;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#30001;&#20808;&#39564;&#21644;&#22810;&#39033;&#24335;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#34920;&#31034;&#30340;&#29702;&#24819;&#29983;&#25104;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;LLMs&#22914;&#20309;&#36924;&#36817;&#35813;&#30697;&#38453;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23884;&#20837;&#21644;&#22810;&#39033;&#24335;&#20998;&#24067;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;Dirichlet&#36924;&#36817;&#23450;&#29702;&#26469;&#36924;&#36817;&#20219;&#20309;&#20808;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#22914;&#20309;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#21407;&#29702;&#19968;&#33268;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#20855;&#20307;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#26356;&#22823;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35270;&#20026;&#38656;&#35201;&#26356;&#26032;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#30340;&#34892;&#20026;&#19982;&#36125;&#21494;&#26031;&#23398;&#20064;&#19968;&#33268;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights
&lt;/p&gt;</description></item><item><title>HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01767</link><description>&lt;p&gt;
HiQA&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#30340;&#20998;&#23618;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;RAG&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01767
&lt;/p&gt;
&lt;p&gt;
HiQA&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#26694;&#26550;&#65292;&#20351;&#29992;&#20998;&#23618;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#21644;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25991;&#26723;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#36805;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#34917;&#20805;&#25991;&#26723;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#23398;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#36825;&#31181;&#36827;&#27493;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#31572;&#36136;&#37327;&#65292;&#24182;&#20943;&#36731;&#20102;&#24187;&#35273;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#22823;&#37327;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#32034;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26377;&#38480;&#65292;&#32473;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#36827;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#65288;MDQA&#65289;&#26694;&#26550;&#65292;&#23558;&#32423;&#32852;&#30340;&#20803;&#25968;&#25454;&#25972;&#21512;&#21040;&#20869;&#23481;&#20013;&#65292;&#21516;&#26102;&#20855;&#22791;&#22810;&#36335;&#24452;&#26816;&#32034;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;MasQA&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#30740;&#31350;MDQA&#12290;&#26368;&#21518;&#65292;HiQA&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language model agents leveraging external tools rapidly evolve, significant progress has been made in question-answering(QA) methodologies utilizing supplementary documents and the Retrieval-Augmented Generation (RAG) approach. This advancement has improved the response quality of language models and alleviates the appearance of hallucination. However, these methods exhibit limited retrieval accuracy when faced with massive indistinguishable documents, presenting notable challenges in their practical application. In response to these emerging challenges, we present HiQA, an advanced framework for multi-document question-answering (MDQA) that integrates cascading metadata into content as well as a multi-route retrieval mechanism. We also release a benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA demonstrates the state-of-the-art performance in multi-document environments.
&lt;/p&gt;</description></item><item><title>IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17612</link><description>&lt;p&gt;
IGCN&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17612
&lt;/p&gt;
&lt;p&gt;
IGCN&#26159;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#29992;&#20110;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32508;&#21512;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#23545;&#20110;&#28041;&#21450;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#26576;&#20123;&#25968;&#25454;&#27169;&#24577;&#22312;&#39044;&#27979;&#19968;&#20010;&#31867;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#21487;&#33021;&#22312;&#39044;&#27979;&#19981;&#21516;&#31867;&#21035;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#38656;&#35201;&#20808;&#36827;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#32508;&#21512;&#24037;&#20855;&#32570;&#20047;&#23545;&#20854;&#29305;&#23450;&#39044;&#27979;&#32972;&#21518;&#21407;&#29702;&#30340;&#20840;&#38754;&#21644;&#36830;&#36143;&#29702;&#35299;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#32593;&#32476;&#30340;&#32508;&#21512;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21517;&#20026;&#32508;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;IGCN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Graph Neural Networks (GNN) have led to a considerable growth in graph data modeling for multi-modal data which contains various types of nodes and edges. Although some integrative prediction solutions have been developed recently for network-structured data, these methods have some restrictions. For a node classification task involving multi-modal data, certain data modalities may perform better when predicting one class, while others might excel in predicting a different class. Thus, to obtain a better learning representation, advanced computational methodologies are required for the integrative analysis of multi-modal data. Moreover, existing integrative tools lack a comprehensive and cohesive understanding of the rationale behind their specific predictions, making them unsuitable for enhancing model interpretability. Addressing these restrictions, we introduce a novel integrative neural network approach for multi-modal data networks, named Integrative Graph Convo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.16327</link><description>&lt;p&gt;
PICL: &#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
PICL: Physics Informed Contrastive Learning for Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26367;&#20195;&#27169;&#22411;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#32780;&#19981;&#26159;&#20989;&#25968;&#26412;&#36523;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#24555;&#36895;&#20934;&#30830;&#22320;&#27714;&#35299;&#22797;&#26434;&#30340;PDE&#12290;&#23613;&#31649;&#22312;&#24191;&#27867;&#30340;&#20195;&#29702;&#24314;&#27169;&#20219;&#21153;&#20013;&#23545;&#31070;&#32463;&#31639;&#23376;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#26159;&#36880;&#20010;&#26041;&#31243;&#35780;&#20272;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#25511;&#21046;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25511;&#21046;&#26041;&#31243;&#31995;&#25968;&#29992;&#20110;&#34913;&#37327;&#31995;&#32479;&#20043;&#38388;&#30340;&#30495;&#23454;&#30456;&#20284;&#24615;&#12290;&#29289;&#29702;&#20449;&#24687;&#31995;&#32479;&#28436;&#21270;&#21644;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#32467;&#21512;&#34987;&#38170;&#23450;&#21040;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#24182;&#29992;&#20110;&#25105;&#20204;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.09340</link><description>&lt;p&gt;
SceneVerse&#65306;&#20026;&#22522;&#20110;&#22330;&#26223;&#30340;&#22330;&#26223;&#29702;&#35299;&#25193;&#23637;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#20197;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#38754;&#20020;&#30340;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#65292;&#21363;&#23558;&#35821;&#35328;&#19982;3D&#29289;&#29702;&#29615;&#22659;&#23545;&#40784;&#65292;&#26159;&#21457;&#23637;&#20855;&#36523;&#20307;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#30340;&#22522;&#30707;&#12290;&#19982;2D&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#30456;&#27604;&#65292;&#23558;&#35821;&#35328;&#19982;3D&#22330;&#26223;&#23545;&#40784;&#38754;&#20020;&#30528;&#20960;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;3D&#22330;&#26223;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#22810;&#26679;&#30340;&#29289;&#20307;&#37197;&#32622;&#12289;&#20016;&#23500;&#30340;&#23646;&#24615;&#21644;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65307;&#65288;ii&#65289;&#25903;&#25345;&#22522;&#20110;&#22330;&#26223;&#23398;&#20064;&#30340;&#37197;&#23545;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65307;&#20197;&#21450;&#65288;iii&#65289;&#32570;&#20047;&#20174;&#22522;&#20110;&#22330;&#26223;&#30340;3D&#25968;&#25454;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#32479;&#19968;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#25193;&#23637;&#23460;&#20869;&#29615;&#22659;&#20013;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20174;&#32780;&#35299;&#20915;3D&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#30340;&#36825;&#19977;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#20171;&#32461;&#39318;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;3D&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;SceneVerse&#65292;&#21253;&#21547;&#32422;68K&#20010;3D&#23460;&#20869;&#22330;&#26223;&#65292;&#21253;&#25324;250&#19975;&#20010;&#35270;&#35273;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09340v2 Announce Type: replace-cross  Abstract: 3D vision-language grounding, which focuses on aligning language with the 3D physical environment, stands as a cornerstone in the development of embodied agents. In comparison to recent advancements in the 2D domain, grounding language in 3D scenes faces several significant challenges: (i) the inherent complexity of 3D scenes due to the diverse object configurations, their rich attributes, and intricate relationships; (ii) the scarcity of paired 3D vision-language data to support grounded learning; and (iii) the absence of a unified learning framework to distill knowledge from grounded 3D data. In this work, we aim to address these three major challenges in 3D vision-language by examining the potential of systematically upscaling 3D vision-language learning in indoor environments. We introduce the first million-scale 3D vision-language dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising 2.5M vision-langu
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2306.01990</link><description>&lt;p&gt;
&#36890;&#36807;&#32447;&#24615;&#19978;&#19979;&#25991;&#21644;&#32452;&#21512;&#21160;&#20316;&#28608;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Exploration with Linear Contexts and Combinatorial Actions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01990
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#20013;&#36890;&#36807;&#32447;&#24615;&#36172;&#21338;&#26426;&#27169;&#22411;&#26367;&#20195;&#20808;&#39564;&#29420;&#31435;&#24615;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#28608;&#21169;&#25506;&#32034;&#25928;&#29575;&#21644;&#26368;&#20248;&#36951;&#25022;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#20851;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#36827;&#20102;&#28608;&#21169;&#24335;&#36172;&#21338;&#26426;&#25506;&#32034;&#30340;&#30740;&#31350;&#65292;&#20854;&#20013;&#25163;&#33218;&#36873;&#25321;&#34987;&#35270;&#20026;&#25512;&#33616;&#65292;&#24182;&#19988;&#35201;&#27714;&#26159;&#36125;&#21494;&#26031;&#28608;&#21169;&#20860;&#23481;&#30340;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#28385;&#36275;&#19968;&#23450;&#29420;&#31435;&#24615;&#20551;&#35774;&#21518;&#65292;&#32463;&#36807;&#36275;&#22815;&#30340;&#21021;&#22987;&#26679;&#26412;&#25910;&#38598;&#65292;&#27969;&#34892;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#21464;&#24471;&#28608;&#21169;&#20860;&#23481;&#12290;&#25105;&#20204;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#25552;&#20379;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#31867;&#27604;&#65292;&#20854;&#20013;&#20808;&#39564;&#30340;&#29420;&#31435;&#24615;&#34987;&#33258;&#28982;&#30340;&#20984;&#24615;&#26465;&#20214;&#21462;&#20195;&#12290;&#36825;&#25171;&#24320;&#20102;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#21644;&#36951;&#25022;&#26368;&#20248;&#30340;&#28608;&#21169;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#21322;&#36172;&#21338;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29992;&#20110;&#21021;&#22987;&#25968;&#25454;&#25910;&#38598;&#30340;&#21069;&#27748;&#26222;&#26862;&#25277;&#26679;&#38454;&#27573;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01990v2 Announce Type: replace-cross  Abstract: We advance the study of incentivized bandit exploration, in which arm choices are viewed as recommendations and are required to be Bayesian incentive compatible. Recent work has shown under certain independence assumptions that after collecting enough initial samples, the popular Thompson sampling algorithm becomes incentive compatible. We give an analog of this result for linear bandits, where the independence of the prior is replaced by a natural convexity condition. This opens up the possibility of efficient and regret-optimal incentivized exploration in high-dimensional action spaces. In the semibandit model, we also improve the sample complexity for the pre-Thompson sampling phase of initial data collection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.11798</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#20132;&#36890;&#39044;&#27979;&#19978;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation on Spatial-Temporal Graph Convolutional Network for Traffic Prediction. (arXiv:2401.11798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#24212;&#29992;&#31354;&#38388;-&#26102;&#38388;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#24605;&#24819;&#33021;&#22815;&#23454;&#29616;&#22312;&#20943;&#23569;&#21442;&#25968;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#25191;&#34892;&#25928;&#29575;&#12290;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#23398;&#29983;&#32593;&#32476;&#23398;&#20064;&#21040;&#22797;&#26434;&#30340;&#20132;&#36890;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#23454;&#26102;&#20132;&#36890;&#39044;&#27979;&#23545;&#20943;&#23569;&#20132;&#36890;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#39044;&#27979;&#20132;&#36890;&#29366;&#20917;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;ST-GNN&#65289;&#23558;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#24314;&#27169;&#20026;&#26102;&#38388;&#22270;&#12290;&#23613;&#31649;ST-GNN&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#20026;&#23454;&#38469;&#20132;&#36890;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#23454;&#26102;&#39044;&#27979;&#26102;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;&#23454;&#26102;&#25968;&#25454;&#21160;&#24577;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;ST-GNN&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25104;&#26412;&#20989;&#25968;&#65292;&#26088;&#22312;&#20351;&#29992;&#22797;&#26434;&#32593;&#32476;&#65288;&#25945;&#24072;&#65289;&#30340;&#33976;&#39311;&#25968;&#25454;&#26469;&#35757;&#32451;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#32593;&#32476;&#65288;&#23398;&#29983;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20934;&#30830;&#24615;&#25509;&#36817;&#25945;&#24072;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#23558;&#25945;&#24072;&#32593;&#32476;&#30340;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#34701;&#20837;&#23398;&#29983;&#32593;&#32476;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#23398;&#20064;&#21040;&#25945;&#24072;&#24863;&#30693;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#38754;&#20020;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient real-time traffic prediction is crucial for reducing transportation time. To predict traffic conditions, we employ a spatio-temporal graph neural network (ST-GNN) to model our real-time traffic data as temporal graphs. Despite its capabilities, it often encounters challenges in delivering efficient real-time predictions for real-world traffic data. Recognizing the significance of timely prediction due to the dynamic nature of real-time data, we employ knowledge distillation (KD) as a solution to enhance the execution time of ST-GNNs for traffic prediction. In this paper, We introduce a cost function designed to train a network with fewer parameters (the student) using distilled data from a complex network (the teacher) while maintaining its accuracy close to that of the teacher. We use knowledge distillation, incorporating spatial-temporal correlations from the teacher network to enable the student to learn the complex patterns perceived by the teacher. However, a challenge a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12858</link><description>&lt;p&gt;
&#38750;&#21018;&#24615;&#25991;&#26412;&#25552;&#31034;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#32534;&#36753;&#27969;&#31243;&#33021;&#22815;&#21019;&#24314;&#19982;&#36755;&#20837;&#38899;&#39057;&#20445;&#25345;&#19968;&#33268;&#30340;&#38899;&#39057;&#32534;&#36753;&#32467;&#26524;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33021;&#22815;&#36827;&#34892;&#28155;&#21152;&#12289;&#39118;&#26684;&#36716;&#25442;&#21644;&#20462;&#22797;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#36825;&#20123;&#32534;&#36753;&#33021;&#22815;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#25991;&#26412;&#25552;&#31034;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;Audio-LDM&#30340;&#32467;&#26524;&#12290;&#23545;&#32467;&#26524;&#30340;&#23450;&#24615;&#26816;&#26597;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#21152;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#21407;&#22987;&#36215;&#22987;&#21644;&#32467;&#26463;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12000</link><description>&lt;p&gt;
Vecchia-Laplace&#36817;&#20284;&#27861;&#22312;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#26159;&#28789;&#27963;&#30340;&#27010;&#29575;&#38750;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#12290;Vecchia&#36817;&#20284;&#26159;&#29992;&#20110;&#20811;&#26381;&#22823;&#25968;&#25454;&#35745;&#31639;&#29942;&#39048;&#30340;&#20934;&#30830;&#36817;&#20284;&#26041;&#27861;&#65292;Laplace&#36817;&#20284;&#26159;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#38750;&#39640;&#26031;&#20284;&#28982;&#20989;&#25968;&#30340;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#30452;&#25509;&#27714;&#35299;&#26041;&#27861;&#65288;&#22914;Cholesky&#20998;&#35299;&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Vecchia-Laplace&#36817;&#20284;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#36229;&#32447;&#24615;&#22320;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#19982;Vecchia-Laplace&#36817;&#20284;&#35745;&#31639;&#30456;&#20851;&#30340;&#36816;&#31639;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#26159;&#26368;&#20934;&#30830;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20250;&#21464;&#24471;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;Vecchia-Laplace&#36817;&#20284;&#25512;&#26029;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Cholesky&#30340;&#35745;&#31639;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2309.00380</link><description>&lt;p&gt;
&#29992;&#25490;&#24207;&#19981;&#21464;&#30340;&#32534;&#30721;&#22120;&#21644;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#23398;&#20064;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (VAE) &#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#23427;&#23398;&#20064;&#33021;&#22815;&#20849;&#21516;&#35299;&#37322;&#22810;&#31181;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#21508;&#31181;&#23458;&#35266;&#20989;&#25968;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#24448;&#24448;&#20197;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#19979;&#30028;&#20197;&#21450;&#20449;&#24687;&#35770;&#26041;&#38754;&#30340;&#32771;&#34385;&#20026;&#21160;&#26426;&#12290;&#20026;&#20102;&#23545;&#19981;&#21516;&#27169;&#24577;&#23376;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#25105;&#20204;&#32463;&#24120;&#20351;&#29992;&#24182;&#23637;&#31034;&#20102;&#20135;&#21697;&#22411;&#19987;&#23478; (PoE) &#25110;&#32773;&#28151;&#21512;&#22411;&#19987;&#23478; (MoE) &#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#29983;&#25104;&#36136;&#37327;&#25110;&#32773;&#22810;&#27169;&#24577;&#19968;&#33268;&#24615;&#31561;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#32534;&#30721;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#25512;&#24191;&#20102; PoE &#25110;&#32773; MoE &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#65292;&#23545;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#20449;&#21495;&#26032;&#29289;&#29702;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#35777;&#26126;&#20102;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.12955</link><description>&lt;p&gt;
&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#35302;&#21457;&#26263;&#28107;&#28020;
&lt;/p&gt;
&lt;p&gt;
Triggering Dark Showers with Conditional Dual Auto-Encoders. (arXiv:2306.12955v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12955
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#21452;&#33258;&#32534;&#30721;&#22120;&#65292;&#23545;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#20449;&#21495;&#26032;&#29289;&#29702;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#35777;&#26126;&#20102;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;(AEs)&#26377;&#28508;&#21147;&#25104;&#20026;&#23545;&#25758;&#26426;&#26032;&#29289;&#29702;&#25628;&#32034;&#30340;&#26377;&#25928;&#21644;&#36890;&#29992;&#24037;&#20855;&#65292;&#38656;&#35201;&#24456;&#23569;&#25110;&#19981;&#38656;&#35201;&#27169;&#22411;&#20381;&#36182;&#30340;&#20551;&#35774;&#12290;&#26032;&#30340;&#29702;&#35770;&#29289;&#29702;&#20449;&#21495;&#21487;&#20197;&#20316;&#20026;&#19982;&#36890;&#24120;&#26399;&#26395;&#29992;&#26469;&#25551;&#36848;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#24050;&#30693;&#32972;&#26223;&#36807;&#31243;&#20559;&#31163;&#30340;&#24322;&#24120;&#20540;&#26469;&#32771;&#34385;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;AE&#23450;&#20041;&#21028;&#23450;&#20107;&#20214;&#29289;&#29702;&#26412;&#36136;&#30340;&#20934;&#21017;&#30340;&#24322;&#24120;&#26816;&#27979;(AD)&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#25506;&#27979;&#22120;&#22270;&#20687;&#36827;&#34892;AD&#25628;&#32034;&#65292;&#23545;&#20110;&#26080;&#27861;&#21033;&#29992;&#20219;&#20309;&#22522;&#20110;&#29289;&#29702;&#30340;&#39044;&#22788;&#29702;&#25110;&#23545;&#20449;&#21495;&#30340;&#20551;&#35774;&#30340;&#22823;&#32780;&#31232;&#30095;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#26263;&#29256;&#24378;&#21147;&#30340;&#34920;&#29616;&#24418;&#24335;&#30340;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36890;&#36807;&#26465;&#20214;&#23398;&#20064;&#32039;&#20945;&#28508;&#22312;&#31354;&#38388;&#30340;&#21452;&#32534;&#30721;&#22120;&#35774;&#35745;&#12290;&#22312;&#22810;&#20010;AD&#25351;&#26631;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#21644;&#20808;&#21069;&#26041;&#27861;&#30340;&#26126;&#26174;&#25913;&#36827;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23637;&#31034;AE&#20855;&#26377;&#20986;&#33394;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-encoders (AEs) have the potential to be effective and generic tools for new physics searches at colliders, requiring little to no model-dependent assumptions. New hypothetical physics signals can be considered anomalies that deviate from the well-known background processes generally expected to describe the whole dataset. We present a search formulated as an anomaly detection (AD) problem, using an AE to define a criterion to decide about the physics nature of an event. In this work, we perform an AD search for manifestations of a dark version of strong force using raw detector images, which are large and very sparse, without leveraging any physics-based pre-processing or assumption on the signals. We propose a dual-encoder design which can learn a compact latent space through conditioning. In the context of multiple AD metrics, we present a clear improvement over competitive baselines and prior approaches. It is the first time that an AE is shown to exhibit excellent discriminati
&lt;/p&gt;</description></item><item><title>OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09301</link><description>&lt;p&gt;
OpenOOD v1.5&#65306;&#22686;&#24378;&#30340;OCC&#65288;Out-of-Distribution Detection&#65289;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09301
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCC&#26816;&#27979;&#23545;&#20110;&#24320;&#25918;&#19990;&#30028;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;OCC&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35780;&#20272;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenOOD v1.5&#65292;&#36825;&#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#30830;&#20445;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#12289;&#26631;&#20934;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenOOD v1.5&#23558;&#20854;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20016;&#23500;&#20102;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04423</link><description>&lt;p&gt;
&#35745;&#31639;&#26368;&#20248;&#26641;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Computing Optimal Tree Ensembles. (arXiv:2306.04423v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#20197;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#38598;&#21512;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#8220;&#35777;&#26126;&#26641;&#25216;&#26415;&#8221;&#26469;&#22823;&#22823;&#25913;&#36827;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#21644;&#20915;&#31574;&#26641;&#38598;&#21512;&#26159;&#20998;&#31867;&#21644;&#22238;&#24402;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#36827;&#23637;&#20801;&#35768;&#35745;&#31639;&#26088;&#22312;&#21508;&#31181;&#24230;&#37327;&#26041;&#38754;&#65288;&#22914;&#22823;&#23567;&#25110;&#28145;&#24230;&#65289;&#26368;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#25105;&#20204;&#19981;&#30693;&#36947;&#26377;&#20851;&#26641;&#38598;&#21512;&#30340;&#27492;&#31867;&#30740;&#31350;&#65292;&#24182;&#26088;&#22312;&#20026;&#35813;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#12290;&#20027;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#19979;&#38480;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33021;&#22815;&#36716;&#31227;&#21644;&#22823;&#22823;&#25913;&#36827;&#20915;&#31574;&#26641;&#30340;&#21487;&#22788;&#29702;&#24615;&#32467;&#26524;&#65292;&#33719;&#24471;&#19968;&#20010; $(6\delta D S)^S \cdot poly$-time &#31639;&#27861;&#65292;&#20854;&#20013; $S$ &#26159;&#26641;&#38598;&#21512;&#20013;&#21106;&#25968;&#65292;$D$ &#26159;&#26368;&#22823;&#22495;&#22823;&#23567;&#65292;$\delta$ &#26159;&#20004;&#20010;&#31034;&#20363;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#29305;&#24449;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35777;&#26126;&#26641;&#25216;&#26415;&#65292;&#36825;&#20284;&#20046;&#23545;&#23454;&#36341;&#20063;&#24456;&#26377;&#21069;&#36884;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#21160;&#24577;&#35268;&#21010;&#23545;&#20110;&#20915;&#31574;&#26641;&#24050;&#32463;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#32780;&#23545;&#20110;&#26641;&#38598;&#21512;&#20063;&#21487;&#33021;&#26159;&#21487;&#34892;&#30340;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010; $\ell^n \cdot poly$-t&#12290;
&lt;/p&gt;
&lt;p&gt;
Random forests and, more generally, (decision\nobreakdash-)tree ensembles are widely used methods for classification and regression. Recent algorithmic advances allow to compute decision trees that are optimal for various measures such as their size or depth. We are not aware of such research for tree ensembles and aim to contribute to this area. Mainly, we provide two novel algorithms and corresponding lower bounds. First, we are able to carry over and substantially improve on tractability results for decision trees, obtaining a $(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest number of features in which two examples differ. To achieve this, we introduce the witness-tree technique which also seems promising for practice. Second, we show that dynamic programming, which has been successful for decision trees, may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-t
&lt;/p&gt;</description></item><item><title>GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13681</link><description>&lt;p&gt;
GUARD: &#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13681
&lt;/p&gt;
&lt;p&gt;
GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#26159;&#30446;&#21069;&#24191;&#27867;&#36941;&#24067;&#19988;&#21253;&#21547;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#30340;&#19968;&#31449;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20840;&#38754;&#28085;&#30422;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#38169;&#30340;&#24615;&#36136;&#65292;&#23558;RL&#31639;&#27861;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#29616;&#23454;&#24212;&#29992;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#20154;&#26426;&#20132;&#20114;&#12289;&#26426;&#22120;&#20154;&#25805;&#20316;&#31561;&#65289;&#36890;&#24120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26368;&#36817;&#65292;&#23433;&#20840;RL&#65288;&#21363;&#32422;&#26463;RL&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#36805;&#36895;&#20986;&#29616;&#65292;&#20854;&#20013;&#20195;&#29702;&#22312;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#65292;&#25506;&#32034;&#29615;&#22659;&#12290;&#30001;&#20110;&#31639;&#27861;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#27604;&#36739;&#29616;&#26377;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GUARD&#65292;&#19968;&#20010;&#24191;&#20041;&#32479;&#19968;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;GUARD&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;GUARD&#26159;&#19968;&#20010;&#24191;&#20041;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;&#65292;&#20855;&#26377;&#21508;&#31181;RL&#20195;&#29702;&#12289;&#20219;&#21153;&#21644;&#23433;&#20840;&#32422;&#26463;&#35268;&#33539;&#12290;&#20854;&#27425;&#65292;GUARD&#20840;&#38754;&#28085;&#30422;&#20102;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#33258;&#21253;&#21547;&#30340;&#23454;&#29616;&#12290;&#31532;&#19977;&#65292;GUARD&#22312;&#20219;&#21153;&#21644;&#31639;&#27861;&#26041;&#38754;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#33258;&#23450;&#20041;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29366;&#24577;&#19979;&#29616;&#26377;&#26041;&#27861;&#22312;GUARD&#19978;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13940</link><description>&lt;p&gt;
1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#30340;&#20027;&#23548;-&#26368;&#23567;&#21270;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Majorization-Minimization Gauss-Newton Method for 1-Bit Matrix Completion. (arXiv:2304.13940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#20013;&#65292;&#26088;&#22312;&#20174;&#37096;&#20998;&#20108;&#36827;&#21046;&#35266;&#27979;&#20540;&#20013;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MMGN&#30340;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#65288;MM&#65289;&#21407;&#21017;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#20135;&#29983;&#19968;&#31995;&#21015;&#26631;&#20934;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#24378;&#21046;&#20551;&#23450;&#30340;&#20302;&#31209;&#32467;&#26500;&#30340;&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#21644;&#23545;&#23454;&#38469;&#25968;&#25454;&#30340;&#24212;&#29992;&#34920;&#26126;&#65292;MMGN&#36755;&#20986;&#30340;&#20272;&#35745;&#32467;&#26524;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#36739;&#20855;&#26377;&#21487;&#27604;&#24615;&#19988;&#26356;&#20934;&#30830;&#12289;&#36895;&#24230;&#36890;&#24120;&#26356;&#24555;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#30697;&#38453;&#30340;&#23574;&#38160;&#24230;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called MMGN. Our method is based on the majorization-minimization (MM) principle, which yields a sequence of standard low-rank matrix completion problems in our setting. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Our numerical studies and application to a real-data example illustrate that MMGN outputs comparable if not more accurate estimates, is often significantly faster, and is less sensitive to the spikiness of the underlying matrix than existing methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03376</link><description>&lt;p&gt;
&#31070;&#32463;&#32676;&#20307;&#21160;&#24577;&#21644;&#20960;&#20309;&#30340;&#21487;&#35299;&#37322;&#32479;&#35745;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretable statistical representations of neural population dynamics and geometry. (arXiv:2304.03376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#20998;&#24067;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#21033;&#29992;&#35813;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#23884;&#20837;&#65292;&#22312;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#31070;&#32463;&#20803;&#32676;&#20307;&#30340;&#21160;&#24577;&#36890;&#24120;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#28436;&#21270;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20960;&#20309;&#21644;&#21160;&#24577;&#23545;&#32534;&#30721;&#30456;&#20851;&#34892;&#20026;&#21464;&#37327;&#30340;&#36129;&#29486;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#30456;&#36712;&#29305;&#24449;&#30340;&#32479;&#35745;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#20960;&#20309;&#24863;&#30693;&#25110;&#20960;&#20309;&#26080;&#24863;&#30693;&#34920;&#31034;&#65292;&#20197;&#23545;&#24050;&#27979;&#37327;&#36712;&#36857;&#36827;&#34892;&#26080;&#20559;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32479;&#35745;&#34920;&#31034;&#21487;&#20197;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#23454;&#20363;&#36827;&#34892;&#25512;&#24191;&#65292;&#20197;&#21306;&#20998;&#35745;&#31639;&#26426;&#21046;&#65292;&#22312;&#20855;&#26377;&#20960;&#20309;&#23545;&#24212;&#30340;&#28789;&#38271;&#31867;&#20284;&#20219;&#21153;&#20013;&#35299;&#37322;&#23884;&#20837;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#24182;&#24320;&#21457;&#20855;&#26377;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20351;&#29992;&#20869;&#22312;&#27969;&#24418;&#32467;&#26500;&#20248;&#20110;&#26102;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamics of neuron populations during diverse tasks often evolve on low-dimensional manifolds. However, it remains challenging to discern the contributions of geometry and dynamics for encoding relevant behavioural variables. Here, we introduce an unsupervised geometric deep learning framework for representing non-linear dynamical systems based on statistical distributions of local phase portrait features. Our method provides robust geometry-aware or geometry-agnostic representations for the unbiased comparison of dynamics based on measured trajectories. We demonstrate that our statistical representation can generalise across neural network instances to discriminate computational mechanisms, obtain interpretable embeddings of neural dynamics in a primate reaching task with geometric correspondence to hand kinematics, and develop a decoding algorithm with state-of-the-art accuracy. Our results highlight the importance of using the intrinsic manifold structure over temporal informati
&lt;/p&gt;</description></item><item><title>&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.08661</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#23376;&#37319;&#26679;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Subsampling Suffices for Adaptive Data Analysis. (arXiv:2302.08661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08661
&lt;/p&gt;
&lt;p&gt;
&#23376;&#37319;&#26679;&#26159;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20165;&#38656;&#22522;&#20110;&#38543;&#26426;&#23376;&#26679;&#26412;&#21644;&#23569;&#37327;&#27604;&#29305;&#36755;&#20986;&#30340;&#26597;&#35810;&#65292;&#21363;&#21487;&#20445;&#35777;&#20195;&#34920;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#20195;&#34920;&#25972;&#20010;&#26679;&#26412;&#24635;&#20307;&#26159;&#32479;&#35745;&#23398;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#32463;&#20856;&#25216;&#26415;&#20551;&#35774;&#25968;&#25454;&#38598;&#19982;&#20998;&#26512;&#24072;&#30340;&#26597;&#35810;&#26080;&#20851;&#65292;&#24182;&#22312;&#22810;&#27425;&#12289;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#26597;&#35810;&#20013;&#22833;&#25928;&#12290;&#36825;&#20010;&#8220;&#33258;&#36866;&#24212;&#25968;&#25454;&#20998;&#26512;&#8221;&#38382;&#39064;&#22312;Dwork&#31561;&#20154;&#65288;STOC&#65292;2015&#65289;&#21644;Hardt&#21644;Ullman&#65288;FOCS&#65292;2014&#65289;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#20013;&#24471;&#21040;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#20551;&#35774;&#38598;&#65292;&#20351;&#24471;&#21363;&#20351;&#22312;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#24773;&#20917;&#19979;&#65292;&#26597;&#35810;&#20173;&#28982;&#20855;&#26377;&#20195;&#34920;&#24615;&#65306;&#21807;&#19968;&#30340;&#35201;&#27714;&#26159;&#27599;&#20010;&#26597;&#35810;&#37319;&#29992;&#38543;&#26426;&#23376;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#23569;&#37327;&#27604;&#29305;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#23376;&#37319;&#26679;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#36275;&#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#21709;&#24212;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#36825;&#31181;&#22522;&#20110;&#23376;&#37319;&#26679;&#30340;&#26694;&#26550;&#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#33021;&#22815;&#27169;&#25311;&#20043;&#21069;&#30740;&#31350;&#25152;&#26410;&#28085;&#30422;&#30340;&#21508;&#31181;&#23454;&#38469;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring that analyses performed on a dataset are representative of the entire population is one of the central problems in statistics. Most classical techniques assume that the dataset is independent of the analyst's query and break down in the common setting where a dataset is reused for multiple, adaptively chosen, queries. This problem of \emph{adaptive data analysis} was formalized in the seminal works of Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014).  We identify a remarkably simple set of assumptions under which the queries will continue to be representative even when chosen adaptively: The only requirements are that each query takes as input a random subsample and outputs few bits. This result shows that the noise inherent in subsampling is sufficient to guarantee that query responses generalize. The simplicity of this subsampling-based framework allows it to model a variety of real-world scenarios not covered by prior work.  In addition to its simplicity, we demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;DiffSBDD&#26469;&#29983;&#25104;&#20855;&#26377;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#26032;&#22411;&#33647;&#29289;&#37197;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13695</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#19982;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure-based Drug Design with Equivariant Diffusion Models. (arXiv:2210.13695v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;DiffSBDD&#26469;&#29983;&#25104;&#20855;&#26377;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#26032;&#22411;&#33647;&#29289;&#37197;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#22810;&#26679;&#21270;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#65288;SBDD&#65289;&#26088;&#22312;&#35774;&#35745;&#19982;&#39044;&#23450;&#30340;&#34507;&#30333;&#38774;&#28857;&#20855;&#26377;&#39640;&#20146;&#21644;&#21147;&#21644;&#29305;&#24322;&#24615;&#30340;&#23567;&#20998;&#23376;&#37197;&#20307;&#12290;&#26412;&#25991;&#23558;SBDD&#34920;&#36848;&#20026;&#19968;&#20010;&#19977;&#32500;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;DiffSBDD&#65292;&#36825;&#26159;&#19968;&#20010;SE(3)-&#31561;&#21464;&#30340;&#19977;&#32500;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#34507;&#30333;&#21475;&#34955;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#26032;&#22411;&#37197;&#20307;&#12290;&#20840;&#38754;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#27169;&#25311;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DiffSBDD&#22312;&#29983;&#25104;&#20855;&#26377;&#20855;&#26377;&#31454;&#20105;&#24615;&#23545;&#25509;&#24471;&#20998;&#30340;&#26032;&#39062;&#21644;&#22810;&#26679;&#30340;&#33647;&#29289;&#26679;&#37197;&#20307;&#26041;&#38754;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25193;&#25955;&#26694;&#26550;&#22312;&#33647;&#29289;&#35774;&#35745;&#27963;&#21160;&#20013;&#26356;&#24191;&#27867;&#20219;&#21153;&#30340;&#28789;&#27963;&#24615;&#65292;&#20363;&#22914;&#21363;&#25554;&#21363;&#29992;&#30340;&#24615;&#36136;&#20248;&#21270;&#21644;&#20174;&#23616;&#37096;&#20998;&#23376;&#35774;&#35745;&#24102;&#26377;&#20462;&#34917;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure-based drug design (SBDD) aims to design small-molecule ligands that bind with high affinity and specificity to pre-determined protein targets. In this paper, we formulate SBDD as a 3D-conditional generation problem and present DiffSBDD, an SE(3)-equivariant 3D-conditional diffusion model that generates novel ligands conditioned on protein pockets. Comprehensive in silico experiments demonstrate the efficiency and effectiveness of DiffSBDD in generating novel and diverse drug-like ligands with competitive docking scores. We further explore the flexibility of the diffusion framework for a broader range of tasks in drug design campaigns, such as off-the-shelf property optimization and partial molecular design with inpainting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.04829</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#23884;&#20837;&#24352;&#37327;&#31215;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#19968;&#20010;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#22810;&#36798;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#28857;&#30340;&#26679;&#26412;&#22823;&#23567;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#20943;&#36731;&#20102;RKHS&#24314;&#27169;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#20135;&#29983;&#20102;&#23450;&#20041;&#33391;&#22909;&#30340;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#23884;&#20837;&#35745;&#31639;&#36895;&#24230;&#24555;&#19988;&#36866;&#29992;&#20110;&#20174;&#39044;&#27979;&#21040;&#20998;&#31867;&#30340;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#26377;&#30410;&#30340;&#25968;&#20540;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
&lt;/p&gt;</description></item></channel></rss>