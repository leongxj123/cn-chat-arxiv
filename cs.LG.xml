<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01302</link><description>&lt;p&gt;
&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Gradient-based Clustering of Distributed Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#26799;&#24230;&#32858;&#31867;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#36890;&#36807;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#21644;&#23450;&#20041;&#32858;&#31867;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32858;&#31867;&#20219;&#21153;&#12290;&#22312;&#25552;&#20379;&#20102;&#32479;&#19968;&#20998;&#26512;&#21644;&#20960;&#20010;&#24378;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25910;&#25947;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#26063;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#32593;&#32476;&#20013;&#24037;&#20316;&#12290;&#22312;&#25552;&#20986;&#30340;&#22330;&#26223;&#20013;&#65292;&#29992;&#25143;&#21253;&#21547;&#19968;&#20010;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21482;&#19982;&#20854;&#30452;&#25509;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#30446;&#26631;&#26159;&#23547;&#25214;&#23436;&#25972;&#25968;&#25454;&#30340;&#32858;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#23478;&#26063;&#31216;&#20026;&#20998;&#24067;&#24335;&#26799;&#24230;&#32858;&#31867;&#65288;DGC-$\mathcal{F}_\rho$&#65289;&#65292;&#30001;&#21442;&#25968;&#21270;&#30340;$\rho\geq1$&#30830;&#23450;&#65292;&#25511;&#21046;&#29992;&#25143;&#20013;&#24515;&#20272;&#35745;&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#32780;$\mathcal{F}$&#30830;&#23450;&#32858;&#31867;&#25439;&#22833;&#12290;&#38024;&#23545;&#27969;&#34892;&#30340;&#32858;&#31867;&#25439;&#22833;&#22914;$K$&#22343;&#20540;&#21644;Huber&#25439;&#22833;&#65292;DGC-$\mathcal{F}_\rho$&#20135;&#29983;&#20102;&#26032;&#30340;&#20998;&#24067;&#24335;&#32858;&#31867;&#31639;&#27861;DGC-KM$_\rho$&#21644;DGC-HL$_\rho$&#65292;&#32780;&#22522;&#20110;&#36923;&#36753;&#20989;&#25968;&#30340;&#26032;&#22411;&#32858;&#31867;&#25439;&#22833;&#23548;&#33268;&#20102;DGC-LL$_\rho$&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#20998;&#26512;&#24182;&#24314;&#31435;&#20102;&#20960;&#20010;&#24378;&#32467;&#26524;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#12290;&#39318;&#20808;&#65292;&#26041;&#27861;&#29983;&#25104;&#30340;&#20013;&#24515;&#24207;&#21015;&#22312;&#20219;&#20309;&#20013;&#24515;&#21021;&#22987;&#21270;&#21644;$...
&lt;/p&gt;
&lt;p&gt;
We develop a family of distributed clustering algorithms that work over networks of users. In the proposed scenario, users contain a local dataset and communicate only with their immediate neighbours, with the aim of finding a clustering of the full, joint data. The proposed family, termed Distributed Gradient Clustering (DGC-$\mathcal{F}_\rho$), is parametrized by $\rho \geq 1$, controling the proximity of users' center estimates, with $\mathcal{F}$ determining the clustering loss. Specialized to popular clustering losses like $K$-means and Huber loss, DGC-$\mathcal{F}_\rho$ gives rise to novel distributed clustering algorithms DGC-KM$_\rho$ and DGC-HL$_\rho$, while a novel clustering loss based on the logistic function leads to DGC-LL$_\rho$. We provide a unified analysis and establish several strong results, under mild assumptions. First, the sequence of centers generated by the methods converges to a well-defined notion of fixed point, under any center initialization and value of $
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13565</link><description>&lt;p&gt;
AdaTrans&#65306;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#29305;&#24449;&#33258;&#36866;&#24212;&#19982;&#26679;&#26412;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13565
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32500;&#22238;&#24402;&#30340;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#33258;&#36866;&#24212;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;&#21644;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#39640;&#32500;&#32972;&#26223;&#19979;&#30340;&#36801;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#29305;&#24449;&#32500;&#24230;&#22823;&#20110;&#26679;&#26412;&#22823;&#23567;&#12290;&#20026;&#20102;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20449;&#24687;&#65292;&#35813;&#20449;&#24687;&#21487;&#33021;&#22312;&#29305;&#24449;&#25110;&#28304;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21644;&#32858;&#21512;&#29305;&#24449;-wise (F-AdaTrans)&#25110;&#26679;&#26412;-wise (S-AdaTrans)&#21487;&#36801;&#31227;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#34701;&#21512;&#24809;&#32602;&#26041;&#27861;&#65292;&#32467;&#21512;&#26435;&#37325;&#65292;&#21487;&#20197;&#26681;&#25454;&#21487;&#36801;&#31227;&#32467;&#26500;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#36873;&#25321;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471; F-AdaTrans &#33021;&#22815;&#36873;&#25321;&#24615;&#22320;&#23558;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#19982;&#30446;&#26631;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#28388;&#38500;&#38750;&#21487;&#36801;&#31227;&#30340;&#20449;&#21495;&#65292;S-AdaTrans&#21017;&#21487;&#20197;&#33719;&#24471;&#27599;&#20010;&#28304;&#26679;&#26412;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#36895;&#29575;&#65292;&#21487;&#20197;&#22312;&#29305;&#27530;&#24773;&#20917;&#19979;&#24674;&#22797;&#29616;&#26377;&#30340;&#36817;&#26368;&#23567;&#20284;&#20046;&#26368;&#20248;&#36895;&#29575;&#12290;&#25928;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13565v1 Announce Type: cross  Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10786</link><description>&lt;p&gt;
ContourDiff&#65306;&#24102;&#36718;&#24275;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10786
&lt;/p&gt;
&lt;p&gt;
ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#65288;&#20363;&#22914;&#20174;CT&#21040;MRI&#65289;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContourDiff&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#26131;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#20294;&#23545;&#20854;&#35299;&#21078;&#20869;&#23481;&#24418;&#25104;&#31934;&#30830;&#30340;&#31354;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#20219;&#24847;&#36755;&#20837;&#39046;&#22495;&#30340;&#22270;&#20687;&#30340;&#36718;&#24275;&#34920;&#31034;&#36716;&#25442;&#20026;&#36755;&#20986;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
&lt;/p&gt;</description></item><item><title>&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.09613</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#35757;&#32451;&#37325;&#26032;&#21796;&#37266;&#30693;&#35782;&#65306;&#20174;&#28798;&#38590;&#24615;&#24178;&#25200;&#20013;&#36827;&#34892;&#39044;&#26399;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09613
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#39044;&#26399;&#34892;&#20026;&#65292;&#33021;&#22815;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#65292;&#25581;&#31034;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25991;&#26723;&#20197;&#22266;&#23450;&#37325;&#22797;&#24207;&#21015;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#31995;&#21015;&#25991;&#26723;&#19978;&#35757;&#32451;&#26102;&#65292;&#32593;&#32476;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#24178;&#25200;&#65307;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#20381;&#27425;&#24494;&#35843;&#30340;LLMs&#34920;&#29616;&#20986;&#19968;&#31181;&#22855;&#29305;&#19988;&#21331;&#36234;&#30340;&#29305;&#24615;&#65306;&#23427;&#20204;&#34920;&#29616;&#20986;&#39044;&#26399;&#30340;&#34892;&#20026;&#65292;&#22312;&#20877;&#27425;&#36935;&#21040;&#20043;&#21069;&#30340;&#25991;&#26723;&#26102;&#20174;&#36951;&#24536;&#20013;&#24674;&#22797;&#36807;&#26469;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#26550;&#26500;&#25193;&#23637;&#20854;&#21442;&#25968;&#25968;&#37327;&#26102;&#36880;&#28176;&#20986;&#29616;&#24182;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#35757;&#32451;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09613v1 Announce Type: cross  Abstract: We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.09603</link><description>&lt;p&gt;
&#25511;&#21046;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#36827;&#34892;&#20048;&#35266;&#21487;&#39564;&#35777;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Optimistic Verifiable Training by Controlling Hardware Nondeterminism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#31934;&#24230;&#19979;&#36827;&#34892;&#35757;&#32451;&#12289;&#22312;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#36827;&#34892;&#22235;&#33293;&#20116;&#20837;&#65292;&#24182;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#65292;&#20197;&#24212;&#23545;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#26085;&#30410;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#23548;&#33268;&#20102;&#20026;&#32570;&#20047;&#24517;&#35201;&#36164;&#28304;&#30340;&#23458;&#25143;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#30340;&#26381;&#21153;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#35757;&#32451;&#30340;&#27491;&#30830;&#24615;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#35757;&#32451;&#26102;&#25915;&#20987;&#65292;&#20363;&#22914;&#25968;&#25454;&#27602;&#21270;&#65292;&#37117;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#39564;&#35777;&#35757;&#32451;&#30340;&#24037;&#20316;&#20027;&#35201;&#20998;&#20026;&#20004;&#31867;&#65306;&#22522;&#20110;&#35777;&#26126;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#38656;&#35201;&#21152;&#23494;&#25216;&#26415;&#32780;&#38590;&#20197;&#25193;&#23637;&#65292;&#20197;&#21450;&#32771;&#34385;&#21040;&#19968;&#20010;&#21487;&#20449;&#31532;&#19977;&#26041;&#23457;&#35745;&#21592;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#30340;&#8220;&#20048;&#35266;&#8221;&#26041;&#27861;&#12290; &#21518;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;GPU&#31867;&#22411;&#20043;&#38388;&#30340;&#30828;&#20214;&#38750;&#30830;&#23450;&#24615;&#38459;&#27490;&#23457;&#35745;&#21592;&#31934;&#30830;&#22797;&#21046;&#35757;&#32451;&#36807;&#31243;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#26041;&#26696;&#19981;&#22815;&#20581;&#22766;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#22312;&#27604;&#30446;&#26631;&#27169;&#22411;&#26356;&#39640;&#30340;&#31934;&#24230;&#19979;&#36827;&#34892;&#65292;&#20013;&#38388;&#35745;&#31639;&#27493;&#39588;&#21518;&#22235;&#33293;&#20116;&#20837;&#65292;&#22522;&#20110;&#33258;&#36866;&#24212;&#38408;&#20540;&#23384;&#20648;&#22235;&#33293;&#20116;&#20837;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
&lt;/p&gt;</description></item><item><title>UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07187</link><description>&lt;p&gt;
UPS: &#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#23454;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07187
&lt;/p&gt;
&lt;p&gt;
UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UPS&#65288;&#32479;&#19968;PDE&#27714;&#35299;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#22495;&#12289;&#32500;&#24230;&#21644;&#20998;&#36776;&#29575;&#19978;&#23450;&#20041;&#30340;&#21508;&#31181;&#26102;&#31354;PDE&#12290;UPS&#23558;&#19981;&#21516;&#30340;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23558;LLMs&#19982;&#29305;&#23450;&#22495;&#31070;&#32463;&#31639;&#23376;&#30456;&#32467;&#21512;&#30340;&#32479;&#19968;&#32593;&#32476;&#26550;&#26500;&#22788;&#29702;&#21508;&#31181;PDE&#25968;&#25454;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#36807;&#31243;&#35757;&#32451;&#32593;&#32476;&#65292;&#21033;&#29992;&#27169;&#24577;&#23545;&#40784;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#36827;&#34892;&#35843;&#25972;&#24182;&#21033;&#29992;&#25991;&#26412;&#24418;&#24335;&#30340;&#20803;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;UPS&#22312;PDEBench&#30340;&#24191;&#27867;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23545;&#32771;&#34385;&#30340;10&#20010;&#20219;&#21153;&#20013;&#30340;8&#20010;&#20219;&#21153;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#23569;&#26679;&#26412;&#24555;&#36895;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07187v1 Announce Type: new  Abstract: We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03949</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#35843;&#21644;&#29616;&#23454;&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#25805;&#20316;&#30340;&#23454;-&#27169;-&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#30417;&#30563;&#26469;&#23398;&#20064;&#23545;&#29289;&#20307;&#23039;&#21183;&#21464;&#21270;&#12289;&#29289;&#29702;&#24178;&#25200;&#21644;&#35270;&#35273;&#25200;&#21160;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#31283;&#20581;&#34892;&#20026;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#36127;&#25285;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RialTo&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#21363;&#23558;&#20174;&#23569;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26500;&#24314;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23454;-&#27169;-&#23454;&#27969;&#27700;&#32447;&#65292;RialTo&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#24555;&#36895;&#25195;&#25551;&#21644;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21453;&#21521;&#25552;&#28860;&#8221;&#36807;&#31243;&#65292;&#29992;&#20110;&#32473;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#24102;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19472</link><description>&lt;p&gt;
&#32456;&#36523;&#22522;&#20934;&#65306;&#22312;&#24555;&#36895;&#36827;&#23637;&#26102;&#20195;&#20013;&#39640;&#25928;&#30340;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19472
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32456;&#36523;&#22522;&#20934;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#21019;&#24314;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;Sort \&amp; Search&#65288;S&amp;S&#65289;&#26469;&#35299;&#20915;&#35780;&#20272;&#25104;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#22522;&#20934;&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#37325;&#22797;&#27979;&#35797;&#65292;&#31639;&#27861;&#23545;&#22522;&#20934;&#30340;&#29305;&#27530;&#24615;&#36807;&#24230;&#21033;&#29992;&#65292;&#20250;&#22686;&#21152;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#32534;&#21046;&#19981;&#26029;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#65288;&#31216;&#20026;&#32456;&#36523;&#22522;&#20934;&#65289;&#26469;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#12290;&#20316;&#20026;&#25105;&#20204;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#32456;&#36523;-CIFAR10&#21644;&#32456;&#36523;-ImageNet&#65292;&#20998;&#21035;&#21253;&#21547;&#65288;&#30446;&#21069;&#65289;1.69&#30334;&#19975;&#21644;1.98&#30334;&#19975;&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#23613;&#31649;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#65292;&#32456;&#36523;&#22522;&#20934;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#35780;&#20272;&#26085;&#30410;&#22686;&#22810;&#30340;&#27169;&#22411;&#22312;&#19981;&#26029;&#25193;&#22823;&#30340;&#26679;&#26412;&#38598;&#19978;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;Sort \&amp; Search (S&amp;S)&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26377;&#36873;&#25321;&#22320;&#23545;&#27979;&#35797;&#26679;&#26412;&#36827;&#34892;&#25490;&#24207;&#21644;&#23376;&#36873;&#25321;&#65292;&#20351;&#24471;&#32456;&#36523;&#22522;&#20934;&#35780;&#20272;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#36890;&#36807;&#23545;31,000&#20010;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19472v1 Announce Type: new  Abstract: Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \&amp; Search (S&amp;S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.19101</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#36328;&#23454;&#20307;&#36328;&#22495;&#25512;&#33616;&#30693;&#35782;&#20256;&#36755;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20307;&#25512;&#33616;&#20013;&#28304;&#23454;&#20307;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#21644;&#29305;&#24449;&#27169;&#24335;&#19981;&#23545;&#40784;&#31561;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#30340;&#25512;&#33616;&#20869;&#23481;&#21464;&#24471;&#36234;&#26469;&#36234;&#20016;&#23500; -- &#21333;&#20010;&#29992;&#25143;&#21453;&#39304;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#23454;&#20307;&#65292;&#22914;&#38144;&#21806;&#20135;&#21697;&#12289;&#30701;&#35270;&#39057;&#21644;&#20869;&#23481;&#24086;&#23376;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#23454;&#20307;&#25512;&#33616;&#38382;&#39064;&#65292;&#19968;&#20010;&#30452;&#35266;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#37319;&#29992;&#22522;&#20110;&#20849;&#20139;&#32593;&#32476;&#30340;&#26550;&#26500;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#12290;&#36825;&#19968;&#24819;&#27861;&#26159;&#23558;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#28304;&#23454;&#20307;&#65289;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#21478;&#19968;&#20010;&#31867;&#22411;&#23454;&#20307;&#65288;&#30446;&#26631;&#23454;&#20307;&#65289;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19101v1 Announce Type: cross  Abstract: In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15402</link><description>&lt;p&gt;
&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#65306;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#37325;&#26032;&#37197;&#32622;&#29289;&#20307;&#21040;&#30001;RGB-D&#22270;&#20687;&#25351;&#23450;&#30340;&#26399;&#26395;&#30446;&#26631;&#37197;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#25506;&#32034;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24863;&#30693;&#35823;&#24046;&#25935;&#24863;&#65292;&#24182;&#19988;&#36739;&#23569;&#20851;&#27880;&#20219;&#21153;&#32423;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#37325;&#26032;&#25490;&#21015;&#26410;&#30693;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22122;&#22768;&#24863;&#30693;&#22914;&#20309;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#24433;&#21709;&#25235;&#21462;&#21644;&#25918;&#32622;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#20998;&#31163;&#32467;&#26500;&#19981;&#23481;&#26131;&#25913;&#21892;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#31163;&#32467;&#26500;&#20316;&#20026;&#20808;&#39564;&#30340;GSP&#65292;&#19968;&#20010;&#21452;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#20869;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#20027;&#21160;&#35266;&#23519;&#31574;&#30053;&#20197;&#25552;&#39640;&#25918;&#32622;&#30340;&#24863;&#30693;&#12290;&#23545;&#20110;&#22806;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#25235;&#21462;&#31574;&#30053;&#65292;&#24847;&#35782;&#21040;&#29289;&#20307;&#21305;&#37197;&#21644;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.14337</link><description>&lt;p&gt;
AURA&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#30340;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
AURA: Natural Language Reasoning for Aleatoric Uncertainty in Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14337
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#22788;&#29702;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#30340;&#26041;&#27861;&#65292;&#23454;&#26045;&#20102;&#20351;&#29992;&#29109;&#20998;&#25968;&#21644;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#25932;&#23545;&#29702;&#30001;&#30340;&#31283;&#20581;&#24615;&#33021;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31574;&#32972;&#21518;&#30340;&#29702;&#30001;&#19981;&#20165;&#35299;&#37322;&#20102;&#27169;&#22411;&#20915;&#31574;&#65292;&#32780;&#19988;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#26080;&#25032;&#21487;&#20987;&#30340;&#29702;&#30001;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#27492;&#22806;&#65292;&#20272;&#35745;&#29702;&#30001;&#36275;&#22815;&#24544;&#23454;&#20197;&#40723;&#21169;&#27169;&#22411;&#34920;&#29616;&#30340;&#31243;&#24230;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25512;&#29702;&#20219;&#21153;&#36890;&#24120;&#36843;&#20351;&#27169;&#22411;&#22312;&#19981;&#29702;&#24819;&#30340;&#29702;&#30001;&#19979;&#36755;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#19988;&#19982;&#27169;&#22411;&#23436;&#20840;&#26377;&#33021;&#21147;&#30340;&#24773;&#20917;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#24212;&#23545;&#24341;&#21457;&#27169;&#24335;&#21512;&#29702;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#23436;&#32654;&#29702;&#30001;&#12290;&#25105;&#20204;&#39318;&#20808;&#29992;&#32473;&#23450;&#29702;&#30001;&#30340;&#29109;&#20998;&#25968;&#26469;&#23450;&#20041;&#27169;&#31946;&#30340;&#29702;&#30001;&#65292;&#20351;&#29992;&#27169;&#22411;&#20808;&#39564;&#20449;&#24565;&#20316;&#20026;&#20449;&#24687;&#37327;&#12290;&#28982;&#21518;&#26681;&#25454;&#29702;&#30001;&#30340;&#27169;&#31946;&#24615;&#26469;&#24341;&#23548;&#27169;&#22411;&#36873;&#25321;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35770;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#29702;&#30001;&#30340;&#25932;&#23545;&#36136;&#37327;&#20135;&#29983;&#20102;&#31283;&#20581;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14337v1 Announce Type: new  Abstract: Rationales behind answers not only explain model decisions but boost language models to reason well on complex reasoning tasks. However, obtaining impeccable rationales is often impossible. Besides, it is non-trivial to estimate the degree to which the rationales are faithful enough to encourage model performance. Thus, such reasoning tasks often compel models to output correct answers under undesirable rationales and are sub-optimal compared to what the models are fully capable of. In this work, we propose how to deal with imperfect rationales causing aleatoric uncertainty. We first define the ambiguous rationales with entropy scores of given rationales, using model prior beliefs as informativeness. We then guide models to select one of two different reasoning models according to the ambiguity of rationales. We empirically argue that our proposed method produces robust performance superiority against the adversarial quality of rationale
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10754</link><description>&lt;p&gt;
&#24403;&#25968;&#25454;&#27969;&#20998;&#26512;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
When Dataflow Analysis Meets Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#20998;&#26512;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20195;&#30721;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#25512;&#26029;&#31243;&#24207;&#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20195;&#30721;&#20248;&#21270;&#12289;&#31243;&#24207;&#29702;&#35299;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLMDFA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#12290;LLMDFA&#21463;&#22522;&#20110;&#25688;&#35201;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20960;&#31181;&#20851;&#38190;&#31574;&#30053;&#26377;&#25928;&#35299;&#20915;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21644;&#24037;&#20855;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#35774;&#35745;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#24182;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10754v1 Announce Type: cross  Abstract: Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark
&lt;/p&gt;</description></item><item><title>&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09721</link><description>&lt;p&gt;
&#35828;&#26381;&#19968;&#20301;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Persuading a Learning Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09721
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#20013;&#65292;&#21363;&#20351;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#22996;&#25176;&#20154;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#26080;&#38480;&#25509;&#36817;&#30340;&#25928;&#26524;&#65307;&#22312;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#37325;&#22797;&#30340;&#36125;&#21494;&#26031;&#35828;&#26381;&#38382;&#39064;&#65288;&#26356;&#19968;&#33324;&#22320;&#65292;&#20219;&#20309;&#20855;&#26377;&#23436;&#20840;&#20449;&#24687;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#27809;&#26377;&#25215;&#35834;&#33021;&#21147;&#65292;&#20195;&#29702;&#20154;&#20351;&#29992;&#31639;&#27861;&#26469;&#23398;&#20064;&#22914;&#20309;&#23545;&#22996;&#25176;&#20154;&#30340;&#20449;&#21495;&#20570;&#20986;&#21709;&#24212;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#24191;&#20041;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#20195;&#29702;&#20154;&#36817;&#20284;&#22320;&#26368;&#20339;&#21709;&#24212;&#12290;&#36890;&#36807;&#36825;&#20010;&#31616;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#65306;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#21487;&#20197;&#20445;&#35777;&#20854;&#25928;&#29992;&#19982;&#32463;&#20856;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#20855;&#26377;&#25215;&#35834;&#30340;&#22996;&#25176;&#20154;&#30340;&#26368;&#20248;&#25928;&#29992;&#20043;&#38388;&#21487;&#20197;&#26080;&#38480;&#25509;&#36817;&#65307;&#22914;&#26524;&#20195;&#29702;&#20154;&#20351;&#29992;&#19978;&#19979;&#25991;&#26080;&#20132;&#25442;&#36951;&#25022;&#23398;&#20064;&#31639;&#27861;&#65292;&#21017;&#22996;&#25176;&#20154;&#26080;&#27861;&#33719;&#24471;&#27604;&#20855;&#26377;&#25215;&#35834;&#30340;&#26080;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#25928;&#29992;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#22996;&#25176;&#20154;&#22312;&#23398;&#20064;&#27169;&#22411;&#19982;&#38750;&#23398;&#20064;&#27169;&#22411;&#20013;&#21487;&#20197;&#33719;&#24471;&#30340;&#25928;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#26159;&#26377;&#30028;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09721v1 Announce Type: cross  Abstract: We study a repeated Bayesian persuasion problem (and more generally, any generalized principal-agent problem with complete information) where the principal does not have commitment power and the agent uses algorithms to learn to respond to the principal's signals. We reduce this problem to a one-shot generalized principal-agent problem with an approximately-best-responding agent. This reduction allows us to show that: if the agent uses contextual no-regret learning algorithms, then the principal can guarantee a utility that is arbitrarily close to the principal's optimal utility in the classic non-learning model with commitment; if the agent uses contextual no-swap-regret learning algorithms, then the principal cannot obtain any utility significantly more than the optimal utility in the non-learning model with commitment. The difference between the principal's obtainable utility in the learning model and the non-learning model is bound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09122</link><description>&lt;p&gt;
&#28151;&#21512;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed-Output Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#20449;&#21495;&#20998;&#31163;&#26041;&#27861;&#65292;&#20854;&#20013;&#20449;&#21495;&#21487;&#20197;&#26681;&#25454;&#28508;&#21464;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22686;&#21152;&#20102;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#65292;&#20197;&#21253;&#25324;&#27599;&#20010;&#25968;&#25454;&#28857;&#30001;&#24050;&#30693;&#25968;&#37327;&#30340;&#32431;&#32452;&#20998;&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#22810;&#20010;&#36755;&#20837;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#20851;&#20110;&#27599;&#20010;&#35266;&#27979;&#26435;&#37325;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#20998;&#25968;&#32452;&#25104;&#30340;&#24635;&#21644;&#20026;&#19968;&#32422;&#26463;&#21644;&#29992;&#20110;&#20998;&#31867;&#30340;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23545;&#20110;&#20809;&#35889;&#23398;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#25913;&#21464;&#26465;&#20214;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#32431;&#32452;&#20998;&#20449;&#21495;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#23637;&#31034;&#23545;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#24212;&#29992;&#65306;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#28201;&#24230;&#30340;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07588</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scaling Laws for Learning in Strategic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21453;&#26144;&#20986;&#19968;&#20010;&#20849;&#35782;&#65306;&#27169;&#22411;&#36234;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#36234;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#65292;&#23601;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#25112;&#30053;&#29615;&#22659;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27169;&#22411;&#19982;&#25112;&#30053;&#20114;&#21160;&#23545;&#27604;&#20363;&#23450;&#24459;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36825;&#20010;&#33258;&#28982;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#27604;&#20363;&#23450;&#24459;&#35266;&#28857;&#65292;&#21363;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#38543;&#30528;&#27169;&#22411;&#30340;&#25193;&#22823;&#21644;/&#25110;&#34920;&#36798;&#33021;&#21147;&#30340;&#22686;&#24378;&#65288;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65289;&#32780;&#21333;&#35843;&#25552;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#25112;&#30053;&#22238;&#24402;&#12289;&#25112;&#30053;&#20998;&#31867;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#20363;&#23376;&#23637;&#31034;&#20102;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#27169;&#22411;&#25110;&#31574;&#30053;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05396</link><description>&lt;p&gt;
TASER: &#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#24555;&#36895;&#20934;&#30830;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05396
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TASER&#26041;&#27861;&#65292;&#23427;&#26159;&#38024;&#23545;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#22312;&#21253;&#25324;&#27450;&#35784;&#26816;&#27979;&#21644;&#20869;&#23481;&#25512;&#33616;&#22312;&#20869;&#30340;&#21508;&#31181;&#37325;&#35201;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#22270;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#26102;&#38388;&#36807;&#26102;&#30340;&#38142;&#25509;&#21644;&#20559;&#26012;&#30340;&#20132;&#20114;&#20998;&#24067;&#12290;&#36825;&#20123;&#22122;&#22768;&#23548;&#33268;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20005;&#37325;&#25439;&#23475;&#20102;TGNN&#30340;&#20934;&#30830;&#24615;&#65306;&#65288;1&#65289;&#27169;&#22411;&#21463;&#21040;&#36739;&#24046;&#20132;&#20114;&#30340;&#30417;&#30563;&#65292;&#65288;2&#65289;&#22122;&#22768;&#36755;&#20837;&#23548;&#33268;&#32858;&#21512;&#28040;&#24687;&#30340;&#39640;&#26041;&#24046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;TGNN&#21435;&#22122;&#25216;&#26415;&#24182;&#26410;&#32771;&#34385;&#27599;&#20010;&#33410;&#28857;&#30340;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#22122;&#22768;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38754;&#20020;&#30528;&#36941;&#21382;&#26356;&#22810;&#37051;&#23621;&#23548;&#33268;&#20135;&#29983;&#36807;&#22810;&#23567;&#25209;&#37327;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#30456;&#20449;&#24555;&#36895;&#20934;&#30830;&#30340;TGNN&#30340;&#35299;&#20915;&#26041;&#27861;&#22312;&#20110;&#26102;&#38388;&#33258;&#36866;&#24212;&#37319;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TASER&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36827;&#34892;&#20248;&#21270;&#30340;TGNN&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
&lt;/p&gt;</description></item><item><title>TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04616</link><description>&lt;p&gt;
TinyLLM: &#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TinyLLM: Learning a Small Student from Multiple Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04616
&lt;/p&gt;
&lt;p&gt;
TinyLLM&#26159;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#65292;&#26088;&#22312;&#35299;&#20915;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#38382;&#39064;&#65292;&#24182;&#40723;&#21169;&#23398;&#29983;&#27169;&#22411;&#29702;&#35299;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26356;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36739;&#23567;&#30340;LLMs&#26356;&#28789;&#27963;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;&#22312;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#30693;&#35782;&#33976;&#39311;&#22240;&#20854;&#20986;&#33394;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#30693;&#35782;&#22810;&#26679;&#24615;&#26377;&#38480;&#21644;&#32570;&#20047;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#20419;&#36827;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyLLM&#65292;&#19968;&#31181;&#20174;&#22810;&#20010;&#22823;&#22411;&#25945;&#24072;LLMs&#20013;&#23398;&#20064;&#23567;&#22411;&#23398;&#29983;LLM&#30340;&#26032;&#22411;&#30693;&#35782;&#33976;&#39311;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#40723;&#21169;&#23398;&#29983;LLM&#19981;&#20165;&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#32780;&#19988;&#29702;&#35299;&#36825;&#20123;&#31572;&#26696;&#32972;&#21518;&#30340;&#21407;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#30340;LLMs&#20855;&#26377;&#19981;&#21516;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#22810;&#20010;&#25945;&#24072;LLMs&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#32769;&#24072;&#24378;&#21046;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02429</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#20316;&#20026;&#31163;&#32447;RL&#21644;&#20803;RL&#30340;&#32467;&#21512;&#65292;&#22312;&#23454;&#29616;RL&#26234;&#33021;&#20307;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#20197;&#21450;&#23433;&#20840;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65288;COMRL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26377;&#25928;&#20219;&#21153;&#34920;&#31034;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;COMRL&#39046;&#22495;&#30340;&#20960;&#20010;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#36825;&#20123;&#30475;&#20284;&#29420;&#31435;&#30340;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20013;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;COMRL&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#36817;&#20284;&#30028;&#38480;&#26469;&#20248;&#21270;&#20219;&#21153;&#21464;&#37327;$\boldsymbol{M}$&#21644;&#20854;&#28508;&#22312;&#34920;&#31034;$\boldsymbol{Z}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#22522;&#20110;&#29702;&#35770;&#27934;&#23519;&#21147;&#21644;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;R&#38382;&#39064;&#35889;&#19978;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\boldsymbol{M}$ and its latent representation $\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of R
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#31574;&#30053;&#21644;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#23454;&#29616;&#20102;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.01964</link><description>&lt;p&gt;
&#19981;&#38656;&#22238;&#39038;&#65306;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
No Need to Look Back: An Efficient and Scalable Approach for Temporal Network Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#26102;&#24577;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#31574;&#30053;&#21644;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#23454;&#29616;&#20102;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;TGRL&#65289;&#23545;&#20110;&#24314;&#27169;&#23454;&#38469;&#32593;&#32476;&#20013;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;TGRL&#26041;&#27861;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#35745;&#31639;&#38656;&#27714;&#21644;&#25512;&#29702;&#24310;&#36831;&#36739;&#39640;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#22238;&#28335;&#27599;&#20010;&#33410;&#28857;&#30340;&#20132;&#20114;&#21382;&#21490;&#26469;&#36827;&#34892;&#26102;&#24577;&#37051;&#23621;&#30340;&#20302;&#25928;&#37319;&#26679;&#25152;&#33268;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#25928;TGRL&#26694;&#26550;&#65292;&#21517;&#20026;No-Looking-Back&#65288;NLB&#65289;&#12290;NLB&#37319;&#29992;&#20102;&#8220;&#21069;&#21521;&#26368;&#36817;&#37319;&#26679;&#8221;&#31574;&#30053;&#65292;&#32469;&#36807;&#20102;&#22238;&#28335;&#21382;&#21490;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#20351;&#29992;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#30340;GPU&#21487;&#25191;&#34892;&#30340;&#22823;&#23567;&#21463;&#38480;&#21704;&#24076;&#34920;&#35760;&#24405;&#19979;&#37319;&#26679;&#21518;&#30340;&#26368;&#36817;&#20132;&#20114;&#65292;&#23454;&#29616;&#23545;&#26597;&#35810;&#30340;&#24555;&#36895;&#21709;&#24212;&#21644;&#26368;&#23567;&#21270;&#25512;&#29702;&#24310;&#36831;&#12290;&#35813;&#21704;&#24076;&#34920;&#30340;&#32500;&#25252;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;$O(1)$&#12290;NLB&#19982;GPU&#22788;&#29702;&#23436;&#20840;&#20860;&#23481;&#65292;&#26368;&#22823;&#21270;&#20102;&#21487;&#32534;&#31243;&#24615;&#12289;&#24182;&#34892;&#24615;&#21644;&#33021;&#25928;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Temporal graph representation learning (TGRL) is crucial for modeling complex, dynamic systems in real-world networks. Traditional TGRL methods, though effective, suffer from high computational demands and inference latency. This is mainly induced by their inefficient sampling of temporal neighbors by backtracking the interaction history of each node when making model inference. This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB). NLB employs a "forward recent sampling" strategy, which bypasses the need for backtracking historical interactions. This strategy is implemented using a GPU-executable size-constrained hash table for each node, recording down-sampled recent interactions, which enables rapid response to queries with minimal inference latency. The maintenance of this hash table is highly efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing, maximizing programmability, parallelism, and power efficiency. Empirical evaluations de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.00261</link><description>&lt;p&gt;
&#20351;&#29992;&#21521;&#37327;&#31354;&#38388;&#21644;&#36870;&#26144;&#23556;&#20102;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#25968;&#23398;&#26041;&#27861;&#26469;&#29702;&#35299;&#22270;&#20687;&#20998;&#26512;&#20013;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#23558;&#31070;&#32463;&#32593;&#32476;&#23618;&#35270;&#20026;&#20449;&#21495;&#31354;&#38388;&#20043;&#38388;&#30340;&#26144;&#23556;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20449;&#21495;&#31354;&#38388;&#26469;&#21487;&#35270;&#21270;&#26435;&#37325;&#31354;&#38388;&#21644;&#21367;&#31215;&#23618;&#21367;&#31215;&#26680;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#36870;&#32593;&#32476;&#30340;&#27010;&#24565;&#21644;&#35745;&#31639;&#20135;&#29983;&#29305;&#23450;&#36755;&#20986;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21487;&#36870;&#32593;&#32476;&#21644;ResNet18&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.17093</link><description>&lt;p&gt;
&#29992;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Out-of-Distribution Detection with Prototypical Semi-Supervised Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#21407;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PAWS-VMK&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#21407;&#22411;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#39592;&#24178;&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#32467;&#26524;&#65292;&#25913;&#36827;&#20102;Predicting View-Assignments With Support Samples&#65288;PAWS&#65289;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;(1) &#21442;&#25968;&#21270;von-Mises Fisher&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;vMF-SNE&#65289;&#26469;&#39044;&#35757;&#32451;&#25237;&#24433;&#22836;&#65292;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;;(2) &#21463;MixMatch&#21551;&#21457;&#30340;&#25439;&#22833;&#65292;&#36890;&#36807;&#23545;&#22810;&#35270;&#22270;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#65292;&#25552;&#20379;&#27604;PAWS&#20013;&#20351;&#29992;&#30340;&#19968;&#33268;&#24615;&#25439;&#22833;&#26356;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;;&#21644;(3) &#31616;&#21333;k-Means&#21407;&#22411;&#36873;&#25321;&#65288;SKMPS&#65289;&#65292;&#19968;&#31181;&#27604;&#20854;&#20182;&#26080;&#30417;&#30563;&#26631;&#31614;&#36873;&#25321;&#26041;&#27861;&#25552;&#20379;&#26356;&#20248;&#36234;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17093v2 Announce Type: replace-cross  Abstract: This paper describes PAWS-VMK, an improved approach to prototypical semi-supervised learning in the field of computer vision, specifically designed to utilize a frozen foundation model as the neural network backbone. This method outperforms previous results in semi-supervised learning and out-of-distribution (OOD) detection, improving upon the Predicting View-Assignments With Support Samples (PAWS) semi-supervised learning method. We introduce (1) parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to pretrain the projection head using the high-quality embeddings of the foundation model; (2) a MixMatch inspired loss, where predictions across multiple views are averaged to provide a more reliable supervision signal compared to the consistency loss used in PAWS and (3) simple $k$-Means prototype selection (SKMPS), a technique that provides superior performance to other unsupervised label selection approaches in t
&lt;/p&gt;</description></item><item><title>DySurv&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#38745;&#24577;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#65292;&#29992;&#20110;&#20272;&#35745;ICU&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;ICU&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2310.18681</link><description>&lt;p&gt;
DySurv&#65306;ICU&#20013;&#29983;&#23384;&#39044;&#27979;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18681
&lt;/p&gt;
&lt;p&gt;
DySurv&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#38745;&#24577;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#65292;&#29992;&#20110;&#20272;&#35745;ICU&#24739;&#32773;&#30340;&#27515;&#20129;&#39118;&#38505;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#30340;ICU&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#20391;&#37325;&#20110;&#20272;&#35745;&#26102;&#38388;&#33267;&#20107;&#20214;&#20998;&#24067;&#65292;&#21487;&#24110;&#21161;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#21160;&#24577;&#39118;&#38505;&#39044;&#27979;&#12290;&#25193;&#23637;&#32463;&#20856;&#30340;Cox&#27169;&#22411;&#65292;&#21457;&#23637;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#27604;&#20363;&#39118;&#38505;&#30340;&#32422;&#26463;&#24615;&#20551;&#35774;&#12290;&#20256;&#32479;&#32479;&#35745;&#27169;&#22411;&#36890;&#24120;&#20165;&#21253;&#21547;&#38745;&#24577;&#20449;&#24687;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DySurv&#30340;&#26032;&#22411;&#22522;&#20110;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#38745;&#24577;&#20449;&#24687;&#21644;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#30340;&#32452;&#21512;&#26469;&#21160;&#24577;&#20272;&#35745;&#27515;&#20129;&#39118;&#38505;&#12290;DySurv&#22312;&#22810;&#20010;&#26102;&#38388;&#33267;&#20107;&#20214;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#25105;&#20204;&#22312;&#26469;&#33258;MIMIC-IV&#21644;eICU&#30340;&#29616;&#23454;&#19990;&#30028;&#37325;&#30151;&#30417;&#25252;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290; DySurv&#30340;&#39044;&#27979;&#33021;&#21147;&#25345;&#32493;&#31283;&#23450;&#65292;&#29983;&#23384;&#20272;&#35745;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18681v2 Announce Type: replace  Abstract: Survival analysis focuses on estimating time-to-event distributions which can help in dynamic risk prediction in healthcare. Extending beyond the classical Cox model, deep learning techniques have been developed which moved away from the constraining assumptions of proportional hazards. Traditional statistical models often only include static information where, in this work, we propose a novel conditional variational autoencoder-based method called DySurv, which uses a combination of static and time-series measurements from patient electronic health records to estimate the risk of death dynamically. DySurv has been tested on several time-to-event benchmarks where it outperforms existing methods, including deep learning methods, and we evaluate it on real-world intensive care unit data from MIMIC-IV and eICU. The predictive capacity of DySurv is consistent and the survival estimates remain disentangled across different datasets suppor
&lt;/p&gt;</description></item><item><title>PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;</title><link>http://arxiv.org/abs/2401.11202</link><description>&lt;p&gt;
PartIR: &#20026;&#26426;&#22120;&#23398;&#20064;&#32452;&#21512;SPMD&#20998;&#21306;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11202
&lt;/p&gt;
&lt;p&gt;
PartIR&#26159;&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#21306;&#31995;&#32479;&#65292;&#20855;&#22791;&#34920;&#36798;&#21147;&#24378;&#21644;&#21487;&#39044;&#27979;&#24615;&#24378;&#30340;&#29305;&#28857;&#12290;&#23427;&#36890;&#36807;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#24182;&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#33021;&#22815;&#32452;&#21512;&#19981;&#21516;&#30340;&#20998;&#29255;&#31574;&#30053;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#33021;&#21147;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#35757;&#32451;&#38656;&#35201;&#32467;&#21512;&#25968;&#25454;&#12289;&#27169;&#22411;&#25110;&#20248;&#21270;&#22120;&#20998;&#29255;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#24403;&#31574;&#30053;&#21464;&#24471;&#22797;&#26434;&#26102;&#65292;&#20998;&#21306;&#24037;&#20855;&#38656;&#35201;&#20855;&#22791;&#20197;&#19979;&#29305;&#28857;&#65306;1&#65289;&#34920;&#36798;&#21147;&#24378;&#65292;&#20801;&#35768;&#32452;&#21512;&#31616;&#21333;&#31574;&#30053;&#65307;2&#65289;&#21487;&#39044;&#27979;&#24615;&#24378;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#31639;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PartIR&#65292;&#19968;&#31181;&#29992;&#20110;NN&#20998;&#21306;&#30340;&#35774;&#35745;&#12290;PartIR&#37319;&#29992;&#22686;&#37327;&#37325;&#20889;&#26041;&#27861;&#65292;&#19982;&#30828;&#20214;&#21644;&#36816;&#34892;&#26102;&#26080;&#20851;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;API&#29992;&#20110;&#32452;&#21512;&#20998;&#29255;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#22120;&#36827;&#34892;&#39564;&#35777;&#12290;&#25972;&#20010;&#36807;&#31243;&#30001;&#39640;&#32423;&#31243;&#24207;&#21592;&#21457;&#20986;&#30340;&#20998;&#21306;&#31574;&#30053;&#39537;&#21160;&#65292;&#26082;&#21487;&#20197;&#25163;&#21160;&#20063;&#21487;&#20197;&#33258;&#21160;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#19982;&#27169;&#22411;&#20195;&#30721;&#20998;&#24320;&#25351;&#23450;&#65292;&#26131;&#20110;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#31181;&#19981;&#21516;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#23637;&#31034;PartIR&#30340;&#21487;&#39044;&#27979;&#24615;&#12289;&#34920;&#36798;&#33021;&#21147;&#21644;&#36798;&#21040;&#23792;&#20540;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10895</link><description>&lt;p&gt;
&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;(SCRA)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#28436;&#21464;&#65292;&#38761;&#26032;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;&#36825;&#31181;&#28436;&#21464;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#22312;&#29616;&#20195;&#20379;&#24212;&#38142;&#20013;&#30830;&#20445;&#36816;&#33829;&#30340;&#38887;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#38656;&#35201;&#31283;&#20581;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#32508;&#36848;&#24050;&#32463;&#27010;&#36848;&#20102;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#29702;&#35299;&#20854;&#22312;SCRA&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;1717&#31687;&#35770;&#25991;&#65292;&#24182;&#20174;2014&#24180;&#33267;2023&#24180;&#20043;&#38388;&#21457;&#34920;&#30340;48&#31687;&#25991;&#31456;&#20013;&#33719;&#24471;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#25506;&#31350;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#26041;&#27861;&#35770;&#12289;&#30740;&#31350;&#32467;&#26524;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
&lt;/p&gt;</description></item><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01599</link><description>&lt;p&gt;
&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#24130;&#24459;&#34928;&#20943;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#65292;&#23545;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21644;&#20854;&#20182;&#20998;&#26512;&#35889;&#31639;&#27861;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#20840;&#38754;&#29305;&#24449;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#36129;&#29486;-&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26576;&#20123;&#26680;&#22238;&#24402;&#26041;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#26088;&#22312;&#30830;&#23450;&#22312;&#19981;&#21516;&#28304;&#26465;&#20214;&#12289;&#22122;&#22768;&#27700;&#24179;&#21644;&#27491;&#21017;&#21270;&#21442;&#25968;&#36873;&#25321;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#30830;&#20999;&#39034;&#24207;&#65292;&#32780;&#19981;&#26159;&#26368;&#23567;&#21270;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20005;&#26684;&#32473;&#20986;&#20102;&#26680;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65288;&#20197;&#21450;&#22823;&#31867;&#20998;&#26512;&#35889;&#31639;&#27861;&#65289;&#22312;&#26680;&#22238;&#24402;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#26354;&#32447;&#30340;&#23436;&#25972;&#29305;&#24449;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#26680;&#25554;&#20540;&#30340;&#36817;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#28548;&#28165;&#20855;&#26377;&#26356;&#39640;&#36164;&#26684;&#30340;&#26680;&#22238;&#24402;&#31639;&#27861;&#30340;&#39281;&#21644;&#25928;&#24212;&#65292;&#31561;&#31561;&#12290;&#30001;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#30340;&#24110;&#21161;&#65292;&#36825;&#20123;&#32467;&#26524;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25105;&#20204;&#23545;&#35757;&#32451;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65292;&#21363;&#20998;&#26512;&#21151;&#33021;&#35770;&#35777;&#65292;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21151;&#33021;&#21644;&#36136;&#37327;&#65292;&#24182;&#35760;&#24405;&#20102;&#25152;&#26377;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#26041;&#27861;&#23558;&#22312;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.03489</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#37096;&#32626;&#32479;&#19968;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#30828;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03489
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21151;&#33021;&#21644;&#36136;&#37327;&#65292;&#24182;&#35760;&#24405;&#20102;&#25152;&#26377;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#26041;&#27861;&#23558;&#22312;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#32423;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#26469;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#65292;&#19981;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197;&#29983;&#25104;&#20855;&#26377;wishbone&#25509;&#21475;&#30340;&#32622;&#25442;&#21516;&#20313;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20223;&#30495;&#21644;Dieharder&#38543;&#26426;&#24615;&#27979;&#35797;&#22871;&#20214;&#39564;&#35777;&#20102;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#35774;&#35745;&#30340;&#21151;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#26696;&#20363;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#35760;&#24405;&#12289;Python&#33050;&#26412;&#12289;Verilog&#33050;&#26412;&#21644;&#20223;&#30495;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#30828;&#20214;&#35774;&#35745;&#29983;&#25104;&#26041;&#27861;&#19982;&#24320;&#28304;&#30789;130&#32435;&#31859;&#35774;&#35745;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#23558;&#25913;&#21464;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26500;&#24314;&#29289;&#32852;&#32593;&#30340;&#39046;&#22495;&#19987;&#29992;&#35745;&#31639;&#21152;&#36895;&#22120;&#21644;&#27010;&#24565;&#39564;&#35777;&#21407;&#22411;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabri
&lt;/p&gt;</description></item><item><title>LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.02058</link><description>&lt;p&gt;
LOTUS&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02058
&lt;/p&gt;
&lt;p&gt;
LOTUS&#26159;&#19968;&#31181;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#25216;&#33021;&#24211;&#65292;&#24182;&#20351;&#29992;&#20803;&#25511;&#21046;&#22120;&#28789;&#27963;&#32452;&#21512;&#25216;&#33021;&#26469;&#25552;&#39640;&#25104;&#21151;&#29575;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LOTUS&#30340;&#25345;&#32493;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#20351;&#24471;&#29289;&#29702;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20854;&#25972;&#20010;&#23551;&#21629;&#20013;&#25345;&#32493;&#32780;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;LOTUS&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#26032;&#20219;&#21153;&#30340;&#23569;&#37327;&#20154;&#31867;&#28436;&#31034;&#26500;&#24314;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25216;&#33021;&#24211;&#12290;LOTUS&#39318;&#20808;&#20351;&#29992;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#20174;&#26410;&#20998;&#27573;&#30340;&#28436;&#31034;&#20013;&#25552;&#21462;&#37325;&#22797;&#20986;&#29616;&#30340;&#25216;&#33021;&#27169;&#24335;&#12290;&#25345;&#32493;&#25216;&#33021;&#21457;&#29616;&#26356;&#26032;&#29616;&#26377;&#25216;&#33021;&#20197;&#36991;&#20813;&#23545;&#20197;&#21069;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#28155;&#21152;&#26032;&#25216;&#33021;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;LOTUS&#35757;&#32451;&#19968;&#20010;&#20803;&#25511;&#21046;&#22120;&#65292;&#22312;&#32456;&#36523;&#23398;&#20064;&#36807;&#31243;&#20013;&#28789;&#27963;&#22320;&#32452;&#21512;&#21508;&#31181;&#25216;&#33021;&#26469;&#35299;&#20915;&#22522;&#20110;&#35270;&#35273;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;LOTUS&#22312;&#25104;&#21151;&#29575;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#22522;&#32447;&#26041;&#27861;11&#65285;&#20197;&#19978;&#65292;&#26174;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce LOTUS, a continual imitation learning algorithm that empowers a physical robot to continuously and efficiently learn to solve new manipulation tasks throughout its lifespan. The core idea behind LOTUS is constructing an ever-growing skill library from a sequence of new tasks with a small number of human demonstrations. LOTUS starts with a continual skill discovery process using an open-vocabulary vision model, which extracts skills as recurring patterns presented in unsegmented demonstrations. Continual skill discovery updates existing skills to avoid catastrophic forgetting of previous tasks and adds new skills to solve novel tasks. LOTUS trains a meta-controller that flexibly composes various skills to tackle vision-based manipulation tasks in the lifelong learning process. Our comprehensive experiments show that LOTUS outperforms state-of-the-art baselines by over 11% in success rate, showing its superior knowledge transfer ability compared to prior methods. More result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2311.01469</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#29615;&#20445;&#34394;&#20551;&#23459;&#20256;
&lt;/p&gt;
&lt;p&gt;
Leveraging Language Models to Detect Greenwashing. (arXiv:2311.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#12290;&#24320;&#21457;&#20102;&#19968;&#31181;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#30340;&#25968;&#23398;&#24418;&#24335;&#65292;&#24314;&#31435;&#20102;&#20248;&#21270;&#30340;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#27604;&#36739;&#20998;&#26512;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#33391;&#22909;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27668;&#20505;&#21464;&#21270;&#30340;&#21518;&#26524;&#36234;&#26469;&#36234;&#24341;&#36215;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20225;&#19994;&#22312;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#20013;&#24378;&#35843;&#20854;&#29615;&#20445;&#21162;&#21147;&#20197;&#22686;&#24378;&#20844;&#20247;&#24418;&#35937;&#12290;&#28982;&#32780;&#65292;&#23545;&#27492;&#31867;&#25253;&#21578;&#30340;&#23457;&#26680;&#32570;&#20047;&#20005;&#26684;&#30340;&#30417;&#31649;&#65292;&#21487;&#33021;&#23548;&#33268;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#36827;&#34892;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#26469;&#37327;&#21270;&#32511;&#33394;&#34394;&#20551;&#23459;&#20256;&#39118;&#38505;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#35813;&#38382;&#39064;&#30340;&#20248;&#21270;ClimateBERT&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#32467;&#26524;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#21487;&#25345;&#32493;&#21457;&#23637;&#25253;&#21578;&#30340;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#23454;&#29616;&#20102;&#24179;&#22343;&#20934;&#30830;&#29575;86.34%&#21644;F1&#20540;0.67&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25506;&#32034;&#30340;&#33391;&#22909;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, climate change repercussions have increasingly captured public interest. Consequently, corporations are emphasizing their environmental efforts in sustainability reports to bolster their public image. Yet, the absence of stringent regulations in review of such reports allows potential greenwashing. In this study, we introduce a novel methodology to train a language model on generated labels for greenwashing risk. Our primary contributions encompass: developing a mathematical formulation to quantify greenwashing risk, a fine-tuned ClimateBERT model for this problem, and a comparative analysis of results. On a test set comprising of sustainability reports, our best model achieved an average accuracy score of 86.34% and F1 score of 0.67, demonstrating that our methods show a promising direction of exploration for this task.
&lt;/p&gt;</description></item><item><title>OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17748</link><description>&lt;p&gt;
&#35753;&#26368;&#32456;&#29992;&#25143;&#25104;&#20026;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#28857;&#65306;OrionBench&#29992;&#20110;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Making the End-User a Priority in Benchmarking: OrionBench for Unsupervised Time Series Anomaly Detection. (arXiv:2310.17748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17748
&lt;/p&gt;
&lt;p&gt;
OrionBench&#26159;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#36890;&#29992;&#25277;&#35937;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21457;&#24067;&#39057;&#32321;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24739;&#32773;&#30417;&#27979;&#12289;&#37329;&#34701;&#20013;&#30340;&#39044;&#27979;&#25110;&#33021;&#28304;&#20013;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#36825;&#23548;&#33268;&#20102;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#27604;&#36739;&#26032;&#24320;&#21457;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#19968;&#27425;&#24615;&#25191;&#34892;&#65292;&#24182;&#19988;&#27604;&#36739;&#20165;&#38480;&#20110;&#23569;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OrionBench&#8212;&#8212;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#12289;&#25345;&#32493;&#32500;&#25252;&#30340;&#26080;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#29992;&#20110;&#34920;&#31034;&#27169;&#22411;&#30340;&#36890;&#29992;&#25277;&#35937;&#12289;&#28155;&#21152;&#26032;&#30340;&#27969;&#27700;&#32447;&#21644;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#12289;&#36229;&#21442;&#25968;&#26631;&#20934;&#21270;&#12289;&#27969;&#27700;&#32447;&#39564;&#35777;&#20197;&#21450;&#21457;&#24067;&#22522;&#20934;&#27979;&#35797;&#30340;&#39057;&#32321;&#29256;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OrionBench&#30340;&#29992;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19977;&#24180;&#26102;&#38388;&#20869;&#21457;&#24067;&#30340;15&#20010;&#29256;&#26412;&#20013;&#27969;&#27700;&#32447;&#30340;&#28436;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series anomaly detection is a prevalent problem in many application domains such as patient monitoring in healthcare, forecasting in finance, or predictive maintenance in energy. This has led to the emergence of a plethora of anomaly detection methods, including more recently, deep learning based methods. Although several benchmarks have been proposed to compare newly developed models, they usually rely on one-time execution over a limited set of datasets and the comparison is restricted to a few models. We propose OrionBench -- a user centric continuously maintained benchmark for unsupervised time series anomaly detection. The framework provides universal abstractions to represent models, extensibility to add new pipelines and datasets, hyperparameter standardization, pipeline verification, and frequent releases with published benchmarks. We demonstrate the usage of OrionBench, and the progression of pipelines across 15 releases published over the course of three years. Moreover,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.15290</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#24555;&#36895;&#21487;&#38752;&#22320;&#29983;&#25104;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Fast and Reliable Generation of EHR Time Series via Diffusion Models. (arXiv:2310.15290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21487;&#38752;&#29983;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#35757;&#32451;&#24037;&#20316;&#30340;&#38656;&#27714;&#26356;&#23569;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#26159;&#20016;&#23500;&#30340;&#24739;&#32773;&#32423;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#23454;&#39564;&#23460;&#26816;&#39564;&#12289;&#33647;&#29289;&#21644;&#35786;&#26029;&#65292;&#20026;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#38544;&#31169;&#30340;&#25285;&#24551;&#24120;&#24120;&#38480;&#21046;&#20102;&#23545;EHR&#30340;&#35775;&#38382;&#65292;&#38459;&#30861;&#20102;&#19979;&#28216;&#20998;&#26512;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20445;&#25252;&#38544;&#31169;&#30340;EHR&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#19971;&#31181;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#21644;&#30495;&#23454;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#26469;&#22686;&#24378;&#19979;&#28216;&#21307;&#30103;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records (EHRs) are rich sources of patient-level data, including laboratory tests, medications, and diagnoses, offering valuable resources for medical data analysis. However, concerns about privacy often restrict access to EHRs, hindering downstream analysis. Researchers have explored various methods for generating privacy-preserving EHR data. In this study, we introduce a new method for generating diverse and realistic synthetic EHR time series data using Denoising Diffusion Probabilistic Models (DDPM). We conducted experiments on six datasets, comparing our proposed method with seven existing methods. Our results demonstrate that our approach significantly outperforms all existing methods in terms of data utility while requiring less training effort. Our approach also enhances downstream medical data analysis by providing diverse and realistic synthetic EHR data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.04407</link><description>&lt;p&gt;
&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Policy-Gradient Training of Language Models for Ranking. (arXiv:2310.04407v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25490;&#24207;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#31639;&#27861;Neural PG-RANK&#65292;&#36890;&#36807;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#26816;&#32034;&#27169;&#22411;&#30340;&#21407;&#21017;&#24615;&#12289;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#26816;&#32034;&#22312;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#21040;&#35821;&#35328;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#32842;&#22825;&#24335;&#32593;&#39029;&#25628;&#32034;&#21040;&#38382;&#31572;&#31995;&#32479;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#36807;&#20856;&#22411;&#30340;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#22522;&#20110;LLM&#30340;&#26816;&#32034;&#22120;&#38656;&#35201;&#22797;&#26434;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21253;&#25324;&#36873;&#25321;&#22256;&#38590;&#30340;&#36127;&#26679;&#26412;&#21644;&#20351;&#29992;&#39069;&#22806;&#30340;&#30417;&#30563;&#20316;&#20026;&#23398;&#20064;&#20449;&#21495;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#21407;&#22240;&#26159;&#23545;&#27604;&#25439;&#22833;&#26412;&#36523;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#19981;&#33021;&#30452;&#25509;&#20248;&#21270;&#22788;&#29702;&#27969;&#31243;&#26411;&#31471;&#20915;&#31574;&#36136;&#37327;&#30340;&#19979;&#28216;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;PG-RANK&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23454;&#20363;&#21270;&#20026;Plackett-Luce&#25490;&#21517;&#31574;&#30053;&#65292;&#23398;&#20064;&#25490;&#24207;&#12290;&#31070;&#32463;PG-RANK&#20026;&#26816;&#32034;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20316;&#20026;&#26356;&#22823;&#30340;&#20915;&#31574;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text retrieval plays a crucial role in incorporating factual knowledge for decision making into language processing pipelines, ranging from chat-based web search to question answering systems. Current state-of-the-art text retrieval models leverage pre-trained large language models (LLMs) to achieve competitive performance, but training LLM-based retrievers via typical contrastive losses requires intricate heuristics, including selecting hard negatives and using additional supervision as learning signals. This reliance on heuristics stems from the fact that the contrastive loss itself is heuristic and does not directly optimize the downstream metrics of decision quality at the end of the processing pipeline. To address this issue, we introduce Neural PG-RANK, a novel training algorithm that learns to rank by instantiating a LLM as a Plackett-Luce ranking policy. Neural PG-RANK provides a principled method for end-to-end training of retrieval models as part of larger decision systems vi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.08949</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Dual-Perspective Approach to Evaluating Feature Attribution Methods. (arXiv:2308.08949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35270;&#35282;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#25581;&#31034;&#20102;&#24402;&#22240;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35782;&#21035;&#30456;&#20851;&#29305;&#24449;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#24314;&#31435;&#19968;&#20010;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20960;&#20010;&#35270;&#35282;&#26469;&#35780;&#20272;&#29305;&#24449;&#24402;&#22240;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#35270;&#35282;&#26159;&#35266;&#23519;&#25200;&#21160;&#24402;&#22240;&#29305;&#24449;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#65288;&#21363;&#24544;&#23454;&#24230;&#65289;&#12290;&#23613;&#31649;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#35265;&#65292;&#20294;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#23384;&#22312;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25581;&#31034;&#30340;&#32570;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24544;&#23454;&#24230;&#33539;&#24335;&#20869;&#30340;&#20004;&#20010;&#26032;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#30452;&#35266;&#30340;&#23646;&#24615;&#65306;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#12290;&#27491;&#30830;&#24615;&#35780;&#20272;&#24402;&#22240;&#29305;&#24449;&#30495;&#27491;&#26159;&#39044;&#27979;&#24615;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#32780;&#23436;&#25972;&#24615;&#26816;&#26597;&#25152;&#24471;&#24402;&#22240;&#22914;&#20309;&#24456;&#22909;&#22320;&#25581;&#31034;&#25152;&#26377;&#39044;&#27979;&#24615;&#29305;&#24449;&#12290;&#36825;&#20004;&#20010;&#35270;&#35282;&#22522;&#20110;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;&#30340;&#23450;&#37327;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods attempt to explain neural network predictions by identifying relevant features. However, establishing a cohesive framework for assessing feature attribution remains a challenge. There are several views through which we can evaluate attributions. One principal lens is to observe the effect of perturbing attributed features on the model's behavior (i.e., faithfulness). While providing useful insights, existing faithfulness evaluations suffer from shortcomings that we reveal in this paper. In this work, we propose two new perspectives within the faithfulness paradigm that reveal intuitive properties: soundness and completeness. Soundness assesses the degree to which attributed features are truly predictive features, while completeness examines how well the resulting attribution reveals all the predictive features. The two perspectives are based on a firm mathematical foundation and provide quantitative metrics that are computable through efficient algorithms. W
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23884;&#20837;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#22312;&#23454;&#29616;&#22810;&#31181;&#27969;&#21160;&#24773;&#20917;&#26102;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.13533</link><description>&lt;p&gt;
&#21487;&#24494;&#28237;&#27969; II
&lt;/p&gt;
&lt;p&gt;
Differentiable Turbulence II. (arXiv:2307.13533v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23884;&#20837;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#22312;&#23454;&#29616;&#22810;&#31181;&#27969;&#21160;&#24773;&#20917;&#26102;&#36827;&#34892;&#27979;&#35797;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#27969;&#20307;&#27169;&#25311;&#22120;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65288;CFD&#65289;&#20013;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#21487;&#24494;&#20998;&#28237;&#27969;&#25110;&#32773;&#35828;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23884;&#20837;CFD&#35299;&#31639;&#31639;&#27861;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#26082;&#20855;&#22791;&#20102;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26377;&#38480;&#30340;&#21069;&#26399;&#25104;&#26412;&#65292;&#21448;&#20855;&#22791;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#33258;&#21160;&#21270;&#35757;&#32451;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#25104;&#21040;&#36890;&#29992;&#26377;&#38480;&#20803;&#25968;&#20540;&#26041;&#26696;&#20013;&#65292;&#29992;&#20110;&#35299;Navier-Stokes&#26041;&#31243;&#65292;&#24212;&#29992;&#35813;&#25216;&#26415;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#21453;&#21521;&#38454;&#26799;&#27969;&#30340;&#23454;&#29616;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#27979;&#35797;&#20102;&#19981;&#21516;&#38647;&#35834;&#25968;&#21644;&#26032;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#38381;&#21512;&#27169;&#22411;&#22312;&#30456;&#24403;&#20110;&#36895;&#24230;&#21152;&#24555;10&#20493;&#30340;&#26356;&#32454;&#32593;&#26684;&#19978;&#21487;&#20197;&#36798;&#21040;&#19982;&#20256;&#32479;&#22823;&#28065;&#27169;&#25311;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable fluid simulators are increasingly demonstrating value as useful tools for developing data-driven models in computational fluid dynamics (CFD). Differentiable turbulence, or the end-to-end training of machine learning (ML) models embedded in CFD solution algorithms, captures both the generalization power and limited upfront cost of physics-based simulations, and the flexibility and automated training of deep learning methods. We develop a framework for integrating deep learning models into a generic finite element numerical scheme for solving the Navier-Stokes equations, applying the technique to learn a sub-grid scale closure using a multi-scale graph neural network. We demonstrate the method on several realizations of flow over a backwards-facing step, testing on both unseen Reynolds numbers and new geometry. We show that the learned closure can achieve accuracy comparable to traditional large eddy simulation on a finer grid that amounts to an equivalent speedup of 10x.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13484</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rational kernel-based interpolation for complex-valued frequency response functions. (arXiv:2307.13484v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#22312;&#22797;&#20540;&#20989;&#25968;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#39057;&#22495;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#26680;&#26041;&#27861;&#36234;&#26469;&#36234;&#24120;&#29992;&#65292;&#28982;&#32780;&#26631;&#20934;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#20540;&#24773;&#20917;&#19979;&#65292;&#24213;&#23618;&#26680;&#23545;&#30340;&#25968;&#23398;&#21547;&#20041;&#21644;&#25968;&#23398;&#25512;&#23548;&#23578;&#24453;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22797;&#20540;&#20989;&#25968;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#23558;&#24102;&#26377;&#26680;&#23545;&#30340;&#22797;&#20540;&#25554;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#36825;&#20123;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25554;&#20540;&#22120;&#19982;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#32467;&#21512;&#65292;&#20854;&#20013;&#38454;&#25968;&#26681;&#25454;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#33258;&#36866;&#24212;&#36873;&#25321;&#12290;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#30005;&#30913;&#23398;&#21644;&#22768;&#23398;&#65289;&#30340;&#20363;&#23376;&#30340;&#25968;&#20540;&#32467;&#26524;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDS&#30340;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25197;&#36716;&#25216;&#26415;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.17775</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#23454;&#29992;&#21644;&#28176;&#36827;&#31934;&#30830;&#26465;&#20214;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Practical and Asymptotically Exact Conditional Sampling in Diffusion Models. (arXiv:2306.17775v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDS&#30340;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25197;&#36716;&#25216;&#26415;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#22312;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20998;&#23376;&#35774;&#35745;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31561;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#23601;&#20027;&#35201;&#20381;&#36182;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#26465;&#20214;&#35757;&#32451;&#25110;&#23481;&#26131;&#20986;&#38169;&#30340;&#21551;&#21457;&#24335;&#36817;&#20284;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#24212;&#35813;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#29305;&#23450;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20026;&#24191;&#27867;&#30340;&#26465;&#20214;&#20998;&#24067;&#25552;&#20379;&#31934;&#30830;&#30340;&#26679;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25197;&#36716;&#24335;&#25193;&#25955;&#37319;&#26679;&#22120;(TDS)&#12290;TDS&#26159;&#19968;&#31181;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;(SMC)&#31639;&#27861;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;&#25197;&#36716;&#65292;&#19968;&#31181;&#20855;&#26377;&#33391;&#22909;&#35745;&#31639;&#25928;&#29575;&#30340;SMC&#25216;&#26415;&#65292;&#26469;&#32467;&#21512;&#21551;&#21457;&#24335;&#36817;&#20284;&#32780;&#19981;&#24433;&#21709;&#28176;&#36827;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#27169;&#25311;&#23454;&#39564;&#21644;MNIST&#22270;&#20687;&#20462;&#22797;&#20197;&#21450;&#31867;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#20013;&#21457;&#29616;&#65292;TDS&#25552;&#20379;&#20102;&#35745;&#31639;&#32479;&#35745;&#26435;&#34913;&#65292;&#20351;&#29992;&#26356;&#22810;&#31890;&#23376;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#36817;&#20284;&#32467;&#26524;&#65292;&#20294;&#21516;&#26102;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been successful on a range of conditional generation tasks including molecular design and text-to-image generation. However, these achievements have primarily depended on task-specific conditional training or error-prone heuristic approximations. Ideally, a conditional generation method should provide exact samples for a broad range of conditional distributions without requiring task-specific training. To this end, we introduce the Twisted Diffusion Sampler, or TDS. TDS is a sequential Monte Carlo (SMC) algorithm that targets the conditional distributions of diffusion models. The main idea is to use twisting, an SMC technique that enjoys good computational efficiency, to incorporate heuristic approximations without compromising asymptotic exactness. We first find in simulation and on MNIST image inpainting and class-conditional generation tasks that TDS provides a computational statistical trade-off, yielding more accurate approximations with many particles but wi
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01646</link><description>&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#23457;&#26680;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auditing for Human Expertise. (arXiv:2306.01646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01646
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19987;&#23478;&#30340;&#20215;&#20540;&#36229;&#20986;&#20102;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#65288;&#20363;&#22914;&#24739;&#32773;&#35786;&#26029;&#65289;&#36890;&#24120;&#30001;&#25509;&#21463;&#22521;&#35757;&#30340;&#20154;&#31867;&#19987;&#23478;&#22788;&#29702;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#33258;&#21160;&#21270;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#65292;&#19987;&#23478;&#21487;&#33021;&#36816;&#29992;&#24456;&#38590;&#24314;&#27169;&#30340;&#30452;&#35273;&#65292;&#24182;&#19988;/&#25110;&#32773;&#21487;&#20197;&#33719;&#21462;&#20449;&#24687;&#65288;&#20363;&#22914;&#19982;&#24739;&#32773;&#30340;&#20132;&#35848;&#65289;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#31639;&#27861;&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65292;&#20154;&#31867;&#19987;&#23478;&#26159;&#21542;&#22686;&#21152;&#20102;&#26080;&#27861;&#34987;&#31639;&#27861;&#39044;&#27979;&#22120;&#25429;&#25417;&#21040;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20026;&#19968;&#20010;&#33258;&#28982;&#30340;&#20551;&#35774;&#26816;&#39564;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#26694;&#26550;&#25152;&#24378;&#35843;&#30340;&#37027;&#26679;&#65292;&#26816;&#27979;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#27604;&#31616;&#21333;&#27604;&#36739;&#19987;&#23478;&#39044;&#27979;&#20934;&#30830;&#24615;&#19982;&#29305;&#23450;&#23398;&#20064;&#31639;&#27861;&#20570;&#20986;&#30340;&#20934;&#30830;&#24615;&#26356;&#21152;&#24494;&#22937;&#12290;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31243;&#24207;&#65292;&#27979;&#35797;&#19987;&#23478;&#39044;&#27979;&#26159;&#21542;&#22312;&#8220;&#29305;&#24449;&#8221;&#21487;&#29992;&#32780;&#26465;&#20214;&#19979;&#26159;&#21542;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#32479;&#35745;&#19978;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#27979;&#35797;&#30340;&#25298;&#32477;&#34920;&#26126;&#20102;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30830;&#23454;&#22686;&#21152;&#20102;&#36229;&#20986;&#31639;&#27861;&#21487;&#25429;&#25417;&#33539;&#22260;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that huma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00353</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#35282;&#24230;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#8212;&#8212;&#31665;&#32422;&#26463; Langevin Monte Carlo&#65288;LMC&#65289;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20960;&#20309;&#36317;&#31163;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36873;&#25321;&#20102;&#35821;&#20041;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;&#20102;&#20010;&#20307;&#23558;&#20854;&#23545;&#35821;&#20041;&#30340;&#29702;&#35299;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#20854;&#22266;&#26377;&#30340;&#21547;&#20041;&#12290;&#22312; MNIST &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#38024;&#23545;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;&#24615;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.10740</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#26041;&#27861;&#30740;&#31350;&#8212;&#8212;&#20197;&#25991;&#26412;&#21644;&#25968;&#23383;&#25968;&#25454;&#27969;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams. (arXiv:2304.10740v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#27169;&#24577;&#30340;&#28145;&#24230;&#23398;&#20064;&#34701;&#21512;&#25216;&#26415;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#65292;&#21516;&#26102;&#22312;&#27604;&#36739;&#31616;&#21333;&#21644;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#20449;&#29992;&#35780;&#32423;&#20998;&#37197;&#20013;&#21738;&#20123;&#22240;&#32032;&#26159;&#37325;&#35201;&#30340;&#21487;&#20197;&#24110;&#21161;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#30340;&#37325;&#28857;&#22823;&#22810;&#38598;&#20013;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#65292;&#36739;&#23569;&#30740;&#31350;&#38750;&#32467;&#26500;&#21270;&#25110;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#30340;&#26377;&#25928;&#26550;&#26500;&#65292;&#20197;&#39044;&#27979;&#20844;&#21496;&#20449;&#29992;&#35780;&#32423;&#26631;&#20934;&#12290;&#22312;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#34701;&#21512;&#31574;&#30053;&#30340;&#32452;&#21512;&#65292;&#21253;&#25324;CNN&#65292;LSTM&#65292;GRU&#21644;BERT&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#34701;&#21512;&#31574;&#30053;&#65288;&#21253;&#25324;&#26089;&#26399;&#21644;&#20013;&#38388;&#34701;&#21512;&#65289;&#20197;&#21450;&#25216;&#26415;&#65288;&#21253;&#25324;&#20018;&#32852;&#21644;&#20132;&#21449;&#27880;&#24847;&#65289;&#31561;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#22522;&#20110;CNN&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#20004;&#31181;&#34701;&#21512;&#31574;&#30053;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#31616;&#21333;&#30340;&#26550;&#26500;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22312;&#20449;&#29992;&#35780;&#32423;&#39044;&#27979;&#20013;&#21457;&#25381;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing which factors are significant in credit rating assignment leads to better decision-making. However, the focus of the literature thus far has been mostly on structured data, and fewer studies have addressed unstructured or multi-modal datasets. In this paper, we present an analysis of the most effective architectures for the fusion of deep learning models for the prediction of company credit rating classes, by using structured and unstructured datasets of different types. In these models, we tested different combinations of fusion strategies with different deep learning models, including CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms of level (including early and intermediate fusion) and techniques (including concatenation and cross-attention). Our results show that a CNN-based multi-modal model with two fusion strategies outperformed other multi-modal techniques. In addition, by comparing simple architectures with more complex ones, we found that more soph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2301.13306</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#30340;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#65306;&#25928;&#29575;&#12289;&#21518;&#24724;&#21644;&#33410;&#22863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#22312;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#19978;&#36827;&#34892;&#21338;&#24328;&#30340;&#24773;&#20917;&#12290;&#27599;&#20010;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#34987;&#36171;&#20104;&#20219;&#21153;&#65292;&#22312;&#22810;&#36718;&#37325;&#22797;&#25293;&#21334;&#20013;&#65292;&#26368;&#22823;&#21270;&#20854;&#24191;&#21578;&#20027;&#30340;&#24635;&#20215;&#20540;&#65292;&#21516;&#26102;&#21463;&#21040;&#39044;&#31639;&#21644;/&#25110;&#25237;&#36164;&#22238;&#25253;&#29575;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#36798;&#21040;&#36880;&#28176;&#20943;&#23567;&#30340;&#20010;&#20307;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#33258;&#21161;&#21453;&#39304;&#65292;&#24182;&#21487;&#19982;&#31532;&#19968;&#25110;&#31532;&#20108;&#20215;&#26684;&#25293;&#21334;&#20197;&#21450;&#20219;&#20309;&#8220;&#20013;&#38388;&#8221;&#25293;&#21334;&#26684;&#24335;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24403;&#36825;&#20123;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#30456;&#20114;&#31454;&#20105;&#26102;&#65292;&#25152;&#26377;&#36718;&#27425;&#30340;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160; welfare &#37117;&#33267;&#23569;&#36798;&#21040;&#20102;&#20219;&#20309;&#20998;&#37197;&#25152;&#23454;&#29616;&#30340;&#26399;&#26395;&#26368;&#20248;&#27969;&#21160; welfare &#30340;&#19968;&#21322;&#12290;&#36825;&#22312;&#20986;&#20215;&#21160;&#24577;&#26159;&#21542;&#25910;&#25947;&#21040;&#22343;&#34913;&#20197;&#21450;&#24191;&#21578;&#20027;&#20272;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#22914;&#20309;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10802</link><description>&lt;p&gt;
BTS&#65306;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#23460;&#20869;&#20004;&#25151;&#38388;&#23384;&#22312;&#26816;&#27979;&#20013;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BTS: Bifold Teacher-Student in Semi-Supervised Learning for Indoor Two-Room Presence Detection Under Time-Varying CSI. (arXiv:2212.10802v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#21452;&#25240;&#21472;&#24072;&#29983;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;CSI&#30340;&#23460;&#20869;&#23384;&#22312;&#26816;&#27979;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#26102;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#23460;&#20869;&#20154;&#20307;&#23384;&#22312;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20381;&#36182;&#20110;CSI&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#23481;&#26131;&#21463;&#21040;&#29615;&#22659;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#22914;&#29289;&#20307;&#31227;&#21160;&#12289;&#22823;&#27668;&#22240;&#32032;&#21644;&#26426;&#22120;&#37325;&#21551;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#32791;&#26102;&#30340;&#26631;&#27880;&#26469;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#35774;&#35745;&#19968;&#20010;&#36830;&#32493;&#30417;&#25511;&#30340;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24605;&#20102;&#19968;&#31181;&#21452;&#25240;&#21472;&#24072;&#29983;&#65288;BTS&#65289;&#23398;&#20064;&#26041;&#27861;&#26469;&#26816;&#27979;&#23384;&#22312;&#20110;&#31995;&#32479;&#20013;&#30340;&#23384;&#22312;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#21407;&#22987;&#23545;&#20598;&#24072;&#29983;&#32593;&#32476;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;CSI&#20013;&#26234;&#33021;&#22320;&#23398;&#20064;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#22686;&#24378;&#30340;&#24809;&#32602;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#29109;&#21644;&#36317;&#31163;&#27979;&#37327;&#26469;&#21306;&#20998;&#28145;&#23618;&#29305;&#24449;&#65292;&#38477;&#20302;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, indoor human presence detection based on supervised learning (SL) and channel state information (CSI) has attracted much attention. However, the existing studies that rely on spatial information of CSI are susceptible to environmental changes, such as object movement, atmospheric factors, and machine rebooting, which degrade prediction accuracy. Moreover, SL-based methods require time-consuming labeling for retraining models. Therefore, it is imperative to design a continuously monitored model life-cycle using a semi-supervised learning (SSL) based scheme. In this paper, we conceive a bifold teacher-student (BTS) learning approach for presence detection systems that combines SSL by utilizing partially labeled and unlabeled datasets. The proposed primal-dual teacher-student network intelligently learns spatial and temporal features from labeled and unlabeled CSI. Additionally, the enhanced penalized loss function leverages entropy and distance measures to distinguish dr
&lt;/p&gt;</description></item></channel></rss>