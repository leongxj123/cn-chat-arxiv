<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#65292;&#23558;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#20107;&#20214;&#30340;&#20998;&#24067;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#26657;&#27491;&#26041;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.18582</link><description>&lt;p&gt;
&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#19968;&#20010;&#24320;&#20851;&#25913;&#36827;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
One flow to correct them all: improving simulations in high-energy physics with a single normalising flow and a switch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18582
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#24402;&#19968;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#65292;&#23558;&#39640;&#33021;&#29289;&#29702;&#27169;&#25311;&#20107;&#20214;&#30340;&#20998;&#24067;&#26377;&#25928;&#22320;&#36716;&#25442;&#20026;&#35266;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#26657;&#27491;&#26041;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20107;&#20214;&#26159;&#20960;&#20046;&#25152;&#26377;&#39640;&#33021;&#29289;&#29702;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#20013;&#30340;&#19981;&#23436;&#32654;&#21487;&#33021;&#23548;&#33268;&#35266;&#27979;&#25968;&#25454;&#21644;&#27169;&#25311;&#20107;&#20214;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#27169;&#25311;&#19981;&#23436;&#20840;&#24615;&#23545;&#30456;&#20851;&#21487;&#35266;&#27979;&#37327;&#30340;&#24433;&#21709;&#24517;&#39035;&#36890;&#36807;&#27604;&#20363;&#22240;&#23376;&#12289;&#26435;&#37325;&#25110;&#20462;&#25913;&#21487;&#35266;&#27979;&#37327;&#21450;&#20854;&#30456;&#20851;&#24615;&#30340;&#20998;&#24067;&#26469;&#26377;&#25928;&#22320;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26657;&#27491;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#21333;&#19968;&#27491;&#24120;&#21270;&#27969;&#21644;&#24067;&#23572;&#26465;&#20214;&#30340;&#31616;&#21333;&#26550;&#26500;&#23558;&#19968;&#20010;&#22810;&#32500;&#20998;&#24067;&#65288;&#27169;&#25311;&#65289;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22810;&#32500;&#20998;&#24067;&#65288;&#25968;&#25454;&#65289;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29609;&#20855;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#23384;&#22312;&#20960;&#20010;&#21487;&#35266;&#27979;&#37327;&#21450;&#20854;&#30456;&#20851;&#24615;&#30340;&#38750;&#24179;&#20961;&#27169;&#25311;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18582v1 Announce Type: cross  Abstract: Simulated events are key ingredients in almost all high-energy physics analyses. However, imperfections in the simulation can lead to sizeable differences between the observed data and simulated events. The effects of such mismodelling on relevant observables must be corrected either effectively via scale factors, with weights or by modifying the distributions of the observables and their correlations. We introduce a correction method that transforms one multidimensional distribution (simulation) into another one (data) using a simple architecture based on a single normalising flow with a boolean condition. We demonstrate the effectiveness of the method on a physics-inspired toy dataset with non-trivial mismodelling of several observables and their correlations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.00892</link><description>&lt;p&gt;
PowerFlowMultiNet&#65306;&#29992;&#20110;&#19981;&#24179;&#34913;&#19977;&#30456;&#37197;&#30005;&#31995;&#32479;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00892
&lt;/p&gt;
&lt;p&gt;
PowerFlowMultiNet&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#22270;&#23884;&#20837;&#26426;&#21046;&#26469;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#35299;&#20915;&#37197;&#30005;&#32593;&#20013;&#19981;&#24179;&#34913;&#30340;&#19977;&#30456;&#21151;&#29575;&#27969;&#38382;&#39064;&#23545;&#20110;&#32593;&#26684;&#20998;&#26512;&#21644;&#20223;&#30495;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#24613;&#38656;&#21487;&#22788;&#29702;&#22823;&#35268;&#27169;&#19981;&#24179;&#34913;&#21151;&#29575;&#32593;&#26684;&#24182;&#33021;&#25552;&#20379;&#20934;&#30830;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23588;&#20854;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#38598;&#20013;&#22312;&#24179;&#34913;&#32593;&#32476;&#19978;&#65292;&#32570;&#20047;&#25903;&#25345;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#32476;&#30340;&#20851;&#38190;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PowerFlowMultiNet&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#19981;&#24179;&#34913;&#19977;&#30456;&#21151;&#29575;&#32593;&#26684;&#35774;&#35745;&#30340;&#26032;&#39062;&#22810;&#22270;GNN&#26694;&#26550;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#22270;&#34920;&#31034;&#20013;&#20998;&#21035;&#23545;&#27599;&#20010;&#30456;&#36827;&#34892;&#24314;&#27169;&#65292;&#26377;&#25928;&#25429;&#25417;&#19981;&#24179;&#34913;&#32593;&#26684;&#20013;&#22266;&#26377;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#24341;&#20837;&#20102;&#21033;&#29992;&#28040;&#24687;&#20256;&#36882;&#25429;&#33719;&#30005;&#21147;&#31995;&#32479;&#32593;&#32476;&#20869;&#37096;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#22270;&#23884;&#20837;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00892v1 Announce Type: cross  Abstract: Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and simulation. There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially Graph Neural Networks (GNNs), have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A graph embedding mechanism utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet out
&lt;/p&gt;</description></item><item><title>inGRASS&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#26080;&#21521;&#22270;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#20851;&#38190;&#36793;&#21644;&#26816;&#27979;&#22810;&#20313;&#36793;&#12290;</title><link>https://arxiv.org/abs/2402.16990</link><description>&lt;p&gt;
inGRASS: &#36890;&#36807;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#23454;&#29616;&#22686;&#37327;&#22270;&#35889;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
inGRASS: Incremental Graph Spectral Sparsification via Low-Resistance-Diameter Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16990
&lt;/p&gt;
&lt;p&gt;
inGRASS&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#26080;&#21521;&#22270;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#65292;&#20854;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#26041;&#26696;&#65292;&#33021;&#22815;&#39640;&#25928;&#35782;&#21035;&#20851;&#38190;&#36793;&#21644;&#26816;&#27979;&#22810;&#20313;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;inGRASS&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#23545;&#22823;&#22411;&#26080;&#21521;&#22270;&#36827;&#34892;&#22686;&#37327;&#35889;&#31232;&#30095;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;inGRASS&#31639;&#27861;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#24182;&#34892;&#21451;&#22909;&#24615;&#65292;&#35774;&#32622;&#38454;&#27573;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20960;&#20046;&#26159;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23545;&#20855;&#26377;N&#20010;&#33410;&#28857;&#30340;&#21407;&#22987;&#22270;&#36827;&#34892;&#22686;&#37327;&#26356;&#25913;&#26102;&#65292;&#20197;$O(\log N)$&#30340;&#26102;&#38388;&#26356;&#26032;&#35889;&#31232;&#30095;&#22120;&#12290;&#22312;inGRASS&#30340;&#35774;&#32622;&#38454;&#27573;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#24341;&#20837;&#20102;&#22810;&#32423;&#38459;&#25239;&#23884;&#20837;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#35782;&#21035;&#35889;&#20851;&#38190;&#36793;&#24182;&#26377;&#25928;&#26816;&#27979;&#22810;&#20313;&#36793;&#65292;&#36825;&#26159;&#36890;&#36807;&#23558;&#21021;&#22987;&#31232;&#30095;&#22120;&#20998;&#35299;&#20026;&#35768;&#22810;&#33410;&#28857;&#32676;&#38598;&#24182;&#21033;&#29992;&#20302;&#38459;&#25239;&#30452;&#24452;&#20998;&#35299;&#65288;LRD&#65289;&#26041;&#26696;&#26469;&#23454;&#29616;&#30340;&#12290;inGRASS&#30340;&#26356;&#26032;&#38454;&#27573;&#21033;&#29992;&#20302;&#32500;&#33410;&#28857;&#23884;&#20837;&#21521;&#37327;&#65292;&#26377;&#25928;&#20272;&#35745;&#27599;&#20010;&#26032;&#28155;&#21152;&#36793;&#30340;&#37325;&#35201;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16990v1 Announce Type: cross  Abstract: This work presents inGRASS, a novel algorithm designed for incremental spectral sparsification of large undirected graphs. The proposed inGRASS algorithm is highly scalable and parallel-friendly, having a nearly-linear time complexity for the setup phase and the ability to update the spectral sparsifier in $O(\log N)$ time for each incremental change made to the original graph with $N$ nodes. A key component in the setup phase of inGRASS is a multilevel resistance embedding framework introduced for efficiently identifying spectrally-critical edges and effectively detecting redundant ones, which is achieved by decomposing the initial sparsifier into many node clusters with bounded effective-resistance diameters leveraging a low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS exploits low-dimensional node embedding vectors for efficiently estimating the importance and uniqueness of each newly added edge. As de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#20248;&#21270;&#22120;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#21487;&#20197;&#35782;&#21035;&#20135;&#29983;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.16565</link><description>&lt;p&gt;
&#20248;&#21270;&#22120;&#30340;&#37096;&#20998;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Partial Rankings of Optimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#26631;&#20934;&#36827;&#34892;&#20248;&#21270;&#22120;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#32570;&#28857;&#65292;&#21487;&#20197;&#35782;&#21035;&#20135;&#29983;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26681;&#25454;&#22810;&#20010;&#26631;&#20934;&#22312;&#21508;&#31181;&#27979;&#35797;&#20989;&#25968;&#19978;&#23545;&#20248;&#21270;&#22120;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#29992;&#20110;&#20559;&#24207;/&#25490;&#24207;&#30340;&#26080;&#38598;&#21512;&#27867;&#20989;&#28145;&#24230;&#20989;&#25968;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#27425;&#24207;&#20449;&#24687;&#24182;&#20801;&#35768;&#19981;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25551;&#36848;&#20102;&#25152;&#26377;&#37096;&#20998;&#39034;&#24207;/&#25490;&#24207;&#30340;&#20998;&#24067;&#65292;&#36991;&#20813;&#20102;&#32858;&#21512;&#30340;&#33261;&#21517;&#26157;&#33879;&#30340;&#32570;&#28857;&#12290;&#36825;&#20801;&#35768;&#35782;&#21035;&#20135;&#29983;&#20248;&#21270;&#22120;&#30340;&#20013;&#24515;&#25110;&#31163;&#32676;&#25490;&#24207;&#30340;&#27979;&#35797;&#20989;&#25968;&#65292;&#24182;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16565v1 Announce Type: cross  Abstract: We introduce a framework for benchmarking optimizers according to multiple criteria over various test functions. Based on a recently introduced union-free generic depth function for partial orders/rankings, it fully exploits the ordinal information and allows for incomparability. Our method describes the distribution of all partial orders/rankings, avoiding the notorious shortcomings of aggregation. This permits to identify test functions that produce central or outlying rankings of optimizers and to assess the quality of benchmarking suites.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09164</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#26159;&#32654;&#65306;&#36890;&#36807;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#20943;&#23569;&#21487;&#35299;&#37322;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Less is More: Fewer Interpretable Region via Submodular Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30340;&#19981;&#20934;&#30830;&#21306;&#22495;&#21644;&#39044;&#27979;&#38169;&#35823;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24402;&#23646;&#31639;&#27861;&#26088;&#22312;&#30830;&#23450;&#19982;&#27169;&#22411;&#20915;&#31574;&#39640;&#24230;&#30456;&#20851;&#30340;&#37325;&#35201;&#21306;&#22495;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#24402;&#23646;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30446;&#26631;&#20803;&#32032;&#20998;&#37197;&#37325;&#35201;&#24615;&#65292;&#20294;&#20173;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;1&#65289;&#29616;&#26377;&#30340;&#24402;&#23646;&#26041;&#27861;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;&#23567;&#21306;&#22495;&#65292;&#20174;&#32780;&#35823;&#23548;&#27491;&#30830;&#24402;&#23646;&#30340;&#26041;&#21521;&#65307;2&#65289;&#27169;&#22411;&#26080;&#27861;&#20026;&#39044;&#27979;&#38169;&#35823;&#30340;&#26679;&#26412;&#20135;&#29983;&#33391;&#22909;&#30340;&#24402;&#23646;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#19978;&#36848;&#22270;&#20687;&#24402;&#23646;&#38382;&#39064;&#37325;&#26032;&#24314;&#27169;&#20026;&#27425;&#27169;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#26088;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#21306;&#22495;&#22686;&#24378;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#23616;&#37096;&#21306;&#22495;&#30340;&#20851;&#27880;&#19981;&#36275;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#19968;&#20010;&#26032;&#30340;&#27425;&#27169;&#20989;&#25968;&#26469;&#21457;&#29616;&#26356;&#20934;&#30830;&#30340;&#31934;&#32454;&#35299;&#37322;&#21306;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;&#25152;&#26377;&#26679;&#26412;&#30340;&#24402;&#23646;&#25928;&#26524;&#65292;&#25105;&#20204;&#36824;&#23545;&#23376;&#21306;&#22495;&#36873;&#25321;&#26045;&#21152;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#32422;&#26463;&#65292;&#21363;&#32622;&#20449;&#24230;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09164v1 Announce Type: cross Abstract: Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate fine-grained interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2312.03690</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#29983;&#25104;&#24314;&#27169;&#23454;&#29616;&#29627;&#29827;&#36716;&#21270;&#28201;&#24230;&#30340;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vitrimer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#20849;&#20215;&#33258;&#36866;&#24212;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#32780;&#20855;&#26377;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#30340;&#26032;&#22411;&#21487;&#25345;&#32493;&#32858;&#21512;&#29289;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26500;&#25104;&#20998;&#23376;&#36873;&#25321;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#31354;&#38388;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#28508;&#22312;&#24212;&#29992;&#30340;&#20805;&#20998;&#23454;&#29616;&#12290;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65288;Tg&#65289;&#25351;&#23548;&#20854;&#36870;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;Vitrimer&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19968;&#30334;&#19975;&#31181;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;MD&#27169;&#25311;&#65292;&#30001;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26657;&#20934;&#65292;&#35745;&#31639;&#20102;8424&#31181;&#30340;Tg&#12290;&#25152;&#25552;&#20986;&#30340;VAE&#37319;&#29992;&#21452;&#22270;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#32500;&#24230;&#37325;&#21472;&#26041;&#26696;&#65292;&#20801;&#35768;&#22810;&#25104;&#20998;Vitrimer&#30340;&#20010;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03690v2 Announce Type: replace-cross  Abstract: Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space conta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2310.19796</link><description>&lt;p&gt;
&#20351;&#29992;Syntheseus&#37325;&#26032;&#35780;&#20272;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-evaluating Retrosynthesis Algorithms with Syntheseus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19796
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Syntheseus&#24314;&#31435;&#30340;&#22522;&#20934;&#24211;&#37325;&#26032;&#35780;&#20272;&#20102;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#24182;&#25552;&#20379;&#20102;&#23545;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#20998;&#23376;&#21512;&#25104;&#35268;&#21010;&#65292;&#20063;&#31216;&#20026;&#22238;&#28335;&#21512;&#25104;&#65292;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#21270;&#23398;&#30028;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#23613;&#31649;&#30475;&#20284;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23384;&#22312;&#19981;&#23436;&#21892;&#30340;&#22522;&#20934;&#21644;&#19981;&#19968;&#33268;&#30340;&#27604;&#36739;&#25513;&#30422;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#31995;&#32479;&#24615;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;syntheseus&#30340;&#22522;&#20934;&#24211;&#65292;&#36890;&#36807;&#40664;&#35748;&#25512;&#24191;&#26368;&#20339;&#23454;&#36341;&#65292;&#23454;&#29616;&#20102;&#23545;&#21333;&#27493;&#21644;&#22810;&#27493;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#30340;&#19968;&#33268;&#32780;&#26377;&#24847;&#20041;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;syntheseus&#37325;&#26032;&#35780;&#20272;&#20102;&#33509;&#24178;&#20808;&#21069;&#30340;&#22238;&#28335;&#21512;&#25104;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#20180;&#32454;&#35780;&#20272;&#26102;&#65292;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#30340;&#25490;&#21517;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#24037;&#20316;&#30340;&#25351;&#23548;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.14557</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#24490;&#29615;&#20869;&#26680;&#25299;&#23637;&#21040;&#19981;&#21516;&#30340;&#20648;&#22791;&#35745;&#31639;&#25299;&#25169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extension of Recurrent Kernels to different Reservoir Computing topologies. (arXiv:2401.14557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#24555;&#36895;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26631;&#20934;&#30340;RC&#22312;&#28176;&#36817;&#26497;&#38480;&#19979;&#24050;&#34987;&#35777;&#26126;&#19982;&#24490;&#29615;&#20869;&#26680;&#31561;&#25928;&#65292;&#36825;&#26377;&#21161;&#20110;&#20998;&#26512;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#65292;&#22914;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#65292;&#23578;&#26410;&#20197;&#36825;&#31181;&#26041;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#26045;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#25910;&#25947;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) has become popular in recent years due to its fast and efficient computational capabilities. Standard RC has been shown to be equivalent in the asymptotic limit to Recurrent Kernels, which helps in analyzing its expressive power. However, many well-established RC paradigms, such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way. This study aims to fill this gap by providing an empirical analysis of the equivalence of specific RC architectures with their corresponding Recurrent Kernel formulation. We conduct a convergence study by varying the activation function implemented in each architecture. Our study also sheds light on the role of sparse connections in RC architectures and propose an optimal sparsity level that depends on the reservoir size. Furthermore, our systematic analysis shows that in Deep RC models, convergence is better achieved with successive reservoirs of decreasing sizes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#32463;&#20856;&#39640;&#26031;&#26680;&#22312;&#20219;&#24847;&#21442;&#25968;&#36873;&#25321;&#19979;&#37117;&#19981;&#26159;&#27491;&#23450;&#30340;&#65292;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20960;&#20309;&#21644;&#20998;&#26512;&#35770;&#35777;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#27491;&#23450;&#24615;&#30340;&#20005;&#26684;&#21051;&#30011;&#20197;&#21450;L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement&#23450;&#29702;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.19270</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#23545;&#31216;&#31354;&#38388;&#19978;&#30340;&#19981;&#21464;&#26680;&#65306;&#19968;&#31181;&#35856;&#27874;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invariant kernels on Riemannian symmetric spaces: a harmonic-analytic approach. (arXiv:2310.19270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#32463;&#20856;&#39640;&#26031;&#26680;&#22312;&#20219;&#24847;&#21442;&#25968;&#36873;&#25321;&#19979;&#37117;&#19981;&#26159;&#27491;&#23450;&#30340;&#65292;&#36890;&#36807;&#21457;&#23637;&#26032;&#30340;&#20960;&#20309;&#21644;&#20998;&#26512;&#35770;&#35777;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#27491;&#23450;&#24615;&#30340;&#20005;&#26684;&#21051;&#30011;&#20197;&#21450;L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement&#23450;&#29702;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#32463;&#20856;&#30340;&#39640;&#26031;&#26680;&#65292;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#23545;&#31216;&#31354;&#38388;&#19978;&#23450;&#20041;&#26102;&#65292;&#23545;&#20110;&#20219;&#24847;&#21442;&#25968;&#36873;&#25321;&#37117;&#19981;&#26159;&#27491;&#23450;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#21457;&#23637;&#20102;&#26032;&#30340;&#20960;&#20309;&#21644;&#20998;&#26512;&#35770;&#35777;&#12290;&#36825;&#20123;&#35770;&#35777;&#25552;&#20379;&#20102;&#39640;&#26031;&#26680;&#27491;&#23450;&#24615;&#30340;&#20005;&#26684;&#21051;&#30011;&#65292;&#20294;&#20165;&#38480;&#20110;&#22312;&#20302;&#32500;&#20013;&#36890;&#36807;&#25968;&#20540;&#35745;&#31639;&#22788;&#29702;&#30340;&#26377;&#38480;&#24773;&#20917;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#32467;&#26524;&#26159;L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement&#23450;&#29702;&#65288;&#20854;&#20013;$p = 1,2$&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#23450;&#20041;&#22312;&#38750;&#32039;&#22411;&#23545;&#31216;&#31354;&#38388;&#19978;&#30340;&#26680;&#26159;&#27491;&#23450;&#30340;&#21487;&#39564;&#35777;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#19968;&#31181;&#33879;&#21517;&#30340;&#23450;&#29702;&#65292;&#26377;&#26102;&#34987;&#31216;&#20026;Bochner-Godement&#23450;&#29702;&#65292;&#24050;&#32463;&#32473;&#20986;&#20102;&#36825;&#26679;&#30340;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#36866;&#29992;&#33539;&#22260;&#19978;&#26356;&#21152;&#24191;&#27867;&#65292;&#20294;&#24212;&#29992;&#36215;&#26469;&#23588;&#20026;&#22256;&#38590;&#12290;&#38500;&#20102;&#19982;&#39640;&#26031;&#26680;&#30340;&#20851;&#32852;&#22806;&#65292;&#22312;&#26412;&#25991;&#20013;&#30340;&#26032;&#32467;&#26524;&#20026;s&#25552;&#20379;&#20102;&#19968;&#20010;&#34013;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to prove that the classical Gaussian kernel, when defined on a non-Euclidean symmetric space, is never positive-definite for any choice of parameter. To achieve this goal, the paper develops new geometric and analytical arguments. These provide a rigorous characterization of the positive-definiteness of the Gaussian kernel, which is complete but for a limited number of scenarios in low dimensions that are treated by numerical computations. Chief among these results are the L$^{\!\scriptscriptstyle p}$-$\hspace{0.02cm}$Godement theorems (where $p = 1,2$), which provide verifiable necessary and sufficient conditions for a kernel defined on a symmetric space of non-compact type to be positive-definite. A celebrated theorem, sometimes called the Bochner-Godement theorem, already gives such conditions and is far more general in its scope, but is especially hard to apply. Beyond the connection with the Gaussian kernel, the new results in this work lay out a blueprint for the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04264</link><description>&lt;p&gt;
C(NN)FD -- &#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#27668;&#21160;&#24615;&#33021;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#31561;&#29289;&#29702;&#27169;&#25311;&#22312;&#24037;&#19994;&#19978;&#30340;&#37325;&#35201;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#30340;&#23454;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#32463;&#35777;&#26126;&#21487;&#25193;&#23637;&#33267;&#24037;&#19994;&#24212;&#29992;&#65292;&#24182;&#36798;&#21040;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20998;&#26512;&#35780;&#20272;&#24615;&#33021;&#24433;&#21709;&#24182;&#28508;&#22312;&#20943;&#23569;&#26114;&#36149;&#29289;&#29702;&#27979;&#35797;&#35201;&#27714;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15638</link><description>&lt;p&gt;
FRS-Nets: Fourier&#21442;&#25968;&#21270;&#30340;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#32593;&#32476;&#29992;&#20110;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
FRS-Nets: Fourier Parameterized Rotation and Scale Equivariant Networks for Retinal Vessel Segmentation. (arXiv:2309.15638v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRS-Nets&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#23454;&#29616;&#20102;&#23545;&#26059;&#36716;&#21644;&#23610;&#24230;&#30340;&#31561;&#21464;&#24615;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;U-Net&#21644;Iter-Net&#20013;&#26367;&#25442;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24179;&#31227;&#31561;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;CNNs&#27809;&#26377;&#23545;&#34880;&#31649;&#24418;&#24577;&#30340;&#20854;&#20182;&#23545;&#31216;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#23610;&#24230;&#23545;&#31216;&#24615;&#12290;&#20026;&#20102;&#22312;CNNs&#20013;&#23884;&#20837;&#26356;&#22810;&#31561;&#21464;&#24615;&#24182;&#28385;&#36275;&#35270;&#32593;&#33180;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#35201;&#27714;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21367;&#31215;&#31639;&#23376;&#65288;FRS-Conv&#65289;&#65292;&#23427;&#26159;&#20613;&#37324;&#21494;&#21442;&#25968;&#21270;&#30340;&#65292;&#24182;&#19988;&#23545;&#26059;&#36716;&#21644;&#32553;&#25918;&#31561;&#21464;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#26041;&#26696;&#65292;&#20351;&#21367;&#31215;&#28388;&#27874;&#22120;&#33021;&#22815;&#20197;&#39640;&#31934;&#24230;&#20219;&#24847;&#36827;&#34892;&#21464;&#25442;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26059;&#36716;&#21644;&#23610;&#24230;&#31561;&#21464;&#21367;&#31215;&#26144;&#23556;&#30340;&#20844;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25552;&#20986;&#30340;&#20844;&#24335;&#26500;&#24314;&#20102;FRS-Conv&#65292;&#24182;&#23558;U-Net&#21644;Iter-Net&#20013;&#30340;&#20256;&#32479;&#21367;&#31215;&#28388;&#27874;&#22120;&#26367;&#25442;&#20026;FRS-Conv&#65288;FRS-Nets&#65289;&#12290;&#25105;&#20204;&#24544;&#23454;&#22320;&#22797;&#29616;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
With translation equivariance, convolution neural networks (CNNs) have achieved great success in retinal vessel segmentation. However, some other symmetries of the vascular morphology are not characterized by CNNs, such as rotation and scale symmetries. To embed more equivariance into CNNs and achieve the accuracy requirement for retinal vessel segmentation, we construct a novel convolution operator (FRS-Conv), which is Fourier parameterized and equivariant to rotation and scaling. Specifically, we first adopt a new parameterization scheme, which enables convolutional filters to arbitrarily perform transformations with high accuracy. Secondly, we derive the formulations for the rotation and scale equivariant convolution mapping. Finally, we construct FRS-Conv following the proposed formulations and replace the traditional convolution filters in U-Net and Iter-Net with FRS-Conv (FRS-Nets). We faithfully reproduce all compared methods and conduct comprehensive experiments on three public
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.13004</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;?
&lt;/p&gt;
&lt;p&gt;
Can Differentiable Decision Trees Learn Interpretable Reward Functions?. (arXiv:2306.13004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#22312;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#26102;&#34920;&#29616;&#21463;&#21040;&#26641;&#30340;&#31163;&#25955;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20154;&#30340;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#22870;&#21169;&#20989;&#25968;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#26694;&#26550;&#20351;&#29992;&#40657;&#30418;&#23398;&#20064;&#26041;&#27861;&#65292;&#38590;&#20197;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#20174;&#20559;&#22909;&#20013;&#23398;&#20064;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#29366;&#24577;&#36755;&#20837;&#12290;&#25105;&#20204;&#22312;Cartpole&#12289;&#35270;&#35273;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#21644;Atari&#28216;&#25103;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DDT&#23398;&#20064;&#21487;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26641;&#24418;&#32467;&#26500;&#26377;&#21161;&#20110;&#30830;&#23450;&#22870;&#21169;&#20989;&#25968;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#31243;&#24230;&#12290;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;DDT&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20294;&#26641;&#30340;&#31163;&#25955;&#24615;&#20250;&#24433;&#21709;&#24378;&#21270;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an increasing interest in learning reward functions that model human intent and human preferences. However, many frameworks use blackbox learning methods that, while expressive, are difficult to interpret. We propose and evaluate a novel approach for learning expressive and interpretable reward functions from preferences using Differentiable Decision Trees (DDTs) for both low- and high-dimensional state inputs. We explore and discuss the viability of learning interpretable reward functions using DDTs by evaluating our algorithm on Cartpole, Visual Gridworld environments, and Atari games. We provide evidence that that the tree structure of our learned reward function is useful in determining the extent to which a reward function is aligned with human preferences. We visualize the learned reward DDTs and find that they are capable of learning interpretable reward functions but that the discrete nature of the trees hurts the performance of reinforcement learning at test time. How
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21521;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;KL&#25955;&#24230;&#39033;&#30340;&#26041;&#27861;&#65292;&#26469;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10943</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#30340;&#27010;&#29575;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Probabilistic matching of real and generated data statistics in generative adversarial networks. (arXiv:2306.10943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21521;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;KL&#25955;&#24230;&#39033;&#30340;&#26041;&#27861;&#65292;&#26469;&#20445;&#35777;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#27492;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#34429;&#28982;&#29983;&#25104;&#26679;&#26412;&#24448;&#24448;&#38590;&#20197;&#21306;&#20998;&#30495;&#23454;&#25968;&#25454;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#23427;&#20204;&#36981;&#24490;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#30830;&#20445;&#26576;&#20123;&#29983;&#25104;&#25968;&#25454;&#32479;&#35745;&#20998;&#24067;&#19982;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#24212;&#20998;&#24067;&#37325;&#21512;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#20102;Kullback-Leibler&#39033;&#65306;KL&#25955;&#24230;&#26159;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20174;&#23567;&#25209;&#37327;&#20540;&#33719;&#24471;&#30340;&#30456;&#24212;&#29983;&#25104;&#20998;&#24067;&#21644;&#30001;&#26465;&#20214;&#33021;&#37327;&#27169;&#22411;&#34920;&#31034;&#30340;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks constitute a powerful approach to generative modeling. While generated samples often are indistinguishable from real data, there is no guarantee that they will follow the true data distribution. In this work, we propose a method to ensure that the distributions of certain generated data statistics coincide with the respective distributions of the real data. In order to achieve this, we add a Kullback-Leibler term to the generator loss function: the KL divergence is taken between the true distributions as represented by a conditional energy-based model, and the corresponding generated distributions obtained from minibatch values at each iteration. We evaluate the method on a synthetic dataset and two real-world datasets and demonstrate improved performance of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.06841</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35270;&#39057;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#35270;&#39057;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#21305;&#37197;&#21253;&#21547;&#30456;&#20284;&#27963;&#21160;&#30340;&#19968;&#23545;&#35270;&#39057;&#30340;&#24103;&#12290;&#35270;&#39057;&#23545;&#40784;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#65292;&#23613;&#31649;&#20004;&#20010;&#35270;&#39057;&#20043;&#38388;&#30340;&#25191;&#34892;&#36807;&#31243;&#21644;&#22806;&#35266;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38656;&#35201;&#24314;&#31435;&#31934;&#30830;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24103;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20154;&#29289;&#26816;&#27979;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;VGG&#32593;&#32476;&#19977;&#31181;&#26426;&#22120;&#35270;&#35273;&#24037;&#20855;&#20026;&#27599;&#20010;&#35270;&#39057;&#24103;&#24341;&#20837;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20195;&#34920;&#35270;&#39057;&#30340;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#26032;&#29256;&#26412;&#65288;Diagonalized Dynamic Time Warping, DDTW&#65289;&#23545;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20219;&#20309;&#26032;&#31867;&#22411;&#30340;&#27963;&#21160;&#32780;&#26080;&#38656;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.04343</link><description>&lt;p&gt;
&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65306;&#30830;&#20445;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Certifiable Black-Box Attack: Ensuring Provably Successful Attack for Adversarial Examples. (arXiv:2304.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#65292;&#33021;&#22815;&#20445;&#35777;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#30772;&#22351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;&#27169;&#22411;&#21644;/&#25110;&#21033;&#29992;&#26412;&#22320;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#26469;&#21046;&#20316;&#23545;&#25239;&#26679;&#26412;&#12290;&#24403;&#23454;&#39564;&#35774;&#35745;&#25915;&#20987;&#26102;&#65292;&#25915;&#20987;&#26159;&#21542;&#25104;&#21151;&#23545;&#25915;&#20987;&#32773;&#26469;&#35828;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#20462;&#25913;&#38543;&#26426;&#24179;&#28369;&#24615;&#29702;&#35770;&#65292;&#39318;&#27425;&#30740;&#31350;&#20102;&#21487;&#35777;&#40657;&#30418;&#25915;&#20987;&#30340;&#26032;&#33539;&#20363;&#65292;&#33021;&#22815;&#20445;&#35777;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#22810;&#31181;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks have shown strong potential to subvert machine learning models. Existing black-box adversarial attacks craft the adversarial examples by iteratively querying the target model and/or leveraging the transferability of a local surrogate model. Whether such attack can succeed remains unknown to the adversary when empirically designing the attack. In this paper, to our best knowledge, we take the first step to study a new paradigm of adversarial attacks -- certifiable black-box attack that can guarantee the attack success rate of the crafted adversarial examples. Specifically, we revise the randomized smoothing to establish novel theories for ensuring the attack success rate of the adversarial examples. To craft the adversarial examples with the certifiable attack success rate (CASR) guarantee, we design several novel techniques, including a randomized query method to query the target model, an initialization method with smoothed self-supervised perturbation to
&lt;/p&gt;</description></item></channel></rss>