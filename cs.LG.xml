<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26469;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>https://arxiv.org/abs/2404.01595</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#26080;&#37197;&#23545;&#20542;&#21521;&#24471;&#20998;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Propensity Score Alignment of Unpaired Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26469;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20381;&#36182;&#20110;&#37197;&#23545;&#26679;&#26412;&#26469;&#23398;&#20064;&#20849;&#21516;&#30340;&#34920;&#31034;&#65292;&#20294;&#22312;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#65292;&#24448;&#24448;&#38590;&#20197;&#25910;&#38598;&#37197;&#23545;&#26679;&#26412;&#65292;&#22240;&#20026;&#27979;&#37327;&#35774;&#22791;&#36890;&#24120;&#20250;&#30772;&#22351;&#26679;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#23545;&#40784;&#19981;&#37197;&#23545;&#26679;&#26412;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#19982;&#22810;&#27169;&#24577;&#35266;&#23519;&#20013;&#30340;&#28508;&#22312;&#35270;&#22270;&#36827;&#34892;&#31867;&#27604;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;Rubin&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#20197;&#21305;&#37197;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20551;&#35774;&#25105;&#20204;&#25910;&#38598;&#20102;&#32463;&#36807;&#22788;&#29702;&#23454;&#39564;&#24178;&#25200;&#30340;&#26679;&#26412;&#65292;&#24182;&#21033;&#29992;&#27492;&#26469;&#20174;&#27599;&#31181;&#27169;&#24577;&#20013;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#65292;&#20854;&#20013;&#21253;&#25324;&#28508;&#22312;&#29366;&#24577;&#21644;&#22788;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#20849;&#20139;&#20449;&#24687;&#65292;&#24182;&#21487;&#29992;&#20110;&#23450;&#20041;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20004;&#31181;&#21033;&#29992;&#36825;&#19968;&#26041;&#27861;&#30340;&#23545;&#40784;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01595v1 Announce Type: new  Abstract: Multimodal representation learning techniques typically rely on paired samples to learn common representations, but paired samples are challenging to collect in fields such as biology where measurement devices often destroy the samples. This paper presents an approach to address the challenge of aligning unpaired samples across disparate modalities in multimodal representation learning. We draw an analogy between potential outcomes in causal inference and potential views in multimodal observations, which allows us to use Rubin's framework to estimate a common space in which to match samples. Our approach assumes we collect samples that are experimentally perturbed by treatments, and uses this to estimate a propensity score from each modality, which encapsulates all shared information between a latent state and treatment and can be used to define a distance between samples. We experiment with two alignment techniques that leverage this di
&lt;/p&gt;</description></item><item><title>QuaRot&#26159;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;LLMs&#20013;&#36827;&#34892;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#37327;&#21270;&#65292;&#24182;&#20445;&#25345;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.00456</link><description>&lt;p&gt;
QuaRot&#65306;&#26059;&#36716;LLMs&#20013;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00456
&lt;/p&gt;
&lt;p&gt;
QuaRot&#26159;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;LLMs&#20013;&#36827;&#34892;&#26080;&#24322;&#24120;&#20540;&#30340;4&#20301;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#37327;&#21270;&#65292;&#24182;&#20445;&#25345;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;QuaRot&#65292;&#19968;&#31181;&#22522;&#20110;&#26059;&#36716;&#30340;&#26032;&#37327;&#21270;&#26041;&#26696;&#65292;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#23558;LLMs&#20013;&#30340;&#25152;&#26377;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#37327;&#21270;&#20026;4&#20301;&#12290;QuaRot&#20197;&#19968;&#31181;&#33021;&#22815;&#21435;&#38500;&#38544;&#34255;&#29366;&#24577;&#20013;&#24322;&#24120;&#20540;&#20294;&#19981;&#25913;&#21464;&#36755;&#20986;&#30340;&#26041;&#24335;&#23545;LLMs&#36827;&#34892;&#26059;&#36716;&#65292;&#20351;&#24471;&#37327;&#21270;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#36825;&#31181;&#35745;&#31639;&#19981;&#21464;&#24615;&#34987;&#24212;&#29992;&#20110;LLM&#30340;&#38544;&#34255;&#29366;&#24577;&#65288;&#27531;&#24046;&#65289;&#65292;&#20197;&#21450;&#21069;&#39304;&#32452;&#20214;&#30340;&#28608;&#27963;&#12289;&#27880;&#24847;&#26426;&#21046;&#30340;&#37096;&#20998;&#20869;&#23481;&#21644;KV&#32531;&#23384;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#37327;&#21270;&#27169;&#22411;&#65292;&#20854;&#20013;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#37117;&#20197;4&#20301;&#36827;&#34892;&#65292;&#26080;&#38656;&#35782;&#21035;&#38656;&#35201;&#20197;&#26356;&#39640;&#31934;&#24230;&#20445;&#30041;&#30340;&#36890;&#36947;&#12290;&#25105;&#20204;&#30340;&#37327;&#21270;LLaMa2-70B&#27169;&#22411;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20165;&#25439;&#22833;0.29&#30340;WikiText-2&#22256;&#24785;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;99%&#30340;&#38646;-shot&#34920;&#29616;&#12290;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#33719;&#24471;&#65306;https://github.com/spcl/QuaRot&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00456v1 Announce Type: new  Abstract: We introduce QuaRot, a new Quantization scheme based on Rotations, which is able to quantize LLMs end-to-end, including all weights, activations, and KV cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output, making quantization easier. This computational invariance is applied to the hidden state (residual) of the LLM, as well as to the activations of the feed-forward components, aspects of the attention mechanism and to the KV cache. The result is a quantized model where all matrix multiplications are performed in 4-bits, without any channels identified for retention in higher precision. Our quantized LLaMa2-70B model has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14623</link><description>&lt;p&gt;
&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;
&lt;/p&gt;
&lt;p&gt;
Simplified Diffusion Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14623
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31616;&#21270;&#21518;&#30340;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#30340;&#32479;&#19968;&#35299;&#20915;&#20102;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#31616;&#21270;&#25193;&#25955;&#34203;&#23450;&#35860;&#26725;&#65288;DSB&#65289;&#65292;&#20415;&#20110;&#23558;&#20854;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#32479;&#19968;&#36215;&#26469;&#65292;&#35299;&#20915;&#20102;DSB&#22312;&#22797;&#26434;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;SGM&#20316;&#20026;DSB&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#30830;&#20445;&#20102;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25913;&#36827;&#20102;SGM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#65292;&#23613;&#31649;&#23384;&#22312;&#29702;&#35770;&#36817;&#20284;&#65292;&#20294;&#23454;&#38469;&#19978;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#35777;&#23454;&#20102;&#31616;&#21270;&#30340;DSB&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#30340;&#36129;&#29486;&#20026;&#20808;&#36827;&#30340;&#29983;&#25104;&#24314;&#27169;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14623v1 Announce Type: new  Abstract: This paper introduces a novel theoretical simplification of the Diffusion Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based Generative Models (SGMs), addressing the limitations of DSB in complex data generation and enabling faster convergence and enhanced performance. By employing SGMs as an initial solution for DSB, our approach capitalizes on the strengths of both frameworks, ensuring a more efficient training process and improving the performance of SGM. We also propose a reparameterization technique that, despite theoretical approximations, practically improves the network's fitting capabilities. Our extensive experimental evaluations confirm the effectiveness of the simplified DSB, demonstrating its significant improvements. We believe the contributions of this work pave the way for advanced generative modeling. The code is available at https://github.com/tzco/Simplified-Diffusion-Schrodinger-Bridge.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.11265</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#21892;&#20316;&#32773;&#39564;&#35777;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forging the Forger: An Attempt to Improve Authorship Verification via Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11265
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#31034;&#20363;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;&#20316;&#32773;&#39564;&#35777;&#20219;&#21153;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20851;&#27880;&#30340;&#26159;&#25512;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#19968;&#20010;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#36824;&#26159;&#30001;&#20854;&#20182;&#20154;&#25776;&#20889;&#12290;&#24050;&#32463;&#26174;&#31034;&#35768;&#22810;AV&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#24694;&#24847;&#20316;&#32773;&#31215;&#26497;&#23581;&#35797;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#26041;&#27861;&#26159;&#38544;&#34255;&#20182;&#20204;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#25110;&#32773;&#27169;&#20223;&#21478;&#19968;&#20301;&#20316;&#32773;&#30340;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#20998;&#31867;&#22120;&#35757;&#32451;&#38598;&#19982;&#65288;&#36127;&#38754;&#30340;&#65289;&#21512;&#25104;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#36825;&#20123;&#21512;&#25104;&#31034;&#20363;&#26159;&#20026;&#20102;&#27169;&#20223;&#24863;&#20852;&#36259;&#30340;&#20316;&#32773;&#30340;&#39118;&#26684;&#32780;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22686;&#24378;&#23545;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#30340;AV&#20219;&#21153;&#20013;&#24102;&#26469;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#25913;&#36827;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65288;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#23567;&#35268;&#27169;transformers&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#27969;&#34892;&#30340;GPT&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11265v1 Announce Type: cross  Abstract: Authorship Verification (AV) is a text classification task concerned with inferring whether a candidate text has been written by one specific author or by someone else. It has been shown that many AV systems are vulnerable to adversarial attacks, where a malicious author actively tries to fool the classifier by either concealing their writing style, or by imitating the style of another author. In this paper, we investigate the potential benefits of augmenting the classifier training set with (negative) synthetic examples. These synthetic examples are generated to imitate the style of the author of interest. We analyze the improvements in classifier prediction that this augmentation brings to bear in the task of AV in an adversarial setting. In particular, we experiment with three different generator architectures (one based on Recurrent Neural Networks, another based on small-scale transformers, and another based on the popular GPT mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.05024</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Hadamard U-Net for MRI Bias Field Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65292;&#24341;&#20837;&#20102;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#65292;&#24182;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#21644;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30913;&#22330;&#19981;&#22343;&#21248;&#24615;&#26657;&#27491;&#22312;MRI&#20998;&#26512;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#26159;&#20026;&#33041;MRI&#35774;&#35745;&#30340;&#65292;&#20551;&#35774;&#30456;&#21516;&#32452;&#32455;&#20013;&#30340;&#22270;&#20687;&#24378;&#24230;&#36981;&#24490;&#22343;&#21248;&#20998;&#24067;&#12290;&#36825;&#31181;&#20551;&#35774;&#19981;&#26131;&#36866;&#29992;&#20110;&#20854;&#20182;&#22120;&#23448;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20307;&#31215;&#23567;&#65292;&#36136;&#22320;&#19981;&#22343;&#21248;&#65288;&#24378;&#24230;&#21464;&#21270;&#22823;&#65289;&#30340;&#22120;&#23448;&#65292;&#27604;&#22914;&#21069;&#21015;&#33146;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21069;&#21015;&#33146;MRI&#20559;&#32622;&#22330;&#26657;&#27491;&#30340;&#27010;&#29575;&#21704;&#36798;&#29595;U-Net&#65288;PHU-Net&#65289;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21704;&#36798;&#29595;U-Net&#65288;HU-Net&#65289;&#20197;&#25552;&#21462;&#20302;&#39057;&#26631;&#37327;&#22330;&#65292;&#23558;&#20854;&#20056;&#20197;&#21407;&#22987;&#36755;&#20837;&#20197;&#33719;&#24471;&#21407;&#22411;&#26657;&#27491;&#22270;&#20687;&#12290;HU-Net&#36890;&#36807;&#21704;&#36798;&#29595;&#21464;&#25442;&#23558;&#36755;&#20837;&#22270;&#20687;&#20174;&#26102;&#22495;&#36716;&#25442;&#20026;&#39057;&#22495;&#12290;&#22312;&#39057;&#22495;&#20013;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#28388;&#27874;&#22120;&#65288;&#32553;&#25918;&#23618;&#65289;&#12289;&#30828;&#38408;&#20540;&#23618;&#28040;&#38500;&#39640;&#39057;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05024v1 Announce Type: cross  Abstract: Magnetic field inhomogeneity correction remains a challenging task in MRI analysis. Most established techniques are designed for brain MRI by supposing that image intensities in the identical tissue follow a uniform distribution. Such an assumption cannot be easily applied to other organs, especially those that are small in size and heterogeneous in texture (large variations in intensity), such as the prostate. To address this problem, this paper proposes a probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field correction. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the low-frequency scalar field, multiplied by the original input to obtain the prototypical corrected image. HU-Net converts the input image from the time domain into the frequency domain via Hadamard transform. In the frequency domain, high-frequency components are eliminated using the trainable filter (scaling layer), hard-thresholding laye
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.03949</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#35843;&#21644;&#29616;&#23454;&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#25805;&#20316;&#30340;&#23454;-&#27169;-&#23454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03949
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RialTo&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#30417;&#30563;&#26469;&#23398;&#20064;&#23545;&#29289;&#20307;&#23039;&#21183;&#21464;&#21270;&#12289;&#29289;&#29702;&#24178;&#25200;&#21644;&#35270;&#35273;&#25200;&#21160;&#40065;&#26834;&#30340;&#31574;&#30053;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#33258;&#20027;&#25506;&#32034;&#29615;&#22659;&#20197;&#23398;&#20064;&#31283;&#20581;&#34892;&#20026;&#65292;&#20294;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#19981;&#23433;&#20840;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#12290;&#20026;&#20102;&#22312;&#27809;&#26377;&#19981;&#23433;&#20840;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37319;&#38598;&#25110;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#36127;&#25285;&#19979;&#23398;&#20064;&#24615;&#33021;&#20248;&#36234;&#12289;&#31283;&#20581;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RialTo&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#21363;&#23558;&#20174;&#23569;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26500;&#24314;&#30340;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#31283;&#20581;&#21270;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#20223;&#23398;&#20064;&#31574;&#30053;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23454;-&#27169;-&#23454;&#27969;&#27700;&#32447;&#65292;RialTo&#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#24555;&#36895;&#25195;&#25551;&#21644;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#25968;&#23383;&#23402;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#21453;&#21521;&#25552;&#28860;&#8221;&#36807;&#31243;&#65292;&#29992;&#20110;&#32473;&#30495;&#23454;&#19990;&#30028;&#28436;&#31034;&#24102;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03949v1 Announce Type: cross  Abstract: Imitation learning methods need significant human supervision to learn policies robust to changes in object poses, physical disturbances, and visual distractors. Reinforcement learning, on the other hand, can explore the environment autonomously to learn robust behaviors but may require impractical amounts of unsafe real-world data collection. To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments. We also introduce a novel "inverse distillation" procedure for bringing real-world demonstrations
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#34013;&#33394;&#21644;&#32511;&#33394;&#20004;&#31181;&#24037;&#20316;&#27169;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01642</link><description>&lt;p&gt;
&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#23454;&#29616;&#30340;&#34013;&#32511;&#27169;&#24335;&#39640;&#33021;&#25928;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;
&lt;/p&gt;
&lt;p&gt;
Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#30340;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#34013;&#33394;&#21644;&#32511;&#33394;&#20004;&#31181;&#24037;&#20316;&#27169;&#24335;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01642v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#29289;&#32852;&#32593;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#24320;&#21457;&#26082;&#39640;&#25928;&#21448;&#33021;&#32988;&#20219;&#30340;&#20248;&#21270;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#37319;&#29992;&#24555;&#36895;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#22996;&#21592;&#20250;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#21033;&#29992;&#24377;&#24615;&#32593;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#31561;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31574;&#30053;&#35782;&#21035;&#20986;&#22312;&#21270;&#23398;&#20256;&#24863;&#22120;&#38453;&#21015;&#20013;&#23545;&#20934;&#30830;&#20998;&#31867;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#20256;&#24863;&#22120;&#65306;&#24341;&#20837;&#21152;&#26435;&#25237;&#31080;&#26426;&#21046;&#26469;&#32858;&#21512;&#20256;&#24863;&#22120;&#36873;&#25321;&#20013;&#30340;&#27169;&#22411;&#24847;&#35265;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24037;&#20316;&#27169;&#24335;&#65292;&#31216;&#20026;&#8220;&#34013;&#33394;&#8221;&#21644;&#8220;&#32511;&#33394;&#8221;&#12290;&#34013;&#33394;&#27169;&#24335;&#21033;&#29992;&#25152;&#26377;&#20256;&#24863;&#22120;&#36827;&#34892;&#26368;&#22823;&#26816;&#27979;&#33021;&#21147;&#65292;&#32780;&#32511;&#33394;&#27169;&#24335;&#20165;&#36873;&#25321;&#24615;&#28608;&#27963;&#20851;&#38190;&#20256;&#24863;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#38477;&#20302;&#33021;&#32791;&#32780;&#19981;&#24433;&#21709;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#29702;&#35770;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01642v1 Announce Type: new  Abstract: The rapid advancement of Internet of Things (IoT) necessitates the development of optimized Chemiresistive Sensor (CRS) arrays that are both energy-efficient and capable. This study introduces a novel optimization strategy that employs a rapid ensemble learning-based model committee approach to achieve these goals. Utilizing machine learning models such as Elastic Net Regression, Random Forests, and XGBoost, among others, the strategy identifies the most impactful sensors in a CRS array for accurate classification: A weighted voting mechanism is introduced to aggregate the models' opinions in sensor selection, thereby setting up wo distinct working modes, termed "Blue" and "Green". The Blue mode operates with all sensors for maximum detection capability, while the Green mode selectively activates only key sensors, significantly reducing energy consumption without compromising detection accuracy. The strategy is validated through theoreti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.18752</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#20844;&#20849;&#25968;&#25454;&#23545;&#26377;&#24046;&#24322;&#38544;&#31169;&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training Differentially Private Models with Limited Public Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18752
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#26174;&#33879;&#32531;&#35299;&#20248;&#21270;&#22120;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#21331;&#36234;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#20351;&#29992;&#65292;&#28982;&#32780;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#38656;&#35201;&#27491;&#24335;&#20445;&#25252;&#30340;&#25935;&#24863;&#12289;&#31169;&#20154;&#21644;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#25552;&#20379;&#30340;&#23433;&#20840;&#31243;&#24230;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#28982;&#32780;&#30001;&#20110;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24212;&#29992;&#24046;&#20998;&#38544;&#31169;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#20854;&#24212;&#29992;&#36890;&#24120;&#20165;&#38480;&#20110;&#27169;&#22411;&#24494;&#35843;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#24046;&#20998;&#38544;&#31169;&#30446;&#21069;&#23578;&#19981;&#33021;&#20445;&#25252;&#21021;&#22987;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#22823;&#37096;&#20998;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20998;&#26512;&#27599;&#27425;&#36845;&#20195;&#30340;&#25439;&#22833;&#25913;&#36827;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#21487;&#20197;&#26174;&#33879;&#32531;&#35299;&#24046;&#20998;&#38544;&#31169;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#24341;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#25345;&#32493;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#36890;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18752v1 Announce Type: new  Abstract: The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, usi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#25913;&#36827;&#20102;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;GAIL&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;MuJoCo&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.16349</link><description>&lt;p&gt;
C-GAIL: &#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#31283;&#23450;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#25511;&#21046;&#29702;&#35770;&#25913;&#36827;&#20102;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;GAIL&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;MuJoCo&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#31574;&#30053;&#26469;&#27169;&#20223;&#19968;&#20010;&#28436;&#31034;&#32773;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#20248;&#21270;&#20174;&#31867;&#20284;GAN&#30340;&#37492;&#21035;&#22120;&#20013;&#23548;&#20986;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;GAIL&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#20854;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615; - &#23427;&#32487;&#25215;&#20102;GAN&#30340;&#22797;&#26434;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;RL&#24341;&#20837;&#30340;&#20998;&#24067;&#36716;&#31227;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25391;&#33633;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#26679;&#26412;&#25928;&#29575;&#21644;&#26368;&#32456;&#31574;&#30053;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#25511;&#21046;&#29702;&#35770;&#21487;&#20197;&#24110;&#21161;GAN&#30340;&#35757;&#32451;&#25910;&#25947;&#12290;&#26412;&#25991;&#24310;&#20280;&#20102;&#36825;&#19968;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#23545;GAIL&#36827;&#34892;&#20102;&#25511;&#21046;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#19981;&#20165;&#23558;GAIL&#25512;&#21521;&#26399;&#26395;&#30340;&#22343;&#34913;&#28857;&#65292;&#36824;&#22312;&#8220;&#21333;&#27493;&#8221;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#28176;&#36817;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#8220;Controlled-GAIL&#8221;&#65288;C-GAIL&#65289;&#12290;&#22312;MuJoCo&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#21463;&#25511;&#21464;&#20307;&#33021;&#22815;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16349v1 Announce Type: new  Abstract: Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of conve
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.11652</link><description>&lt;p&gt;
&#22240;&#26524;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#31283;&#20581;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Inference in Causal Latent Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11652
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#24615;&#36136;&#65292;&#24182;&#22312;&#21442;&#25968;&#36895;&#29575;&#19979;&#23558;&#20854;&#35823;&#24046;&#25910;&#25947;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#29616;&#20195;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#20272;&#35745;&#23384;&#22312;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#19979;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#29615;&#22659;&#20855;&#26377;&#22823;&#37327;&#21333;&#20301;&#21644;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#37327;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#65292;&#32467;&#21512;&#20102;&#32467;&#26524;&#22635;&#34917;&#12289;&#20498;&#25968;&#27010;&#29575;&#21152;&#26435;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#34917;&#20840;&#30340;&#26032;&#22411;&#20132;&#21449;&#37197;&#23545;&#31243;&#24207;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20272;&#35745;&#37327;&#30340;&#35823;&#24046;&#25910;&#25947;&#21040;&#21442;&#25968;&#36895;&#29575;&#19979;&#30340;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#12290;&#27169;&#25311;&#32467;&#26524;&#23637;&#31034;&#20102;&#26412;&#25991;&#20998;&#26512;&#30340;&#20272;&#35745;&#37327;&#30340;&#24418;&#24335;&#29305;&#24615;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11652v1 Announce Type: cross  Abstract: This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.
&lt;/p&gt;</description></item><item><title>OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11427</link><description>&lt;p&gt;
OptEx: &#21033;&#29992;&#36817;&#20284;&#24182;&#34892;&#21270;&#36845;&#20195;&#21152;&#36895;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11427
&lt;/p&gt;
&lt;p&gt;
OptEx&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#19968;&#38454;&#20248;&#21270;&#30340;&#36845;&#20195;&#29942;&#39048;&#24182;&#22686;&#24378;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#23454;&#29616;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270;&#65292;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#38454;&#20248;&#21270;&#65288;FOO&#65289;&#31639;&#27861;&#22312;&#35832;&#22914;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#21435;&#22122;&#31561;&#20247;&#22810;&#35745;&#31639;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31561;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#22240;&#20026;&#38656;&#35201;&#35768;&#22810;&#39034;&#24207;&#36845;&#20195;&#20197;&#23454;&#29616;&#25910;&#25947;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#38454;&#20248;&#21270;&#21152;&#36895;&#36817;&#20284;&#24182;&#34892;&#36845;&#20195;&#65288;OptEx&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#20943;&#36731;&#20854;&#36845;&#20195;&#29942;&#39048;&#32780;&#22686;&#24378;FOO&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;OptEx&#37319;&#29992;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#26469;&#21033;&#29992;&#26799;&#24230;&#21382;&#21490;&#36827;&#34892;&#26410;&#26469;&#26799;&#24230;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#36845;&#20195;&#30340;&#24182;&#34892;&#21270; -- &#36825;&#26159;&#19968;&#31181;&#26366;&#32463;&#34987;&#35748;&#20026;&#30001;&#20110;FOO&#20013;&#22266;&#26377;&#30340;&#36845;&#20195;&#20381;&#36182;&#32780;&#19981;&#20999;&#23454;&#38469;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26680;&#21270;&#26799;&#24230;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#21644;&#22522;&#20110;SGD&#30340;OptEx&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#30830;&#35748;&#20102;&#20854;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02425</link><description>&lt;p&gt;
EuLagNet: &#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#27431;&#25289;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02425
&lt;/p&gt;
&lt;p&gt;
EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#23545;&#27668;&#35937;&#23398;&#12289;&#28023;&#27915;&#23398;&#21644;&#31354;&#27668;&#21160;&#21147;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#36890;&#24120;&#20174;&#27431;&#25289;&#35282;&#24230;&#35266;&#23519;&#65292;&#20854;&#27963;&#36291;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#22312;&#38745;&#27490;&#30340;&#32593;&#26684;&#20013;&#20005;&#37325;&#34987;&#25513;&#30422;&#21644;&#28151;&#28102;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#20026;&#23548;&#21521;&#30340;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#21452;&#37325;&#36882;&#24402;&#32593;&#32476;&#65288;EuLagNet&#65289;&#65292;&#36890;&#36807;&#36319;&#36394;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#24182;&#38543;&#26102;&#38388;&#31215;&#32047;&#21160;&#21147;&#23398;&#20449;&#24687;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EuLag&#22359;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#26102;&#21051;&#21644;&#23610;&#24230;&#19978;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27431;&#25289;&#21644;&#25289;&#26684;&#26391;&#26085;&#29305;&#24449;&#65292;&#20854;&#20013;&#36319;&#36394;&#31890;&#23376;&#30340;&#36816;&#21160;&#26159;&#20174;&#27431;&#25289;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#31215;&#32047;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.02103</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
D\'ej\`a Vu Memorization in Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02103
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#35757;&#32451;&#30340;VLMs&#65292;&#27169;&#22411;&#30830;&#23454;&#20250;&#20445;&#30041;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#30340;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#25991;&#26412;&#38543;&#26426;&#21270;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#35760;&#24518;&#29616;&#35937;&#32780;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#20855;&#26377;&#35832;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20250;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20063;&#23545;&#27867;&#21270;&#26377;&#30528;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;VLMs&#20013;&#35760;&#24518;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#12290;&#23545;&#20110;&#22312;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#30340;VLMs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30830;&#23454;&#20445;&#30041;&#20102;&#20851;&#20110;&#35757;&#32451;&#22270;&#20687;&#20013;&#20010;&#21035;&#23545;&#35937;&#30340;&#20449;&#24687;&#65292;&#36229;&#20986;&#20102;&#20174;&#30456;&#20851;&#24615;&#25110;&#22270;&#20687;&#26631;&#39064;&#20013;&#21487;&#20197;&#25512;&#26029;&#20986;&#30340;&#33539;&#30068;&#12290;&#25105;&#20204;&#22312;&#26679;&#26412;&#21644;&#24635;&#20307;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#24515;&#29702;&#29616;&#35937;&#35760;&#24518;&#65292;&#24182;&#23637;&#31034;&#20102;OpenCLIP&#22312;&#22810;&#36798;5000&#19975;&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#19978;&#35757;&#32451;&#26102;&#30340;&#26174;&#33879;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25991;&#26412;&#38543;&#26426;&#21270;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#35760;&#24518;&#65292;&#21516;&#26102;&#23545;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20135;&#29983;&#20102;&#36866;&#24230;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have emerged as the state-of-the-art representation learning solution, with myriads of downstream applications such as image classification, retrieval and generation. A natural question is whether these models memorize their training data, which also has implications for generalization. We propose a new method for measuring memorization in VLMs, which we call d\'ej\`a vu memorization. For VLMs trained on image-caption pairs, we show that the model indeed retains information about individual objects in the training images beyond what can be inferred from correlations or the image caption. We evaluate d\'ej\`a vu memorization at both sample and population level, and show that it is significant for OpenCLIP trained on as many as 50M image-caption pairs. Finally, we show that text randomization considerably mitigates memorization while only moderately impacting the model's downstream task performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.05821</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#27010;&#24565;&#29942;&#39048;&#29992;&#20110;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents. (arXiv:2401.05821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05821
&lt;/p&gt;
&lt;p&gt;
SCoBots&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65292;&#33021;&#22815;&#36879;&#26126;&#21270;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#65292;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#21644;&#35268;&#33539;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#31232;&#30095;&#24615;&#12289;&#38590;&#20197;&#24402;&#22240;&#30340;&#38382;&#39064;&#20197;&#21450;&#19981;&#23545;&#40784;&#31561;&#31561;&#37117;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#21442;&#19982;&#65292;&#36825;&#20123;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24182;&#32416;&#27491;&#38169;&#35823;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36830;&#32493;&#27010;&#24565;&#29942;&#39048;&#20195;&#29702;&#65288;SCoBots&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#36830;&#32493;&#30340;&#27010;&#24565;&#29942;&#39048;&#23618;&#65292;&#20351;&#25972;&#20010;&#20915;&#31574;&#27969;&#31243;&#36879;&#26126;&#21270;&#12290;SCoBots&#19981;&#20165;&#21033;&#29992;&#30456;&#20851;&#30340;&#23545;&#35937;&#23646;&#24615;&#65292;&#36824;&#21033;&#29992;&#20851;&#31995;&#27010;&#24565;&#12290;&#23454;&#39564;&#32467;&#26524;&#24378;&#26377;&#21147;&#22320;&#35777;&#26126;&#65292;SCoBots&#20351;&#39046;&#22495;&#19987;&#23478;&#33021;&#22815;&#26377;&#25928;&#29702;&#35299;&#21644;&#35268;&#33539;&#20182;&#20204;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#21487;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SCoBots&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#26368;&#31616;&#21333;&#19988;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;Pong&#20013;&#30340;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#21152;&#20197;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward sparsity, difficult credit assignment, and misalignment are only a few of the many issues that make it difficult, if not impossible, for deep reinforcement learning (RL) agents to learn optimal policies. Unfortunately, the black-box nature of deep networks impedes the inclusion of domain experts who could interpret the model and correct wrong behavior. To this end, we introduce Successive Concept Bottlenecks Agents (SCoBots), which make the whole decision pipeline transparent via the integration of consecutive concept bottleneck layers. SCoBots make use of not only relevant object properties but also of relational concepts. Our experimental results provide strong evidence that SCoBots allow domain experts to efficiently understand and regularize their behavior, resulting in potentially better human-aligned RL. In this way, SCoBots enabled us to identify a misalignment problem in the most simple and iconic video game, Pong, and resolve it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2311.01404</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#23558;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#24402;&#19968;&#21270;&#27969;"&#19968;&#35789;&#19982;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#30456;&#20851;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;$W_2$-&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#24674;&#22797;&#20026;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#21512;&#36866;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#27979;&#24230;$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$&#21644;&#21463;&#25511;&#21521;&#37327;&#22330;&#65292;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#21253;&#21547;&#22312;&#31995;&#32479;&#20135;&#29983;&#30340;&#27969;&#21160;&#30340;$C^0_c$&#38381;&#21253;&#20013;&#12290;&#20551;&#35774;&#21407;&#22987;&#27979;&#24230;$\mu,\nu$&#30340;&#31163;&#25955;&#36817;&#20284;$\mu_N,\nu_N$&#21487;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;$\gamma_N$&#26469;&#23450;&#20041;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;$\Gamma$-&#25910;&#25947;&#35770;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#35299;&#23545;&#24212;&#20110;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#30340;&#27969;&#21160;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;Pontryagin&#26368;&#22823;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#20540;&#26041;&#26696;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the reso
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#22806;&#37096;&#26377;&#25928;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.14763</link><description>&lt;p&gt;
&#22806;&#37096;&#39564;&#35777;&#31574;&#30053;&#35780;&#20272;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Externally Valid Policy Evaluation Combining Trial and Observational Data. (arXiv:2310.14763v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14763
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#35797;&#39564;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#22806;&#37096;&#26377;&#25928;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35797;&#39564;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35780;&#20272;&#20915;&#31574;&#31574;&#30053;&#24433;&#21709;&#30340;&#37329; standard&#12290;&#28982;&#32780;&#65292;&#35797;&#39564;&#25968;&#25454;&#26469;&#33258;&#21487;&#33021;&#19982;&#30446;&#26631;&#20154;&#32676;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#36825;&#24341;&#21457;&#20102;&#22806;&#37096;&#25928;&#24230;&#65288;&#20063;&#31216;&#20026;&#27867;&#21270;&#33021;&#21147;&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#35797;&#39564;&#25968;&#25454;&#23545;&#30446;&#26631;&#20154;&#32676;&#19978;&#30340;&#25919;&#31574;&#32467;&#26524;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#30446;&#26631;&#20154;&#32676;&#30340;&#39069;&#22806;&#21327;&#21464;&#37327;&#25968;&#25454;&#29992;&#20110;&#27169;&#25311;&#35797;&#39564;&#30740;&#31350;&#20013;&#20010;&#20307;&#30340;&#25277;&#26679;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20219;&#20309;&#25351;&#23450;&#30340;&#27169;&#22411;&#26410;&#26657;&#20934;&#33539;&#22260;&#20869;&#20135;&#29983;&#21487;&#39564;&#35777;&#30340;&#22522;&#20110;&#35797;&#39564;&#30340;&#25919;&#31574;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#21363;&#20351;&#26679;&#26412;&#26159;&#26377;&#38480;&#30340;&#65292;&#26377;&#25928;&#24615;&#20063;&#24471;&#21040;&#20445;&#35777;&#12290;&#20351;&#29992;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#35828;&#26126;&#20102;&#35748;&#35777;&#30340;&#25919;&#31574;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized trials are widely considered as the gold standard for evaluating the effects of decision policies. Trial data is, however, drawn from a population which may differ from the intended target population and this raises a problem of external validity (aka. generalizability). In this paper we seek to use trial data to draw valid inferences about the outcome of a policy on the target population. Additional covariate data from the target population is used to model the sampling of individuals in the trial study. We develop a method that yields certifiably valid trial-based policy evaluations under any specified range of model miscalibrations. The method is nonparametric and the validity is assured even with finite samples. The certified policy evaluations are illustrated using both simulated and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#37325;&#25972;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32553;&#25918;&#30340;&#26230;&#26684;&#37197;&#32622;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#20351;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#19979;&#30340;&#31934;&#30830;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.12631</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#31995;&#32479;&#30340;&#36870;&#37325;&#25972;&#32676;
&lt;/p&gt;
&lt;p&gt;
Inverse Renormalization Group of Disordered Systems. (arXiv:2310.12631v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#37325;&#25972;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32553;&#25918;&#30340;&#26230;&#26684;&#37197;&#32622;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#20351;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#19979;&#30340;&#31934;&#30830;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36870;&#37325;&#25972;&#32676;&#21464;&#25442;&#65292;&#29992;&#20110;&#26500;&#24314;&#23578;&#26410;&#34987;&#36229;&#32423;&#35745;&#31639;&#26426;&#25110;&#22823;&#35268;&#27169;&#27169;&#25311;&#26041;&#27861;&#25152;&#35775;&#38382;&#30340;&#26230;&#26684;&#20307;&#31215;&#30340;&#36817;&#20284;&#37197;&#32622;&#65292;&#20197;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#32500;&#29233;&#24503;&#21326;-&#23433;&#24503;&#26862;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#20307;&#31215;&#20026;$V=8^{3}$&#30340;&#26230;&#26684;&#24320;&#22987;&#65292;&#25105;&#20204;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32463;&#36807;&#32553;&#25918;&#30340;&#26230;&#26684;&#65292;&#26368;&#22823;&#21040;$V'=128^{3}$&#65292;&#24182;&#25552;&#21462;&#20102;&#20004;&#20010;&#20020;&#30028;&#25351;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#36870;&#37325;&#25972;&#32676;&#26041;&#27861;&#20013;&#34701;&#20837;&#25968;&#20540;&#31934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#25552;&#20379;&#20102;&#25506;&#32034;&#21487;&#25345;&#32493;&#12289;&#33410;&#33021;&#30340;&#31934;&#30830;&#37197;&#32622;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose inverse renormalization group transformations to construct approximate configurations for lattice volumes that have not yet been accessed by supercomputers or large-scale simulations in the study of spin glasses. Specifically, starting from lattices of volume $V=8^{3}$ in the case of the three-dimensional Edwards-Anderson model we employ machine learning algorithms to construct rescaled lattices up to $V'=128^{3}$, which we utilize to extract two critical exponents. We conclude by discussing how to incorporate numerical exactness within inverse renormalization group approaches of disordered systems, thus opening up the opportunity to explore a sustainable and energy-efficient generation of exact configurations for increasing lattice volumes without the use of dedicated supercomputers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.07852</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#39640;&#32500;&#31169;&#26377;&#27169;&#22411;&#36873;&#25321;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#25351;&#25968;&#26426;&#21046;&#26469;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20854;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#25351;&#25968;&#26426;&#21046;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#36827;&#34892;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#22312;&#38382;&#39064;&#21442;&#25968;$n$&#12289;$p$&#21644;$s$&#20013;&#24314;&#31435;&#20102;&#20854;&#21040;&#31283;&#24577;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20854;&#28151;&#21512;&#24615;&#36136;&#24314;&#31435;&#20102;Metropolis-Hastings&#38543;&#26426;&#34892;&#36208;&#30340;&#26368;&#32456;&#20272;&#35745;&#30340;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#35828;&#26126;&#24615;&#27169;&#25311;&#65292;&#21360;&#35777;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.05495</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#32852;&#37030;&#24179;&#22343;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Neural Tangent Kernel View on Federated Averaging for Deep Linear Neural Network. (arXiv:2310.05495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#35270;&#35282;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#22312;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#26469;&#33258;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#22914;&#20170;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#24615;&#33021;&#65292;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;FedAvg&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#29978;&#33267;&#26159;&#38750;&#20809;&#28369;&#30340;&#12290;&#27492;&#22806;&#65292;FedAvg&#24635;&#26159;&#28041;&#21450;&#22810;&#20010;&#23458;&#25143;&#31471;&#21644;&#26412;&#22320;&#26356;&#26032;&#65292;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#36825;&#20123;&#23646;&#24615;&#32473;&#20998;&#26512;FedAvg&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#29702;&#35770;&#24050;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#29702;&#35299;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#38750;&#20984;&#38382;&#39064;&#20013;&#30340;&#19968;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#29702;&#35770;&#23398;&#31185;&#20013;&#30340;&#32463;&#20856;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#20844;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#23545;&#20110;FedAvg&#30340;&#25910;&#25947;&#24615;&#30446;&#21069;&#36824;&#27809;&#26377;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated averaging (FedAvg) is a widely employed paradigm for collaboratively training models from distributed clients without sharing data. Nowadays, the neural network has achieved remarkable success due to its extraordinary performance, which makes it a preferred choice as the model in FedAvg. However, the optimization problem of the neural network is often non-convex even non-smooth. Furthermore, FedAvg always involves multiple clients and local updates, which results in an inaccurate updating direction. These properties bring difficulties in analyzing the convergence of FedAvg in training neural networks. Recently, neural tangent kernel (NTK) theory has been proposed towards understanding the convergence of first-order methods in tackling the non-convex problem of neural networks. The deep linear neural network is a classical model in theoretical subject due to its simple formulation. Nevertheless, there exists no theoretical result for the convergence of FedAvg in training the d
&lt;/p&gt;</description></item><item><title>FeDEQ&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15659</link><description>&lt;p&gt;
&#32852;&#37030;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#65306;&#36793;&#32536;&#36890;&#20449;&#25928;&#29575;&#30340;&#32039;&#20945;&#20849;&#20139;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Federated Deep Equilibrium Learning: A Compact Shared Representation for Edge Communication Efficiency. (arXiv:2309.15659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15659
&lt;/p&gt;
&lt;p&gt;
FeDEQ&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#36890;&#36807;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#30340;&#36890;&#20449;&#29942;&#39048;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21331;&#36234;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20419;&#36827;&#20102;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#20043;&#38388;&#30340;&#21327;&#20316;&#65292;&#20197;&#22312;&#19981;&#38598;&#20013;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#36716;&#31227;&#21040;&#32593;&#32476;&#36793;&#32536;&#65292;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20102;&#40065;&#26834;&#21644;&#21709;&#24212;&#36805;&#36895;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22686;&#24378;&#20102;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#37096;&#32626;&#28145;&#24230;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#21463;&#21040;&#36890;&#20449;&#29942;&#39048;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#20869;&#23384;&#38480;&#21046;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FeDEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#26377;&#25928;&#22320;&#37319;&#29992;&#28145;&#24230;&#24179;&#34913;&#23398;&#20064;&#21644;&#20849;&#35782;&#20248;&#21270;&#65292;&#22312;&#36793;&#32536;&#33410;&#28857;&#20043;&#38388;&#21033;&#29992;&#32039;&#20945;&#30340;&#20849;&#20139;&#25968;&#25454;&#34920;&#31034;&#65292;&#20801;&#35768;&#27966;&#29983;&#20986;&#38024;&#23545;&#27599;&#20010;&#33410;&#28857;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#30001;&#19968;&#20010;&#24179;&#34913;&#23618;&#21644;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#23618;&#32452;&#25104;&#12290;&#22312;&#36825;&#37324;&#65292;&#24179;&#34913;&#23618;&#20805;&#24403;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#65292;&#36793;&#32536;&#33410;&#28857;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36827;&#34892;&#20010;&#24615;&#21270;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a prominent distributed learning paradigm facilitating collaboration among nodes within an edge network to co-train a global model without centralizing data. By shifting computation to the network edge, FL offers robust and responsive edge-AI solutions and enhance privacy-preservation. However, deploying deep FL models within edge environments is often hindered by communication bottlenecks, data heterogeneity, and memory limitations. To address these challenges jointly, we introduce FeDEQ, a pioneering FL framework that effectively employs deep equilibrium learning and consensus optimization to exploit a compact shared data representation across edge nodes, allowing the derivation of personalized models specific to each node. We delve into a unique model structure composed of an equilibrium layer followed by traditional neural network layers. Here, the equilibrium layer functions as a global feature representation that edge nodes can adapt to personalize thei
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2307.11972</link><description>&lt;p&gt;
&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#21306;&#22806;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Optimality of Invariant Risk Minimization. (arXiv:2307.11972v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#32487;&#25215;&#35757;&#32451;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#27867;&#21270;&#21040;&#20855;&#26377;&#19982;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#30340;&#39046;&#22495;&#19981;&#21516;&#30340;&#26410;&#30693;&#22495;&#12290;M. Arjovsky&#31561;&#20154;&#65288;2019&#24180;&#65289;&#24341;&#20837;&#20102;&#21306;&#22806;&#65288;o.o.d.&#65289;&#39118;&#38505;&#30340;&#27010;&#24565;&#65292;&#21363;&#25152;&#26377;&#22495;&#20013;&#30340;&#26368;&#22823;&#39118;&#38505;&#65292;&#24182;&#23558;&#30001;&#34394;&#20551;&#30456;&#20851;&#24615;&#24341;&#36215;&#30340;&#38382;&#39064;&#35268;&#23450;&#20026;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#38382;&#39064;&#12290;&#19981;&#21464;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;IRM&#65289;&#34987;&#35748;&#20026;&#26159;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65306;IRM&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#20272;&#35745;&#26368;&#23567;&#21270;&#30340;&#21306;&#22806;&#39118;&#38505;&#12290;&#23613;&#31649;IRM&#20197;&#23454;&#35777;&#25104;&#21151;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#32570;&#20047;&#19968;&#20123;&#29702;&#35770;&#20445;&#35777;&#12290;&#29305;&#21035;&#26159;&#65292;&#36824;&#27809;&#26377;&#30830;&#31435;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#32473;&#20986;&#26368;&#23567;&#21270;&#21306;&#22806;&#39118;&#38505;&#30340;&#22362;&#23454;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;IRM&#30340;&#29702;&#35770;&#39564;&#35777;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36890;&#36807;&#22312;&#22823;&#20223;&#30495;&#36319;&#36394;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#23454;&#26102;&#20223;&#30495;&#65292;&#20854;&#21253;&#25324;&#23545;&#21608;&#22260;&#29615;&#22659;&#30340;&#30452;&#25509;&#24863;&#30693;&#65292;&#23545;&#28508;&#22312;&#36335;&#32447;&#35268;&#21010;&#30340;&#31574;&#30053;&#35748;&#35782;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#22810;&#36710;&#36742;&#20132;&#20114;&#65292;&#20197;&#23454;&#29616;&#35813;&#38382;&#39064;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks often inherit spurious correlations embedded in training data and hence may fail to generalize to unseen domains, which have different distributions from the domain to provide training data. M. Arjovsky et al. (2019) introduced the concept out-of-distribution (o.o.d.) risk, which is the maximum risk among all domains, and formulated the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. Invariant Risk Minimization (IRM) is considered to be a promising approach to minimize the o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization problem. While IRM has attracted considerable attention with empirical success, it comes with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level optimization problem gives the minimum of the o.o.d. risk has not yet been established. Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2307.08925</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Federated Large Language Model: A Position Paper. (arXiv:2307.08925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08925
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#20844;&#20849;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#38480;&#21046;&#21644;&#31169;&#26377;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#36825;&#19977;&#20010;&#32452;&#20214;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#26045;&#31574;&#30053;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21644;&#28508;&#22312;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#33719;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#24182;&#25214;&#21040;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#65292;&#20294;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#24320;&#21457;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#20844;&#20849;&#39046;&#22495;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#21294;&#20047;&#20197;&#21450;&#23545;&#31169;&#26377;&#39046;&#22495;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#39033;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#33021;&#22815;&#22312;&#20445;&#25345;&#20998;&#25955;&#25968;&#25454;&#30340;&#21516;&#26102;&#23454;&#29616;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#24335;LLM&#30340;&#27010;&#24565;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#32852;&#37030;&#24335;LLM&#39044;&#35757;&#32451;&#12289;&#32852;&#37030;&#24335;LLM&#24494;&#35843;&#21644;&#32852;&#37030;&#24335;LLM&#25552;&#31034;&#24037;&#31243;&#12290;&#23545;&#20110;&#27599;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#30456;&#23545;&#20110;&#20256;&#32479;LLM&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24037;&#31243;&#31574;&#30053;&#26469;&#23454;&#26045;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FL&#21644;LLM&#38598;&#25104;&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#20998;&#26512;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#30830;&#23450;&#21487;&#33021;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Large scale language models (LLM) have received significant attention and found diverse applications across various domains, but their development encounters challenges in real-world scenarios. These challenges arise due to the scarcity of public domain data availability and the need to maintain privacy with respect to private domain data. To address these issues, federated learning (FL) has emerged as a promising technology that enables collaborative training of shared models while preserving decentralized data. We propose the concept of federated LLM, which comprises three key components, i.e., federated LLM pre-training, federated LLM fine-tuning, and federated LLM prompt engineering. For each component, we discuss its advantage over traditional LLM training methods and propose specific engineering strategies for implementation. Furthermore, we explore the novel challenges introduced by the integration of FL and LLM. We analyze existing solutions and identify potential obstacles fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#26377;&#38480;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;&#21450;&#20854;&#26222;&#36866;&#24615;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26377;&#38480;&#32593;&#32476;&#20013;&#20173;&#28982;&#23384;&#22312;&#30528;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20250;&#21453;&#26144;&#22312;&#36807;&#28193;&#30340;&#26222;&#36866;&#31867;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.02284</link><description>&lt;p&gt;
&#20154;&#24037;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
Absorbing Phase Transitions in Artificial Deep Neural Networks. (arXiv:2307.02284v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#26377;&#38480;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21560;&#25910;&#30456;&#21464;&#21450;&#20854;&#26222;&#36866;&#24615;&#65292;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#26377;&#38480;&#32593;&#32476;&#20013;&#20173;&#28982;&#23384;&#22312;&#30528;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#36807;&#28193;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#20250;&#21453;&#26144;&#22312;&#36807;&#28193;&#30340;&#26222;&#36866;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33879;&#21517;&#30340;&#24179;&#22343;&#22330;&#29702;&#35770;&#65292;&#23545;&#20110;&#21508;&#31181;&#20307;&#31995;&#30340;&#26080;&#38480;&#23485;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#30340;&#29702;&#35770;&#29702;&#35299;&#24050;&#32463;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26356;&#23454;&#38469;&#21644;&#29616;&#23454;&#37325;&#35201;&#24615;&#26356;&#24378;&#30340;&#26377;&#38480;&#32593;&#32476;&#65292;&#32570;&#20047;&#28165;&#26224;&#30452;&#35266;&#30340;&#26694;&#26550;&#26469;&#24310;&#20280;&#25105;&#20204;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#21487;&#20197;&#29992;&#21560;&#25910;&#30456;&#21464;&#20013;&#30340;&#26222;&#36941;&#20020;&#30028;&#29616;&#35937;&#26469;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20174;&#26377;&#24207;&#29366;&#24577;&#21040;&#28151;&#27788;&#29366;&#24577;&#30340;&#30456;&#21464;&#65292;&#24182;&#24378;&#35843;&#20102;&#20307;&#31995;&#26550;&#26500;&#30340;&#24046;&#24322;&#19982;&#30456;&#21464;&#30340;&#26222;&#36866;&#31867;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#25104;&#21151;&#22320;&#24212;&#29992;&#20102;&#26377;&#38480;&#23610;&#24230;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#20102;&#30452;&#35266;&#30340;&#29616;&#35937;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical understanding of the behavior of infinitely-wide neural networks has been rapidly developed for various architectures due to the celebrated mean-field theory. However, there is a lack of a clear, intuitive framework for extending our understanding to finite networks that are of more practical and realistic importance. In the present contribution, we demonstrate that the behavior of properly initialized neural networks can be understood in terms of universal critical phenomena in absorbing phase transitions. More specifically, we study the order-to-chaos transition in the fully-connected feedforward neural networks and the convolutional ones to show that (i) there is a well-defined transition from the ordered state to the chaotics state even for the finite networks, and (ii) difference in architecture is reflected in that of the universality class of the transition. Remarkably, the finite-size scaling can also be successfully applied, indicating that intuitive phenomenologic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15769</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#20026;&#20309;&#19982;LAION&#32593;&#32476;&#25130;&#28982;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
What Makes ImageNet Look Unlike LAION. (arXiv:2306.15769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#65292;&#23581;&#35797;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#24182;&#21457;&#29616;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#30456;&#27604;&#65292;&#26032;&#24314;&#30340;LAIONet&#20855;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#26159;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#26102;&#65292;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32593;&#26159;&#36890;&#36807;Flickr&#22270;&#20687;&#25628;&#32034;&#32467;&#26524;&#21019;&#24314;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#20165;&#26681;&#25454;&#22270;&#20687;&#25551;&#36848;&#37325;&#26032;&#21019;&#24314;&#22270;&#20687;&#32593;&#65292;&#25628;&#32034;&#22823;&#35268;&#27169;&#30340;LAION&#25968;&#25454;&#38598;&#20250;&#21457;&#29983;&#20160;&#20040;&#21602;&#65311;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#36825;&#20010;&#21453;&#20107;&#23454;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#37325;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#32593;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LAIONet&#65292;&#19982;&#21407;&#22987;&#22270;&#20687;&#32593;&#26377;&#26126;&#26174;&#19981;&#21516;&#20043;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21407;&#22987;&#22270;&#20687;&#32593;&#20013;&#22270;&#20687;&#30340;&#31867;&#20869;&#30456;&#20284;&#24615;&#36828;&#39640;&#20110;LAIONet&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#20687;&#32593;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;LAIONet&#19978;&#34920;&#29616;&#26126;&#26174;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#35299;&#37322;&#36825;&#31181;&#24046;&#24322;&#30340;&#35266;&#28857;&#65292;&#24182;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#20104;&#20197;&#25903;&#25345;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#22522;&#20110;&#22270;&#20687;&#25551;&#36848;&#36827;&#34892;&#25628;&#32034;&#20250;&#20135;&#29983;&#20449;&#24687;&#29942;&#39048;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#22522;&#20110;&#22270;&#20687;&#36807;&#28388;&#26102;&#23384;&#22312;&#30340;&#36873;&#25321;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
ImageNet was famously created from Flickr image search results. What if we recreated ImageNet instead by searching the massive LAION dataset based on image captions alone? In this work, we carry out this counterfactual investigation. We find that the resulting ImageNet recreation, which we call LAIONet, looks distinctly unlike the original. Specifically, the intra-class similarity of images in the original ImageNet is dramatically higher than it is for LAIONet. Consequently, models trained on ImageNet perform significantly worse on LAIONet. We propose a rigorous explanation for the discrepancy in terms of a subtle, yet important, difference in two plausible causal data-generating processes for the respective datasets, that we support with systematic experimentation. In a nutshell, searching based on an image caption alone creates an information bottleneck that mitigates the selection bias otherwise present in image-based filtering. Our explanation formalizes a long-held intuition in th
&lt;/p&gt;</description></item><item><title>S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.15220</link><description>&lt;p&gt;
S-TLLR: &#21463;&#21040;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;STDP&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks. (arXiv:2306.15220v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15220
&lt;/p&gt;
&lt;p&gt;
S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#21487;&#29992;&#20110;&#36793;&#32536;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#38754;&#20020;&#30528;&#31934;&#30830;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#29992;&#20998;&#37197;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;BPTT&#31639;&#27861;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23427;&#20135;&#29983;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BPTT&#21450;&#20854;&#36817;&#20284;&#20165;&#21033;&#29992;&#20174;&#33033;&#20914;&#27963;&#21160;&#20013;&#23548;&#20986;&#30340;&#22240;&#26524;&#20449;&#24687;&#26469;&#35745;&#31639;&#31361;&#35302;&#26356;&#26032;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S-TLLR&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;Spike-Timing Dependent Plasticity&#65288;STDP&#65289;&#26426;&#21046;&#21551;&#21457;&#30340;&#26032;&#22411;&#19977;&#22240;&#32032;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26088;&#22312;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;SNN&#35757;&#32451;&#12290;S-TLLR&#21516;&#26102;&#32771;&#34385;&#20102;&#21069;&#21518;&#31361;&#35302;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for the deployment for energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses a significant challenge due to the necessity for precise temporal and spatial credit assignment. Back-propagation through time (BPTT) algorithm, whilst being the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. Moreover, BPTT and its approximations solely utilize causal information derived from the spiking activity to compute the synaptic updates, thus neglecting non-causal relationships. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training SNNs on event-based learning tasks. S-TLLR considers both causal and non-causal relationships between pre and post-syn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15260</link><description>&lt;p&gt;
&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;: &#19968;&#31181;&#22312;&#32447;&#31163;&#32447;&#36716;&#31227;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative World Models: An Online-Offline Transfer RL Approach. (arXiv:2305.15260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#24182;&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#21644;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#30001;&#20110;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#21644;&#20215;&#20540;&#20989;&#25968;&#20013;&#30340;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21327;&#20316;&#19990;&#30028;&#27169;&#22411;&#65288;CoWorld&#65289;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#26465;&#20214;&#19979;&#35270;&#35273;RL&#30340;&#24615;&#33021;&#12290;&#20854;&#26680;&#24515;&#24819;&#27861;&#26159;&#20351;&#29992;&#26131;&#20110;&#20132;&#20114;&#12289;&#29616;&#25104;&#30340;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#36741;&#21161;RL&#27169;&#22411;&#20316;&#20026;&#31163;&#32447;&#31574;&#30053;&#22312;&#30446;&#26631;&#22495;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#8220;&#27979;&#35797;&#24202;&#8221;&#65292;&#36825;&#20026;&#20215;&#20540;&#20989;&#25968;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#32422;&#26463;&#8212;&#8212;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#24819;&#22312;&#19981;&#22952;&#30861;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#30340;&#21160;&#20316;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#32531;&#35299;&#31163;&#32447;&#25968;&#25454;&#20998;&#24067;&#20043;&#22806;&#30340;&#20215;&#20540;&#20989;&#25968;&#36807;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CoWorld&#25191;&#34892;&#22495;&#21327;&#20316;&#34920;&#31034;&#23398;&#20064;&#20197;&#24357;&#21512;&#22312;&#32447;&#21644;&#31163;&#32447;&#38544;&#34255;&#29366;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#23427;&#25191;&#34892;&#22495;&#21327;&#20316;&#34892;&#20026;&#23398;&#20064;&#65292;&#20351;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#22806;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#23398;&#20064;&#22312;&#32447;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training visual reinforcement learning (RL) models in offline datasets is challenging due to overfitting issues in representation learning and overestimation problems in value function. In this paper, we propose a transfer learning method called Collaborative World Models (CoWorld) to improve the performance of visual RL under offline conditions. The core idea is to use an easy-to-interact, off-the-shelf simulator to train an auxiliary RL model as the online "test bed" for the offline policy learned in the target domain, which provides a flexible constraint for the value function -- Intuitively, we want to mitigate the overestimation problem of value functions outside the offline data distribution without impeding the exploration of actions with potential advantages. Specifically, CoWorld performs domain-collaborative representation learning to bridge the gap between online and offline hidden state distributions. Furthermore, it performs domain-collaborative behavior learning that enab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07598</link><description>&lt;p&gt;
RHINO&#65306;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#30340;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#30340;&#26059;&#36716;DETR
&lt;/p&gt;
&lt;p&gt;
RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection. (arXiv:2305.07598v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#27169;&#22411;RHINO&#12290;&#24182;&#36890;&#36807;&#21256;&#29273;&#21033;&#21305;&#37197;&#21644;&#26597;&#35810;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21160;&#24577;&#38477;&#22122;&#65292;&#35299;&#20915;&#20102;&#37325;&#22797;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;DINO&#30340;&#21457;&#24067;&#65292;&#19968;&#31181;DETR&#30340;&#21464;&#20307;&#65292;&#26816;&#27979;&#21464;&#21387;&#22120;&#27491;&#22312;&#36890;&#36807;&#20854;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#24615;&#22312;&#30446;&#26631;&#26816;&#27979;&#22522;&#20934;&#20013;&#21047;&#26032;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#39044;&#35745;&#20174;&#20854;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#33719;&#24471;&#26356;&#22810;&#30340;&#22909;&#22788;&#65292;&#22914;&#28040;&#38500;NMS&#21644;&#19982;&#38170;&#30456;&#20851;&#30340;&#25104;&#26412;&#65292;&#20294;&#23578;&#26410;&#24443;&#24213;&#30740;&#31350;DETR&#22312;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#30340;&#25193;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#38754;&#21521;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#30340;DINO&#22522;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20351;&#29992;DETR&#36827;&#34892;&#23450;&#21521;&#30446;&#26631;&#26816;&#27979;&#24182;&#19981;&#33021;&#20445;&#35777;&#19981;&#37325;&#22797;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25104;&#26412;&#26469;&#20943;&#36731;&#36825;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#21256;&#29273;&#21033;&#21305;&#37197;&#26469;&#36807;&#28388;&#20887;&#20313;&#30340;&#24102;&#22122;&#22768;&#30340;&#26597;&#35810;&#65292;&#24182;&#20351;&#29992;&#26597;&#35810;&#23545;&#40784;&#26469;&#20445;&#25345;Transformer&#35299;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#21305;&#37197;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26059;&#36716;DETR&#21644;&#20854;&#20182;&#23545;&#25163;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the publication of DINO, a variant of the Detection Transformer (DETR), Detection Transformers are breaking the record in the object detection benchmark with the merits of their end-to-end design and scalability. However, the extension of DETR to oriented object detection has not been thoroughly studied although more benefits from its end-to-end architecture are expected such as removing NMS and anchor-related costs. In this paper, we propose a first strong DINO-based baseline for oriented object detection. We found that straightforward employment of DETRs for oriented object detection does not guarantee non-duplicate prediction, and propose a simple cost to mitigate this. Furthermore, we introduce a novel denoising strategy that uses Hungarian matching to filter redundant noised queries and query alignment to preserve matching consistency between Transformer decoder layers. Our proposed model outperforms previous rotated DETRs and other counterparts, achieving state-of-the-art pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07472</link><description>&lt;p&gt;
&#36890;&#29992;&#26680;&#23398;&#20064;&#30340;&#39640;&#25928;&#20984;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Convex Algorithms for Universal Kernel Learning. (arXiv:2304.07472v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#30340;&#39640;&#25928;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#29992;&#20110;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26680;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#33021;&#22815;&#20248;&#21270;&#30340;&#26680;&#38598;&#12290;&#29702;&#24819;&#30340;&#26680;&#38598;&#24212;&#35813;&#65306;&#20855;&#26377;&#32447;&#24615;&#21442;&#25968;&#21270;&#65288;&#20197;&#20415;&#20110;&#21487;&#22788;&#29702;&#24615;&#65289;&#65307;&#22312;&#25152;&#26377;&#26680;&#38598;&#20013;&#23494;&#38598;&#65288;&#20197;&#20415;&#20110;&#40065;&#26834;&#24615;&#65289;&#65307;&#26159;&#36890;&#29992;&#30340;&#65288;&#20197;&#20415;&#20110;&#20934;&#30830;&#24615;&#65289;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#23450;&#30697;&#38453;&#26469;&#21442;&#25968;&#21270;&#19968;&#31867;&#27491;&#21322;&#20998;&#31163;&#26680;&#12290;&#23613;&#31649;&#27492;&#31867;&#26680;&#33021;&#22815;&#28385;&#36275;&#25152;&#26377;&#19977;&#20010;&#26631;&#20934;&#65292;&#20294;&#20043;&#21069;&#29992;&#20110;&#20248;&#21270;&#27492;&#31867;&#26680;&#30340;&#31639;&#27861;&#20165;&#38480;&#20110;&#20998;&#31867;&#65292;&#24182;&#19988;&#36824;&#20381;&#36182;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23398;&#20064;&#21322;&#20998;&#31163;&#26680;&#30340;&#38382;&#39064;&#20316;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;SVD-QCQP&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#20854;&#19982;&#20043;&#21069;&#22522;&#20110;SDP&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#26680;&#23398;&#20064;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy and complexity of machine learning algorithms based on kernel optimization are determined by the set of kernels over which they are able to optimize. An ideal set of kernels should: admit a linear parameterization (for tractability); be dense in the set of all kernels (for robustness); be universal (for accuracy). Recently, a framework was proposed for using positive matrices to parameterize a class of positive semi-separable kernels. Although this class can be shown to meet all three criteria, previous algorithms for optimization of such kernels were limited to classification and furthermore relied on computationally complex Semidefinite Programming (SDP) algorithms. In this paper, we pose the problem of learning semiseparable kernels as a minimax optimization problem and propose a SVD-QCQP primal-dual algorithm which dramatically reduces the computational complexity as compared with previous SDP-based approaches. Furthermore, we provide an efficient implementation of thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.01315</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Empirical Design in Reinforcement Learning. (arXiv:2304.01315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;&#19981;&#26159;&#23567;&#20219;&#21153;&#12290;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#38656;&#35201;&#35762;&#31350;&#32454;&#33410;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26102;&#20505;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24120;&#29992;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#21644;&#23454;&#29616;&#32454;&#33410;&#25935;&#24863;&#65292;&#24182;&#19988;&#24120;&#35265;&#30340;&#23454;&#35777;&#20570;&#27861;&#20250;&#23548;&#33268;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#25991;&#19981;&#20165;&#21628;&#21505;&#34892;&#21160;&#65292;&#32780;&#19988;&#26159;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for proper statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018). Here we take this one step further.  This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. 
&lt;/p&gt;</description></item><item><title>Het-node2vec&#26159;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#22810;&#22270;&#19978;&#36827;&#34892;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#65292;&#33021;&#22815;&#25429;&#33719;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#36793;&#30340;&#35821;&#20041;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2101.01425</link><description>&lt;p&gt;
Het-node2vec&#65306;&#24322;&#26500;&#22810;&#22270;&#23884;&#20837;&#30340;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Het-node2vec: second order random walk sampling for heterogeneous multigraphs embedding. (arXiv:2101.01425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01425
&lt;/p&gt;
&lt;p&gt;
Het-node2vec&#26159;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#24322;&#26500;&#22810;&#22270;&#19978;&#36827;&#34892;&#20108;&#38454;&#38543;&#26426;&#28216;&#36208;&#37319;&#26679;&#65292;&#33021;&#22815;&#25429;&#33719;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#36793;&#30340;&#35821;&#20041;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#20026;&#24322;&#26500;&#22270;&#24320;&#21457;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#22810;&#20010;&#19978;&#19979;&#25991;&#20013;&#65292;&#22270;&#30001;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#25152;&#29305;&#24449;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31639;&#27861;&#26694;&#26550;&#65288;Het-node2vec&#65289;&#65292;&#23558;&#21407;&#22987;&#30340;node2vec&#33410;&#28857;&#37051;&#22495;&#37319;&#26679;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#24322;&#26500;&#22810;&#22270;&#19978;&#12290;&#25152;&#24471;&#21040;&#30340;&#38543;&#26426;&#28216;&#36208;&#26679;&#26412;&#25429;&#33719;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24449;&#20197;&#21450;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#35821;&#20041;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#32858;&#28966;&#20110;&#29305;&#23450;&#30340;&#33410;&#28857;&#25110;&#36793;&#31867;&#22411;&#65292;&#20026;&#25152;&#30740;&#31350;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#26377;&#20852;&#36259;&#30340;&#23569;&#25968;&#33410;&#28857;/&#36793;&#31867;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#20016;&#23500;&#32780;&#26377;&#38024;&#23545;&#24615;&#30340;&#34920;&#31034;&#21487;&#20197;&#22686;&#24378;&#23545;&#24322;&#26500;&#22270;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Graph Representation Learning methods for heterogeneous graphs is fundamental in several real-world applications, since in several contexts graphs are characterized by different types of nodes and edges. We introduce a an algorithmic framework (Het-node2vec) that extends the original node2vec node-neighborhood sampling method to heterogeneous multigraphs. The resulting random walk samples capture both the structural characteristics of the graph and the semantics of the different types of nodes and edges. The proposed algorithms can focus their attention on specific node or edge types, allowing accurate representations also for underrepresented types of nodes/edges that are of interest for the prediction problem under investigation. These rich and well-focused representations can boost unsupervised and supervised learning on heterogeneous graphs.
&lt;/p&gt;</description></item></channel></rss>