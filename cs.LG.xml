<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15576</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Data-centric Prediction Explanation via Kernelized Stein Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#28508;&#22312;&#34920;&#31034;&#26469;&#36830;&#25509;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#39044;&#27979;&#21407;&#22240;&#30340;&#32447;&#32034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#27604;&#22914;&#20135;&#29983;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#37322;&#65288;HD-Explain&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#65288;KSD&#65289;&#23646;&#24615;&#30340;&#31616;&#21333;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;KSD&#21807;&#19968;&#22320;&#20026;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#29992;&#20110;&#32534;&#30721;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#39046;&#22495;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;HD-Explain&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
&lt;/p&gt;</description></item><item><title>ScatterMoE&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08245</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Scattered Mixture-of-Experts Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08245
&lt;/p&gt;
&lt;p&gt;
ScatterMoE&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ScatterMoE&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#12290;ScatterMoE&#22312;&#29616;&#26377;&#23454;&#29616;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#20811;&#26381;&#20102;&#19968;&#20123;&#38480;&#21046;&#20197;&#25552;&#39640;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#23454;&#29616;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ParallelLinear&#65292;&#36825;&#26159;&#25105;&#20204;&#29992;&#26469;&#26500;&#24314;&#23454;&#29616;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#21152;&#36895;&#25805;&#20316;&#30340;&#21508;&#31181;&#20869;&#26680;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#23454;&#29616;&#36827;&#34892;&#20102;&#19982;Megablocks&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ParallelLinear&#22914;&#20309;&#36890;&#36807;&#23637;&#31034;Mixture of Attention&#30340;&#23454;&#29616;&#26469;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08245v1 Announce Type: new  Abstract: We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input.   We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.05571</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36712;&#36857;&#20248;&#21270;&#38754;&#20020;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#35774;&#32622;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#65292;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#39069;&#22806;&#32422;&#26463;&#36829;&#21453;&#25439;&#22833;&#30340;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#26088;&#22312;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#36817;&#20284;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32422;&#26463;&#36829;&#21453;&#12290;&#28982;&#21518;&#29992;&#26679;&#26412;&#20316;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#26469;&#20248;&#21270;&#24182;&#24471;&#20986;&#26368;&#32456;&#35299;&#65292;&#24182;&#39564;&#35777;&#21487;&#34892;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15951</link><description>&lt;p&gt;
GreenLLaMA: &#19968;&#31181;&#24102;&#26377;&#35299;&#37322;&#30340;&#35299;&#27602;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA: A Framework for Detoxification with Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15951
&lt;/p&gt;
&lt;p&gt;
GreenLLaMA&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#35299;&#27602;&#30340;&#30740;&#31350;&#24037;&#20316;&#20998;&#25955;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#27809;&#26377;&#28085;&#30422;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#25152;&#38656;&#30340;&#25152;&#26377;&#35299;&#27602;&#26041;&#38754;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#24320;&#21457;&#35299;&#27602;&#27169;&#22411;&#30340;&#20219;&#21153;&#23616;&#38480;&#22312;&#20165;&#35265;&#36807;&#30340;&#24179;&#21488;&#23376;&#38598;&#19978;&#65292;&#27809;&#26377;&#25506;&#35752;&#27169;&#22411;&#22312;&#26410;&#30693;&#24179;&#21488;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#24037;&#20316;&#27809;&#26377;&#35299;&#20915;&#19981;&#21487;&#35299;&#27602;&#24615;&#36825;&#19968;&#29616;&#35937;&#65292;&#21363;&#27602;&#24615;&#25991;&#26412;&#26080;&#27861;&#22312;&#19981;&#25913;&#21464;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35299;&#27602;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreenLLaMA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35299;&#27602;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;&#24179;&#21488;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#22810;&#27493;&#25968;&#25454;&#22788;&#29702;&#21644;&#29983;&#25104;&#31574;&#30053;&#21033;&#29992;ChatGPT&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36328;&#24179;&#21488;&#35821;&#26009;&#24211;&#35757;&#32451;&#19968;&#22871;&#35299;&#27602;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35299;&#27602;&#27169;&#22411;&#20248;&#20110;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15951v1 Announce Type: cross  Abstract: Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.15000</link><description>&lt;p&gt;
&#21010;&#20998;&#36824;&#26159;&#24449;&#26381;&#65311;&#20320;&#24212;&#35813;&#25552;&#28860;LLM&#30340;&#21738;&#19968;&#37096;&#20998;&#65311;
&lt;/p&gt;
&lt;p&gt;
Divide-or-Conquer? Which Part Should You Distill Your LLM?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#30340;&#31574;&#30053;&#65292;&#21457;&#29616;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#30456;&#27604;&#38382;&#39064;&#35299;&#20915;&#26356;&#23481;&#26131;&#25552;&#28860;&#20026;&#36739;&#23567;&#27169;&#22411;&#65292;&#24182;&#35777;&#23454;&#35813;&#31574;&#30053;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34987;&#40723;&#21169;&#20808;&#35299;&#20915;&#20027;&#35201;&#20219;&#21153;&#30340;&#23376;&#20219;&#21153;&#26102;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#31574;&#30053;&#65292;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#35299;&#20026;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#21644;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#65292;&#24182;&#23637;&#31034;&#35813;&#31574;&#30053;&#33021;&#22815;&#32988;&#36807;&#21333;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20551;&#35774;&#19982;&#35299;&#20915;&#38382;&#39064;&#30456;&#27604;&#65292;&#20998;&#35299;&#38454;&#27573;&#26356;&#23481;&#26131;&#34987;&#25552;&#28860;&#20026;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#21518;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#21069;&#32773;&#21482;&#38656;&#35201;&#23398;&#20064;&#19968;&#33324;&#30340;&#38382;&#39064;&#35299;&#20915;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25552;&#28860;&#36825;&#20004;&#31181;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#25512;&#29702;&#32467;&#26524;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#25552;&#28860;&#38382;&#39064;&#20998;&#35299;&#38454;&#27573;&#65292;&#24182;&#21516;&#26102;&#22312;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#35201;&#25552;&#28860;&#38382;&#39064;&#35299;&#20915;&#38454;&#27573;&#23601;&#26356;&#22256;&#38590;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15000v1 Announce Type: new  Abstract: Recent methods have demonstrated that Large Language Models (LLMs) can solve reasoning tasks better when they are encouraged to solve subtasks of the main task first. In this paper we devise a similar strategy that breaks down reasoning tasks into a problem decomposition phase and a problem solving phase and show that the strategy is able to outperform a single stage solution. Further, we hypothesize that the decomposition should be easier to distill into a smaller model compared to the problem solving because the latter requires large amounts of domain knowledge while the former only requires learning general problem solving strategies. We propose methods to distill these two capabilities and evaluate their impact on reasoning outcomes and inference cost. We find that we can distill the problem decomposition phase and at the same time achieve good generalization across tasks, datasets, and models. However, it is harder to distill the pr
&lt;/p&gt;</description></item><item><title>LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14086</link><description>&lt;p&gt;
LexC-Gen: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21452;&#35821;&#35789;&#27719;&#34920;&#20026;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#29983;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
LexC-Gen: Generating Data for Extremely Low-Resource Languages with Large Language Models and Bilingual Lexicons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14086
&lt;/p&gt;
&lt;p&gt;
LexC-Gen&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25968;&#25454;&#21294;&#20047;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#26631;&#35760;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#36880;&#23383;&#32763;&#35793;&#26469;&#35299;&#20915;&#65292;&#28982;&#32780;&#65292;&#21452;&#35821;&#35789;&#20856;&#36890;&#24120;&#19982;&#20219;&#21153;&#25968;&#25454;&#26377;&#38480;&#30340;&#35789;&#27719;&#37325;&#21472;&#65292;&#23548;&#33268;&#32763;&#35793;&#35206;&#30422;&#21644;&#35789;&#20856;&#21033;&#29992;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;LexC-Gen&#30340;&#35789;&#20856;&#26465;&#20214;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LexC-Gen&#39318;&#20808;&#20351;&#29992;&#21452;&#35821;&#35789;&#20856;&#20013;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#21333;&#35789;&#29983;&#25104;&#19982;&#35789;&#20856;&#20860;&#23481;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#28982;&#21518;&#36890;&#36807;&#21333;&#35789;&#32763;&#35793;&#23558;&#20854;&#32763;&#35793;&#25104;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;17&#31181;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;LexC-Gen&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#23478;&#32763;&#35793;&#30340;&#40644;&#37329;&#25968;&#25454;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#19988;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#19978;&#24179;&#22343;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#35789;&#20856;&#30340;&#21333;&#35789;&#32763;&#35793;&#26041;&#27861;&#25552;&#39640;&#20102;5.6&#21644;8.9&#20010;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14086v1 Announce Type: cross  Abstract: Data scarcity in low-resource languages can be addressed with word-to-word translations from labeled task data in high-resource languages using bilingual lexicons. However, bilingual lexicons often have limited lexical overlap with task data, which results in poor translation coverage and lexicon utilization. We propose lexicon-conditioned data generation (LexC-Gen), a method that generates low-resource-language classification task data at scale. Specifically, LexC-Gen first uses high-resource-language words from bilingual lexicons to generate lexicon-compatible task data, and then it translates them into low-resource languages with bilingual lexicons via word translation. Across 17 extremely low-resource languages, LexC-Gen generated data is competitive with expert-translated gold data, and yields on average 5.6 and 8.9 points improvement over existing lexicon-based word translation methods on sentiment analysis and topic classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;</title><link>https://arxiv.org/abs/2402.13728</link><description>&lt;p&gt;
&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#22349;&#22604;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Average gradient outer product as a mechanism for deep neural collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Collapse (DNC)&#25351;&#30340;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26368;&#21518;&#20960;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#24778;&#20154;&#21018;&#24615;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#37117;&#24471;&#21040;&#20102;&#27979;&#37327;&#65292;&#20294;&#20854;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20805;&#20998;&#35777;&#25454;&#65292;&#34920;&#26126;DNC&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;(AGOP)&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#21457;&#29983;&#30340;&#12290;&#30456;&#27604;&#20110;&#35299;&#37322;&#31070;&#32463;&#22349;&#22604;&#30340;&#29305;&#24449;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#22914;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65292;&#36825;&#19968;&#36827;&#23637;&#26356;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#32487;&#32493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#26435;&#37325;&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#20540;&#26159;DNN&#20013;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36825;&#31181;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#30340;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AGOP&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#21457;&#31070;&#32463;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.13037</link><description>&lt;p&gt;
&#23545;&#40784;&#24744;&#30340;&#24847;&#22270;&#65306;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align Your Intents: Offline Imitation Learning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;AILOT&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26126;&#30830;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36890;&#36807;&#23398;&#20064;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#21363;&#20351;&#32570;&#20047;&#26126;&#30830;&#30340;&#22870;&#21169;&#25110;&#21160;&#20316;&#26631;&#31614;&#65292;&#27169;&#20223;&#20195;&#29702;&#20063;&#21487;&#20197;&#20165;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#26469;&#23398;&#20064;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;AILOT&#65288;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23545;&#40784;&#27169;&#20223;&#23398;&#20064;&#65289;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24847;&#22270;&#30340;&#29305;&#27530;&#29366;&#24577;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#20869;&#30340;&#20004;&#20004;&#31354;&#38388;&#36317;&#31163;&#12290;&#22312;&#32473;&#23450;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#19987;&#23478;&#21644;&#20195;&#29702;&#36712;&#36857;&#20043;&#38388;&#30340;&#26368;&#20248;&#20256;&#36755;&#36317;&#31163;&#23450;&#20041;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#31216;AILOT&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13037v1 Announce Type: cross  Abstract: Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and im
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12821</link><description>&lt;p&gt;
&#22312;&#25688;&#35201;&#20013;&#35782;&#21035;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65306;&#26397;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#25277;&#35937;&#24615;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21830;&#19994;&#37096;&#32626;&#26500;&#25104;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#23637;&#24320;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#31934;&#28860;&#19968;&#20010;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21151;&#25928;&#24615;&#30340;&#26356;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;&#39318;&#20808;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#38646;&#26679;&#26412;&#33539;&#24335;&#65292;&#36328;&#36234;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65306;&#30452;&#25509;&#25512;&#29702;&#25972;&#20010;&#25688;&#35201;&#25110;&#27599;&#20010;&#25688;&#35201;&#31383;&#21475;&#65307;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#36827;&#34892;&#23454;&#20307;&#39564;&#35777;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#33021;&#22815;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24179;&#22343;&#36229;&#36807;&#24378;&#22823;&#30340;&#35757;&#32451;&#22522;&#32447;2.8%&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#38024;&#23545;&#31934;&#28860;&#26356;&#23567;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19968;&#27425;&#24615;&#39640;&#20934;&#30830;&#22320;&#35780;&#20998;&#25972;&#20010;&#25688;&#35201;&#65292;&#32988;&#36807;&#38646;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11789</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20551;&#35774;&#30340;&#32479;&#35745;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Generated Hypotheses by Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#22312;&#32771;&#34385;&#29983;&#25104;&#22270;&#20687;&#26159;&#30001;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#26465;&#20214;&#19979;&#65292;&#37327;&#21270;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30340;&#22686;&#24378;&#24615;&#33021;&#21152;&#36895;&#20102;&#20854;&#34701;&#20837;&#31185;&#23398;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#21019;&#24314;&#31185;&#23398;&#20551;&#35774;&#26159;&#24456;&#26377;&#21069;&#36884;&#30340;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;AI&#29983;&#25104;&#30340;&#20551;&#35774;&#36827;&#34892;&#20851;&#38190;&#20915;&#31574;&#65288;&#22914;&#21307;&#23398;&#35786;&#26029;&#65289;&#26102;&#65292;&#39564;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#21307;&#23398;&#35786;&#26029;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#37327;&#21270;&#20854;&#21487;&#38752;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20351;&#29992;&#36873;&#25321;&#24615;&#25512;&#26029;&#26694;&#26550;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#30001;&#32463;&#36807;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#36825;&#19968;&#20107;&#23454;&#26465;&#20214;&#19979;&#30340;&#32479;&#35745;&#26816;&#39564;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#32467;&#26524;&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#25511;&#21046;&#38169;&#35823;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07594</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Foundational Inference Models for Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#26500;&#25104;&#20102;&#20316;&#20026;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#27169;&#22411;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#20986;&#26368;&#20339;&#25551;&#36848;&#32473;&#23450;&#29616;&#35937;&#30340;&#19968;&#32452;&#22122;&#22768;&#35266;&#23519;&#30340;ODE&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24448;&#24448;&#20063;&#38750;&#24120;&#19987;&#19994;&#21270;&#21644;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;ODE&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#21021;&#22987;&#26465;&#20214;&#31354;&#38388;&#21644;&#23450;&#20041;&#23427;&#20204;&#30340;&#21521;&#37327;&#22330;&#31354;&#38388;&#30340;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#29983;&#25104;&#22823;&#22411;&#19968;&#32500;ODE&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#31243;&#30340;&#35299;&#30340;&#22122;&#22768;&#35266;&#23519;&#19982;&#20854;&#30456;&#24212;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#31070;&#32463;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#27169;&#22411;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#65288;i&#65289;&#27839;&#26102;&#38388;&#32500;&#22797;&#21046;&#21644;&#21305;&#37197;&#20197;&#22686;&#21152;&#20998;&#36776;&#29575;&#65307;&#65288;ii&#65289;&#22797;&#21046;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#35299;&#20915;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#65292;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#32858;&#31867;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04732</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#23454;&#29616;&#20855;&#26377;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#30340;&#22270;&#21106;
&lt;/p&gt;
&lt;p&gt;
Graph Cuts with Arbitrary Size Constraints Through Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#35299;&#20915;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#65292;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#32858;&#31867;&#31561;&#24212;&#29992;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#24120;&#35265;&#20998;&#21106;&#26041;&#27861;&#26159;&#26368;&#23567;&#21106;&#12290;&#32463;&#20856;&#26368;&#23567;&#21106;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#29983;&#25104;&#23567;&#30340;&#20998;&#32452;&#65292;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#26356;&#24179;&#34913;&#30340;&#21464;&#20307;&#65292;&#22914;&#24402;&#19968;&#21270;&#21106;&#21644;&#27604;&#20363;&#21106;&#21462;&#24471;&#20102;&#26356;&#22810;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#26576;&#20123;&#24212;&#29992;&#65292;&#22914;&#38750;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#65292;&#36825;&#20123;&#21464;&#20307;&#30340;&#24179;&#34913;&#32422;&#26463;&#21487;&#33021;&#36807;&#20110;&#38480;&#21046;&#65292;&#32780;&#23545;&#20110;&#23547;&#25214;&#23436;&#32654;&#24179;&#34913;&#20998;&#21306;&#26469;&#35828;&#19981;&#22815;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20219;&#24847;&#22823;&#23567;&#32422;&#26463;&#19979;&#23545;&#22270;&#36827;&#34892;&#20998;&#21106;&#12290;&#25105;&#20204;&#23558;&#22270;&#21106;&#38382;&#39064;&#21046;&#23450;&#20026;&#27491;&#21017;&#21270;&#30340;Gromov-Wasserstein&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21152;&#36895;&#30340;&#36817;&#31471;GD&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#20135;&#29983;&#31232;&#30095;&#35299;&#65292;&#24182;&#19988;&#21482;&#27604;&#32463;&#20856;&#35889;&#32858;&#31867;&#31639;&#27861;&#22810;&#28040;&#32791;$\mathcal{O}(\log(n))$&#30340;&#38468;&#21152;&#27604;&#29575;&#65292;&#20294;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common way of partitioning graphs is through minimum cuts. One drawback of classical minimum cut methods is that they tend to produce small groups, which is why more balanced variants such as normalized and ratio cuts have seen more success. However, we believe that with these variants, the balance constraints can be too restrictive for some applications like for clustering of imbalanced datasets, while not being restrictive enough for when searching for perfectly balanced partitions. Here, we propose a new graph cut algorithm for partitioning graphs under arbitrary size constraints. We formulate the graph cut problem as a regularized Gromov-Wasserstein problem. We then propose to solve it using accelerated proximal GD algorithm which has global convergence guarantees, results in sparse solutions and only incurs an additional ratio of $\mathcal{O}(\log(n))$ compared to the classical spectral clustering algorithm but was seen to be more efficient.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04390</link><description>&lt;p&gt;
&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Densely Multiplied Physics Informed Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04390
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#26377;&#25928;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;PINN&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;Physics-Informed Neural Networks, PINNs&#65289;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#29616;&#31934;&#24230;&#19981;&#36275;&#25110;&#33719;&#21462;&#19981;&#27491;&#30830;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#35813;&#35770;&#25991;&#25913;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;PINN&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23494;&#38598;&#20056;&#27861;PINN&#65288;DM-PINN&#65289;&#26550;&#26500;&#65292;&#23427;&#23558;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#19982;&#25152;&#26377;&#21518;&#38754;&#30340;&#38544;&#34255;&#23618;&#30340;&#36755;&#20986;&#30456;&#20056;&#12290;&#22312;&#19981;&#24341;&#20837;&#26356;&#22810;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26377;&#25928;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;PINN&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#22235;&#20010;&#22522;&#20934;&#31034;&#20363;&#65288;Allan-Cahn&#26041;&#31243;&#65292;Helmholtz&#26041;&#31243;&#65292;Burgers&#26041;&#31243;&#21644;1D&#23545;&#27969;&#26041;&#31243;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23558;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#19981;&#21516;&#30340;PINN&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although physics-informed neural networks (PINNs) have shown great potential in dealing with nonlinear partial differential equations (PDEs), it is common that PINNs will suffer from the problem of insufficient precision or obtaining incorrect outcomes. Unlike most of the existing solutions trying to enhance the ability of PINN by optimizing the training process, this paper improved the neural network architecture to improve the performance of PINN. We propose a densely multiply PINN (DM-PINN) architecture, which multiplies the output of a hidden layer with the outputs of all the behind hidden layers. Without introducing more trainable parameters, this effective mechanism can significantly improve the accuracy of PINNs. The proposed architecture is evaluated on four benchmark examples (Allan-Cahn equation, Helmholtz equation, Burgers equation and 1D convection equation). Comparisons between the proposed architecture and different PINN structures demonstrate the superior performance of 
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.14557</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#24490;&#29615;&#20869;&#26680;&#25299;&#23637;&#21040;&#19981;&#21516;&#30340;&#20648;&#22791;&#35745;&#31639;&#25299;&#25169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Extension of Recurrent Kernels to different Reservoir Computing topologies. (arXiv:2401.14557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#31561;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#23578;&#26410;&#36827;&#34892;&#20998;&#26512;&#30340;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#35813;&#30740;&#31350;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#24555;&#36895;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26631;&#20934;&#30340;RC&#22312;&#28176;&#36817;&#26497;&#38480;&#19979;&#24050;&#34987;&#35777;&#26126;&#19982;&#24490;&#29615;&#20869;&#26680;&#31561;&#25928;&#65292;&#36825;&#26377;&#21161;&#20110;&#20998;&#26512;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24050;&#24314;&#31435;&#30340;RC&#33539;&#20363;&#65292;&#22914;Leaky RC&#12289;Sparse RC&#21644;Deep RC&#65292;&#23578;&#26410;&#20197;&#36825;&#31181;&#26041;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#29305;&#23450;RC&#20307;&#31995;&#32467;&#26500;&#19982;&#30456;&#24212;&#24490;&#29615;&#20869;&#26680;&#24418;&#24335;&#31561;&#20215;&#24615;&#30340;&#32463;&#39564;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#20307;&#31995;&#32467;&#26500;&#20013;&#23454;&#26045;&#30340;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#25910;&#25947;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#31232;&#30095;&#36830;&#25509;&#22312;RC&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#20648;&#22791;&#22823;&#23567;&#30340;&#26368;&#20339;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;Deep RC&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#20943;&#23567;&#23610;&#23544;&#30340;&#36830;&#32493;&#20648;&#22791;&#21487;&#20197;&#26356;&#22909;&#22320;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) has become popular in recent years due to its fast and efficient computational capabilities. Standard RC has been shown to be equivalent in the asymptotic limit to Recurrent Kernels, which helps in analyzing its expressive power. However, many well-established RC paradigms, such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way. This study aims to fill this gap by providing an empirical analysis of the equivalence of specific RC architectures with their corresponding Recurrent Kernel formulation. We conduct a convergence study by varying the activation function implemented in each architecture. Our study also sheds light on the role of sparse connections in RC architectures and propose an optimal sparsity level that depends on the reservoir size. Furthermore, our systematic analysis shows that in Deep RC models, convergence is better achieved with successive reservoirs of decreasing sizes.
&lt;/p&gt;</description></item><item><title>MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09624</link><description>&lt;p&gt;
MITS-GAN: &#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;
&lt;/p&gt;
&lt;p&gt;
MITS-GAN: Safeguarding Medical Imaging from Tampering with Generative Adversarial Networks. (arXiv:2401.09624v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09624
&lt;/p&gt;
&lt;p&gt;
MITS-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;&#21307;&#23398;&#24433;&#20687;&#20813;&#21463;&#31713;&#25913;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#20316;&#20026;&#38450;&#25252;&#25514;&#26045;&#65292;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#33021;&#22815;&#29983;&#25104;&#32784;&#31713;&#25913;&#22270;&#20687;&#65292;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#28508;&#22312;&#30340;&#24694;&#24847;&#20351;&#29992;&#30340;&#25285;&#24551;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#24433;&#20687;&#31561;&#25935;&#24863;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;MITS-GAN&#65292;&#29992;&#20110;&#38450;&#27490;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#31713;&#25913;&#65292;&#29305;&#21035;&#20851;&#27880;CT&#25195;&#25551;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#19981;&#21487;&#23519;&#35273;&#20294;&#31934;&#30830;&#30340;&#25200;&#21160;&#26469;&#25171;&#20081;&#25915;&#20987;&#32773;&#30340;CT-GAN&#26550;&#26500;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#36866;&#24403;&#30340;&#39640;&#26031;&#22122;&#22768;&#24341;&#20837;&#21040;&#36755;&#20837;&#20013;&#20316;&#20026;&#23545;&#21508;&#31181;&#25915;&#20987;&#30340;&#20445;&#25252;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#38450;&#31713;&#25913;&#33021;&#21147;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#20248;&#21183;&#12290;&#23545;CT&#25195;&#25551;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MITS-GAN&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24378;&#35843;&#20102;&#20854;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#21487;&#24573;&#30053;&#20266;&#24433;&#30340;&#32784;&#31713;&#25913;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#31713;&#25913;&#24102;&#26469;&#20102;&#21361;&#21450;&#29983;&#21629;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#30340;&#20027;&#21160;&#38450;&#25252;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in generative models, particularly Generative Adversarial Networks (GANs), opened new possibilities for image generation but raised concerns about potential malicious uses, especially in sensitive areas like medical imaging. This study introduces MITS-GAN, a novel approach to prevent tampering in medical images, with a specific focus on CT scans. The approach disrupts the output of the attacker's CT-GAN architecture by introducing imperceptible but yet precise perturbations. Specifically, the proposed approach involves the introduction of appropriate Gaussian noise to the input as a protective measure against various attacks. Our method aims to enhance tamper resistance, comparing favorably to existing techniques. Experimental results on a CT scan dataset demonstrate MITS-GAN's superior performance, emphasizing its ability to generate tamper-resistant images with negligible artifacts. As image tampering in medical domains poses life-threatening risks, our proactive approac
&lt;/p&gt;</description></item><item><title>&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2401.08632</link><description>&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement Learning. (arXiv:2401.08632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08632
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#19982;&#25551;&#36848;&#31526;&#26465;&#20214;&#21152;&#24378;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#20811;&#26381;&#36827;&#21270;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#30340;&#22522;&#26412;&#29305;&#24449;&#20043;&#19968;&#26159;&#25214;&#21040;&#26032;&#39062;&#21644;&#26377;&#21019;&#36896;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;&#25361;&#25112;&#25110;&#36866;&#24212;&#26410;&#39044;&#26009;&#21040;&#30340;&#24773;&#20917;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#20248;&#21270;&#26159;&#19968;&#31867;&#36827;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#26082;&#22810;&#26679;&#21448;&#39640;&#24615;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#20854;&#20013;&#65292;MAP-Elites&#26159;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#12290;&#28982;&#32780;&#65292;MAP-Elites&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#30340;&#38543;&#26426;&#31361;&#21464;&#36827;&#34892;&#21457;&#25955;&#25628;&#32034;&#65292;&#22240;&#27492;&#20165;&#38480;&#20110;&#36827;&#21270;&#20302;&#32500;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#12290;PGA-MAP-Elites&#36890;&#36807;&#21463;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21551;&#21457;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#20811;&#26381;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#21270;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#24615;&#33021;&#20248;&#31168;&#65292;&#20294;PGA-MAP-Elites&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#22833;&#36133;&#65292;&#20854;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#21464;&#24322;&#31639;&#23376;&#30340;&#25910;&#25947;&#25628;&#32034;&#38459;&#30861;&#20102;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
A fundamental trait of intelligence involves finding novel and creative solutions to address a given challenge or to adapt to unforeseen situations. Reflecting this, Quality-Diversity optimization is a family of Evolutionary Algorithms, that generates collections of both diverse and high-performing solutions. Among these, MAP-Elites is a prominent example, that has been successfully applied to a variety of domains, including evolutionary robotics. However, MAP-Elites performs a divergent search with random mutations originating from Genetic Algorithms, and thus, is limited to evolving populations of low-dimensional solutions. PGA-MAP-Elites overcomes this limitation using a gradient-based variation operator inspired by deep reinforcement learning which enables the evolution of large neural networks. Although high-performing in many environments, PGA-MAP-Elites fails on several tasks where the convergent search of the gradient-based variation operator hinders diversity. In this work, we
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00317</link><description>&lt;p&gt;
&#29992;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#21644;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Code Translation with Comparable Corpora and Multiple References. (arXiv:2311.00317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#32763;&#35793;&#12290;&#36890;&#36807;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#21644;&#22686;&#21152;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#19968;&#31181;&#26159;&#26500;&#24314;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65288;&#21363;&#20855;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#20195;&#30721;&#23545;&#65289;&#65292;&#21478;&#19968;&#31181;&#26159;&#29992;&#22810;&#20010;&#21442;&#32771;&#32763;&#35793;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#20998;&#26512;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#21487;&#27604;&#36739;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26723;&#20013;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#23545;&#21333;&#20010;&#21442;&#32771;&#32763;&#35793;&#30340;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#33258;&#21160;&#29983;&#25104;&#20102;&#21487;&#29992;&#24179;&#34892;&#25968;&#25454;&#30340;&#39069;&#22806;&#32763;&#35793;&#21442;&#32771;&#65292;&#24182;&#36890;&#36807;&#21333;&#20803;&#27979;&#35797;&#23545;&#32763;&#35793;&#36827;&#34892;&#31579;&#36873;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30446;&#26631;&#32763;&#35793;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;CodeT5&#22312;Java&#12289;Python&#21644;C++&#20043;&#38388;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#25552;&#21319;&#20102;7.5%&#30340;&#35745;&#31639;&#20934;&#30830;&#24615;&#65288;CA@1&#65289;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;&#38382;&#39064;&#65292;&#38024;&#23545;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#20013;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2310.02407</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Automated Bug Generation in the era of Large Language Models. (arXiv:2310.02407v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#33258;&#21160;&#32570;&#38519;&#29983;&#25104;&#38382;&#39064;&#65292;&#38024;&#23545;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#20013;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65307;&#36807;&#21435;&#20960;&#21313;&#24180;&#30340;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#20462;&#22797;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#32570;&#38519;&#30340;&#26041;&#27861;&#12290;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#22797;&#26434;&#30340;&#32570;&#38519;&#65292;&#21363;&#37027;&#20123;&#24456;&#38590;&#36890;&#36807;&#27979;&#35797;&#21644;&#35843;&#35797;&#26469;&#26816;&#27979;&#21644;&#20462;&#22797;&#30340;&#32570;&#38519;&#12290;&#20174;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#19982;&#27491;&#30830;&#30340;&#20195;&#30721;&#22312;&#22810;&#20010;&#20301;&#32622;&#19978;&#26377;&#25152;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#23450;&#20301;&#21644;&#20462;&#22797;&#12290;&#32780;&#38590;&#20197;&#26816;&#27979;&#30340;&#32570;&#38519;&#21017;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#36755;&#20837;&#21644;&#21487;&#36798;&#26465;&#20214;&#19979;&#23637;&#29616;&#20986;&#26469;&#12290;&#36825;&#20004;&#20010;&#30446;&#26631;&#65292;&#21363;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#21644;&#38590;&#20197;&#20462;&#22797;&#30340;&#32570;&#38519;&#65292;&#22823;&#22810;&#25968;&#26159;&#19968;&#33268;&#30340;&#65307;&#32570;&#38519;&#29983;&#25104;&#25216;&#26415;&#21487;&#20197;&#23558;&#22810;&#20010;&#35821;&#21477;&#26356;&#25913;&#20026;&#20165;&#22312;&#29305;&#23450;&#36755;&#20837;&#38598;&#21512;&#19979;&#34987;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#26469;&#35828;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#26159;&#30456;&#20114;&#20914;&#31361;&#30340;&#65306;&#19968;&#20010;&#32570;&#38519;&#24212;&#35813;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#27491;&#30830;&#20195;&#30721;&#30456;&#20284;&#30340;&#20195;&#30721;&#34920;&#31034;&#65292;&#20197;&#25361;&#25112;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bugs are essential in software engineering; many research studies in the past decades have been proposed to detect, localize, and repair bugs in software systems. Effectiveness evaluation of such techniques requires complex bugs, i.e., those that are hard to detect through testing and hard to repair through debugging. From the classic software engineering point of view, a hard-to-repair bug differs from the correct code in multiple locations, making it hard to localize and repair. Hard-to-detect bugs, on the other hand, manifest themselves under specific test inputs and reachability conditions. These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs, are mostly aligned; a bug generation technique can change multiple statements to be covered only under a specific set of inputs. However, these two objectives are conflicting for learning-based techniques: A bug should have a similar code representation to the correct code in the training data to challenge a bug predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation through Optimal Transport. (arXiv:2310.01632v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;ILfO&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#23519;&#20013;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;ILfO&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#32773;&#35797;&#22270;&#22312;&#27809;&#26377;&#30452;&#25509;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454;&#27169;&#20223;&#19987;&#23478;&#34892;&#20026;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;IL&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26681;&#25454;&#23398;&#20064;&#32773;&#21644;&#19987;&#23478;&#30340;&#29366;&#24577;&#36712;&#36857;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#29983;&#25104;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#20026;&#29983;&#25104;&#26080;&#38656;&#23398;&#20064;&#27169;&#22411;&#25110;&#23545;&#25239;&#23398;&#20064;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;ILfO&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#31616;&#21333;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#21482;&#35266;&#23519;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#32780;&#27809;&#26377;&#21160;&#20316;&#65292;&#23427;&#22312;ILfO&#35774;&#32622;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#19987;&#23478;&#32423;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning from Observation (ILfO) is a setting in which a learner tries to imitate the behavior of an expert, using only observational data and without the direct guidance of demonstrated actions. In this paper, we re-examine the use of optimal transport for IL, in which a reward is generated based on the Wasserstein distance between the state trajectories of the learner and expert. We show that existing methods can be simplified to generate a reward function without requiring learned models or adversarial learning. Unlike many other state-of-the-art methods, our approach can be integrated with any RL algorithm, and is amenable to ILfO. We demonstrate the effectiveness of this simple approach on a variety of continuous control tasks and find that it surpasses the state of the art in the IlfO setting, achieving expert-level performance across a range of evaluation domains even when observing only a single expert trajectory without actions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCS-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#36755;&#20986;&#23618;&#26356;&#26032;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#23458;&#25143;&#31471;&#30340;&#32858;&#31867;&#36873;&#25321;&#65292;&#20197;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.00198</link><description>&lt;p&gt;
&#21033;&#29992;&#24322;&#26500;&#24341;&#23548;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Non-IID Federated Learning via Heterogeneity-Guided Client Sampling. (arXiv:2310.00198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HiCS-FL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#36755;&#20986;&#23618;&#26356;&#26032;&#26469;&#20272;&#35745;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#36827;&#34892;&#23458;&#25143;&#31471;&#30340;&#32858;&#31867;&#36873;&#25321;&#65292;&#20197;&#21152;&#36895;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#31471;&#35774;&#22791;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#20351;&#24471;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#20013;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26159;&#65292;&#22312;&#30001;&#20110;&#36164;&#28304;&#38480;&#21046;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#23458;&#25143;&#31471;&#33021;&#21442;&#19982;&#20219;&#20309;&#19968;&#36718;FL&#30340;&#35774;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#33268;&#21147;&#20110;&#35757;&#32451;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;FL&#31995;&#32479;&#20013;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#23427;&#20204;&#30528;&#37325;&#20110;&#24320;&#21457;&#37319;&#26679;&#26356;&#20855;&#20449;&#24687;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#25216;&#26415;&#35201;&#20040;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#21482;&#22312;&#23458;&#25143;&#31471;&#20855;&#26377;&#31867;&#20284;&#24322;&#36136;&#24615;&#37197;&#32622;&#25991;&#20214;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HiCS-FL&#65288;&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#37319;&#26679;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#20351;&#29992;&#23458;&#25143;&#31471;&#32593;&#32476;&#36755;&#20986;&#23618;&#30340;&#26356;&#26032;&#26469;&#20272;&#35745;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#24182;&#20381;&#36182;&#27492;&#20449;&#24687;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical heterogeneity of data present at client devices in a federated learning (FL) system renders the training of a global model in such systems difficult. Particularly challenging are the settings where due to resource constraints only a small fraction of clients can participate in any given round of FL. Recent approaches to training a global model in FL systems with non-IID data have focused on developing client selection methods that aim to sample clients with more informative updates of the model. However, existing client selection techniques either introduce significant computation overhead or perform well only in the scenarios where clients have data with similar heterogeneity profiles. In this paper, we propose HiCS-FL (Federated Learning via Hierarchical Clustered Sampling), a novel client selection method in which the server estimates statistical heterogeneity of a client's data using the client's update of the network's output layer and relies on this information to clu
&lt;/p&gt;</description></item><item><title>MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.13042</link><description>&lt;p&gt;
MosaicFusion: &#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#22120;
&lt;/p&gt;
&lt;p&gt;
MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation. (arXiv:2309.13042v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13042
&lt;/p&gt;
&lt;p&gt;
MosaicFusion&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MosaicFusion&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26080;&#38656;&#35757;&#32451;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26631;&#31614;&#30417;&#30563;&#12290;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#26377;&#29992;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23545;&#35937;&#23454;&#20363;&#21644;&#33945;&#29256;&#27880;&#37322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#30011;&#24067;&#20998;&#20026;&#20960;&#20010;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;&#19968;&#36718;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#22522;&#20110;&#19981;&#21516;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#22810;&#20010;&#23454;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#19982;&#23545;&#35937;&#25552;&#31034;&#30456;&#20851;&#32852;&#30340;&#36328;&#27880;&#24847;&#21147;&#22270;&#22312;&#23618;&#21644;&#25193;&#25955;&#26102;&#38388;&#27493;&#19978;&#65292;&#28982;&#21518;&#36827;&#34892;&#31616;&#21333;&#30340;&#38408;&#20540;&#22788;&#29702;&#21644;&#36793;&#32536;&#24863;&#30693;&#30340;&#32454;&#21270;&#22788;&#29702;&#65292;&#24471;&#21040;&#30456;&#24212;&#30340;&#23454;&#20363;&#33945;&#29256;&#12290;&#25105;&#20204;&#30340;MosaicFusion&#21487;&#20197;&#20026;&#31232;&#32570;&#21644;&#26032;&#39062;&#31867;&#21035;&#20135;&#29983;&#22823;&#37327;&#30340;&#21512;&#25104;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#22797;&#26434;&#30340;&#22788;&#29702;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;LVIS&#38271;&#23614;&#21644;&#24320;&#25918;&#35789;&#27719;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MosaicFusion, a simple yet effective diffusion-based data augmentation approach for large vocabulary instance segmentation. Our method is training-free and does not rely on any label supervision. Two key designs enable us to employ an off-the-shelf text-to-image diffusion model as a useful dataset generator for object instances and mask annotations. First, we divide an image canvas into several regions and perform a single round of diffusion process to generate multiple instances simultaneously, conditioning on different text prompts. Second, we obtain corresponding instance masks by aggregating cross-attention maps associated with object prompts across layers and diffusion time steps, followed by simple thresholding and edge-aware refinement processing. Without bells and whistles, our MosaicFusion can produce a significant amount of synthetic labeled data for both rare and novel categories. Experimental results on the challenging LVIS long-tailed and open-vocabulary benchma
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.06577</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20197;&#36991;&#20813;&#21442;&#25968;&#29190;&#28856;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#25110;&#22823;&#22810;&#25968;&#33410;&#28857;&#19982;&#36755;&#20837;&#25110;&#36755;&#20986;&#26377;&#36830;&#25509;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#35813;&#23618;&#30340;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;&#33539;&#22260;&#12290;&#36825;&#20010;&#33539;&#25968;&#30340;&#35745;&#31639;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#21487;&#20197;&#23436;&#20840;&#25110;&#37096;&#20998;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#65292;&#24182;&#26816;&#26597;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Python&#20989;&#25968;&#65292;&#22312;i3BQuantum&#23384;&#20648;&#24211;&#30340;Jupyter Notebook&#20013;&#21487;&#20197;&#36816;&#34892;&#23427;&#65306;https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;
&lt;p&gt;
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.10219</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#30340;&#19981;&#21464;&#20851;&#31995;&#30693;&#35782;&#19978;&#25506;&#32034;&#36229;&#20851;&#31995;&#26102;&#38388;&#30693;&#35782;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge. (arXiv:2307.10219v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10219
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#26102;&#38388;KG&#21644;&#36229;&#20851;&#31995;KG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20851;&#31995;&#30693;&#35782;&#22270;(HKGs)&#26159;&#20256;&#32479;&#30693;&#35782;&#22270;(KGs)&#30340;&#24310;&#20280;&#65292;&#20026;&#27599;&#20010;KG&#20107;&#23454;&#25552;&#20379;&#39069;&#22806;&#30340;&#38190;&#20540;&#23545;(&#21363;&#38480;&#23450;&#35789;)&#65292;&#20197;&#26356;&#22909;&#22320;&#38480;&#21046;&#20107;&#23454;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#22312;HKGs&#19978;&#36827;&#34892;&#22270;&#25512;&#29702;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30001;&#20110;&#19990;&#30028;&#30693;&#35782;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#22823;&#37327;&#24179;&#34892;&#24037;&#20316;&#38598;&#20013;&#22312;&#23545;&#26102;&#38388;KGs(TKGs)&#36827;&#34892;&#25512;&#29702;&#65292;&#20854;&#20013;&#27599;&#20010;TKG&#20107;&#23454;&#21487;&#20197;&#34987;&#35270;&#20026;&#24102;&#26377;&#26102;&#38388;&#25139;(&#25110;&#26102;&#38388;&#27573;)&#30340;KG&#20107;&#23454;&#65292;&#25351;&#23450;&#20854;&#26102;&#38388;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;HKG&#25512;&#29702;&#26041;&#27861;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#20026;&#22312;&#20043;&#21069;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#26174;&#24335;&#22320;&#25351;&#23450;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#21069;&#30340;TKG&#25512;&#29702;&#26041;&#27861;&#21482;&#37325;&#35270;&#26102;&#38388;&#25512;&#29702;&#65292;&#24182;&#27809;&#26377;&#21150;&#27861;&#20174;&#38480;&#23450;&#35789;&#20013;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22635;&#34917;TKG&#25512;&#29702;&#21644;HKG&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#36229;&#20851;&#31995;TKG(HTKG)&#25968;&#25454;&#38598;&#65292;&#21363;Wiki-hy&#21644;...
&lt;/p&gt;
&lt;p&gt;
Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs) provide additional key-value pairs (i.e., qualifiers) for each KG fact that help to better restrict the fact validity. In recent years, there has been an increasing interest in studying graph reasoning over HKGs. In the meantime, due to the ever-evolving nature of world knowledge, extensive parallel works have been focusing on reasoning over temporal KGs (TKGs), where each TKG fact can be viewed as a KG fact coupled with a timestamp (or time period) specifying its time validity. The existing HKG reasoning approaches do not consider temporal information because it is not explicitly specified in previous benchmark datasets. Besides, all the previous TKG reasoning methods only lay emphasis on temporal reasoning and have no way to learn from qualifiers. To this end, we aim to fill the gap between TKG reasoning and HKG reasoning. We develop two new benchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.07191</link><description>&lt;p&gt;
&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#30340;&#22522;&#20934;&#21644;&#33258;&#23450;&#20041;&#21253;
&lt;/p&gt;
&lt;p&gt;
Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36127;&#33655;&#39044;&#27979;&#22312;&#30005;&#21147;&#34892;&#19994;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#21487;&#20197;&#20026;&#21518;&#32493;&#20219;&#21153;&#22914;&#30005;&#32593;&#35843;&#24230;&#25552;&#20379;&#21442;&#32771;&#65292;&#20174;&#32780;&#24102;&#26469;&#24040;&#22823;&#30340;&#32463;&#27982;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#36127;&#33655;&#39044;&#27979;&#19982;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20043;&#38388;&#23384;&#22312;&#35768;&#22810;&#24046;&#24322;&#12290;&#19968;&#26041;&#38754;&#65292;&#36127;&#33655;&#39044;&#27979;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21518;&#32493;&#20219;&#21153;&#65288;&#22914;&#30005;&#32593;&#35843;&#24230;&#65289;&#30340;&#25104;&#26412;&#65292;&#32780;&#19981;&#20165;&#20165;&#36861;&#27714;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36127;&#33655;&#21463;&#21040;&#35768;&#22810;&#22806;&#37096;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#28201;&#24230;&#25110;&#26085;&#21382;&#21464;&#37327;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#30340;&#35268;&#27169;&#65288;&#22914;&#24314;&#31569;&#32423;&#36127;&#33655;&#21644;&#32858;&#21512;&#32423;&#36127;&#33655;&#65289;&#20063;&#20250;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#36127;&#33655;&#39044;&#27979;&#23384;&#26723;&#65292;&#20854;&#20013;&#21253;&#25324;&#36127;&#33655;&#39046;&#22495;&#29305;&#23450;&#30340;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#24110;&#21161;&#39044;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#27169;&#25311;&#36127;&#33655;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#20165;&#36861;&#27714;&#20934;&#30830;&#24615;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;...
&lt;/p&gt;
&lt;p&gt;
Load forecasting is of great significance in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between load forecasting and traditional time series forecasting. On the one hand, load forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. On the other hand, the load is largely influenced by many external factors, such as temperature or calendar variables. In addition, the scale of predictions (such as building-level loads and aggregated-level loads) can also significantly impact the predicted results. In this paper, we provide a comprehensive load forecasting archive, which includes load domain-specific feature engineering to help forecasting models better model load data. In addition, different from the traditional loss function which only aims for accuracy, we also provide a method to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17210</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scattering Spectra Models for Physics. (arXiv:2306.17210v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29289;&#29702;&#23398;&#20013;&#30340;&#25955;&#23556;&#35889;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#22330;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#32467;&#21512;&#20102;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#65292;&#33021;&#22815;&#20934;&#30830;&#19988;&#31283;&#20581;&#22320;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#23478;&#24120;&#24120;&#38656;&#35201;&#27010;&#29575;&#27169;&#22411;&#26469;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#25110;&#29983;&#25104;&#19968;&#20010;&#22330;&#30340;&#26032;&#23454;&#29616;&#12290;&#38024;&#23545;&#39640;&#24230;&#38750;&#39640;&#26031;&#22330;&#30340;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25955;&#23556;&#35889;&#27169;&#22411;&#29992;&#20110;&#24179;&#31283;&#22330;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#29289;&#29702;&#23398;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#22330;&#30340;&#20934;&#30830;&#19988;&#31283;&#20581;&#30340;&#32479;&#35745;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#25955;&#23556;&#31995;&#25968;&#30340;&#21327;&#26041;&#24046;&#65292;&#21363;&#22330;&#30340;&#23567;&#27874;&#20998;&#35299;&#21644;&#28857;&#20301;&#27169;&#12290;&#22312;&#20171;&#32461;&#21033;&#29992;&#26059;&#36716;&#21644;&#32553;&#25918;&#19979;&#22330;&#30340;&#35268;&#24459;&#24615;&#36827;&#34892;&#26377;&#29992;&#30340;&#32500;&#24230;&#32422;&#31616;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#22810;&#23610;&#24230;&#29289;&#29702;&#22330;&#19978;&#30340;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#37325;&#29616;&#26631;&#20934;&#32479;&#35745;&#37327;&#65292;&#21253;&#25324;&#22235;&#38454;&#31354;&#38388;&#30697;&#12290;&#36825;&#20123;&#25955;&#23556;&#35889;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#32500;&#32467;&#26500;&#21270;&#34920;&#31034;&#65292;&#25429;&#25417;&#20102;&#20851;&#38190;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicists routinely need probabilistic models for a number of tasks such as parameter inference or the generation of new realizations of a field. Establishing such models for highly non-Gaussian fields is a challenge, especially when the number of samples is limited. In this paper, we introduce scattering spectra models for stationary fields and we show that they provide accurate and robust statistical descriptions of a wide range of fields encountered in physics. These models are based on covariances of scattering coefficients, i.e. wavelet decomposition of a field coupled with a point-wise modulus. After introducing useful dimension reductions taking advantage of the regularity of a field under rotation and scaling, we validate these models on various multi-scale physical fields and demonstrate that they reproduce standard statistics, including spatial moments up to 4th order. These scattering spectra provide us with a low-dimensional structured representation that captures key prop
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10716</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#26500;&#24314;&#22823;&#35268;&#27169;&#12289;&#33391;&#22909;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36880;&#28176;&#24341;&#36215;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;TS-PTMs&#65289;&#65292;&#26088;&#22312;&#25351;&#23548;&#20102;&#35299;&#12289;&#24212;&#29992;&#21644;&#30740;&#31350;TS-PTMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;TSM&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#39044;&#35757;&#32451;&#25216;&#26415;&#27010;&#36848;&#20102;TS-PTMs&#12290;&#25105;&#20204;&#25506;&#35752;&#30340;&#20027;&#35201;&#31867;&#21035;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;TS-PTMs&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.03097</link><description>&lt;p&gt;
&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Federated Ensemble-Directed Offline Reinforcement Learning. (arXiv:2305.03097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;FEDORA&#30340;&#32852;&#37030;&#38598;&#25104;&#25351;&#23548;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#22330;&#26223;&#19979;&#65292;&#20998;&#24067;&#24335;&#30340;&#23398;&#20064;&#20195;&#29702;&#24517;&#39035;&#20165;&#20351;&#29992;&#30001;&#19981;&#21516;&#30340;&#26410;&#30693;&#30340;&#34892;&#20026;&#31574;&#30053;&#29983;&#25104;&#30340;&#23567;&#22411;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#21327;&#20316;&#23398;&#20064;&#20986;&#39640;&#36136;&#37327;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#31528;&#25305;&#22320;&#23558;&#26631;&#20934;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19982;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#32452;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22240;&#27492;&#35774;&#35745;&#20102;Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA)&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#28860;&#23458;&#25143;&#32676;&#20307;&#30340;&#38598;&#20307;&#26234;&#24935;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;FEDORA&#20195;&#30721;&#24211;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#19978;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FEDORA&#22312;&#21508;&#31181;&#22797;&#26434;&#30340;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#21253;&#25324;&#22312;&#21512;&#24182;&#30340;&#25968;&#25454;&#27719;&#24635;&#20013;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;FEDORA&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of federated offline reinforcement learning (RL), a scenario under which distributed learning agents must collaboratively learn a high-quality control policy only using small pre-collected datasets generated according to different unknown behavior policies. Naively combining a standard offline RL approach with a standard federated learning approach to solve this problem can lead to poorly performing policies. In response, we develop the Federated Ensemble-Directed Offline Reinforcement Learning Algorithm (FEDORA), which distills the collective wisdom of the clients using an ensemble learning approach. We develop the FEDORA codebase to utilize distributed compute resources on a federated learning platform. We show that FEDORA significantly outperforms other approaches, including offline RL over the combined data pool, in various complex continuous control environments and real world datasets. Finally, we demonstrate the performance of FEDORA in the real-world on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#30340;&#20840;&#23616;&#21644;&#22522;&#20110;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#21487;&#26377;&#25928;&#25628;&#32034;&#21487;&#34892;&#22495;</title><link>http://arxiv.org/abs/2302.04686</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#23454;&#29616;&#28151;&#21512;&#21464;&#37327;&#30340;&#20840;&#23616;&#21644;&#20248;&#20808;&#32423;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global and Preference-based Optimization with Mixed Variables using Piecewise Affine Surrogates. (arXiv:2302.04686v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#30340;&#20840;&#23616;&#21644;&#22522;&#20110;&#20559;&#22909;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#31639;&#27861;&#36890;&#36807;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#21487;&#26377;&#25928;&#25628;&#32034;&#21487;&#34892;&#22495;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#22797;&#26434;&#38480;&#21046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#28041;&#21450;&#28151;&#21512;&#21464;&#37327;&#65288;&#21363;&#25968;&#20540;&#21644;&#20998;&#31867;&#24615;&#30340;&#21464;&#37327;&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#21487;&#33021;&#38590;&#20197;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#26159;&#22797;&#26434;&#27169;&#25311;&#25110;&#23454;&#39564;&#30340;&#32467;&#26524;&#26102;&#65292;&#35780;&#20272;&#20195;&#20215;&#21487;&#33021;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20195;&#29702;&#20840;&#23616;&#20248;&#21270;&#31639;&#27861;&#65292;&#22522;&#20110;&#23545;&#21487;&#34892;&#26679;&#26412;&#19978;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#27573;&#20223;&#23556;&#20195;&#29702;&#26500;&#24314;&#26469;&#35299;&#20915;&#32447;&#24615;&#32422;&#26463;&#30340;&#28151;&#21512;&#21464;&#37327;&#38382;&#39064;&#65292;&#21487;&#35299;&#20915;&#20013;&#21040;&#22823;&#35268;&#27169;&#38382;&#39064;&#65288;&#32534;&#30721;&#21518;&#32422;100&#20010;&#21464;&#37327;&#21644;20&#20010;&#32422;&#26463;&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#25506;&#32034;&#20989;&#25968;&#26469;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#26377;&#25928;&#22320;&#25628;&#32034;&#21487;&#34892;&#22495;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#31639;&#27861;&#29256;&#26412;&#65292;&#24403;&#21482;&#33021;&#33719;&#24471;&#26679;&#26412;&#38388;&#30340;&#25104;&#23545;&#27604;&#36739;&#32780;&#26410;&#37327;&#21270;&#24213;&#23618;&#35201;&#26368;&#23567;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#21487;&#20351;&#29992;&#35813;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization problems involving mixed variables, i.e., variables of numerical and categorical nature, can be challenging to solve, especially in the presence of complex constraints. Moreover, when the objective function is the result of a complicated simulation or experiment, it may be expensive to evaluate. This paper proposes a novel surrogate-based global optimization algorithm to solve linearly constrained mixed-variable problems up to medium-large size (around 100 variables after encoding and 20 constraints) based on constructing a piecewise affine surrogate of the objective function over feasible samples. We introduce two types of exploration functions to efficiently search the feasible domain via mixed-integer linear programming solvers. We also provide a preference-based version of the algorithm, which can be used when only pairwise comparisons between samples can be acquired while the underlying objective function to minimize remains unquantified. The two algorithms are tested
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item></channel></rss>