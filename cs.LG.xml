<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01138</link><description>&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20010;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks in EEG-based Emotion Recognition: A Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01138
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#39046;&#22495;&#12290;&#26412;&#32508;&#36848;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;&#24050;&#26377;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#30340;&#26126;&#30830;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#23545;&#20110;&#20854;&#20182;&#27169;&#24335;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#21487;&#20197;&#30452;&#35266;&#22320;&#21709;&#24212;&#20154;&#33041;&#20013;&#30340;&#24773;&#32490;&#27169;&#24335;&#65292;&#22240;&#27492;&#25104;&#20026;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#39046;&#22495;&#26368;&#20851;&#27880;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#20381;&#36182;&#19982;&#24773;&#32490;&#23494;&#20999;&#30456;&#20851;&#65292;&#22240;&#27492;&#21457;&#23637;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#24773;&#32490;&#24615;&#33041;&#30005;&#22270;&#20013;&#30340;&#22823;&#33041;&#21306;&#22495;&#20381;&#36182;&#20855;&#26377;&#29983;&#29702;&#22522;&#30784;&#65292;&#20351;&#24471;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;GNNs&#19982;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;GNNs&#26377;&#25152;&#21306;&#21035;&#12290;&#27492;&#22806;&#65292;&#22312;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#24773;&#32490;&#35782;&#21035;&#20013;&#26082;&#27809;&#26377;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20063;&#27809;&#26377;&#26500;&#24314;GNNs&#30340;&#25351;&#23548;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#24050;&#26377;&#26041;&#27861;&#22312;&#22270;&#26500;&#36896;&#30340;&#32479;&#19968;&#26694;&#26550;&#19979;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#25581;&#31034;&#20986;&#20854;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#20174;&#26694;&#26550;&#30340;&#19977;&#20010;&#38454;&#27573;&#20998;&#26512;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#26500;&#24314;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;GNNs&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#20123;...
&lt;/p&gt;
&lt;p&gt;
Compared to other modalities, EEG-based emotion recognition can intuitively respond to the emotional patterns in the human brain and, therefore, has become one of the most concerning tasks in the brain-computer interfaces field. Since dependencies within brain regions are closely related to emotion, a significant trend is to develop Graph Neural Networks (GNNs) for EEG-based emotion recognition. However, brain region dependencies in emotional EEG have physiological bases that distinguish GNNs in this field from those in other time series fields. Besides, there is neither a comprehensive review nor guidance for constructing GNNs in EEG-based emotion recognition. In the survey, our categorization reveals the commonalities and differences of existing approaches under a unified framework of graph construction. We analyze and categorize methods from three stages in the framework to provide clear guidance on constructing GNNs in EEG-based emotion recognition. In addition, we discuss several 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12212</link><description>&lt;p&gt;
&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#27604;&#36739;&#20998;&#26512;&#24052;&#35199;&#20844;&#21496;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#19978;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Named Entity Recognition: Comparative Analysis of Mono- and Multilingual Transformer Models on Brazilian Corporate Earnings Call Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22312;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#20351;&#29992;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26159;&#19968;&#31181;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20851;&#20110;NER&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#25991;&#26723;&#19978;&#65292;&#23548;&#33268;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#36130;&#21153;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#37329;&#34701;&#39046;&#22495;&#20869;NER&#38656;&#27714;&#65292;&#24182;&#20391;&#37325;&#20110;&#20174;&#24052;&#35199;&#38134;&#34892;&#36130;&#25253;&#30005;&#35805;&#36716;&#24405;&#20013;&#25552;&#21462;&#30340;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#12290;&#36890;&#36807;&#25972;&#29702;&#21253;&#25324;384&#20010;&#36716;&#24405;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27880;&#37322;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#33889;&#33796;&#29273;&#35821;&#65288;BERTimbau&#21644;PTT5&#65289;&#35757;&#32451;&#30340;&#21333;&#35821;&#27169;&#22411;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#65288;mBERT&#21644;mT5&#65289;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#37325;&#26032;&#26500;&#24314;&#20026;&#25991;&#26412;&#29983;&#25104;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;T5&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#22312;&#27169;&#22411;&#24494;&#35843;&#20043;&#21518;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12212v1 Announce Type: cross  Abstract: Named Entity Recognition (NER) is a Natural Language Processing technique for extracting information from textual documents. However, much of the existing research on NER has been centered around English-language documents, leaving a gap in the availability of datasets tailored to the financial domain in Portuguese. This study addresses the need for NER within the financial domain, focusing on Portuguese-language texts extracted from earnings call transcriptions of Brazilian banks. By curating a comprehensive dataset comprising 384 transcriptions and leveraging weak supervision techniques for annotation, we evaluate the performance of monolingual models trained on Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5). Notably, we introduce a novel approach that reframes the token classification task as a text generation problem, enabling fine-tuning and evaluation of T5 models. Following the fine-tuning of the models,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;</title><link>https://arxiv.org/abs/2403.06087</link><description>&lt;p&gt;
&#23398;&#20064;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Learning the irreversible progression trajectory of Alzheimer's disease
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06087
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21487;&#36870;&#36827;&#23637;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#19968;&#31181;&#38543;&#30528;30&#24180;&#26102;&#38388;&#36880;&#28176;&#23637;&#24320;&#30340;&#36827;&#34892;&#24615;&#19981;&#21487;&#36870;&#33041;&#37096;&#30142;&#30149;&#12290;&#22240;&#27492;&#65292;&#20851;&#38190;&#26159;&#22312;&#26089;&#26399;&#25429;&#33719;&#30142;&#30149;&#30340;&#36827;&#23637;&#65292;&#20197;&#20415;&#22312;&#30151;&#29366;&#20986;&#29616;&#20043;&#21069;&#21487;&#20197;&#26045;&#21152;&#24178;&#39044;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#27979;AD&#30340;&#21457;&#20316;&#26041;&#38754;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#38543;&#35775;&#30340;&#21463;&#35797;&#32773;&#65292;&#29616;&#26377;&#30340;AD&#20998;&#31867;&#25216;&#26415;&#21482;&#38024;&#23545;&#20934;&#30830;&#30340;&#32452;&#20998;&#37197;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#22312;&#38543;&#35775;&#36807;&#31243;&#20013;&#36882;&#22686;&#39118;&#38505;&#30340;&#21333;&#35843;&#22686;&#21152;&#12290;&#22312;&#38543;&#35775;&#38388;&#20986;&#29616;&#30340;&#27874;&#21160;&#39118;&#38505;&#35780;&#20998;&#36829;&#32972;&#20102;AD&#30340;&#19981;&#21487;&#36870;&#24615;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20063;&#23545;&#29702;&#35299;&#30142;&#30149;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#24456;&#23569;&#30340;&#20215;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#39044;&#27979;AD&#30340;&#32437;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26088;&#22312;&#22312;&#30149;&#24773;&#36827;&#23637;&#26399;&#38388;&#20445;&#25345;&#39044;&#26399;&#30340;&#21333;&#35843;&#22686;&#21152;&#30142;&#30149;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06087v1 Announce Type: new  Abstract: Alzheimer's disease (AD) is a progressive and irreversible brain disorder that unfolds over the course of 30 years. Therefore, it is critical to capture the disease progression in an early stage such that intervention can be applied before the onset of symptoms. Machine learning (ML) models have been shown effective in predicting the onset of AD. Yet for subjects with follow-up visits, existing techniques for AD classification only aim for accurate group assignment, where the monotonically increasing risk across follow-up visits is usually ignored. Resulted fluctuating risk scores across visits violate the irreversibility of AD, hampering the trustworthiness of models and also providing little value to understanding the disease progression. To address this issue, we propose a novel regularization approach to predict AD longitudinally. Our technique aims to maintain the expected monotonicity of increasing disease risk during progression w
&lt;/p&gt;</description></item><item><title>&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.04261</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#21306;&#25361;&#25112;&#25512;&#21160;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advancing Biomedical Text Mining with Community Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04261
&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#31215;&#32047;&#20102;&#22823;&#37327;&#26469;&#33258;&#31185;&#23398;&#25991;&#29486;&#12289;&#30005;&#23376;&#30149;&#21382;&#12289;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21644;&#31038;&#20132;&#23186;&#20307;&#31561;&#21508;&#26041;&#38754;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#28982;&#32780;&#25163;&#21160;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#20123;&#24222;&#22823;&#19988;&#22797;&#26434;&#30340;&#36164;&#28304;&#26159;&#32791;&#26102;&#19988;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65292;&#20063;&#31216;&#20026;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22791;&#21463;&#20851;&#27880;&#12290;&#31038;&#21306;&#25361;&#25112;&#35780;&#20272;&#31454;&#36187;&#22312;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#30740;&#31350;&#20013;&#30340;&#25216;&#26415;&#21019;&#26032;&#21644;&#36328;&#23398;&#31185;&#21512;&#20316;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#25361;&#25112;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#25366;&#25496;&#21644;&#20449;&#24687;&#22788;&#29702;&#30340;&#26368;&#26032;&#35299;&#20915;&#26041;&#26696;&#30340;&#24179;&#21488;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19982;&#20013;&#25991;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#26377;&#20851;&#30340;&#26368;&#26032;&#31038;&#21306;&#25361;&#25112;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04261v1 Announce Type: new  Abstract: The field of biomedical research has witnessed a significant increase in the accumulation of vast amounts of textual data from various sources such as scientific literatures, electronic health records, clinical trial reports, and social media. However, manually processing and analyzing these extensive and complex resources is time-consuming and inefficient. To address this challenge, biomedical text mining, also known as biomedical natural language processing, has garnered great attention. Community challenge evaluation competitions have played an important role in promoting technology innovation and interdisciplinary collaboration in biomedical text mining research. These challenges provide platforms for researchers to develop state-of-the-art solutions for data mining and information processing in biomedical research. In this article, we review the recent advances in community challenges specific to Chinese biomedical text mining. Firs
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23450;&#20301;&#30446;&#26631;&#21152;&#23494;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#65292;&#21363;&#20351;&#23384;&#22312;&#36319;&#36394;&#21464;&#24418;&#12290;</title><link>https://arxiv.org/abs/2402.19037</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23450;&#20301;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23494;&#30721;&#25805;&#20316;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Deep-Learning Technique to Locate Cryptographic Operations in Side-Channel Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19037
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#20391;&#20449;&#36947;&#36319;&#36394;&#20013;&#23450;&#20301;&#30446;&#26631;&#21152;&#23494;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#65292;&#21363;&#20351;&#23384;&#22312;&#36319;&#36394;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20391;&#20449;&#36947;&#25915;&#20987;&#36890;&#36807;&#30456;&#20851;&#37096;&#20998;&#24050;&#30693;&#35745;&#31639;&#25968;&#25454;&#21644;&#24050;&#27979;&#37327;&#30340;&#20391;&#20449;&#36947;&#20449;&#21495;&#65292;&#20801;&#35768;&#20174;&#21152;&#23494;&#21407;&#35821;&#30340;&#25191;&#34892;&#20013;&#25552;&#21462;&#31192;&#23494;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23450;&#20301;&#20391;&#20449;&#36947;&#36861;&#36394;&#20013;&#25191;&#34892;&#30340;&#30446;&#26631;&#35745;&#31639;&#23494;&#30721;&#25805;&#20316;&#30340;&#26102;&#38388;&#30636;&#38388;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#36890;&#36807;&#38543;&#26426;&#24310;&#36831;&#25554;&#20837;&#25216;&#26415;&#33719;&#24471;&#30340;&#36319;&#36394;&#21464;&#24418;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19037v1 Announce Type: cross  Abstract: Side-channel attacks allow extracting secret information from the execution of cryptographic primitives by correlating the partially known computed data and the measured side-channel signal. However, to set up a successful side-channel attack, the attacker has to perform i) the challenging task of locating the time instant in which the target cryptographic primitive is executed inside a side-channel trace and then ii)the time-alignment of the measured data on that time instant. This paper presents a novel deep-learning technique to locate the time instant in which the target computed cryptographic operations are executed in the side-channel trace. In contrast to state-of-the-art solutions, the proposed methodology works even in the presence of trace deformations obtained through random delay insertion techniques. We validated our proposal through a successful attack against a variety of unprotected and protected cryptographic primitive
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;&#20272;&#35745;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;</title><link>https://arxiv.org/abs/2402.17295</link><description>&lt;p&gt;
&#25345;&#20037;&#22270;&#30340;&#37327;&#23376;&#36317;&#31163;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Quantum Distance Approximation for Persistence Diagrams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17295
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#22312;&#20272;&#35745;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#23545;&#20110;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#21487;&#33021;&#20250;&#24456;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#24635;&#32467;&#20851;&#20110;&#28508;&#22312;&#22797;&#26434;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#24418;&#29366;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#20108;&#32500;&#25345;&#20037;&#22270;&#12290;&#25345;&#20037;&#22270;&#30340;&#31354;&#38388;&#21487;&#20197;&#36171;&#20104;&#21508;&#31181;&#24230;&#37327;&#65292;&#27604;&#22914;Wasserstein&#36317;&#31163;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#20801;&#35768;&#23558;&#36825;&#20123;&#24635;&#32467;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#20004;&#20010;&#25345;&#20037;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#28041;&#21450;&#25214;&#21040;&#20004;&#20010;&#22270;&#30340;&#28857;&#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#26041;&#24335;&#65292;&#23545;&#20110;&#20256;&#32479;&#35745;&#31639;&#26426;&#26469;&#35828;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#19968;&#39033;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#37327;&#23376;&#35745;&#31639;&#26426;&#35780;&#20272;&#25345;&#20037;&#22270;&#20043;&#38388;&#36317;&#31163;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;Wasserstein&#36317;&#31163;&#21644;$d^{c}_{p}$&#36317;&#31163;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17295v1 Announce Type: cross  Abstract: Topological Data Analysis methods can be useful for classification and clustering tasks in many different fields as they can provide two dimensional persistence diagrams that summarize important information about the shape of potentially complex and high dimensional data sets. The space of persistence diagrams can be endowed with various metrics such as the Wasserstein distance which admit a statistical structure and allow to use these summaries for machine learning algorithms. However, computing the distance between two persistence diagrams involves finding an optimal way to match the points of the two diagrams and may not always be an easy task for classical computers. In this work we explore the potential of quantum computers to estimate the distance between persistence diagrams, in particular we propose variational quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$ distance. Our implementation is a weighted 
&lt;/p&gt;</description></item><item><title>Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;</title><link>https://arxiv.org/abs/2402.15478</link><description>&lt;p&gt;
Transformer&#26159;&#34920;&#29616;&#21147;&#24378;&#22823;&#30340;&#65292;&#20294;&#26159;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#26469;&#35828;&#34920;&#29616;&#21147;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transformers are Expressive, But Are They Expressive Enough for Regression?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15478
&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#20173;&#26377;&#24453;&#32771;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#25351;&#30340;&#26159;&#23427;&#33021;&#22815;&#36924;&#36817;&#30340;&#20989;&#25968;&#31867;&#12290;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26159;&#23436;&#20840;&#34920;&#29616;&#21147;&#30340;&#65292;&#22914;&#26524;&#23427;&#21487;&#20197;&#20805;&#24403;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#23581;&#35797;&#20998;&#26512;Transformer&#30340;&#34920;&#29616;&#21147;&#12290;&#19982;&#29616;&#26377;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#22312;&#21487;&#38752;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20381;&#36182;&#20110;&#20855;&#26377;&#21487;&#35266;&#21306;&#38388;&#30340;&#20998;&#27573;&#24120;&#25968;&#36924;&#36817;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#8220;Transformer&#26159;&#21542;&#30495;&#27491;&#26159;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#35843;&#26597;&#65292;&#36890;&#36807;&#23454;&#39564;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#8230;&#8230;&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15478v1 Announce Type: new  Abstract: Transformers have become pivotal in Natural Language Processing, demonstrating remarkable success in applications like Machine Translation and Summarization. Given their widespread adoption, several works have attempted to analyze the expressivity of Transformers. Expressivity of a neural network is the class of functions it can approximate. A neural network is fully expressive if it can act as a universal function approximator. We attempt to analyze the same for Transformers. Contrary to existing claims, our findings reveal that Transformers struggle to reliably approximate continuous functions, relying on piecewise constant approximations with sizable intervals. The central question emerges as: "\textit{Are Transformers truly Universal Function Approximators}?" To address this, we conduct a thorough investigation, providing theoretical insights and supporting evidence through experiments. Our contributions include a theoretical analysi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.13192</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#34920;&#31034;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generative Design of Crystal Structures by Point Cloud Representations and Diffusion Model. (arXiv:2401.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#35774;&#35745;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#37325;&#24314;&#36755;&#20837;&#32467;&#26500;&#21644;&#29983;&#25104;&#20840;&#26032;&#26448;&#26009;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#33021;&#37327;&#31283;&#23450;&#30340;&#26230;&#20307;&#32467;&#26500;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26230;&#26684;&#20013;&#21407;&#23376;&#30340;&#24040;&#22823;&#25490;&#21015;&#12290;&#20026;&#20102;&#20419;&#36827;&#31283;&#23450;&#26448;&#26009;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#21487;&#21512;&#25104;&#26448;&#26009;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28857;&#20113;&#34920;&#31034;&#26469;&#32534;&#30721;&#22797;&#26434;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#25903;&#26609;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#36755;&#20837;&#32467;&#26500;&#65292;&#24182;&#20005;&#26684;&#39564;&#35777;&#20854;&#39640;&#37325;&#24314;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#20840;&#26032;&#30340;&#26448;&#26009;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#26230;&#20307;&#25193;&#25955;(PCCD)&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#21512;&#25104;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#26448;&#26009;&#35774;&#35745;&#21644;&#21512;&#25104;&#30340;&#25512;&#36827;&#20013;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#29983;&#25104;&#35774;&#35745;&#26041;&#27861;&#65292;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently generating energetically stable crystal structures has long been a challenge in material design, primarily due to the immense arrangement of atoms in a crystal lattice. To facilitate the discovery of stable material, we present a framework for the generation of synthesizable materials, leveraging a point cloud representation to encode intricate structural information. At the heart of this framework lies the introduction of a diffusion model as its foundational pillar. To gauge the efficacy of our approach, we employ it to reconstruct input structures from our training datasets, rigorously validating its high reconstruction performance. Furthermore, we demonstrate the profound potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely new materials, emphasizing their synthesizability. Our research stands as a noteworthy contribution to the advancement of materials design and synthesis through the cutting-edge avenue of generative design instead of the con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10726</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#29992;&#24037;&#20855;&#22686;&#24378;&#32858;&#21512;&#22120;&#33021;&#21147;&#65306;&#21033;&#29992;&#32858;&#21512;&#19982;&#20998;&#25955;&#30340;&#28789;&#27963;&#24615;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32858;&#21512;&#22120;&#21644;&#24314;&#31569;&#29289;&#23621;&#20303;&#32773;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#26041;&#26696;&#28608;&#27963;&#28789;&#27963;&#24615;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#30528;&#37325;&#20110;&#23454;&#29616;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#21644;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#24314;&#31569;&#29289;&#23621;&#27665;&#30340;&#27963;&#21160;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;DR&#20107;&#20214;&#26399;&#38388;&#20379;&#28909;&#36890;&#39118;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#31934;&#30830;&#30340;&#35774;&#22791;&#32423;&#20998;&#26512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20026;&#32858;&#21512;&#22120;&#22312;&#25972;&#20010;&#24314;&#31569;&#29289;&#28040;&#36153;&#20165;&#26377;&#19968;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#28789;&#27963;&#24615;&#26381;&#21153;&#25552;&#20379;&#20102;&#19968;&#26465;&#38750;&#20405;&#20837;&#24615;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05735</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Diffusion for Efficient Video Editing. (arXiv:2401.05735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#37197;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#32473;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#26469;&#23454;&#29616;&#35270;&#39057;&#32534;&#36753;&#30340;&#39640;&#25928;&#29575;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#35270;&#39057;&#32534;&#36753;&#24050;&#32463;&#36798;&#21040;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#32534;&#36753;&#25552;&#31034;&#26469;&#36716;&#25442;&#35270;&#39057;&#30340;&#20840;&#23616;&#39118;&#26684;&#12289;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26469;&#29983;&#25104;&#20855;&#26377;&#26102;&#24207;&#19968;&#33268;&#24615;&#30340;&#24103;&#65292;&#21487;&#33021;&#28041;&#21450;&#25193;&#25955;&#21453;&#28436;&#21644;/&#25110;&#36328;&#24103;&#27880;&#24847;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20302;&#25928;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#21516;&#26102;&#20445;&#25345;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#23545;&#35937;&#30340;&#25193;&#25955;&#25216;&#26415;&#65288;OCD&#65289;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#36164;&#28304;&#26356;&#22810;&#22320;&#20998;&#37197;&#32473;&#23545;&#24863;&#30693;&#36136;&#37327;&#26356;&#37325;&#35201;&#30340;&#21069;&#26223;&#32534;&#36753;&#21306;&#22495;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#24310;&#36831;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#25552;&#26696;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65306;i&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;&#37319;&#26679;&#65292;&#23558;&#29992;&#20110;&#26174;&#33879;&#21306;&#22495;&#25110;&#32972;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#19982;&#29992;&#20110;&#21069;&#26223;&#30340;&#25193;&#25955;&#27493;&#39588;&#20998;&#31163;&#24320;&#26469;&#65292;&#23558;&#22823;&#37096;&#20998;&#27169;&#22411;&#23481;&#37327;&#20998;&#37197;&#32473;&#21069;&#32773;&#65307;ii&#65289;&#38754;&#21521;&#23545;&#35937;&#30340;3D&#20196;&#29260;&#21512;&#24182;&#65292;&#29992;&#20110;&#25913;&#21892;&#21069;&#26223;&#21644;&#32972;&#26223;&#20043;&#38388;&#30340;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based video editing have reached impressive quality and can transform either the global style, local structure, and attributes of given video inputs, following textual edit prompts. However, such solutions typically incur heavy memory and computational costs to generate temporally-coherent frames, either in the form of diffusion inversion and/or cross-frame attention. In this paper, we conduct an analysis of such inefficiencies, and suggest simple yet effective modifications that allow significant speed-ups whilst maintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as OCD, to further reduce latency by allocating computations more towards foreground edited regions that are arguably more important for perceptual quality. We achieve this by two novel proposals: i) Object-Centric Sampling, decoupling the diffusion steps spent on salient regions or background, allocating most of the model capacity to the former, and ii) Object-Centric 3D Token Merging, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;ICP&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#21644;&#24341;&#20837;LoLICaP&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.05218</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Prediction with Locally Linear Models. (arXiv:2401.05218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;ICP&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#21644;&#24341;&#20837;LoLICaP&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#65292;&#20174;&#19968;&#32452;&#20505;&#36873;&#21464;&#37327;&#20013;&#35782;&#21035;&#20986;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#20505;&#36873;&#21464;&#37327;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#29615;&#22659;&#21487;&#20197;&#23545;&#24212;&#20110;&#26426;&#22120;&#30340;&#19981;&#21516;&#35774;&#32622;&#25110;&#32773;&#21160;&#24577;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#38388;&#38548;&#31561;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#30340;&#29615;&#22659;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#35266;&#23519;&#31995;&#32479;&#30340;&#24178;&#39044;&#12290;&#25105;&#20204;&#20551;&#35774;&#30446;&#26631;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;&#27599;&#20010;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#21516;&#65292;&#20294;&#22240;&#26524;&#32467;&#26500;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#26159;Peters&#31561;&#20154;[2016]&#25552;&#20986;&#30340;ICP&#65288;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#65289;&#21407;&#21017;&#30340;&#25193;&#23637;&#65292;&#21518;&#32773;&#20551;&#35774;&#25152;&#26377;&#29615;&#22659;&#19979;&#23384;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22240;&#26524;&#29238;&#33410;&#28857;&#21487;&#36776;&#35782;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LoLICaP&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data. Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process. Under certain assumptions different environments can be regarded as interventions on the observed system. We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. This is an extension of the ICP ($\textbf{I}$nvariant $\textbf{C}$ausal $\textbf{P}$rediction) principle by Peters et al. [2016], who assumed a fixed linear relationship across all environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12404</link><description>&lt;p&gt;
Loop Copilot: &#29992;&#20110;&#38899;&#20048;&#29983;&#25104;&#21644;&#36845;&#20195;&#32534;&#36753;&#30340;AI&#21512;&#22863;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12404
&lt;/p&gt;
&lt;p&gt;
Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#38899;&#20048;&#26159;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AI&#38899;&#20048;&#31995;&#32479;&#22312;&#32452;&#32455;&#22810;&#20010;&#23376;&#31995;&#32479;&#20197;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Loop Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#12289;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#30340;&#26032;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#27599;&#20010;&#21518;&#31471;&#27169;&#22411;&#37117;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#32858;&#21512;&#36215;&#26469;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#65292;&#20851;&#38190;&#23646;&#24615;&#34987;&#20445;&#30041;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#30340;&#35775;&#35848;&#21644;&#38382;&#21367;&#35843;&#26597;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#20419;&#36827;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29305;&#27530;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;&#22359;&#26469;&#23398;&#20064;&#32570;&#22833;&#30340;&#26102;&#31354;&#36830;&#25509;&#65292;&#23558;&#20854;&#20016;&#23500;&#21518;&#30340;&#22359;&#37051;&#25509;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;</title><link>http://arxiv.org/abs/2310.02606</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Learning adjacency matrix for dynamic graph neural network. (arXiv:2310.02606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#37051;&#25509;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29305;&#27530;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;&#22359;&#26469;&#23398;&#20064;&#32570;&#22833;&#30340;&#26102;&#31354;&#36830;&#25509;&#65292;&#23558;&#20854;&#20016;&#23500;&#21518;&#30340;&#22359;&#37051;&#25509;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;[1] &#24341;&#20837;&#20102;&#20351;&#29992;&#22359;&#37051;&#25509;&#30697;&#38453;&#65288;BA&#65289;&#26469;&#34920;&#31034;&#26102;&#31354;&#25968;&#25454;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#20182;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#20018;&#32852;&#20102;&#37051;&#25509;&#30697;&#38453;&#65292;&#20197;&#23553;&#35013;&#21333;&#20010;&#22270;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#20294;&#23427;&#24418;&#25104;&#20102;&#19968;&#20010;&#19981;&#36830;&#36890;&#30340;&#22270;&#12290;&#36825;&#20010;&#38480;&#21046;&#22952;&#30861;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#23646;&#20110;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27809;&#26377;&#26102;&#38388;&#38142;&#25509;&#23384;&#22312;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#36825;&#20123;&#32570;&#22833;&#30340;&#26102;&#38388;&#38142;&#25509;&#30340;&#32534;&#30721;&#22120;&#22359;&#12290;&#32534;&#30721;&#22120;&#22359;&#22788;&#29702;BA&#24182;&#39044;&#27979;&#20043;&#21069;&#26410;&#36830;&#25509;&#30340;&#23376;&#22270;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#23500;&#21270;&#30340;&#26102;&#31354;&#22359;&#37051;&#25509;&#30697;&#38453;&#65288;STBAM&#65289;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20010;&#23500;&#21270;&#30340;&#30697;&#38453;&#36755;&#20837;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#30340;&#22797;&#26434;&#26102;&#31354;&#25299;&#25169;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;surgVisDom&#21644;C2D2&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#31245;&#39640;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent work, [1] introduced the concept of using a Block Adjacency Matrix (BA) for the representation of spatio-temporal data. While their method successfully concatenated adjacency matrices to encapsulate spatio-temporal relationships in a single graph, it formed a disconnected graph. This limitation hampered the ability of Graph Convolutional Networks (GCNs) to perform message passing across nodes belonging to different time steps, as no temporal links were present. To overcome this challenge, we introduce an encoder block specifically designed to learn these missing temporal links. The encoder block processes the BA and predicts connections between previously unconnected subgraphs, resulting in a Spatio-Temporal Block Adjacency Matrix (STBAM). This enriched matrix is then fed into a Graph Neural Network (GNN) to capture the complex spatio-temporal topology of the network. Our evaluations on benchmark datasets, surgVisDom and C2D2, demonstrate that our method, with slightly higher
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>EEGMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#22522;&#20110;EEG-Mixup&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#22810;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06496</link><description>&lt;p&gt;
EEGMatch: &#23398;&#20064;&#19981;&#23436;&#25972;&#26631;&#35760;&#30340;&#21322;&#30417;&#30563;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EEGMatch: Learning with Incomplete Labels for Semi-Supervised EEG-based Cross-Subject Emotion Recognition. (arXiv:2304.06496v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06496
&lt;/p&gt;
&lt;p&gt;
EEGMatch&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#33041;&#30005;&#24773;&#32490;&#35782;&#21035;&#12290;&#36890;&#36807;&#22522;&#20110;EEG-Mixup&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#22810;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#24773;&#32490;&#35782;&#21035;&#30340;&#23458;&#35266;&#24037;&#20855;&#65292;&#24182;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#22522;&#20110;EEG&#30340;&#24773;&#32490;&#35782;&#21035;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65288;EEGMatch&#65289;&#65292;&#20197;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;EEG&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;EEG-Mixup&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#29992;&#20110;&#27169;&#22411;&#23398;&#20064;&#30340;&#26377;&#25928;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20004;&#27493;&#25104;&#23545;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21407;&#22411;&#24335;&#21644;&#23454;&#20363;&#21270;&#24335;&#25104;&#23545;&#23398;&#20064;&#36830;&#25509;&#36215;&#26469;&#65292;&#20854;&#20013;&#21407;&#22411;&#24335;&#25104;&#23545;&#23398;&#20064;&#34913;&#37327;EEG&#25968;&#25454;&#19982;&#27599;&#20010;&#24773;&#24863;&#31867;&#21035;&#30340;&#21407;&#22411;&#34920;&#31034;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#32780;&#23454;&#20363;&#21270;&#24335;&#25104;&#23545;&#23398;&#20064;&#25429;&#25417;EEG&#25968;&#25454;&#20043;&#38388;&#30340;&#23616;&#37096;&#20869;&#22312;&#20851;&#31995;&#12290;&#31532;&#19977;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#22810;&#22495;&#33258;&#36866;&#24212;&#65292;&#20197;&#23545;&#40784;&#22810;&#20010;&#22495;&#65288;&#26631;&#35760;&#30340;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65289;&#20043;&#38388;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EEGMatch&#22312;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) is an objective tool for emotion recognition and shows promising performance. However, the label scarcity problem is a main challenge in this field, which limits the wide application of EEG-based emotion recognition. In this paper, we propose a novel semi-supervised learning framework (EEGMatch) to leverage both labeled and unlabeled EEG data. First, an EEG-Mixup based data augmentation method is developed to generate more valid samples for model learning. Second, a semi-supervised two-step pairwise learning method is proposed to bridge prototype-wise and instance-wise pairwise learning, where the prototype-wise pairwise learning measures the global relationship between EEG data and the prototypical representation of each emotion class and the instance-wise pairwise learning captures the local intrinsic relationship among EEG data. Third, a semi-supervised multi-domain adaptation is introduced to align the data representation among multiple domains (labeled
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.15799</link><description>&lt;p&gt;
&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#32858;&#21512;&#26799;&#24230;&#30340;&#24555;&#36895;&#25910;&#25947;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22343;&#20540;&#22330;&#26041;&#27861;&#26469;&#23436;&#25104;&#21442;&#25968;&#21644;&#26799;&#24230;&#30340;&#32858;&#21512;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20351;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#22312;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#21516;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#30340;&#25968;&#25454;&#26679;&#26412;&#20197;&#21450;&#21442;&#19982;&#32773;&#20043;&#38388;&#39057;&#32321;&#30340;&#36890;&#20449;&#23558;&#20943;&#32531;&#25910;&#25947;&#36895;&#29575;&#24182;&#22686;&#21152;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#24120;&#35268;&#26412;&#22320;&#26356;&#26032;&#35268;&#21017;&#20013;&#24341;&#20837;&#32858;&#21512;&#26799;&#24230;&#26469;&#25913;&#21892;&#26412;&#22320;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36827;&#19968;&#27493;&#32771;&#34385;&#26412;&#22320;&#21442;&#25968;&#21644;&#20840;&#23616;&#21442;&#25968;&#30340;&#20559;&#24046;&#12290;&#20197;&#19978;&#31574;&#30053;&#35201;&#27714;&#22312;&#27599;&#20010;&#26412;&#22320;&#36845;&#20195;&#20013;&#25910;&#38598;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#21442;&#25968;&#21644;&#26799;&#24230;&#65292;&#30001;&#20110;&#26412;&#22320;&#26356;&#26032;&#26399;&#38388;&#27809;&#26377;&#36890;&#20449;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22343;&#20540;&#22330;&#26041;&#27861;&#65292;&#24341;&#20837;&#31216;&#20026;&#20840;&#23616;&#22343;&#20540;&#22330;&#21644;&#26412;&#22320;&#22343;&#20540;&#22330;&#30340;&#20004;&#20010;&#22343;&#20540;&#22330;&#26415;&#35821;&#26469;&#23436;&#25104;&#32858;&#21512;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
&lt;/p&gt;</description></item><item><title>LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.11428</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;LHCb&#36229;&#24555;&#36895;&#27169;&#25311;&#31995;&#32479;Lamarr&#22312;Gauss&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss. (arXiv:2303.11428v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11428
&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#20013;&#30340;90%&#35745;&#31639;&#36164;&#28304;&#29992;&#20110;&#29983;&#20135;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#65292;&#32780;Lamarr&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LHCb&#23454;&#39564;&#21487;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#32422;90%&#29992;&#20110;&#29983;&#20135;Large Hadron Collider&#65288;LHC&#65289;&#36816;&#34892;2&#30340;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#21319;&#32423;&#21518;&#30340;LHCb&#25506;&#27979;&#22120;&#23558;&#33021;&#22815;&#25910;&#38598;&#26356;&#22810;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#27169;&#25311;&#20107;&#20214;&#26469;&#20998;&#26512;&#23558;&#22312;&#36816;&#34892;3&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#27169;&#25311;&#26159;&#20998;&#26512;&#30340;&#20851;&#38190;&#38656;&#27714;&#65292;&#20197;&#35299;&#37322;&#20449;&#21495;&#19982;&#32972;&#26223;&#24182;&#27979;&#37327;&#25928;&#29575;&#12290;&#36825;&#31181;&#38656;&#35201;&#30340;&#27169;&#25311;&#23558;&#36828;&#36828;&#36229;&#20986;&#24050;&#25215;&#35834;&#30340;&#36164;&#28304;&#65292;&#38656;&#35201;&#25216;&#26415;&#21644;&#25216;&#24039;&#30340;&#28436;&#21464;&#26469;&#29983;&#20135;&#36825;&#20123;&#27169;&#25311;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Lamarr&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Gaudi&#26694;&#26550;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#36890;&#36807;&#23545;LHCb&#23454;&#39564;&#30340;&#25506;&#27979;&#22120;&#21709;&#24212;&#21644;&#37325;&#24314;&#31639;&#27861;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#21152;&#24555;&#20102;&#27169;&#25311;&#20135;&#20986;&#12290;&#20351;&#29992;&#22522;&#20110;&#22810;&#31181;&#31639;&#27861;&#21644;&#31574;&#30053;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#21442;&#25968;&#21270;&#20102;LHCb&#25506;&#27979;&#22120;&#21333;&#20010;&#32452;&#20214;&#30340;&#39640;&#32423;&#21709;&#24212;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
About 90% of the computing resources available to the LHCb experiment has been spent to produce simulated data samples for Run 2 of the Large Hadron Collider at CERN. The upgraded LHCb detector will be able to collect larger data samples, requiring many more simulated events to analyze the data to be collected in Run 3. Simulation is a key necessity of analysis to interpret signal vs background and measure efficiencies. The needed simulation will far exceed the pledged resources, requiring an evolution in technologies and techniques to produce these simulated data samples. In this contribution, we discuss Lamarr, a Gaudi-based framework to speed-up the simulation production parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Deep Generative Models powered by several algorithms and strategies are employed to effectively parametrize the high-level response of the single components of the LHCb detector, encoding within neural networks the exp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.11498</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#24211;&#29289;&#27969;&#20013;&#19982;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers. (arXiv:2212.11498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20179;&#24211;&#29289;&#27969;&#20013;&#30340;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#21516;&#20107;&#21512;&#20316;&#12290;&#20182;&#20204;&#36890;&#36807;&#20998;&#23618;&#30340;MARL&#31639;&#27861;&#65292;&#35753;&#32463;&#29702;&#21644;&#24037;&#20154;&#20195;&#29702;&#26681;&#25454;&#20840;&#23616;&#30446;&#26631;&#36827;&#34892;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#26368;&#22823;&#21270;&#25315;&#36135;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#24819;&#19968;&#20010;&#20179;&#24211;&#37324;&#26377;&#25968;&#21313;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20998;&#25315;&#21592;&#19968;&#36215;&#24037;&#20316;&#65292;&#25910;&#38598;&#21644;&#20132;&#20184;&#20179;&#24211;&#20869;&#30340;&#29289;&#21697;&#12290;&#25105;&#20204;&#35201;&#35299;&#20915;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#31216;&#20026;&#25315;&#36135;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#24037;&#20316;&#20195;&#29702;&#20154;&#22914;&#20309;&#22312;&#20179;&#24211;&#20013;&#21327;&#35843;&#20182;&#20204;&#30340;&#31227;&#21160;&#21644;&#34892;&#20026;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#65288;&#20363;&#22914;&#35746;&#21333;&#21534;&#21520;&#37327;&#65289;&#12290;&#20256;&#32479;&#30340;&#34892;&#19994;&#26041;&#27861;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#31243;&#21162;&#21147;&#26469;&#20026;&#22266;&#26377;&#21487;&#21464;&#30340;&#20179;&#24211;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#21487;&#20197;&#28789;&#27963;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20179;&#24211;&#37197;&#32622;&#65288;&#20363;&#22914;&#22823;&#23567;&#65292;&#24067;&#23616;&#65292;&#24037;&#20154;&#25968;&#37327;/&#31867;&#22411;&#65292;&#29289;&#21697;&#34917;&#20805;&#39057;&#29575;&#65289;&#65292;&#22240;&#20026;&#20195;&#29702;&#20154;&#36890;&#36807;&#32463;&#39564;&#23398;&#20064;&#22914;&#20309;&#26368;&#20248;&#22320;&#30456;&#20114;&#21512;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20998;&#23618;MARL&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#31649;&#29702;&#32773;&#20026;&#24037;&#20154;&#20195;&#29702;&#20998;&#37197;&#30446;&#26631;&#65292;&#24182;&#19988;&#31649;&#29702;&#32773;&#21644;&#24037;&#20154;&#30340;&#31574;&#30053;&#34987;&#20849;&#21516;&#35757;&#32451;&#20197;&#26368;&#22823;&#21270;&#20840;&#23616;&#30446;&#26631;&#65288;&#20363;&#22914;&#25315;&#36135;&#36895;&#29575;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We envision a warehouse in which dozens of mobile robots and human pickers work together to collect and deliver items within the warehouse. The fundamental problem we tackle, called the order-picking problem, is how these worker agents must coordinate their movement and actions in the warehouse to maximise performance (e.g. order throughput). Established industry methods using heuristic approaches require large engineering efforts to optimise for innately variable warehouse configurations. In contrast, multi-agent reinforcement learning (MARL) can be flexibly applied to diverse warehouse configurations (e.g. size, layout, number/types of workers, item replenishment frequency), as the agents learn through experience how to optimally cooperate with one another. We develop hierarchical MARL algorithms in which a manager assigns goals to worker agents, and the policies of the manager and workers are co-trained toward maximising a global objective (e.g. pick rate). Our hierarchical algorith
&lt;/p&gt;</description></item><item><title>MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2211.09944</link><description>&lt;p&gt;
MelHuBERT: &#19968;&#31181;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;HuBERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MelHuBERT: A simplified HuBERT on Mel spectrograms. (arXiv:2211.09944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09944
&lt;/p&gt;
&lt;p&gt;
MelHuBERT&#26159;&#22522;&#20110;Mel&#39057;&#35889;&#22270;&#30340;&#31616;&#21270;&#29256;HuBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#65292;&#22312;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#21033;&#34920;&#29616;&#65292;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#21644;33.5%&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#30417;&#30563;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#22810;&#20010;GPU&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20005;&#37325;&#38480;&#21046;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#20943;&#23569;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;HuBERT&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#21151;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#25913;&#36827;&#24182;&#31616;&#21270;&#20102;&#20960;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25439;&#22833;&#20989;&#25968;&#12289;&#36755;&#20837;&#34920;&#31034;&#21644;&#22810;&#38454;&#27573;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MelHuBERT&#22312;&#38899;&#32032;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#22343;&#33021;&#21462;&#24471;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;31.2%&#30340;&#39044;&#35757;&#32451;&#26102;&#38388;&#65292;&#25110;&#31561;&#25928;&#22320;&#27599;&#31186;&#35821;&#38899;&#33410;&#30465;&#20102;33.5%&#30340;MACs&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/nervjack2/MelHuBERT&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised models have had great success in learning speech representations that can generalize to various downstream tasks. However, most self-supervised models require a large amount of compute and multiple GPUs to train, significantly hampering the development of self-supervised learning. In an attempt to reduce the computation of training, we revisit the training of HuBERT, a highly successful self-supervised model. We improve and simplify several key components, including the loss function, input representation, and training in multiple stages. Our model, MelHuBERT, is able to achieve favorable performance on phone recognition, speaker identification, and automatic speech recognition against HuBERT, while saving 31.2% of the pre-training time, or equivalently 33.5% MACs per one second speech. The code and pre-trained models are available in https://github.com/nervjack2/MelHuBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.05442</link><description>&lt;p&gt;
Wasserstein&#22810;&#20803;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#21450;&#20854;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#21253;&#25324;&#19968;&#32452;&#22312;&#23454;&#32447;&#26377;&#30028;&#38388;&#38548;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#22810;&#20010;&#31995;&#21015;&#65292;&#24182;&#19988;&#34987;&#19981;&#21516;&#26102;&#38388;&#30636;&#38388;&#25152;&#32034;&#24341;&#12290;&#27010;&#29575;&#27979;&#24230;&#34987;&#24314;&#27169;&#20026;Wasserstein&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Lebesgue&#27979;&#24230;&#30340;&#20999;&#31354;&#38388;&#20013;&#24314;&#31435;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#39318;&#20808;&#23545;&#25152;&#26377;&#21407;&#22987;&#27979;&#24230;&#36827;&#34892;&#23621;&#20013;&#22788;&#29702;&#65292;&#20197;&#20415;&#23427;&#20204;&#30340;Fr&#233;chet&#24179;&#22343;&#20540;&#25104;&#20026;Lebesgue&#27979;&#24230;&#12290;&#21033;&#29992;&#36845;&#20195;&#38543;&#26426;&#20989;&#25968;&#31995;&#32479;&#30340;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#24179;&#31283;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#22411;&#31995;&#25968;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#38500;&#20102;&#23545;&#27169;&#25311;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27169;&#22411;&#28436;&#31034;&#65306;&#19968;&#20010;&#26159;&#19981;&#21516;&#22269;&#23478;&#24180;&#40836;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#24052;&#40654;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
&lt;/p&gt;</description></item></channel></rss>