<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2404.02785</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization through Meta-Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02785
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#23454;&#29616;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#24555;&#36895;&#36866;&#24212;&#65292;&#20026;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#26102;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#20294;&#26159;&#24403;&#38754;&#23545;&#20998;&#24067;&#20043;&#22806;(out-of-distribution, OOD)&#25968;&#25454;&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#19981;&#21487;&#36991;&#20813;&#65292;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#34987;&#20551;&#23450;&#20026;&#20849;&#20139;&#30456;&#21516;&#20998;&#24067;&#30340;&#24120;&#35265;&#24773;&#20917;&#12290;&#23613;&#31649;DNNs&#22312;&#22823;&#37327;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#24456;&#38590;&#24212;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#20803;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#33719;&#21462;&#21487;&#36716;&#31227;&#30693;&#35782;&#30340;&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#38656;&#35201;&#20174;&#22836;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#20803;&#23398;&#20064;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02785v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have revolutionized artificial intelligence but often lack performance when faced with out-of-distribution (OOD) data, a common scenario due to the inevitable domain shifts in real-world applications. This limitation stems from the common assumption that training and testing data share the same distribution-an assumption frequently violated in practice. Despite their effectiveness with large amounts of data and computational power, DNNs struggle with distributional shifts and limited labeled data, leading to overfitting and poor generalization across various tasks and domains. Meta-learning presents a promising approach by employing algorithms that acquire transferable knowledge across various tasks for fast adaptation, eliminating the need to learn each task from scratch. This survey paper delves into the realm of meta-learning with a focus on its contribution to domain generalization. We first clarify the 
&lt;/p&gt;</description></item><item><title>skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.18540</link><description>&lt;p&gt;
skscope&#65306;Python&#20013;&#30340;&#24555;&#36895;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
skscope: Fast Sparsity-Constrained Optimization in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18540
&lt;/p&gt;
&lt;p&gt;
skscope&#26159;&#19968;&#20010;Python&#24211;&#65292;&#36890;&#36807;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#65292;&#23601;&#33021;&#24555;&#36895;&#23454;&#29616;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#19979;&#65292;&#20854;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#27714;&#35299;&#22120;&#33021;&#22815;&#36805;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#36895;&#24230;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#24555;80&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#32422;&#26463;&#20248;&#21270;&#65288;SCO&#65289;&#19978;&#24212;&#29992;&#36845;&#20195;&#27714;&#35299;&#22120;&#38656;&#35201;&#32321;&#29712;&#30340;&#25968;&#23398;&#25512;&#23548;&#21644;&#20180;&#32454;&#30340;&#32534;&#31243;/&#35843;&#35797;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#27714;&#35299;&#22120;&#30340;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24211;skscope&#65292;&#20197;&#20811;&#26381;&#27492;&#38556;&#30861;&#12290;&#20511;&#21161;skscope&#65292;&#29992;&#25143;&#21482;&#38656;&#32534;&#20889;&#30446;&#26631;&#20989;&#25968;&#21363;&#21487;&#35299;&#20915;SCO&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20363;&#23376;&#28436;&#31034;&#20102;skscope&#30340;&#26041;&#20415;&#20043;&#22788;&#65292;&#20854;&#20013;&#21482;&#38656;&#22235;&#34892;&#20195;&#30721;&#23601;&#21487;&#20197;&#35299;&#20915;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#36235;&#21183;&#36807;&#28388;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;skscope&#30340;&#39640;&#25928;&#23454;&#29616;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#24555;&#36895;&#33719;&#24471;&#31232;&#30095;&#35299;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#21442;&#25968;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#65292;skscope&#20013;&#30340;&#21487;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#23454;&#29616;&#27604;&#22522;&#20934;&#20984;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#31454;&#20105;&#26494;&#24347;&#35299;&#39640;&#36798;80&#20493;&#30340;&#21152;&#36895;&#24230;&#12290;skscope&#24050;&#32463;&#21457;&#24067;&#22312;Python&#36719;&#20214;&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#21644;Conda&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18540v1 Announce Type: cross  Abstract: Applying iterative solvers on sparsity-constrained optimization (SCO) requires tedious mathematical deduction and careful programming/debugging that hinders these solvers' broad impact. In the paper, the library skscope is introduced to overcome such an obstacle. With skscope, users can solve the SCO by just programming the objective function. The convenience of skscope is demonstrated through two examples in the paper, where sparse linear regression and trend filtering are addressed with just four lines of code. More importantly, skscope's efficient implementation allows state-of-the-art solvers to quickly attain the sparse solution regardless of the high dimensionality of parameter space. Numerical experiments reveal the available solvers in skscope can achieve up to 80x speedup on the competing relaxation solutions obtained via the benchmarked convex solver. skscope is published on the Python Package Index (PyPI) and Conda, and its 
&lt;/p&gt;</description></item><item><title>Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.11901</link><description>&lt;p&gt;
Larimar: &#20855;&#26377;&#24773;&#33410;&#35760;&#24518;&#25511;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Larimar: Large Language Models with Episodic Memory Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11901
&lt;/p&gt;
&lt;p&gt;
Larimar&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#22686;&#24378;LLMs&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#30340;&#30693;&#35782;&#26356;&#26032;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#19988;&#22312;&#36895;&#24230;&#21644;&#28789;&#27963;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Larimar - &#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#21463;&#22823;&#33041;&#21551;&#21457;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20998;&#24067;&#24335;&#24773;&#33410;&#35760;&#24518;&#12290; Larimar&#30340;&#35760;&#24518;&#20801;&#35768;&#21160;&#24577;&#12289;&#19968;&#27425;&#24615;&#26356;&#26032;&#30693;&#35782;&#65292;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#20107;&#23454;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Larimar&#22312;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322; - &#26681;&#25454;&#22522;&#30784;LLM&#30340;&#19981;&#21516;&#65292;&#36895;&#24230;&#25552;&#21319;&#20026;4-10&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#25552;&#20986;&#30340;&#26550;&#26500;&#31616;&#21333;&#12289;&#19981;&#20381;&#36182;&#20110;LLM&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#20107;&#23454;&#36951;&#24536;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#38271;&#24230;&#27010;&#25324;&#26426;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11901v1 Announce Type: cross  Abstract: Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 4-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting and input context length generalization with Larimar and show their effectiveness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.11449</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Partial Label Learning with Potential Cause Discovering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11449
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#22312;&#35299;&#20915;&#21508;&#39046;&#22495;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#26631;&#27880;&#22270;&#25968;&#25454;&#20197;&#36827;&#34892;&#35757;&#32451;&#30001;&#20110;&#22270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#30456;&#20114;&#20851;&#32852;&#24615;&#32780;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;GNN&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#65288;PLL&#65289;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#12290; PLL&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#21253;&#25324;&#30495;&#23454;&#26631;&#31614;&#21644;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#22240;&#26524;&#25552;&#21462;&#26469;&#33719;&#21462;&#20855;&#26377;&#26356;&#39640;&#22240;&#26524;&#20851;&#31995;&#21487;&#33021;&#24615;&#30340;&#22270;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#21462;&#30340;&#22270;&#30340;&#36741;&#21161;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11449v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains. However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs. To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted gra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00225</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#25216;&#33021;&#25193;&#25955;&#23454;&#29616;&#31283;&#20581;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Policy Learning via Offline Skill Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00225
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#36890;&#36807;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#38271;&#26102;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#20998;&#23618;&#32467;&#26500;&#12290;&#36825;&#20123;&#25216;&#33021;&#26159;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26080;&#20851;&#20219;&#21153;&#22320;&#23398;&#20064;&#30340;&#65292;&#21487;&#20197;&#21152;&#24555;&#38024;&#23545;&#26032;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25216;&#33021;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#38480;&#20110;&#23545;&#25968;&#25454;&#38598;&#30340;&#22266;&#26377;&#20381;&#36182;&#65292;&#24403;&#23581;&#35797;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#21516;&#20110;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30446;&#26631;&#39046;&#22495;&#23398;&#20064;&#22522;&#20110;&#25216;&#33021;&#30340;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#25361;&#25112;&#23601;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31163;&#32447;&#25216;&#33021;&#23398;&#20064;&#26694;&#26550;DuSkill&#65292;&#23427;&#37319;&#29992;&#20102;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#20174;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#25216;&#33021;&#25193;&#23637;&#20986;&#30340;&#36890;&#29992;&#25216;&#33021;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#19981;&#21516;&#39046;&#22495;&#20219;&#21153;&#30340;&#31574;&#30053;&#23398;&#20064;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24341;&#23548;&#25193;&#25955;&#25216;&#33021;&#35299;&#30721;&#22120;&#65292;&#32467;&#21512;&#20998;&#23618;&#32534;&#30721;&#65292;&#20197;&#35299;&#24320;&#25216;&#33021;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00225v1 Announce Type: new  Abstract: Skill-based reinforcement learning (RL) approaches have shown considerable promise, especially in solving long-horizon tasks via hierarchical structures. These skills, learned task-agnostically from offline datasets, can accelerate the policy learning process for new tasks. Yet, the application of these skills in different domains remains restricted due to their inherent dependency on the datasets, which poses a challenge when attempting to learn a skill-based policy via RL for a target domain different from the datasets' domains. In this paper, we present a novel offline skill learning framework DuSkill which employs a guided Diffusion model to generate versatile skills extended from the limited skills in datasets, thereby enhancing the robustness of policy learning for tasks in different domains. Specifically, we devise a guided diffusion-based skill decoder in conjunction with the hierarchical encoding to disentangle the skill embeddi
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07862</link><description>&lt;p&gt;
AI&#22686;&#24378;&#39044;&#27979;&#65306;LLM&#21161;&#25163;&#25552;&#39640;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#19982;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#22686;&#24378;&#21028;&#26029;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;GPT-4-Turbo&#21161;&#25163;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#24314;&#35758;&#65288;&#36229;&#32423;&#39044;&#27979;&#65289;&#65292;&#21478;&#19968;&#20010;&#26088;&#22312;&#36807;&#20110;&#33258;&#20449;&#21644;&#22522;&#26412;&#27010;&#29575;&#24573;&#35270;&#12290;&#21442;&#19982;&#32773;&#65288;N = 991&#65289;&#21487;&#20197;&#22312;&#25972;&#20010;&#30740;&#31350;&#36807;&#31243;&#20013;&#21672;&#35810;&#20182;&#20204;&#34987;&#20998;&#37197;&#30340;LLM&#21161;&#25163;&#65292;&#32780;&#23545;&#29031;&#32452;&#21017;&#20351;&#29992;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;DaVinci-003&#65289;&#65292;&#19981;&#25552;&#20379;&#30452;&#25509;&#30340;&#39044;&#27979;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#27880;&#20876;&#20998;&#26512;&#26174;&#31034;&#65292;LLM&#22686;&#24378;&#26174;&#33879;&#25552;&#39640;&#20102;23%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#20219;&#20309;&#19968;&#31181;&#21161;&#25163;&#31867;&#22411;&#65292;&#30456;&#27604;&#20110;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#25913;&#36827;&#21457;&#29983;&#22312;&#36229;&#32423;&#39044;&#27979;&#21161;&#25163;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#22686;&#24378;&#30340;&#25928;&#30410;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2311.09386</link><description>&lt;p&gt;
&#36229;&#36234;PCA&#65306;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#30340;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#22312;&#25968;&#25454;&#20013;&#23384;&#22312;&#38750;&#32447;&#24615;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#29575;&#24615;Gram-Schmidt (GS)&#31867;&#22411;&#30340;&#27491;&#20132;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#21644;&#26144;&#23556;&#20986;&#20887;&#20313;&#32500;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#19968;&#26063;&#20989;&#25968;&#19978;&#24212;&#29992;GS&#36807;&#31243;&#65292;&#35813;&#26063;&#20989;&#25968;&#39044;&#35745;&#25429;&#25417;&#21040;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#22823;&#26041;&#24046;&#26041;&#21521;&#65292;&#25110;&#32773;&#23558;&#36825;&#20123;&#20381;&#36182;&#24615;&#20174;&#20027;&#25104;&#20998;&#20013;&#21435;&#38500;&#12290;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29109;&#20943;&#23569;&#30340;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#22312;&#25152;&#36873;&#25321;&#20989;&#25968;&#26063;&#30340;&#32447;&#24615;&#24352;&#25104;&#31354;&#38388;&#20013;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#12290;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a probabilistic Gram-Schmidt (GS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the GS process over a family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to identify new large-variance directions, or to remove those dependencies from the principal components. In the former case, we provide information-theoretic guarantees in terms of entropy reduction. In the latter, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.07511</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#21355;&#26143;&#38477;&#27700;&#25554;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in satellite precipitation interpolation with machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#38477;&#27700;&#25968;&#25454;&#38598;&#30340;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#21355;&#26143;&#21644;&#27979;&#31449;&#25968;&#25454;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#38477;&#27700;&#25968;&#25454;&#38598;&#65292;&#20294;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24448;&#24448;&#32570;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20845;&#31181;&#31639;&#27861;&#65292;&#22823;&#37096;&#20998;&#26159;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#31639;&#27861;&#65292;&#26469;&#37327;&#21270;&#31354;&#38388;&#25554;&#20540;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36830;&#32493;&#32654;&#22269;&#30340;15&#24180;&#26376;&#24230;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20998;&#20301;&#25968;&#22238;&#24402;&#65288;QR&#65289;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65288;QRF&#65289;&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#65288;GRF&#65289;&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBM&#65289;&#12289;&#36731;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;LightGBM&#65289;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#12290;&#23427;&#20204;&#33021;&#22815;&#22312;&#20061;&#20010;&#20998;&#20301;&#27700;&#24179;&#65288;0.025&#12289;0.050&#12289;0.100&#12289;0.250&#12289;0.500&#12289;0.750&#12289;0.900&#12289;0.950&#12289;0.975&#65289;&#19978;&#21457;&#24067;&#39044;&#27979;&#38477;&#27700;&#20998;&#20301;&#25968;&#65292;&#20197;&#36817;&#20284;&#23436;&#25972;&#27010;&#29575;&#20998;&#24067;&#65292;&#35780;&#20272;&#26102;&#37319;&#29992;&#20998;&#20301;&#25968;&#35780;&#20998;&#20989;&#25968;&#21644;&#20998;&#20301;&#25968;&#35780;&#20998;&#35268;&#21017;&#12290;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#21355;&#26143;&#38477;&#27700;&#65288;PERSIA
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07511v2 Announce Type: replace-cross  Abstract: Merging satellite and gauge data with machine learning produces high-resolution precipitation datasets, but uncertainty estimates are often missing. We address this gap by benchmarking six algorithms, mostly novel for this task, for quantifying predictive uncertainty in spatial interpolation. On 15 years of monthly data over the contiguous United States (CONUS), we compared quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neural networks (QRNN). Their ability to issue predictive precipitation quantiles at nine quantile levels (0.025, 0.050, 0.100, 0.250, 0.500, 0.750, 0.900, 0.950, 0.975), approximating the full probability distribution, was evaluated using quantile scoring functions and the quantile scoring rule. Feature importance analysis revealed satellite precipitation (PERSIA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2302.09193</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#30340;&#21487;&#36716;&#31227;&#27169;&#22411;&#29992;&#20110;&#21512;&#25104;&#20154;&#21475;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Copula-based transferable models for synthetic population generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.09193
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#20154;&#21475;&#26679;&#26412;&#20197;&#21450;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#65292;&#24341;&#20837;&#31354;&#38388;&#32452;&#20214;&#24182;&#32771;&#34385;&#22810;&#31181;&#20449;&#24687;&#28304;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#30446;&#26631;&#20154;&#21475;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32508;&#21512;&#28041;&#21450;&#29983;&#25104;&#24494;&#35266;&#20195;&#29702;&#30446;&#26631;&#20154;&#21475;&#30340;&#21512;&#25104;&#20294;&#29616;&#23454;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#34892;&#20026;&#24314;&#27169;&#21644;&#27169;&#25311;&#12290; &#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#30446;&#26631;&#20154;&#21475;&#26679;&#26412;&#65292;&#22914;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#25110;&#26053;&#34892;&#35843;&#26597;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#21644;&#36739;&#23567;&#30340;&#26679;&#26412;&#37327;&#65292;&#22312;&#36739;&#23567;&#30340;&#22320;&#29702;&#23610;&#24230;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20165;&#24050;&#30693;&#32463;&#39564;&#36793;&#38469;&#20998;&#24067;&#30340;&#30446;&#26631;&#20154;&#21475;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290; &#35813;&#26041;&#27861;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#30456;&#20284;&#36793;&#38469;&#20381;&#36182;&#24615;&#30340;&#19981;&#21516;&#20154;&#21475;&#30340;&#26679;&#26412;&#65292;&#23558;&#31354;&#38388;&#32452;&#20214;&#24341;&#20837;&#21040;&#20154;&#21475;&#32508;&#21512;&#20013;&#65292;&#24182;&#32771;&#34385;&#21508;&#31181;&#20449;&#24687;&#28304;&#29992;&#20110;&#26356;&#30495;&#23454;&#30340;&#29983;&#25104;&#22120;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#36807;&#31243;&#28041;&#21450;&#23558;&#25968;&#25454;&#26631;&#20934;&#21270;&#24182;&#23558;&#20854;&#35270;&#20026;&#32473;&#23450;Copula&#30340;&#23454;&#29616;&#65292;&#28982;&#21518;&#22312;&#34701;&#20837;&#20851;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.09193v2 Announce Type: replace-cross  Abstract: Population synthesis involves generating synthetic yet realistic representations of a target population of micro-agents for behavioral modeling and simulation. Traditional methods, often reliant on target population samples, such as census data or travel surveys, face limitations due to high costs and small sample sizes, particularly at smaller geographical scales. We propose a novel framework based on copulas to generate synthetic data for target populations where only empirical marginal distributions are known. This method utilizes samples from different populations with similar marginal dependencies, introduces a spatial component into population synthesis, and considers various information sources for more realistic generators. Concretely, the process involves normalizing the data and treat it as realizations of a given copula, and then training a generative model before incorporating the information on the marginals of the
&lt;/p&gt;</description></item><item><title>MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.15960</link><description>&lt;p&gt;
MoTCoder: &#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks. (arXiv:2312.15960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15960
&lt;/p&gt;
&lt;p&gt;
MoTCoder&#26159;&#19968;&#20010;&#20351;&#29992;&#24605;&#32500;&#27169;&#22359;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#20013;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25351;&#20196;&#35843;&#25972;&#20419;&#36827;&#20219;&#21153;&#30340;&#20998;&#35299;&#21644;&#27169;&#22359;&#21270;&#65292;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22359;&#21270;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#31616;&#21333;&#30340;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32534;&#31243;&#38382;&#39064;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#20316;&#20026;&#21333;&#19968;&#20195;&#30721;&#22359;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Modular-of-Thought Coder (MoTCoder)&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;MoT&#25351;&#20196;&#35843;&#25972;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#36923;&#36753;&#23376;&#20219;&#21153;&#21644;&#23376;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22521;&#20859;&#21644;&#21033;&#29992;&#23376;&#27169;&#22359;&#65292;MoTCoder&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27169;&#22359;&#21270;&#21644;&#27491;&#30830;&#24615;&#65292;&#23548;&#33268;&#22312;APPS&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;12.9%&#65292;&#22312;CodeContests&#19978;&#30456;&#23545;pass@1&#25913;&#36827;&#20102;9.43%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/dvlab-research/MoTCoder&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive capabilities in handling straightforward programming tasks. However, their performance tends to falter when confronted with more challenging programming problems. We observe that conventional models often generate solutions as monolithic code blocks, restricting their effectiveness in tackling intricate questions. To overcome this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a pioneering framework for MoT instruction tuning, designed to promote the decomposition of tasks into logical sub-tasks and sub-modules. Our investigations reveal that, through the cultivation and utilization of sub-modules, MoTCoder significantly improves both the modularity and correctness of the generated solutions, leading to substantial relative pass@1 improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are available at https://github.com/dvlab-research/MoTCoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;(CANN)&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12671</link><description>&lt;p&gt;
&#21033;&#29992;&#39057;&#29575;&#21644;&#20005;&#37325;&#24615;&#25968;&#25454;&#36827;&#34892;&#20445;&#38505;&#23450;&#20215;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#20174;&#25968;&#25454;&#39044;&#22788;&#29702;&#21040;&#25216;&#26415;&#23450;&#20215;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff. (arXiv:2310.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;(CANN)&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38505;&#20844;&#21496;&#36890;&#24120;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26469;&#24314;&#27169;&#32034;&#36180;&#30340;&#39057;&#29575;&#21644;&#20005;&#37325;&#24615;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31934;&#31639;&#24037;&#20855;&#31665;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20026;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20445;&#38505;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#26377;&#22810;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#39057;&#29575;-&#20005;&#37325;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22312;&#20998;&#31665;&#36755;&#20837;&#25968;&#25454;&#12289;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNN&#65289;&#21644;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;CANN&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;CANN&#23558;&#36890;&#36807;GLM&#21644;GBM&#20998;&#21035;&#24314;&#31435;&#30340;&#22522;&#32447;&#39044;&#27979;&#19982;&#31070;&#32463;&#32593;&#32476;&#26657;&#27491;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#24120;&#23384;&#22312;&#20110;&#34920;&#26684;&#20445;&#38505;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#27604;&#22914;&#37038;&#32534;&#21644;&#25968;&#23383;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insurers usually turn to generalized linear models for modelling claim frequency and severity data. Due to their success in other fields, machine learning techniques are gaining popularity within the actuarial toolbox. Our paper contributes to the literature on frequency-severity insurance pricing with machine learning via deep learning structures. We present a benchmark study on four insurance data sets with frequency and severity targets in the presence of multiple types of input features. We compare in detail the performance of: a generalized linear model on binned input data, a gradient-boosted tree model, a feed-forward neural network (FFNN), and the combined actuarial neural network (CANN). Our CANNs combine a baseline prediction established with a GLM and GBM, respectively, with a neural network correction. We explain the data preprocessing steps with specific focus on the multiple types of input features typically present in tabular insurance data sets, such as postal codes, nu
&lt;/p&gt;</description></item><item><title>FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00109</link><description>&lt;p&gt;
FedAIoT: &#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things. (arXiv:2310.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00109
&lt;/p&gt;
&lt;p&gt;
FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#24182;&#19981;&#26159;&#22522;&#20110;&#20174;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#29420;&#29305;&#27169;&#24335;&#21644;&#22266;&#26377;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedAIoT&#65292;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;FL&#22522;&#20934;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#24046;&#36317;&#12290;FedAIoT&#21253;&#25324;&#20174;&#21508;&#31181;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#29289;&#32852;&#32593;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#24182;&#38024;&#23545;AIoT&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;FedAIoT&#36824;&#21253;&#25324;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;FedAIoT&#33021;&#25104;&#20026;&#22312;FL for AIoT&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#29645;&#36149;&#36164;&#28304;&#12290;FedAIoT&#30340;&#20195;&#30721;&#20179;&#24211;&#20301;&#20110;https://github.com/AIoT-MLSys-Lab/FedAIoT&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2309.13080</link><description>&lt;p&gt;
SPICED: &#20855;&#26377;&#22810;&#20010;&#20027;&#39064;&#21644;&#22797;&#26434;&#31243;&#24230;&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13080
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SPICED&#30340;&#26032;&#38395;&#30456;&#20284;&#24615;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20351;&#29992;&#26234;&#33021;&#31995;&#32479;&#26469;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#26222;&#36941;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;&#20307;&#39564;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#26032;&#38395;&#23186;&#20307;&#30340;&#34028;&#21187;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26032;&#38395;&#30340;&#24322;&#36136;&#24615;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#34394;&#20551;&#21457;&#29616;&#65306;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#27604;&#22914;&#19968;&#23545;&#26032;&#38395;&#26159;&#21542;&#37117;&#28041;&#21450;&#25919;&#27835;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#20294;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#23558;&#26032;&#38395;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#20998;&#21106;&#25104;&#20027;&#39064;&#21487;&#20197;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#22312;&#26356;&#29421;&#31364;&#30340;&#39046;&#22495;&#20013;&#21306;&#20998;&#26174;&#33879;&#29305;&#24449;&#26469;&#25913;&#36827;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23384;&#22312;&#30446;&#21069;&#32570;&#20047;&#30340;&#19987;&#39064;&#29305;&#23450;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30456;&#20284;&#26032;&#38395;&#25968;&#25454;&#38598;SPICED&#65292;&#20854;&#20013;&#21253;&#25324;&#19971;&#20010;&#20027;&#39064;&#65306;&#29359;&#32618;&#19982;&#27861;&#24459;&#12289;&#25991;&#21270;&#19982;&#23089;&#20048;&#12289;&#28798;&#38590;&#19982;&#20107;&#25925;&#12289;&#32463;&#27982;&#19982;&#21830;&#19994;&#12289;&#25919;&#27835;&#19982;&#20914;&#31361;&#12289;&#31185;&#23398;&#19982;&#25216;&#26415;&#20197;&#21450;&#20307;&#32946;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime &amp; Law, Culture &amp; Entertainment, Disasters &amp; Accidents, Economy &amp; Business, Politics &amp; Conflicts, Science &amp; Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08731</link><description>&lt;p&gt;
&#25351;&#24341;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#25913;&#36827;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#12290;&#34429;&#28982;&#30446;&#21069;&#23450;&#20301;&#30340;&#25216;&#26415;&#27700;&#24179;&#26159;&#23558;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#26159;&#38647;&#36798;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23545;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#20855;&#26377;&#26356;&#24378;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#21516;&#26102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#20445;&#25345;&#24615;&#33021;&#65292;&#23558;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38647;&#36798;&#27979;&#37327;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#20266;&#24433;&#65292;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#19968;&#30452;&#38590;&#20197;&#36798;&#21040;&#19982;&#28608;&#20809;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#24037;&#20316;&#22312;&#22522;&#20110;ICP&#30340;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#31995;&#32479;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26681;&#25454;&#39640;&#23618;&#27425;&#30340;&#25195;&#25551;&#20449;&#24687;&#23545;&#38647;&#36798;&#28857;&#36827;&#34892;&#21152;&#26435;&#12290;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20998;&#26512;&#26041;&#27861;&#19982;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20943;&#23567;&#20102;&#38647;&#36798;&#23450;&#20301;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08560</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#19988;&#20844;&#24179;&#20998;&#37197;&#21307;&#30103;&#36164;&#28304;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Efficient and Fair Allocation of Health Care Resources. (arXiv:2309.08560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#20010;&#20307;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#65292;&#26469;&#20248;&#21270;&#21307;&#30103;&#36164;&#28304;&#30340;&#20998;&#37197;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36991;&#20813;&#30340;&#37197;&#32473;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#36890;&#27668;&#26426;&#30340;&#20379;&#24212;&#36890;&#24120;&#26377;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#32039;&#24613;&#24773;&#20917;&#25110;&#36164;&#28304;&#26377;&#38480;&#30340;&#21307;&#30103;&#29615;&#22659;&#20013;&#65292;&#22914;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#21307;&#30103;&#36164;&#28304;&#20998;&#37197;&#30340;&#21327;&#35758;&#24182;&#27809;&#26377;&#26222;&#36941;&#25509;&#21463;&#30340;&#26631;&#20934;&#65292;&#23548;&#33268;&#21508;&#22269;&#25919;&#24220;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#22522;&#20110;&#21551;&#21457;&#24335;&#21327;&#35758;&#26469;&#20248;&#20808;&#32771;&#34385;&#24739;&#32773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#20197;&#20844;&#24179;&#26377;&#25928;&#22320;&#37197;&#32473;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#20010;&#20307;&#24739;&#32773;&#30340;&#30149;&#24773;&#36827;&#23637;&#21644;&#24739;&#32773;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25928;&#24212;&#25972;&#21512;&#21040;&#37325;&#30151;&#25252;&#29702;&#36164;&#28304;&#20998;&#37197;&#20013;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#20998;&#37197;&#30340;&#20844;&#24179;&#24615;&#21644;&#25972;&#20307;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#36807;&#24230;&#37197;&#32473;&#36164;&#28304;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of health care resources could result in the unavoidable consequence of rationing. For example, ventilators are often limited in supply, especially during public health emergencies or in resource-constrained health care settings, such as amid the pandemic of COVID-19. Currently, there is no universally accepted standard for health care resource allocation protocols, resulting in different governments prioritizing patients based on various criteria and heuristic-based protocols. In this study, we investigate the use of reinforcement learning for critical care resource allocation policy optimization to fairly and effectively ration resources. We propose a transformer-based deep Q-network to integrate the disease progression of individual patients and the interaction effects among patients during the critical care resource allocation. We aim to improve both fairness of allocation and overall patient outcomes. Our experiments demonstrate that our method significantly reduces exces
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20184;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#30340;&#30452;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#26102;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19979;&#30028;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.13100</link><description>&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#65306;&#23545;&#20462;&#27491;&#30340;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Label Noise: Correcting a Correction. (arXiv:2307.13100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20184;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#30340;&#30452;&#25509;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#26102;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#65292;&#25552;&#20986;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19979;&#30028;&#20197;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26126;&#30830;&#19988;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#26631;&#31614;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#26356;&#40065;&#26834;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#26367;&#20195;&#26041;&#26696;&#37117;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#24182;&#19988;&#20173;&#28982;&#23481;&#26131;&#36807;&#25311;&#21512;&#25110;&#27424;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#30452;&#25509;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30001;&#26631;&#31614;&#22122;&#22768;&#24341;&#36215;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#26631;&#31614;&#22122;&#22768;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#22122;&#22768;&#24191;&#20041;&#39118;&#38505;&#30340;&#19979;&#30028;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#32463;&#39564;&#39118;&#38505;&#26045;&#21152;&#19968;&#20010;&#19979;&#30028;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#20026;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#26368;&#23567;&#21487;&#23454;&#29616;&#22122;&#22768;&#39118;&#38505;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20351;&#29992;&#36825;&#20123;&#30028;&#38480;&#26174;&#33879;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#65292;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01497</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Accelerated stochastic approximation with state-dependent noise. (arXiv:2307.01497v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20855;&#26377;&#29366;&#24577;&#30456;&#20851;&#22122;&#22768;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#26041;&#38754;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#19968;&#33324;&#22122;&#22768;&#20551;&#35774;&#30340;&#38543;&#26426;&#24179;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31867;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#35266;&#27979;&#30340;&#22122;&#22768;&#30340;&#26041;&#24046;&#19982;&#31639;&#27861;&#20135;&#29983;&#30340;&#36817;&#20284;&#35299;&#30340;"&#20122;&#26368;&#20248;&#24615;" &#30456;&#20851;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#32479;&#35745;&#23398;&#20013;&#30340;&#24191;&#20041;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#22312;&#31934;&#24230;&#12289;&#38382;&#39064;&#21442;&#25968;&#21644;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#37117;&#26410;&#36798;&#21040;&#26368;&#20248;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#38750;&#27431;&#20960;&#37324;&#24471;&#21152;&#36895;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#8212;&#8212;&#38543;&#26426;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#65288;SAGD&#65289;&#21644;&#38543;&#26426;&#26799;&#24230;&#22806;&#25512;&#65288;SGE&#65289;&#8212;&#8212;&#23427;&#20204;&#20855;&#26377;&#19968;&#31181;&#29305;&#27530;&#30340;&#23545;&#20598;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
We consider a class of stochastic smooth convex optimization problems under rather general assumptions on the noise in the stochastic gradient observation. As opposed to the classical problem setting in which the variance of noise is assumed to be uniformly bounded, herein we assume that the variance of stochastic gradients is related to the "sub-optimality" of the approximate solutions delivered by the algorithm. Such problems naturally arise in a variety of applications, in particular, in the well-known generalized linear regression problem in statistics. However, to the best of our knowledge, none of the existing stochastic approximation algorithms for solving this class of problems attain optimality in terms of the dependence on accuracy, problem parameters, and mini-batch size.  We discuss two non-Euclidean accelerated stochastic approximation routines--stochastic accelerated gradient descent (SAGD) and stochastic gradient extrapolation (SGE)--which carry a particular duality rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06329</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65306;&#21151;&#33021;&#21644;&#34920;&#31034;&#24615;&#27979;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Network Models: A Survey of Functional and Representational Measures. (arXiv:2305.06329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#19988;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20197;&#20102;&#35299;&#21644;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#35266;&#28857;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#26159;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20004;&#20010;&#20114;&#34917;&#30340;&#35266;&#28857;&#65292;&#21363;(i) &#34920;&#31034;&#24615;&#30456;&#20284;&#65292;&#32771;&#34385;&#20013;&#38388;&#31070;&#32463;&#23618;&#30340;&#28608;&#27963;&#24046;&#24322;&#65292;&#21644;(ii) &#21151;&#33021;&#30456;&#20284;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24615;&#27979;&#37327;&#30340;&#23478;&#26063;&#12290;&#38500;&#20102;&#25552;&#20379;&#29616;&#26377;&#27979;&#37327;&#30340;&#35814;&#32454;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#21487;&#20197;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#20026;&#25105;&#20204;&#30340;&#31038;&#21306;&#21442;&#19982;&#26356;&#22810;&#26377;&#29992;&#30340;&#24037;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity of neural networks has become an issue of great importance and research interest to understand and utilize differences of neural networks. While there are several perspectives on how neural networks can be similar, we specifically focus on two complementing perspectives, i.e., (i) representational similarity, which considers how activations of intermediate neural layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In this survey, we provide a comprehensive overview of these two families of similarity measures for neural network models. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties and relationships of these measures, and point to open research problems. Further, we provide practical recommendations that can guide researchers as well as practitioners in applying the measures. We hope our work lays a foundation for our community to engage in more s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#25928;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20026;&#22797;&#26434;&#38598;&#32676;&#20998;&#24067;&#36866;&#37197;&#39640;&#26031;&#23376;&#38598;&#32676;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#26377;&#30340;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#25928;&#26524;&#20063;&#26356;&#22909;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2304.13886</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#22788;&#29702;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving the Utility of Differentially Private Clustering through Dynamical Processing. (arXiv:2304.13886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#25552;&#39640;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#25928;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20026;&#22797;&#26434;&#38598;&#32676;&#20998;&#24067;&#36866;&#37197;&#39640;&#26031;&#23376;&#38598;&#32676;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#26377;&#30340;&#31616;&#21333;&#32858;&#31867;&#26041;&#27861;&#65292;&#20854;&#25928;&#26524;&#20063;&#26356;&#22909;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#24046;&#20998;&#31169;&#26377;&#32858;&#31867;&#20219;&#21153;&#20013;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#31616;&#21333;&#30340;&#38598;&#32676;&#26041;&#27861;&#65292;&#23545;&#20110;&#38750;&#20984;&#38598;&#32676;&#30340;&#32858;&#31867;&#25928;&#26524;&#36739;&#24046;&#12290;&#36890;&#36807;&#21033;&#29992; Morse &#29702;&#35770;&#65292;&#25105;&#20204;&#23558;&#39640;&#26031;&#23376;&#38598;&#32676;&#25353;&#23618;&#27425;&#36830;&#25509;&#20197;&#36866;&#24212;&#26356;&#22797;&#26434;&#30340;&#38598;&#32676;&#20998;&#24067;&#12290;&#30001;&#20110;&#24046;&#20998;&#31169;&#26377;&#23376;&#32676;&#26159;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#33719;&#24471;&#30340;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#22686;&#21152;&#38544;&#31169;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32972;&#26223;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#24402;&#32435;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#25968;&#37327;&#30340;&#32676;&#38598;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#30456;&#21516;&#30340;&#38544;&#31169;&#27700;&#24179;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to alleviate the trade-off between utility and privacy in the task of differentially private clustering. Existing works focus on simple clustering methods, which show poor clustering performance for non-convex clusters. By utilizing Morse theory, we hierarchically connect the Gaussian sub-clusters to fit complex cluster distributions. Because differentially private sub-clusters are obtained through the existing methods, the proposed method causes little or no additional privacy loss. We provide a theoretical background that implies that the proposed method is inductive and can achieve any desired number of clusters. Experiments on various datasets show that our framework achieves better clustering performance at the same privacy level, compared to the existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12767</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;ChatGPT&#30340;&#35780;&#20272;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#31532;&#19968;&#20010;&#34987;&#24191;&#27867;&#37319;&#32435;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#38381;&#21512;&#24615;&#20197;&#21450;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#26356;&#26032;&#65292;&#35780;&#20272;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;ChatGPT&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#21644;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#21644;&#30830;&#20445;&#20844;&#24179;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.02899</link><description>&lt;p&gt;
&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Clustering of Wireless Spectrum Activity. (arXiv:2210.02899v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#26080;&#32447;&#30005;&#39057;&#35889;&#25968;&#25454;&#20197;&#35299;&#20915;&#35748;&#30693;&#26080;&#32447;&#30005;&#32593;&#32476;&#30456;&#20851;&#38382;&#39064;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#12289;&#35843;&#21046;&#20998;&#31867;&#12289;&#25216;&#26415;&#20998;&#31867;&#21644;&#35774;&#22791;&#25351;&#32441;&#31561;&#39046;&#22495;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#37117;&#26159;&#22522;&#20110;&#21463;&#25511;&#21046;&#30340;&#12289;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#27979;&#37327;&#30340;&#39057;&#35889;&#25968;&#25454;&#39640;&#24230;&#19981;&#30830;&#23450;&#65292;&#20854;&#26631;&#35760;&#26159;&#19968;&#39033;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#22240;&#27492;&#25104;&#20026;&#22312;&#35813;&#39046;&#22495;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21033;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#25506;&#32034;&#39057;&#35889;&#27963;&#21160;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181; SSL &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#19968;&#31181;&#22522;&#20110; DeepCluster &#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#65292;&#21478;&#19968;&#31181;&#36866;&#29992;&#20110;&#39057;&#35889;&#27963;&#21160;&#35782;&#21035;&#21644;&#32858;&#31867;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512; SSL &#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#19977;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#39057;&#35889;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#28151;&#21512;&#26041;&#27861;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks, such as anomaly detection, modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the use of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data. In particular, we compare the performance of two SSL models, one based on a reference DeepCluster architecture and one adapted for spectrum activity identification and clustering, and a 
&lt;/p&gt;</description></item></channel></rss>