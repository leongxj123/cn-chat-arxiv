<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01607</link><description>&lt;p&gt;
&#20855;&#26377;&#24517;&#35201;&#22238;&#28335;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
Natural Counterfactuals With Necessary Backtracking
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#26694;&#26550;&#21644;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#65292;&#29983;&#25104;&#19982;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#30340;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#29702;&#23545;&#20110;&#20154;&#31867;&#35748;&#30693;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#23545;&#20110;&#25552;&#20379;&#35299;&#37322;&#21644;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;Judea Pearl&#30340;&#30740;&#31350;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#24456;&#20248;&#38597;&#65292;&#20294;&#20854;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#26223;&#24448;&#24448;&#38656;&#35201;&#36807;&#20110;&#33073;&#31163;&#23454;&#38469;&#24773;&#26223;&#30340;&#24178;&#39044;&#65292;&#22240;&#27492;&#38590;&#20197;&#23454;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26694;&#26550;&#21644;&#19968;&#31181;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#25913;&#36827;&#65292;&#20801;&#35768;&#23545;&#22240;&#26524;&#21069;&#32622;&#21464;&#37327;&#36827;&#34892;&#25913;&#21464;&#20197;&#26368;&#23567;&#21270;&#19982;&#23454;&#38469;&#24773;&#26223;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#29983;&#25104;&#33258;&#28982;&#21453;&#20107;&#23454;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#24615;&#20934;&#21017;&#20801;&#35768;&#20294;&#25511;&#21046;&#22238;&#28335;&#30340;&#33539;&#22260;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual reasoning is pivotal in human cognition and especially important for providing explanations and making decisions. While Judea Pearl's influential approach is theoretically elegant, its generation of a counterfactual scenario often requires interventions that are too detached from the real scenarios to be feasible. In response, we propose a framework of natural counterfactuals and a method for generating counterfactuals that are natural with respect to the actual world's data distribution. Our methodology refines counterfactual reasoning, allowing changes in causally preceding variables to minimize deviations from realistic scenarios. To generate natural counterfactuals, we introduce an innovative optimization framework that permits but controls the extent of backtracking with a naturalness criterion. Empirical experiments indicate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01093</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Specialized Language Models with Cheap Inference from Limited Domain Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#24265;&#20215;&#25512;&#29702;&#30340;&#19987;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#19979;&#25214;&#21040;&#20102;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#26356;&#20248;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#65292;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#22256;&#24785;&#24230;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#32570;&#20047;&#22823;&#35268;&#27169;&#25512;&#29702;&#39044;&#31639;&#21644;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#30340;&#20219;&#21153;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#36825;&#20123;&#38480;&#21046;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#22235;&#20010;&#37325;&#35201;&#30340;&#21464;&#37327;&#65306;&#39044;&#35757;&#32451;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21069;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#19987;&#29992;&#39044;&#31639;&#65288;&#29992;&#20110;&#22312;&#30446;&#26631;&#39046;&#22495;&#20986;&#29616;&#20043;&#21518;&#36827;&#34892;&#35757;&#32451;&#65289;&#65292;&#25512;&#29702;&#39044;&#31639;&#21644;&#39046;&#22495;&#20869;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#21463;&#21040;&#25512;&#29702;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27604;&#35757;&#32451;&#38750;&#24120;&#22823;&#30340;&#22522;&#26412;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#26631;&#20934;&#20570;&#27861;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36229;&#32593;&#32476;&#21644;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#39044;&#31639;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#22256;&#24785;&#24230;&#65292;&#32780;&#22312;&#22823;&#22411;&#19987;&#29992;&#39044;&#31639;&#19979;&#65292;&#35757;&#32451;&#37325;&#35201;&#26679;&#26412;&#25968;&#25454;&#38598;&#19978;&#30340;&#23567;&#22411;&#27169;&#22411;&#26356;&#20855;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged as a versatile tool but are challenging to apply to tasks lacking large inference budgets and large in-domain training sets. This work formalizes these constraints and distinguishes four important variables: the pretraining budget (for training before the target domain is known), the specialization budget (for training after the target domain is known), the inference budget, and the in-domain training set size. Across these settings, we compare different approaches from the machine learning literature. Limited by inference cost, we find better alternatives to the standard practice of training very large vanilla transformer models. In particular, we show that hyper-networks and mixture of experts have better perplexity for large pretraining budgets, while small models trained on importance sampled datasets are attractive for large specialization budgets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.11574</link><description>&lt;p&gt;
&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Offline Multitask Representation Learning for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11574
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#34987;&#25552;&#20379;&#26469;&#33258;&#20849;&#20139;&#36890;&#29992;&#34920;&#31034;&#30340;&#19981;&#21516;&#20219;&#21153;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#24182;&#34987;&#35201;&#27714;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#23545;&#31163;&#32447;&#22810;&#20219;&#21153;&#20302;&#31209;RL&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MORL&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#22810;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#22870;&#21169;&#20813;&#36153;&#12289;&#31163;&#32447;&#21644;&#22312;&#32447;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#19979;&#28216;RL&#65292;&#20854;&#20013;&#21521;&#20195;&#29702;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982;&#19978;&#28216;&#31163;&#32447;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20174;&#19978;&#28216;&#31163;&#32447;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#22909;&#22788;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23398;&#20064;&#20302;&#31209;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11574v1 Announce Type: new  Abstract: We study offline multitask representation learning in reinforcement learning (RL), where a learner is provided with an offline dataset from different tasks that share a common representation and is asked to learn the shared representation. We theoretically investigate offline multitask low-rank RL, and propose a new algorithm called MORL for offline multitask representation learning. Furthermore, we examine downstream RL in reward-free, offline and online scenarios, where a new task is introduced to the agent that shares the same representation as the upstream offline tasks. Our theoretical results demonstrate the benefits of using the learned representation from the upstream offline task instead of directly learning the representation of the low-rank model.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10332</link><description>&lt;p&gt;
GreedyML&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GreedyML: A Parallel Algorithm for Maximizing Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#22312;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#35299;&#20915;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#27714;&#30340;&#21551;&#21457;&#65292;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25968;&#25454;&#25688;&#35201;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Barbosa&#12289;Ene&#12289;Nguyen&#21644;Ward&#65288;2015&#65289;&#25552;&#20986;&#30340;&#38543;&#26426;&#20998;&#24067;&#24335;RandGreedI&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#25968;&#25454;&#38543;&#26426;&#20998;&#21306;&#21040;&#25152;&#26377;&#22788;&#29702;&#22120;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#21333;&#20010;&#32047;&#31215;&#27493;&#39588;&#35745;&#31639;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#22788;&#29702;&#22120;&#23558;&#23427;&#20204;&#30340;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#21457;&#36865;&#32473;&#19968;&#20010;&#22788;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#38382;&#39064;&#65292;&#32047;&#31215;&#27493;&#39588;&#21487;&#33021;&#36229;&#36807;&#22788;&#29702;&#22120;&#19978;&#21487;&#29992;&#30340;&#20869;&#23384;&#65292;&#24182;&#19988;&#25191;&#34892;&#32047;&#31215;&#30340;&#22788;&#29702;&#22120;&#21487;&#33021;&#25104;&#20026;&#35745;&#31639;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10332v1 Announce Type: cross  Abstract: We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   Here, we propose a generalization of the R
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06328</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#23454;&#29616;&#21487;&#36801;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferable Reinforcement Learning via Generalized Occupancy Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#24517;&#39035;&#26159;&#36890;&#29992;&#30340; - &#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#27010;&#25324;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#20869;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#23398;&#20064;&#19990;&#30028;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21160;&#24577;&#27169;&#22411;&#65292;&#21407;&#21017;&#19978;&#20351;&#23427;&#20204;&#33021;&#22815;&#27010;&#25324;&#21040;&#20219;&#24847;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#19968;&#27493;&#27169;&#22411;&#33258;&#28982;&#20250;&#21463;&#21040;&#32047;&#31215;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#20351;&#23427;&#20204;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#38382;&#39064;&#19978;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#22411;&#27169;&#22411; - &#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65288;GOMs&#65289;&#65292;&#20445;&#30041;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32047;&#31215;&#24615;&#38169;&#35823;&#12290;GOMs&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#19979;&#65292;&#24314;&#27169;&#32473;&#23450;&#29366;&#24577;&#30340;&#25152;&#26377;&#21487;&#33021;&#38271;&#26399;&#32467;&#26524;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#23454;&#29616;&#32473;&#23450;&#29366;&#24577;&#30340;&#29305;&#23450;&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36805;&#36895;&#29992;&#20110;&#20026;&#20219;&#24847;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20248;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#32047;&#31215;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.04690</link><description>&lt;p&gt;
&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;: &#22312;&#32447;&#31243;&#22359;&#32423;&#21035;&#20943;&#23569;&#33258;&#27880;&#24847;&#21147;&#30340;O(n^2)&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04690
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#37051;&#22495;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#38480;&#21046;&#22312;&#26368;&#36817;&#30340;&#37051;&#23621;&#20043;&#38388;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37051;&#22495;&#27880;&#24847;&#21147;&#36890;&#36807;&#38480;&#21046;&#27599;&#20010;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#33539;&#22260;&#20026;&#20854;&#26368;&#36817;&#30340;&#37051;&#23621;&#26469;&#38477;&#20302;&#33258;&#27880;&#24847;&#21147;&#30340;&#25104;&#26412;&#12290;&#35813;&#38480;&#21046;&#30001;&#31383;&#21475;&#22823;&#23567;&#21644;&#25193;&#24352;&#22240;&#23376;&#21442;&#25968;&#21270;&#65292;&#20171;&#20110;&#32447;&#24615;&#25237;&#24433;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#32472;&#21046;&#20102;&#21487;&#33021;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#35889;&#12290;&#37051;&#22495;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#26356;&#19968;&#33324;&#22320;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#22522;&#30784;&#35774;&#26045;&#26041;&#38754;&#38271;&#26399;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#39640;&#31209;&#30340;&#31354;&#38388;&#65288;2-D&#21644;3-D&#65289;&#65292;&#20419;&#20351;&#24320;&#21457;&#23450;&#21046;&#20869;&#26680;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#20869;&#26680;&#22312;&#21151;&#33021;&#25110;&#24615;&#33021;&#26041;&#38754;&#21463;&#38480;&#65292;&#22914;&#26524;&#19981;&#26159;&#20004;&#32773;&#37117;&#26377;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#37051;&#22495;&#27880;&#24847;&#21147;&#21487;&#20197;&#34920;&#31034;&#20026;&#25209;&#37327;&#21270;&#30340;GEMM&#38382;&#39064;&#65292;&#31867;&#20284;&#20110;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#24182;&#20026;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#23454;&#29616;&#23427;&#12290;&#19982;&#29616;&#26377;&#30340;&#31616;&#21333;&#20869;&#26680;&#30456;&#27604;&#65292;&#36825;&#20123;&#20869;&#26680;&#24179;&#22343;&#25552;&#20379;&#20102;&#20998;&#21035;&#26159;1-D&#21644;2-D&#37051;&#22495;&#27880;&#24847;&#21147;&#30340;&#20840;&#31934;&#24230;&#24310;&#36831;&#25913;&#36827;&#20998;&#21035;&#20026;895%&#21644;272%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04690v1 Announce Type: cross  Abstract: Neighborhood attention reduces the cost of self attention by restricting each token's attention span to its nearest neighbors. This restriction, parameterized by a window size and dilation factor, draws a spectrum of possible attention patterns between linear projection and self attention. Neighborhood attention, and more generally sliding window attention patterns, have long been bounded by infrastructure, particularly in higher-rank spaces (2-D and 3-D), calling for the development of custom kernels, which have been limited in either functionality, or performance, if not both. In this work, we first show that neighborhood attention can be represented as a batched GEMM problem, similar to standard attention, and implement it for 1-D and 2-D neighborhood attention. These kernels on average provide 895% and 272% improvement in full precision latency compared to existing naive kernels for 1-D and 2-D neighborhood attention respectively. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;</title><link>https://arxiv.org/abs/2403.04082</link><description>&lt;p&gt;
&#36890;&#36807;&#25554;&#20540;&#36827;&#34892;&#25512;&#26029;&#65306;&#23545;&#27604;&#34920;&#31034;&#21487;&#35777;&#26126;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference via Interpolation: Contrastive Representations Provably Enable Planning and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#20174;&#32780;&#21551;&#29992;&#35268;&#21010;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#25105;&#20204;&#22914;&#20309;&#22238;&#31572;&#35832;&#22914;&#8220;&#26410;&#26469;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#25105;&#20204;&#26159;&#22914;&#20309;&#21040;&#36798;&#36825;&#37324;&#30340;&#65311;&#8221;&#36825;&#31867;&#27010;&#29575;&#25512;&#26029;&#38382;&#39064;&#22312;&#35266;&#27979;&#20540;&#20026;&#39640;&#32500;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#38382;&#39064;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#30340;&#32039;&#20945;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#23545;&#27604;&#23398;&#20064;&#30340;&#21464;&#20307;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#32534;&#30721;&#20102;&#27010;&#29575;&#27604;&#12290;&#36890;&#36807;&#23558;&#20043;&#21069;&#30340;&#24037;&#20316;&#25193;&#23637;&#20197;&#34920;&#26126;&#34920;&#31034;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#34920;&#31034;&#30340;&#32852;&#21512;&#20998;&#24067;&#20063;&#26159;&#39640;&#26031;&#20998;&#24067;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#65292;&#36890;&#36807;&#26102;&#38388;&#23545;&#27604;&#23398;&#20064;&#23398;&#21040;&#30340;&#34920;&#31034;&#36981;&#24490;&#39640;&#26031;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#19968;&#31181;&#22270;&#24418;&#27169;&#22411;&#65292;&#20854;&#20013;&#23545;&#34920;&#31034;&#36827;&#34892;&#30340;&#25512;&#26029;&#65288;&#20363;&#22914;&#39044;&#27979;&#12289;&#35268;&#21010;&#65289;&#23545;&#24212;&#20110;&#21453;&#28436;&#20302;&#32500;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04082v1 Announce Type: new  Abstract: Given time series data, how can we answer questions like "what will happen in the future?" and "how did we get here?" These sorts of probabilistic inference questions are challenging when observations are high-dimensional. In this paper, we show how these questions can have compact, closed form solutions in terms of learned representations. The key idea is to apply a variant of contrastive learning to time series data. Prior work already shows that the representations learned by contrastive learning encode a probability ratio. By extending prior work to show that the marginal distribution over representations is Gaussian, we can then prove that joint distribution of representations is also Gaussian. Taken together, these results show that representations learned via temporal contrastive learning follow a Gauss-Markov chain, a graphical model where inference (e.g., prediction, planning) over representations corresponds to inverting a low-
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03333</link><description>&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#39640;&#24230;&#24322;&#26500;&#30340;&#23458;&#25143;&#20998;&#24067;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22256;&#38590;&#37096;&#20998;&#28304;&#20110;&#20004;&#20010;&#30475;&#20284;&#30683;&#30462;&#30340;&#30446;&#26631;&#65306;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#23398;&#20064;&#24212;&#36866;&#24212;&#27599;&#20010;&#26412;&#22320;&#20998;&#24067;&#30340;&#26412;&#22320;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#26469;&#28040;&#38500;&#36825;&#31181;&#30683;&#30462;&#12290;&#22522;&#20110;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;SosicFL&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21333;&#32431;&#24418;&#20013;&#30340;&#23376;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;FL&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#33258;&#30001;&#24230;&#33539;&#22260;&#20869;&#20855;&#26377;&#20854;&#29305;&#24449;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#36890;&#29992;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SosicFL&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#24182;&#21152;&#36895;&#20102;&#20840;&#23616;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
&lt;/p&gt;</description></item><item><title>&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00540</link><description>&lt;p&gt;
Epsilon-Greedy Thompson Sampling&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Epsilon-Greedy Thompson Sampling to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00540
&lt;/p&gt;
&lt;p&gt;
&#23558;$\varepsilon$-greedy&#31574;&#30053;&#24341;&#20837;Thompson&#37319;&#26679;&#20197;&#25913;&#36827;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24320;&#21457;&#21151;&#33021;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Thompson&#37319;&#26679;&#65288;TS&#65289;&#34987;&#35748;&#20026;&#26159;&#35299;&#20915;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24320;&#21457;-&#25506;&#32034;&#22256;&#22659;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#34429;&#28982;&#23427;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#21644;&#26368;&#22823;&#21270;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21518;&#39564;&#30340;&#26679;&#26412;&#36335;&#24452;&#26469;&#20248;&#20808;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;TS&#22312;&#27599;&#27425;&#25191;&#34892;&#25506;&#32034;&#21518;&#36890;&#36807;&#25910;&#38598;&#20851;&#20110;&#30495;&#23454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#26469;&#24369;&#21270;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#26412;&#30740;&#31350;&#23558;&#22312;TS&#20013;&#24341;&#20837;$\varepsilon$-greedy&#31574;&#30053;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25913;&#36827;&#20854;&#24320;&#21457;&#21151;&#33021;&#12290; &#25105;&#20204;&#39318;&#20808;&#25551;&#36848;&#20102;TS&#24212;&#29992;&#20110;BO&#30340;&#20004;&#20010;&#26497;&#31471;&#65292;&#21363;&#36890;&#29992;TS&#21644;&#26679;&#26412;&#24179;&#22343;TS&#12290;&#21069;&#32773;&#21644;&#21518;&#32773;&#20998;&#21035;&#25552;&#20513;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290; &#28982;&#21518;&#25105;&#20204;&#20351;&#29992;$\varepsilon$-greedy&#31574;&#30053;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#38543;&#26426;&#20999;&#25442;&#12290; $\varepsilon \in (0,1)$&#30340;&#23567;&#20540;&#20248;&#20808;&#32771;&#34385;&#24320;&#21457;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290; &#25105;&#20204;&#23454;&#35777;&#34920;&#26126;$\varepsilon$-greedy T
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00540v1 Announce Type: new  Abstract: Thompson sampling (TS) serves as a solution for addressing the exploitation-exploration dilemma in Bayesian optimization (BO). While it prioritizes exploration by randomly generating and maximizing sample paths of Gaussian process (GP) posteriors, TS weakly manages its exploitation by gathering information about the true objective function after each exploration is performed. In this study, we incorporate the epsilon-greedy ($\varepsilon$-greedy) policy, a well-established selection strategy in reinforcement learning, into TS to improve its exploitation. We first delineate two extremes of TS applied for BO, namely the generic TS and a sample-average TS. The former and latter promote exploration and exploitation, respectively. We then use $\varepsilon$-greedy policy to randomly switch between the two extremes. A small value of $\varepsilon \in (0,1)$ prioritizes exploitation, and vice versa. We empirically show that $\varepsilon$-greedy T
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.18729</link><description>&lt;p&gt;
&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#28237;&#27969;&#23553;&#38381;&#27169;&#22411;&#36827;&#34892;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18729
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#23545;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#20851;&#38190;&#21464;&#37327;&#30340;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20026;&#22823;&#28065;&#27169;&#25311;&#65288;LES&#65289;&#20013;&#30340;&#23376;&#28388;&#27874;&#23610;&#24230;&#65288;SFS&#65289;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#29289;&#29702;&#30340;&#23553;&#38381;&#27169;&#22411;&#24418;&#24335;&#65292;&#20294;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#65288;DNS&#65289;&#25552;&#20379;&#30340;&#22823;&#37327;&#25968;&#25454;&#20026;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#25216;&#26415;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#28789;&#27963;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20173;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#20989;&#25968;&#24418;&#24335;&#12290;&#37319;&#29992;&#36825;&#31181;&#27169;&#22411;&#30340;&#22686;&#21152;&#38656;&#35201;&#21487;&#38752;&#22320;&#20272;&#35745;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#20013;&#25968;&#25454;&#30693;&#35782;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#26469;&#25429;&#25417;&#21453;&#24212;&#27969;&#21160;&#27169;&#22411;&#20013;&#30340;&#36923;&#36753;&#19981;&#30830;&#23450;&#24615;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#22312;&#28237;&#28065;&#39044;&#28151;&#28779;&#28976;&#21160;&#24577;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#28388;&#27874;&#36827;&#23637;&#21464;&#37327;&#26631;&#37327;&#32791;&#25955;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BNN&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#25968;&#25454;&#39537;&#21160;&#23553;&#38381;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#32467;&#26500;&#30340;&#29420;&#29305;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18729v1 Announce Type: cross  Abstract: While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;</title><link>https://arxiv.org/abs/2402.18551</link><description>&lt;p&gt;
&#38544;&#24615;&#20559;&#35265;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Next-Token Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18551
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#35757;&#32451;&#19979;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#20013;&#65292;&#30830;&#23450;&#20102;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#26799;&#24230;&#19979;&#38477;&#33021;&#22815;&#23454;&#29616;&#20854;&#19979;&#30028;&#65307;&#21516;&#26102;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#65288;NTP&#65289;&#26159;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39318;&#36873;&#33539;&#24335;&#65292;&#23427;&#28041;&#21450;&#39044;&#27979;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#19982;&#20256;&#32479;&#30340;&#29420;&#28909;&#20998;&#31867;&#19981;&#21516;&#65292;&#22312;NTP&#20013;&#65292;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#39057;&#29575;&#30340;&#26631;&#35760;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#21518;&#32487;&#12290;&#26412;&#25991;&#23558;NTP&#35757;&#32451;&#26694;&#26550;&#21270;&#20026;&#36328;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#29109;&#26368;&#23567;&#21270;&#65292;&#27599;&#20010;&#19978;&#19979;&#25991;&#37117;&#19982;&#26377;&#38480;&#35789;&#27719;&#34920;&#20013;&#30340;&#31232;&#30095;&#32463;&#39564;&#27010;&#29575;&#21521;&#37327;&#30456;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#23427;&#25506;&#35752;&#20102;&#20197;&#19979;&#38382;&#39064;&#65306;&#24403;NTP&#35757;&#32451;&#25439;&#22833;&#36798;&#21040;&#20854;&#19979;&#30028;&#65288;&#29109;&#65289;&#26102;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#26159;&#21542;&#20250;&#23545;&#20855;&#26377;&#29305;&#23450;&#32467;&#26500;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#20559;&#35265;&#65311;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#32447;&#24615;NTP&#27169;&#22411;&#65292;&#25105;&#20204;&#20570;&#20986;&#20197;&#19979;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#19978;&#30340;NTP&#21487;&#20998;&#31163;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;GD&#33021;&#22815;&#36798;&#21040;&#20854;&#19979;&#30028;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#22312;&#36807;&#21442;&#25968;&#21270;&#26102;&#20173;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18551v1 Announce Type: cross  Abstract: Next-token prediction (NTP), the go-to training paradigm in training large language models, involves predicting the next token in a sequence. Departing from traditional one-hot classification, in NTP, multiple tokens with varying frequencies follow each given context. This work frames NTP training as cross-entropy minimization over distinct contexts, each associated with a sparse empirical probability vector across a finite vocabulary. It then addresses the following question: do gradient-based optimizers exhibit a bias towards solutions with specific structure as the NTP training loss reaches its lower bound (entropy)? Specifically, for linear NTP models trained using gradient descent (GD), we make the following contributions: Firstly, we determine NTP-separability conditions on the data, under which GD can attain its lower bound. We also demonstrate that these conditions hold under overparameterization. Secondly, we establish that th
&lt;/p&gt;</description></item><item><title>Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.17966</link><description>&lt;p&gt;
Conformer&#65306;&#23558;&#36830;&#32493;&#27880;&#24847;&#21147;&#23884;&#20837;&#35270;&#35273;Transformer&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17966
&lt;/p&gt;
&lt;p&gt;
Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#24615;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;Transformers&#26159;&#31163;&#25955;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#20854;&#23398;&#20064;&#21160;&#24577;&#22825;&#27668;&#31995;&#32479;&#36830;&#32493;&#26102;&#31354;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Conformer&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#12290;Conformer&#26088;&#22312;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#34987;&#32534;&#30721;&#20026;Transformer&#26550;&#26500;&#20013;&#30340;&#21487;&#24494;&#20998;&#20989;&#25968;&#65292;&#20197;&#24314;&#27169;&#22797;&#26434;&#30340;&#22825;&#27668;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;Conformer&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#21644;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;Conformer&#22312;&#25152;&#26377;&#21069;&#23548;&#26102;&#38388;&#19978;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17966v1 Announce Type: new  Abstract: Operational weather forecasting system relies on computationally expensive physics-based models. Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#65292;&#20351;&#29992;Latent Prompt Transformer (LPT)&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.17179</link><description>&lt;p&gt;
&#21452;&#31354;&#38388;&#20248;&#21270;&#65306;&#36890;&#36807;&#28508;&#22312;&#25552;&#31034;&#21464;&#25442;&#22120;&#25913;&#36827;&#20998;&#23376;&#24207;&#21015;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dual-Space Optimization: Improved Molecule Sequence Design by Latent Prompt Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#65292;&#20351;&#29992;Latent Prompt Transformer (LPT)&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29702;&#24819;&#24615;&#36136;&#65288;&#22914;&#33647;&#29289;&#26679;&#24615;&#21644;&#23545;&#34507;&#30333;&#38774;&#28857;&#30340;&#39640;&#32467;&#21512;&#20146;&#21644;&#21147;&#65289;&#30340;&#20998;&#23376;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#20248;&#21270;&#65288;DSO&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21644;&#25968;&#25454;&#31354;&#38388;&#36873;&#25321;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DSO&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36880;&#27493;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#31227;&#21521;&#25152;&#38656;&#24615;&#36136;&#25968;&#20540;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#37319;&#29992;&#28508;&#22312;&#25552;&#31034;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#30340;&#24418;&#24335;&#65292;&#20854;&#20013;&#28508;&#22312;&#21521;&#37327;&#20805;&#24403;&#22240;&#26524;&#21464;&#25442;&#22120;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#30446;&#26631;&#12289;&#22810;&#30446;&#26631;&#21644;&#32422;&#26463;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#20013;&#26641;&#31435;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17179v1 Announce Type: new  Abstract: Designing molecules with desirable properties, such as drug-likeliness and high binding affinities towards protein targets, is a challenging problem. In this paper, we propose the Dual-Space Optimization (DSO) method that integrates latent space sampling and data space selection to solve this problem. DSO iteratively updates a latent space generative model and a synthetic dataset in an optimization process that gradually shifts the generative model and the synthetic data towards regions of desired property values. Our generative model takes the form of a Latent Prompt Transformer (LPT) where the latent vector serves as the prompt of a causal transformer. Our extensive experiments demonstrate effectiveness of the proposed method, which sets new performance benchmarks across single-objective, multi-objective and constrained molecule design tasks.
&lt;/p&gt;</description></item><item><title>NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15393</link><description>&lt;p&gt;
NeuralThink: &#22312;&#19968;&#33324;&#20219;&#21153;&#20013;&#36827;&#34892;&#22806;&#25512;&#30340;&#31639;&#27861;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
NeuralThink: Algorithm Synthesis that Extrapolates in General Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15393
&lt;/p&gt;
&lt;p&gt;
NeuralThink &#26159;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25797;&#38271;&#27169;&#24335;&#35782;&#21035;&#65292;&#20294;&#22312;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#26041;&#24335;&#19978;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#24605;&#32500;&#26041;&#27861;&#23637;&#29616;&#20102;&#23398;&#20064;&#21487;&#20197;&#22806;&#25512;&#30340;&#31639;&#27861;&#30340;&#28508;&#21147;&#65306;&#22312;&#36739;&#23567;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#24182;&#22312;&#36739;&#22823;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#23398;&#21040;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23616;&#38480;&#20110;&#23545;&#31216;&#20219;&#21153;&#65292;&#21363;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#30456;&#21516;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; NeuralThink&#65292;&#19968;&#31181;&#26032;&#30340;&#36882;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#19968;&#36143;&#22320;&#23545;&#23545;&#31216;&#21644;&#19981;&#23545;&#31216;&#20219;&#21153;&#36827;&#34892;&#22806;&#25512;&#65292;&#20854;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#23545;&#31216;&#20219;&#21153;&#22806;&#25512;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NeuralThink &#22312;&#31283;&#23450;&#22320;&#20174;&#36739;&#23567;&#30340;&#35757;&#32451;&#35268;&#27169;&#23545;&#22823;&#35266;&#27979;&#36827;&#34892;&#22806;&#25512;&#26041;&#38754;&#19968;&#30452;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24605;&#32500;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15393v1 Announce Type: cross  Abstract: While machine learning methods excel at pattern recognition, they struggle with complex reasoning tasks in a scalable, algorithmic manner. Recent Deep Thinking methods show promise in learning algorithms that extrapolate: learning in smaller environments and executing the learned algorithm in larger environments. However, these works are limited to symmetrical tasks, where the input and output dimensionalities are the same. To address this gap, we propose NeuralThink, a new recurrent architecture that can consistently extrapolate to both symmetrical and asymmetrical tasks, where the dimensionality of the input and output are different. We contribute with a novel benchmark of asymmetrical tasks for extrapolation. We show that NeuralThink consistently outperforms the prior state-of-the-art Deep Thinking architectures, in regards to stable extrapolation to large observations from smaller training sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12711</link><description>&lt;p&gt;
&#20855;&#26377;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;&#20445;&#35777;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#36825;&#19968;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21516;&#26102;&#35777;&#26126;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#23548;&#33268;&#20102;&#22312;&#21508;&#31181;&#24615;&#33021;&#24230;&#37327;&#19978;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#24615;&#33021;&#24230;&#37327;&#65292;&#22914;&#36951;&#25022;&#12289;PAC&#30028;&#38480;&#25110;&#32479;&#19968;PAC(Dann&#31561;&#20154;&#65292;2017)&#65292;&#36890;&#24120;&#35780;&#20272;&#32047;&#31215;&#24615;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#22312;&#20219;&#24847;&#26377;&#38480;&#26102;&#38388;t&#20869;&#29609;&#24369;&#21155;&#30340;&#33218;&#12290;&#36825;&#31181;&#34892;&#20026;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#21487;&#33021;&#36896;&#25104;&#20005;&#37325;&#25439;&#22833;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26356;&#24378;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#32479;&#19968;&#26368;&#21518;&#36845;&#20195;(ULI)&#20445;&#35777;&#65292;&#25429;&#25417;&#36172;&#21338;&#31639;&#27861;&#30340;&#32047;&#31215;&#21644;&#30636;&#26102;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ULI&#34920;&#24449;&#20102;&#30636;&#26102;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#25152;&#29609;&#24369;&#21155;&#33218;&#30340;&#27599;&#36718;&#36951;&#25022;&#21463;&#21040;&#19968;&#20010;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#65288;&#22823;&#65289;&#36718;&#27425;t&#21333;&#35843;&#36882;&#20943;&#65292;&#22312;&#26377;&#36275;&#22815;&#26679;&#26412;&#21487;&#29992;&#26102;&#38450;&#27490;&#37325;&#22797;&#35775;&#38382;&#21155;&#36136;&#33218;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25509;&#36817;&#26368;&#20248;&#30340;ULI&#20445;&#35777;&#30452;&#25509;&#24847;&#21619;&#30528;&#22312;&#19978;&#36848;&#24615;&#33021;&#24230;&#37327;&#20013;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#32047;&#31215;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;ULI&#22312;&#26377;&#38480;&#33218;&#38598;&#19978;&#30340;&#21487;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12711v1 Announce Type: new  Abstract: Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10095</link><description>&lt;p&gt;
&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Classification Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;&#65288;CDMs&#65289;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#30340;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#21462;&#24471;&#20102;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.10095v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#19968;&#31181;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#30340;&#31361;&#20986;&#26041;&#27861;&#23478;&#26063;&#20381;&#36182;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#65288;DRE&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#20998;&#31867;}$&#25968;&#25454;&#26679;&#26412;&#21644;&#26469;&#33258;&#26576;&#20010;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#31616;&#21333;&#30340;&#20302;&#32500;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#23398;&#20064;&#20998;&#24067;&#30340;&#21478;&#19968;&#31181;&#26041;&#27861;&#23478;&#26063;&#26159;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDM&#65289;&#65292;&#20854;&#20013;&#27169;&#22411;&#34987;&#35757;&#32451;&#26469;$\textit{&#21435;&#22122;}$&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{&#20998;&#31867;&#25193;&#25955;&#27169;&#22411;}$&#65288;CDMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#25216;&#26415;&#65292;&#23427;&#37319;&#29992;&#20102;DDM&#30340;&#21435;&#22122;&#22522;&#26412;&#24418;&#24335;&#65292;&#21516;&#26102;&#21033;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#21152;&#22312;&#24178;&#20928;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#37327;&#65292;&#31867;&#20284;&#20110;DRE&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#35266;&#23519;&#65292;&#21363;MSE&#26368;&#20248;&#21270;&#30340;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10095v1 Announce Type: new  Abstract: A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal d
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09299</link><description>&lt;p&gt;
&#26410;&#32463;&#26412;&#20154;&#21516;&#24847;&#30340;&#35757;&#32451;&#65306;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09299
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#22914;&#20309;&#22312;&#35757;&#32451;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#20195;&#30721;&#21253;&#21547;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#23457;&#35745;&#26102;&#30340;&#29256;&#26435;&#20405;&#26435;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#35745;&#36890;&#36807;&#39564;&#35777;&#24320;&#21457;&#30340;&#20195;&#30721;&#26159;&#21542;&#31526;&#21512;&#26631;&#20934;&#12289;&#27861;&#35268;&#21644;&#29256;&#26435;&#20445;&#25252;&#65292;&#30830;&#20445;&#20854;&#19981;&#21253;&#21547;&#26469;&#33258;&#21463;&#20445;&#25252;&#26469;&#28304;&#30340;&#20195;&#30721;&#12290;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#32534;&#30721;&#21161;&#25163;&#30340;&#20986;&#29616;&#32473;&#20195;&#30721;&#23457;&#35745;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#26469;&#33258;&#20844;&#24320;&#21487;&#29992;&#30340;&#26469;&#28304;&#12290;&#36825;&#24341;&#21457;&#20102;&#30693;&#35782;&#20135;&#26435;&#20405;&#26435;&#38382;&#39064;&#65292;&#22240;&#20026;&#24320;&#21457;&#32773;&#30340;&#20195;&#30721;&#24050;&#21253;&#21547;&#22312;&#25968;&#25454;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#24320;&#21457;&#30340;&#20195;&#30721;&#23457;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#24320;&#21457;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;LLM&#26159;&#21542;&#24050;&#32463;&#22312;&#29305;&#23450;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#20026;&#25105;&#20204;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#37492;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#23494;&#24615;&#65292;&#20256;&#32479;&#30340;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#31561;&#26041;&#27861;&#26080;&#27861;&#30830;&#20445;&#29256;&#26435;&#20405;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09299v1 Announce Type: cross Abstract: Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers' codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08406</link><description>&lt;p&gt;
&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition Constrained Bayesian Optimization via Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36807;&#28193;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20102;&#30001;&#20110;&#36716;&#21464;&#32422;&#26463;&#23548;&#33268;&#30340;&#25628;&#32034;&#31354;&#38388;&#20381;&#36182;&#21382;&#21490;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20851;&#27880;&#30340;&#26159;&#21487;&#20197;&#20219;&#24847;&#26597;&#35810;&#25628;&#32034;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#24182;&#19981;&#20855;&#22791;&#36825;&#31181;&#28789;&#27963;&#24615;&#65307;&#29305;&#21035;&#26159;&#65292;&#19979;&#19968;&#20010;&#26597;&#35810;&#30340;&#25628;&#32034;&#31354;&#38388;&#21487;&#33021;&#21462;&#20915;&#20110;&#20808;&#21069;&#30340;&#26597;&#35810;&#12290;&#29289;&#29702;&#31185;&#23398;&#39046;&#22495;&#30340;&#20363;&#23376;&#20013;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#23616;&#37096;&#31227;&#21160;&#38480;&#21046;&#12289;&#29305;&#23450;&#21464;&#37327;&#30340;&#21333;&#35843;&#24615;&#35201;&#27714;&#20197;&#21450;&#36716;&#21464;&#24433;&#21709;&#27979;&#37327;&#31934;&#24230;&#12290;&#24635;&#20043;&#65292;&#36825;&#20123;&#36807;&#28193;&#32422;&#26463;&#38656;&#35201;&#19968;&#31181;&#35268;&#21010;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26694;&#26550;&#25193;&#23637;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36845;&#20195;&#22320;&#35299;&#20915;&#25105;&#20204;&#30446;&#26631;&#30340;&#19968;&#20010;&#21487;&#34892;&#32447;&#24615;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#33021;&#22815;&#25552;&#21069;&#35268;&#21010;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#31574;&#30053;&#12290;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20381;&#36182;&#21382;&#21490;&#30340;&#21644;&#38750;&#39532;&#23572;&#21487;&#22827;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21270;&#23398;&#21453;&#24212;&#22120;&#20248;&#21270;&#12289;&#20449;&#24687;&#21270;&#36335;&#24452;&#35268;&#21010;&#12289;&#26426;&#22120;&#26657;&#20934;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization is a methodology to optimize black-box functions. Traditionally, it focuses on the setting where you can arbitrarily query the search space. However, many real-life problems do not offer this flexibility; in particular, the search space of the next query may depend on previous ones. Example challenges arise in the physical sciences in the form of local movement constraints, required monotonicity in certain variables, and transitions influencing the accuracy of measurements. Altogether, such transition constraints necessitate a form of planning. This work extends Bayesian optimization via the framework of Markov Decision Processes, iteratively solving a tractable linearization of our objective using reinforcement learning to obtain a policy that plans ahead over long horizons. The resulting policy is potentially history-dependent and non-Markovian. We showcase applications in chemical reactor optimization, informative path planning, machine calibration, and other s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07963</link><description>&lt;p&gt;
SMX: &#19987;&#23478;&#36845;&#20195;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SMX: Sequential Monte Carlo Planning for Expert Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#35268;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#21019;&#24314;&#20102;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;&#23427;&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#39640;&#24182;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#33021;&#22815;&#22312;&#20915;&#31574;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#21033;&#29992;&#35268;&#21010;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26641;&#29366;&#25628;&#32034;&#26041;&#27861;&#21644;&#33258;&#25105;&#23545;&#24328;&#23398;&#20064;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25628;&#32034;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#36136;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#34429;&#28982;&#23454;&#36341;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#37096;&#20998;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;SMX&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35745;&#21010;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#25193;&#23637;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#25105;&#23398;&#20064;&#26426;&#21046;&#12290;SMX&#22522;&#20110;&#25511;&#21046;&#20316;&#20026;&#25512;&#26029;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#21463;&#30410;&#20110;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23427;&#22522;&#20110;&#37319;&#26679;&#30340;&#25628;&#32034;&#26041;&#27861;&#20351;&#20854;&#36866;&#24212;&#20855;&#26377;&#31163;&#25955;&#21644;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;SMX&#20801;&#35768;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#21487;&#20197;&#36816;&#34892;&#20110;&#21508;&#31867;&#35745;&#31639;&#26426;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing agents that can leverage planning abilities during their decision and learning processes is critical to the advancement of Artificial Intelligence. Recent works have demonstrated the effectiveness of combining tree-based search methods and self-play learning mechanisms. Yet, these methods typically face scaling challenges due to the sequential nature of their search. While practical engineering solutions can partly overcome this, they still demand extensive computational resources, which hinders their applicability. In this paper, we introduce SMX, a model-based planning algorithm that utilises scalable Sequential Monte Carlo methods to create an effective self-learning mechanism. Grounded in the theoretical framework of control as inference, SMX benefits from robust theoretical underpinnings. Its sampling-based search approach makes it adaptable to environments with both discrete and continuous action spaces. Furthermore, SMX allows for high parallelisation and can run on h
&lt;/p&gt;</description></item><item><title>&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07307</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07307
&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20855;&#26377;&#30456;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#37319;&#21462;&#30456;&#21516;&#30340;&#34892;&#21160;&#12290;&#31526;&#21512;&#39044;&#27979;&#24110;&#21161;&#20915;&#31574;&#32773;&#37327;&#21270;&#21160;&#20316;&#30340;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#21463;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#23427;&#20135;&#29983;&#20102;&#26082;&#31526;&#21512;Venn-Abers&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#24341;&#21457;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21518;&#39564;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#22120;&#65292;&#25552;&#20379;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#38388;&#30340;&#25928;&#29575;&#21644;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.07067</link><description>&lt;p&gt;
&#23398;&#20064;&#20005;&#26684;&#20984;&#30340;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#39044;&#26399;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Learning the Expected Core of Strictly Convex Stochastic Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20998;&#37197;&#65292;&#20063;&#31216;&#20026;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#65292;&#26159;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#20449;&#29992;&#20998;&#37197;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#26159;&#26680;&#24515;&#65292;&#23427;&#26159;&#31283;&#23450;&#20998;&#37197;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#27809;&#26377;&#20195;&#29702;&#26377;&#21160;&#26426;&#20174;&#22823;&#32852;&#30431;&#20013;&#20559;&#31163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#31283;&#23450;&#20998;&#37197;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#34987;&#25551;&#36848;&#20026;&#20855;&#26377;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#36820;&#22238;&#26597;&#35810;&#32852;&#30431;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;oracle&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#65292;&#21363;&#22312;&#26399;&#26395;&#19978;&#31283;&#23450;&#30340;&#20998;&#37197;&#38598;&#21512;&#12290;&#22312;&#20005;&#26684;&#20984;&#21338;&#24328;&#31867;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#26512;&#28041;&#21450;&#21040;&#20984;&#20960;&#20309;&#20013;&#30340;&#20960;&#20010;&#26032;&#32467;&#26524;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in credit assignment is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In this paper, we consider the stable allocation learning problem of stochastic cooperative games, where the reward function is characterised as a random variable with an unknown distribution. Given an oracle that returns a stochastic reward for an enquired coalition each round, our goal is to learn the expected core, that is, the set of allocations that are stable in expectation. Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a stable allocation given a polynomial number of samples, with high probability. The analysis of our algorithm involves the development of several new results in convex geometry, including an e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.04647</link><description>&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65306;&#35268;&#21010;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Plan Transformer: Planning as Latent Variable Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04647
&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#38271;&#26399;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#35268;&#21010;&#21464;&#24471;&#24517;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35268;&#21010;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32570;&#20047;&#36880;&#27493;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#26469;&#36830;&#25509;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#12290;LPT&#21487;&#20197;&#36890;&#36807;&#36712;&#36857;-&#22238;&#25253;&#23545;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#65292;&#23613;&#31649;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#65292;&#33258;&#28982;&#22320;&#32858;&#38598;&#23376;&#36712;&#36857;&#20197;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#36890;&#36807;&#39044;&#26399;&#22238;&#25253;&#23545;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#20316;&#20026;&#25512;&#26029;&#30340;&#24605;&#24819;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#25972;&#20010;&#22238;&#21512;&#20013;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#65292;&#36215;&#21040;&#19968;&#20010;&#35745;&#21010;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LPT&#21487;&#20197;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03864</link><description>&lt;p&gt;
&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#38454;&#27573;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#35270;&#35282;&#26159;&#30740;&#31350;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35757;&#32451;&#21160;&#24577;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35270;&#35282;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;PINNs&#27714;&#35299;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#19981;&#21516;&#34892;&#20026;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#24378;&#35843;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#25361;&#25112;&#12290;&#25152;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#37117;&#24471;&#21040;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDEs&#30340;&#25968;&#20540;&#31034;&#20363;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03545</link><description>&lt;p&gt;
&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#25913;&#21892;&#22312;&#32447;&#65288;&#24191;&#20041;&#65289;&#26631;&#31614;&#36716;&#31227;&#36866;&#24212;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Feature Updates Improve Online (Generalized) Label Shift Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#35774;&#32622;&#20013;&#26631;&#31614;&#36716;&#31227;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#20854;&#20013;&#23384;&#22312;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21450;&#26102;&#33719;&#24471;&#26631;&#31614;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#25110;&#26356;&#26032;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#30340;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#33258;&#36866;&#24212;&#19982;&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#65288;OLS-OFU&#65289;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#39044;&#27979;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#65292;OLS-OFU&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#20248;&#21270;&#65292;&#20943;&#23569;&#20102;&#31639;&#27861;&#36951;&#25022;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#22312;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#21644;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#26465;&#20214;&#19979;&#65292;&#24378;&#35843;&#20102;OLS-OFU&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03256</link><description>&lt;p&gt;
&#23398;&#20064;Predict-then-Optimize&#26694;&#26550;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Best-in-Class Policies for the Predict-then-Optimize Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03256
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#20854;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#24863;&#30693;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#65292;&#31216;&#20026;Perturbation Gradient&#65288;PG&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;predict-then-optimize&#26694;&#26550;&#12290;&#36825;&#20123;&#25439;&#22833;&#30452;&#25509;&#36817;&#20284;&#20102;&#19979;&#28216;&#20915;&#31574;&#25439;&#22833;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#29616;&#26377;&#30340;&#26367;&#20195;&#25439;&#22833;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#30340;&#36817;&#20284;&#35823;&#24046;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#28040;&#22833;&#12290;&#36825;&#24847;&#21619;&#30528;&#20248;&#21270;&#25105;&#20204;&#30340;&#26367;&#20195;&#25439;&#22833;&#21487;&#20197;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#24471;&#21040;&#26368;&#20339;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#35823;&#35774;&#32622;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#35823;&#35774;&#32622;&#19979;&#30340;&#36825;&#26679;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#35777;&#25454;&#35777;&#23454;&#20102;&#24403;&#22522;&#30784;&#27169;&#22411;&#35823;&#35774;&#32622;&#19988;&#22122;&#22768;&#19981;&#26159;&#20013;&#24515;&#23545;&#31216;&#26102;&#65292;&#25105;&#20204;&#30340;PG&#25439;&#22833;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#26696;&#12290;&#37492;&#20110;&#22312;&#23454;&#36341;&#20013;&#35823;&#35774;&#32622;&#24456;&#24120;&#35265;--&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#21487;&#33021;&#26356;&#21916;&#27426;&#19968;&#20010;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26102;--PG&#25439;&#22833;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#29702;&#35770;&#19978;&#26377;&#20381;&#25454;&#30340;&#12289;&#21487;&#35745;&#31639;&#30340;&#20915;&#31574;&#24863;&#30693;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel family of decision-aware surrogate losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize framework. These losses directly approximate the downstream decision loss and can be optimized using off-the-shelf gradient-based methods. Importantly, unlike existing surrogate losses, the approximation error of our PG losses vanishes as the number of samples grows. This implies that optimizing our surrogate loss yields a best-in-class policy asymptotically, even in misspecified settings. This is the first such result in misspecified settings and we provide numerical evidence confirming our PG losses substantively outperform existing proposals when the underlying model is misspecified and the noise is not centrally symmetric. Insofar as misspecification is commonplace in practice -- especially when we might prefer a simpler, more interpretable model -- PG losses offer a novel, theoretically justified, method for computationally tractable decision-aware 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02518</link><description>&lt;p&gt;
&#28508;&#22312;&#22270;&#25193;&#25955;&#65306;&#19968;&#31181;&#22312;&#22270;&#19978;&#29983;&#25104;&#21644;&#39044;&#27979;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Diffusion: A Unified Framework for Generation and Prediction on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#28508;&#22312;&#22270;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21644;&#39044;&#27979;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#35299;&#20915;&#21508;&#32423;&#21035;&#65288;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#65289;&#21644;&#21508;&#31867;&#22411;&#65288;&#29983;&#25104;&#12289;&#22238;&#24402;&#21644;&#20998;&#31867;&#65289;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#22270;&#25193;&#25955;&#65288;LGD&#65289;&#65292;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#32423;&#21035;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#22270;&#32467;&#26500;&#21644;&#29305;&#24449;&#23884;&#20837;&#28508;&#22312;&#31354;&#38388;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#35299;&#30721;&#65292;&#28982;&#21518;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#20010;&#30446;&#26631;&#12290;LGD&#36824;&#21487;&#20197;&#36890;&#36807;&#29305;&#27530;&#35774;&#35745;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22238;&#24402;&#21644;&#20998;&#31867;&#31561;&#39044;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#65288;&#26465;&#20214;&#65289;&#29983;&#25104;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;LGD&#33021;&#22815;&#36890;&#36807;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#26469;&#35299;&#20915;&#21508;&#32423;&#21035;&#21644;&#21508;&#31867;&#22411;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#25110;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results acr
&lt;/p&gt;</description></item><item><title>AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02370</link><description>&lt;p&gt;
AutoTimes: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02370
&lt;/p&gt;
&lt;p&gt;
AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#21457;&#23637;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#30456;&#20284;&#39034;&#24207;&#32467;&#26500;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#30340;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#23545;LLM&#28508;&#21147;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20174;&#35821;&#35328;&#24314;&#27169;&#20013;&#23398;&#21040;&#30340;&#36890;&#29992;&#20196;&#29260;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTimes&#65292;&#23558;LLM&#37325;&#26032;&#29992;&#20316;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36825;&#19982;LLM&#30340;&#33719;&#21462;&#21644;&#21033;&#29992;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#30340;&#31995;&#21015;&#38271;&#24230;&#65292;&#24182;&#23454;&#29616;&#19982;&#27969;&#34892;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
&lt;/p&gt;</description></item><item><title>NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.10112</link><description>&lt;p&gt;
NM-FlowGAN: &#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28151;&#21512;&#26041;&#27861;&#23545;sRGB&#22122;&#22768;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10112
&lt;/p&gt;
&lt;p&gt;
NM-FlowGAN&#26159;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#24314;&#27169;sRGB&#22122;&#22768;&#65292;&#24357;&#34917;&#20102;&#21333;&#19968;&#29983;&#25104;&#27169;&#22411;&#22266;&#26377;&#29305;&#24615;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#21512;&#25104;&#30495;&#23454;&#30340;sRGB&#22122;&#22768;&#23545;&#20110;&#21508;&#31181;&#20302;&#32423;&#21035;&#35270;&#35273;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26500;&#24314;&#29992;&#20110;&#35757;&#32451;&#22270;&#20687;&#21435;&#22122;&#31995;&#32479;&#30340;&#25968;&#25454;&#38598;&#12290;&#30495;&#23454;sRGB&#22122;&#22768;&#30340;&#20998;&#24067;&#26497;&#20026;&#22797;&#26434;&#65292;&#24182;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#20351;&#24471;&#20854;&#20934;&#30830;&#24314;&#27169;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30740;&#31350;&#30456;&#27604;&#20256;&#32479;&#30340;&#22122;&#22768;&#24314;&#27169;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;sRGB&#22122;&#22768;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NM-FlowGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;GAN&#21644;&#27491;&#35268;&#21270;&#27969;&#30340;&#20248;&#21183;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#37319;&#29992;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#20687;&#32032;&#32423;&#22122;&#22768;&#24314;&#27169;&#32593;&#32476;&#65292;&#20197;&#21450;&#22522;&#20110;GAN&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#24314;&#27169;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10112v2 Announce Type: replace-cross  Abstract: Modeling and synthesizing real sRGB noise is crucial for various low-level vision tasks, such as building datasets for training image denoising systems. The distribution of real sRGB noise is highly complex and affected by a multitude of factors, making its accurate modeling extremely challenging. Therefore, recent studies have proposed methods that employ data-driven generative models, such as generative adversarial networks (GAN) and Normalizing Flows. These studies achieve more accurate modeling of sRGB noise compared to traditional noise modeling methods. However, there are performance limitations due to the inherent characteristics of each generative model. To address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the strengths of both GAN and Normalizing Flows. We simultaneously employ a pixel-wise noise modeling network based on Normalizing Flows, and spatial correlation modeling networks based on GAN
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;</title><link>https://arxiv.org/abs/2311.08376</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#33218;&#30340;&#38598;&#25104;&#25277;&#26679;&#65306;&#23567;&#38598;&#25104;&#36275;&#30691;
&lt;/p&gt;
&lt;p&gt;
Ensemble sampling for linear bandits: small ensembles suffice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#29615;&#22659;&#20013;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#39318;&#27425;&#23454;&#29992;&#21644;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#37319;&#29992;&#35268;&#27169;&#20026;$d \log T$&#30340;&#38598;&#25104;&#25277;&#26679;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;$\sqrt{T}$&#38454;&#30340;&#21518;&#24724;&#65292;&#32780;&#19981;&#38656;&#35201;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#35774;&#23450;&#19979;&#30340;&#38598;&#25104;&#25277;&#26679;&#36827;&#34892;&#20102;&#26377;&#29992;&#19988;&#20005;&#35880;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#20132;&#20114;&#20316;&#29992;&#26102;&#38388;&#36328;&#24230;$T$&#30340;$d$&#32500;&#38543;&#26426;&#32447;&#24615;&#36172;&#33218;&#65292;&#37319;&#29992;&#38598;&#25104;&#22823;&#23567;&#20026;$\smash{d \log T}$&#30340;&#38598;&#25104;&#25277;&#26679;&#65292;&#36973;&#21463;&#30340;&#21518;&#24724;&#26368;&#22810;&#20026;$\smash{(d \log T)^{5/2} \sqrt{T}}$&#38454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#20219;&#20309;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#31532;&#19968;&#20010;&#19981;&#35201;&#27714;&#38598;&#25104;&#22823;&#23567;&#19982;$T$&#32447;&#24615;&#25193;&#23637;&#30340;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#38598;&#25104;&#25277;&#26679;&#22833;&#21435;&#24847;&#20041;&#65292;&#21516;&#26102;&#33719;&#24471;&#20102;&#25509;&#36817;$\smash{\sqrt{T}}$&#38454;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20063;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#26080;&#38480;&#21160;&#20316;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08376v2 Announce Type: replace-cross  Abstract: We provide the first useful and rigorous analysis of ensemble sampling for the stochastic linear bandit setting. In particular, we show that, under standard assumptions, for a $d$-dimensional stochastic linear bandit with an interaction horizon $T$, ensemble sampling with an ensemble of size of order $\smash{d \log T}$ incurs regret at most of the order $\smash{(d \log T)^{5/2} \sqrt{T}}$. Ours is the first result in any structured setting not to require the size of the ensemble to scale linearly with $T$ -- which defeats the purpose of ensemble sampling -- while obtaining near $\smash{\sqrt{T}}$ order regret. Ours is also the first result that allows infinite action sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10862</link><description>&lt;p&gt;
&#22522;&#20110;&#21098;&#26525;&#30340;&#20445;&#25252;: &#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#22686;&#21152;&#23545;&#40784;&#30340;LLMs&#30340;&#36234;&#29425;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning. (arXiv:2401.10862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21098;&#26525;&#23545;&#40784;&#30340;LLMs&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#21457;&#29616;&#21098;&#26525;LLM&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20854;&#25269;&#25239;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20854;&#20182;LLM&#34892;&#20026;&#20063;&#21487;&#33021;&#26377;&#26356;&#26222;&#36941;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26377;&#23475;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#35825;&#20351;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#21644;&#36829;&#27861;&#20869;&#23481;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#21098;&#26525;LLM&#21442;&#25968;&#22810;&#36798;20&#65285;&#21487;&#20197;&#26174;&#33879;&#22686;&#21152;&#23427;&#20204;&#23545;&#27492;&#31867;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#24182;&#19988;&#19981;&#25439;&#23475;&#20854;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21098;&#26525;&#21518;&#35266;&#23519;&#21040;&#30340;&#22686;&#24378;&#23433;&#20840;&#24615;&#19982;&#27169;&#22411;&#30340;&#21021;&#22987;&#23433;&#20840;&#35757;&#32451;&#27700;&#24179;&#30456;&#20851;&#65292;&#36825;&#26263;&#31034;&#21098;&#26525;&#30340;&#25928;&#26524;&#21487;&#33021;&#26356;&#26222;&#36941;&#65292;&#20063;&#21487;&#33021;&#36866;&#29992;&#20110;&#36229;&#20986;&#23433;&#20840;&#24615;&#33539;&#30068;&#30340;&#20854;&#20182;LLM&#34892;&#20026;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#31867;&#21035;&#12289;&#25554;&#20837;&#21040;&#21313;&#20010;&#19981;&#21516;&#36234;&#29425;&#25552;&#31034;&#20013;&#30340;225&#20010;&#26377;&#23475;&#20219;&#21153;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#65292;&#34920;&#26126;&#21098;&#26525;&#26377;&#21161;&#20110;LLMs&#38598;&#20013;&#27880;&#24847;&#21147;&#22312;&#36234;&#29425;&#25552;&#31034;&#20013;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#26631;&#35760;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#31361;&#20986;&#30340;&#32842;&#22825;&#27169;&#22411;&#65288;&#22914;LLaMA-2 Chat&#65292;Vicuna&#21644;Mistral Instruct&#65289;&#20855;&#26377;&#24456;&#39640;&#30340;&#26131;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type of attack that can coax these models into generating harmful and illegal content. In this paper, we show that pruning up to 20% of LLM parameters markedly increases their resistance to such attacks without additional training and without sacrificing their performance in standard benchmarks. Intriguingly, we discovered that the enhanced safety observed post-pruning correlates to the initial safety training level of the model, hinting that the effect of pruning could be more general and may hold for other LLM behaviors beyond safety. Additionally, we introduce a curated dataset of 225 harmful tasks across five categories, inserted into ten different Jailbreaking prompts, showing that pruning aids LLMs in concentrating attention on task-relevant tokens in jailbreaking prompts. Lastly, our experiments reveal that the prominent chat models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high susceptibi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2310.17638</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Fractional Diffusion Models. (arXiv:2310.17638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#34920;&#31034;&#20026;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#23454;&#29616;&#20102;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#25928;&#26524;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#65288;FBM&#65289;&#30340;&#36830;&#32493;&#26102;&#38388;&#26694;&#26550;&#25512;&#24191;&#21040;&#22522;&#20110;&#20998;&#25968;&#24067;&#26391;&#36816;&#21160;&#30340;&#36817;&#20284;&#24418;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;FBM&#34920;&#31034;&#20026;&#23478;&#26063;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#36807;&#31243;&#30340;&#38543;&#26426;&#31215;&#20998;&#65292;&#25512;&#23548;&#20986;&#36830;&#32493;&#20877;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;&#36870;&#26102;&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;&#20855;&#26377;&#39537;&#21160;&#22122;&#22768;&#25910;&#25947;&#21040;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#29983;&#25104;&#20998;&#25968;&#38454;&#25193;&#25955;&#27169;&#22411;&#65288;GFDM&#65289;&#12290;FBM&#30340;&#36203;&#26031;&#29305;&#25351;&#25968;$H \in (0,1)$ &#21487;&#20197;&#25511;&#21046;&#36335;&#24452;&#21464;&#25442;&#20998;&#24067;&#30340;&#31895;&#31961;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#20855;&#26377;&#26080;&#38480;&#20108;&#27425;&#21464;&#24046;&#30340;&#38543;&#26426;&#36807;&#31243;&#19978;&#24314;&#31435;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the continuous time framework for score-based generative models from an underlying Brownian motion (BM) to an approximation of fractional Brownian motion (FBM). We derive a continuous reparameterization trick and the reverse time model by representing FBM as a stochastic integral over a family of Ornstein-Uhlenbeck processes to define generative fractional diffusion models (GFDM) with driving noise converging to a non-Markovian process of infinite quadratic variation. The Hurst index $H\in(0,1)$ of FBM enables control of the roughness of the distribution transforming path. To the best of our knowledge, this is the first attempt to build a generative model upon a stochastic process with infinite quadratic variation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16965</link><description>&lt;p&gt;
&#25511;&#21046;&#32452;&#21512;&#20248;&#21270;&#30340;&#36830;&#32493;&#25918;&#26494;
&lt;/p&gt;
&lt;p&gt;
Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30456;&#23545;&#23494;&#38598;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#25152;&#26377;&#21464;&#37327;&#20026;&#38646;&#30340;&#23616;&#37096;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25214;&#21040;&#36817;&#20284;&#35299;&#30340;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNN&#65288;PI-GNN&#65289;&#27714;&#35299;&#22120;&#22312;&#22823;&#35268;&#27169;CO&#38382;&#39064;&#19978;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30456;&#23545;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#65292;&#36138;&#23146;&#31639;&#27861;&#30340;&#24615;&#33021;&#24694;&#21270;&#65292;&#20294;&#23545;&#20110;PI-GNN&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#21364;&#27809;&#26377;&#22826;&#22810;&#35752;&#35770;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PI-GNN&#27714;&#35299;&#22120;&#37319;&#29992;&#20102;&#25918;&#26494;&#31574;&#30053;&#65292;&#23398;&#20064;&#21518;&#38656;&#35201;&#20174;&#36830;&#32493;&#31354;&#38388;&#20154;&#24037;&#36716;&#25442;&#22238;&#21407;&#22987;&#31163;&#25955;&#31354;&#38388;&#65292;&#21487;&#33021;&#20250;&#30772;&#22351;&#35299;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;PI-GNN&#27714;&#35299;&#22120;&#22312;&#23494;&#38598;&#22270;&#19978;&#30340;CO&#38382;&#39064;&#30340;&#23398;&#20064;&#26089;&#26399;&#21487;&#33021;&#38519;&#20837;&#23616;&#37096;&#35299;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#25152;&#26377;&#21464;&#37327;&#37117;&#20026;&#38646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#36830;&#32493;&#24615;&#21644;&#31163;&#25955;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04459</link><description>&lt;p&gt;
&#23376;&#35789;&#20316;&#20026;&#25216;&#24039;&#65306;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#35789;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning. (arXiv:2309.04459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#24182;&#37319;&#29992;&#20998;&#35789;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#29983;&#25104;&#25216;&#24039;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20943;&#23569;&#25506;&#32034;&#30340;&#38590;&#24230;&#65292;&#24182;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#20855;&#26377;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#36890;&#36807;&#38271;&#26399;&#30340;&#12289;&#21327;&#35843;&#30340;&#34892;&#21160;&#24207;&#21015;&#25165;&#33021;&#33719;&#24471;&#20219;&#20309;&#22870;&#21169;&#12290;&#32780;&#19988;&#65292;&#22312;&#36830;&#32493;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#65292;&#21487;&#33021;&#30340;&#34892;&#21160;&#25968;&#37327;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#36825;&#21482;&#20250;&#22686;&#21152;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#31867;&#26041;&#27861;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#25910;&#38598;&#30340;&#20132;&#20114;&#25968;&#25454;&#20013;&#24418;&#25104;&#26102;&#38388;&#19978;&#24310;&#20280;&#30340;&#34892;&#21160;&#65292;&#36890;&#24120;&#31216;&#20026;&#25216;&#24039;&#65292;&#24182;&#22312;&#36825;&#20010;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#30340;&#20248;&#21270;&#12290;&#36890;&#24120;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#20013;&#38656;&#35201;&#19968;&#20010;&#28459;&#38271;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#24320;&#22987;&#20043;&#21069;&#24418;&#25104;&#25216;&#24039;&#12290;&#37492;&#20110;&#20808;&#21069;&#30340;&#35777;&#25454;&#34920;&#26126;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24182;&#19981;&#38656;&#35201;&#23436;&#25972;&#30340;&#36830;&#32493;&#34892;&#21160;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#24039;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#23558;&#34892;&#21160;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20511;&#37492;&#26469;&#30340;&#20998;&#35789;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration in sparse-reward reinforcement learning is difficult due to the requirement of long, coordinated sequences of actions in order to achieve any reward. Moreover, in continuous action spaces there are an infinite number of possible actions, which only increases the difficulty of exploration. One class of methods designed to address these issues forms temporally extended actions, often called skills, from interaction data collected in the same domain, and optimizes a policy on top of this new action space. Typically such methods require a lengthy pretraining phase, especially in continuous action spaces, in order to form the skills before reinforcement learning can begin. Given prior evidence that the full range of the continuous action space is not required in such tasks, we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#35299;&#30721;&#22120;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35299;&#30721;&#26102;&#38388;&#24182;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09463</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#30340;&#20302;&#28201;&#23384;&#20648;&#25935;&#21270;&#31070;&#32463;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Cryogenic Memristive Neural Decoder for Fault-tolerant Quantum Error Correction. (arXiv:2307.09463v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#65292;&#35813;&#35299;&#30721;&#22120;&#29992;&#20110;&#23481;&#38169;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35299;&#30721;&#26102;&#38388;&#24182;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;(QEC)&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#20998;&#31867;&#20174;&#38169;&#35823;&#32416;&#27491;&#32534;&#30721;&#20013;&#25552;&#21462;&#30340;&#32508;&#21512;&#24449;&#65292;&#24182;&#25214;&#21040;&#21512;&#36866;&#30340;&#24674;&#22797;&#25805;&#20316;&#21592;&#26469;&#20445;&#25252;&#36923;&#36753;&#20449;&#24687;&#20813;&#21463;&#38169;&#35823;&#24433;&#21709;&#12290;&#23613;&#31649;&#31070;&#32463;&#35299;&#30721;&#22120;&#24615;&#33021;&#33391;&#22909;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#30340;&#23454;&#38469;&#35201;&#27714;&#65292;&#22914;&#23558;&#35299;&#30721;&#26102;&#38388;&#26368;&#23567;&#21270;&#20197;&#28385;&#36275;&#37325;&#22797;&#38169;&#35823;&#32416;&#27491;&#26041;&#26696;&#20013;&#30340;&#32508;&#21512;&#24449;&#29983;&#25104;&#36895;&#24230;&#65292;&#20197;&#21450;&#30830;&#20445;&#35299;&#30721;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#38543;&#30528;&#32534;&#30721;&#36317;&#31163;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#35774;&#35745;&#19968;&#20010;&#19987;&#29992;&#30340;&#38598;&#25104;&#30005;&#36335;&#20197;&#19982;&#37327;&#23376;&#22788;&#29702;&#22120;&#20849;&#21516;&#23436;&#25104;&#35299;&#30721;&#20219;&#21153;&#20284;&#20046;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#23558;&#20449;&#21495;&#20174;&#20302;&#28201;&#29615;&#22659;&#20013;&#24341;&#20986;&#24182;&#36827;&#34892;&#22806;&#37096;&#22788;&#29702;&#20250;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#24310;&#36831;&#21644;&#26368;&#32456;&#30340;&#24067;&#32447;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#35745;&#31639;&#30340;&#31070;&#32463;&#35299;&#30721;&#22120;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#35774;&#35745;&#21644;&#24615;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural decoders for quantum error correction (QEC) rely on neural networks to classify syndromes extracted from error correction codes and find appropriate recovery operators to protect logical information against errors. Despite the good performance of neural decoders, important practical requirements remain to be achieved, such as minimizing the decoding time to meet typical rates of syndrome generation in repeated error correction schemes, and ensuring the scalability of the decoding approach as the code distance increases. Designing a dedicated integrated circuit to perform the decoding task in co-integration with a quantum processor appears necessary to reach these decoding time and scalability requirements, as routing signals in and out of a cryogenic environment to be processed externally leads to unnecessary delays and an eventual wiring bottleneck. In this work, we report the design and performance analysis of a neural decoder inference accelerator based on an in-memory comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01762</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#25552;&#32431;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer for adversarial purification. (arXiv:2306.01762v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#38450;&#24481;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#26041;&#26696;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;Transformer&#24494;&#35843;&#26469;&#25552;&#32431;&#23545;&#25239;&#26679;&#26412;&#65292;&#20351;&#20854;&#36924;&#36817;&#28165;&#27905;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#37096;&#32626;&#20026;&#21508;&#31181;&#26085;&#24120;&#26381;&#21153;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#36867;&#36991;&#25915;&#20987;&#26159;&#26368;&#26222;&#36941;&#30340;&#19968;&#31181;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#25110;&#21033;&#29992;&#22823;&#37327;&#28165;&#27905;&#25968;&#25454;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#20854;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#37096;&#32626;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23545;&#22312;&#32447;&#26381;&#21153;&#36896;&#25104;&#37325;&#22823;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#24403;&#26816;&#27979;&#21040;&#26576;&#31181;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#26102;&#65292;&#26381;&#21153;&#25552;&#20379;&#32773;&#21482;&#33021;&#33719;&#24471;&#26377;&#38480;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#32780;&#22823;&#37327;&#30340;&#28165;&#27905;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#21517;&#20026;RaPiD&#65288;Rapid Plug-in Defender&#65289;&#65292;&#26088;&#22312;&#24555;&#36895;&#38450;&#24481;&#20855;&#26377;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#31034;&#20363;&#38480;&#21046;&#30340;&#21407;&#22987;&#26381;&#21153;&#27169;&#22411;&#30340;&#26576;&#31181;&#25915;&#20987;&#12290;&#21463;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#36716;&#31227;&#23398;&#20064;&#33391;&#22909;&#21021;&#22987;&#21270;&#30340;&#36890;&#29992;&#36235;&#21183;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;Transformer&#26469;&#25552;&#32431;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#39044;&#35757;&#32451;&#30340;Transformer&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#40723;&#21169;&#25552;&#32431;&#21518;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#25509;&#36817;&#28165;&#26224;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RaPiD&#22312;&#38450;&#24481;&#21508;&#31181;&#20855;&#26377;&#38480;&#25968;&#25454;&#30340;&#25915;&#20987;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more deep neural networks being deployed as various daily services, their reliability is essential. It's frightening that deep neural networks are vulnerable and sensitive to adversarial attacks, the most common one of which for the services is evasion-based. Recent works usually strengthen the robustness by adversarial training or leveraging the knowledge of an amount of clean data. However, in practical terms, retraining and redeploying the model need a large computational budget, leading to heavy losses to the online service. In addition, when adversarial examples of a certain attack are detected, only limited adversarial examples are available for the service provider, while much clean data may not be accessible. Given the mentioned problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples. Motivated by the general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11512</link><description>&lt;p&gt;
&#20016;&#23500;&#35299;&#32544;&#32467;&#65306;&#20174;&#23450;&#20041;&#21040;&#24230;&#37327;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enriching Disentanglement: Definitions to Metrics. (arXiv:2305.11512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#24212;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#32467;&#34920;&#31034;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#20998;&#31163;&#22810;&#20010;&#21464;&#21270;&#22240;&#32032;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#23398;&#20064;&#21644;&#35780;&#20272;&#35299;&#32544;&#32467;&#34920;&#31034;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#30495;&#27491;&#37327;&#21270;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27604;&#36739;&#23427;&#20204;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#19968;&#38454;&#26041;&#31243;&#35859;&#35789;&#23450;&#20041;&#30340;&#35299;&#32544;&#32467;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20016;&#23500;&#33539;&#30068;&#35770;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#26041;&#31243;&#23450;&#20041;&#36716;&#21270;&#20026;&#20860;&#23481;&#30340;&#23450;&#37327;&#24230;&#37327;&#26631;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#29992;&#24230;&#37327;&#25110;&#31163;&#25955;&#24230;&#26367;&#25442;(i) &#31561;&#24335;&#65292;&#29992;&#25490;&#24207;&#25805;&#20316;&#26367;&#25442; (ii) &#36923;&#36753;&#32852;&#32467;&#35789;&#65292;&#29992;&#32858;&#21512;&#26367;&#25442; (iii) &#36890;&#29992;&#37327;&#35789;&#65292;&#29992;&#26368;&#20339;&#36924;&#36817;&#26367;&#25442; (iv) &#23384;&#22312;&#37327;&#35789;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#29992;&#20110;&#27979;&#37327;&#35299;&#32544;&#32467;&#34920;&#31034;&#25552;&#21462;&#22120;&#25152;&#38656;&#23646;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#23637;&#31034;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#33021;&#22815;&#23558;&#35299;&#32544;&#32467;&#34920;&#31034;&#23450;&#20041;&#36716;&#21270;&#20026;&#21487;&#27604;&#36739;&#30340;&#65292;&#24182;&#34913;&#37327;&#19968;&#31181;&#26041;&#27861;&#20013;&#35299;&#32544;&#32467;&#23646;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled representation learning is a challenging task that involves separating multiple factors of variation in complex data. Although various metrics for learning and evaluating disentangled representations have been proposed, it remains unclear what these metrics truly quantify and how to compare them. In this work, we study the definitions of disentanglement given by first-order equational predicates and introduce a systematic approach for transforming an equational definition into a compatible quantitative metric based on enriched category theory. Specifically, we show how to replace (i) equality with metric or divergence, (ii) logical connectives with order operations, (iii) universal quantifier with aggregation, and (iv) existential quantifier with the best approximation. Using this approach, we derive metrics for measuring the desired properties of a disentangled representation extractor and demonstrate their effectiveness on synthetic data. Our proposed approach provides p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2303.16510</link><description>&lt;p&gt;
&#22312;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#19981;&#21487;&#34892;&#30830;&#23450;&#24615;&#12289;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Infeasible Deterministic, Stochastic, and Variance-Reduction Algorithms for Optimization under Orthogonality Constraints. (arXiv:2303.16510v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#36845;&#20195;&#30340;Landing&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#30340;&#21516;&#26102;&#39034;&#30021;&#22320;&#21560;&#24341;&#21040;&#27491;&#20132;&#32422;&#26463;&#27969;&#24418;&#19978;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#31181;&#31639;&#27861;&#20197;&#25903;&#25345;&#26031;&#25176;&#33778;&#23572;&#65288;Stiefel&#65289;&#27969;&#24418;&#65292;&#24182;&#25552;&#20379;&#20102;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30456;&#21516;&#20294;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#32422;&#26463;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#37117;&#20250;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#20174;&#20027;&#25104;&#20998;&#20998;&#26512;&#21040;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#38382;&#39064;&#38656;&#35201;&#20351;&#29992;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#26469;&#27714;&#35299;&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21046;&#25191;&#34892;&#32422;&#26463;&#26102;&#26368;&#32791;&#36153;&#26102;&#38388;&#12290;&#26368;&#36817;&#65292;Ablin&#65286;Peyr\'e&#65288;2022&#65289;&#25552;&#20986;&#20102;Landing&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#24265;&#20215;&#36845;&#20195;&#26041;&#27861;&#65292;&#23427;&#19981;&#24378;&#21046;&#25191;&#34892;&#27491;&#20132;&#32422;&#26463;&#65292;&#20294;&#20250;&#20197;&#24179;&#28369;&#30340;&#26041;&#24335;&#21560;&#24341;&#21040;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;Landing&#31639;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#23454;&#29992;&#21644;&#29702;&#35770;&#21457;&#23637;&#12290;&#39318;&#20808;&#65292;&#35813;&#26041;&#27861;&#34987;&#25193;&#23637;&#21040;&#26031;&#25176;&#33778;&#23572;&#27969;&#24418;&#65292;&#21363;&#30697;&#24418;&#27491;&#20132;&#30697;&#38453;&#30340;&#38598;&#21512;&#12290;&#24403;&#25104;&#26412;&#20989;&#25968;&#26159;&#35768;&#22810;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#26102;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#38543;&#26426;&#21644;&#26041;&#24046;&#32422;&#20943;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#23427;&#20204;&#30340;&#40654;&#26364;&#20248;&#21270;&#31639;&#27861;&#30456;&#21516;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonality constraints naturally appear in many machine learning problems, from Principal Components Analysis to robust neural network training. They are usually solved using Riemannian optimization algorithms, which minimize the objective function while enforcing the constraint. However, enforcing the orthogonality constraint can be the most time-consuming operation in such algorithms. Recently, Ablin &amp; Peyr\'e (2022) proposed the Landing algorithm, a method with cheap iterations that does not enforce the orthogonality constraint but is attracted towards the manifold in a smooth manner. In this article, we provide new practical and theoretical developments for the landing algorithm. First, the method is extended to the Stiefel manifold, the set of rectangular orthogonal matrices. We also consider stochastic and variance reduction algorithms when the cost function is an average of many functions. We demonstrate that all these methods have the same rate of convergence as their Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.14658</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Russo&#21644;Xu&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35777;&#26126;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#19978;&#30028;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#8220;&#24930;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#30340;&#26399;&#26395;&#25910;&#25947;&#36895;&#24230;&#30340;&#24418;&#24335;&#20026;$O(\sqrt{\lambda/n})$&#65292;&#20854;&#20013;$\lambda$&#26159;&#19968;&#20123;&#20449;&#24687;&#29702;&#35770;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#21495;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#20351;&#29992;&#36825;&#20010;&#30028;&#38480;&#26469;&#24471;&#21040;$O(\lambda/n)$&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36798;&#21040;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;$(\eta,c)$-&#20013;&#24515;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#28857;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;MHP&#21442;&#25968;&#21644;&#35889;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2111.02062</link><description>&lt;p&gt;
&#36328;&#25968;&#25454;&#31890;&#24230;&#38142;&#25509;&#65306;&#25311;&#21512;&#22810;&#21464;&#37327;Hawkes&#36807;&#31243;&#21040;&#37096;&#20998;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data. (arXiv:2111.02062v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.02062
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#28857;&#36807;&#31243;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;MHP&#21442;&#25968;&#21644;&#35889;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;Hawkes&#36807;&#31243;(MHP)&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#26512;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#27969;&#65292;&#20854;&#20013;&#20107;&#20214;&#22312;&#33258;&#36523;&#32500;&#24230;&#20869;&#65288;&#36890;&#36807;&#33258;&#28608;&#65289;&#25110;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#65288;&#36890;&#36807;&#20132;&#21449;&#28608;&#21457;&#65289;&#29983;&#25104;&#26032;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#26576;&#20123;&#32500;&#24230;&#20013;&#30340;&#20010;&#21035;&#20107;&#20214;&#26102;&#38388;&#25139;&#26159;&#19981;&#21487;&#35266;&#27979;&#30340;&#65292;&#21482;&#26377;&#22312;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#20107;&#20214;&#35745;&#25968;&#26159;&#24050;&#30693;&#30340;&#65292;&#34987;&#31216;&#20026;&#37096;&#20998;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#12290;MHP&#19981;&#36866;&#29992;&#20110;&#22788;&#29702;&#36825;&#31181;&#25968;&#25454;&#65292;&#22240;&#20026;&#20854;&#20272;&#35745;&#38656;&#35201;&#20107;&#20214;&#26102;&#38388;&#25139;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Partial Mean Behavior Poisson (PMBP)&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#36807;&#31243;&#65292;&#19982;MHP&#20849;&#20139;&#21442;&#25968;&#31561;&#25928;&#24615;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26102;&#38388;&#25139;&#21644;&#21306;&#38388;&#23631;&#34109;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;PMBP&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PMBP&#36807;&#31243;&#21487;&#20197;&#36817;&#20284;MHP&#21442;&#25968;&#24182;&#24674;&#22797;&#35889;&#21322;&#24452;&#65292;&#20351;&#29992;&#21512;&#25104;&#20107;&#20214;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multivariate Hawkes process (MHP) is widely used for analyzing data streams that interact with each other, where events generate new events within their own dimension (via self-excitation) or across different dimensions (via cross-excitation). However, in certain applications, the timestamps of individual events in some dimensions are unobservable, and only event counts within intervals are known, referred to as partially interval-censored data. The MHP is unsuitable for handling such data since its estimation requires event timestamps. In this study, we introduce the Partial Mean Behavior Poisson (PMBP) process, a novel point process which shares parameter equivalence with the MHP and can effectively model both timestamped and interval-censored data. We demonstrate the capabilities of the PMBP process using synthetic and real-world datasets. Firstly, we illustrate that the PMBP process can approximate MHP parameters and recover the spectral radius using synthetic event histories. 
&lt;/p&gt;</description></item></channel></rss>