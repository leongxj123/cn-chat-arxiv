<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;</title><link>https://rss.arxiv.org/abs/2312.12616</link><description>&lt;p&gt;
&#22312;&#32447;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Variational Sequential Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.12616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#22312;&#32447;VSMC&#65292;&#23427;&#22522;&#20110;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#25968;&#25454;&#27969;&#26102;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#26159;AI&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#32463;&#20856;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;&#21442;&#25968;&#23398;&#20064;&#25110;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#65292;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#22797;&#26434;&#30340;&#28508;&#22312;&#29366;&#24577;&#21518;&#39564;&#20998;&#24067;&#12290;&#26412;&#25991;&#22312;&#21464;&#20998;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;VSMC&#65289;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#31890;&#23376;&#26041;&#27861;&#21644;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#21644;&#36125;&#21494;&#26031;&#28508;&#22312;&#29366;&#24577;&#25512;&#26029;&#12290;&#20256;&#32479;&#30340;VSMC&#26041;&#27861;&#22312;&#31163;&#32447;&#27169;&#24335;&#19979;&#36816;&#34892;&#65292;&#36890;&#36807;&#37325;&#22797;&#22788;&#29702;&#32473;&#23450;&#30340;&#25968;&#25454;&#25209;&#27425;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#23558;VSMC&#20195;&#29702;ELBO&#30340;&#26799;&#24230;&#36924;&#36817;&#20998;&#24067;&#21040;&#26102;&#38388;&#19978;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#27969;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#21517;&#20026;&#22312;&#32447;VSMC&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#31890;&#23376;&#25552;&#35758;&#36866;&#24212;&#65292;&#32780;&#19988;&#23436;&#20840;&#23454;&#26102;&#22788;&#29702;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Being the most classical generative model for serial data, state-space models (SSM) are fundamental in AI and statistical machine learning. In SSM, any form of parameter learning or latent state inference typically involves the computation of complex latent-state posteriors. In this work, we build upon the variational sequential Monte Carlo (VSMC) method, which provides computationally efficient and accurate model parameter estimation and Bayesian latent-state inference by combining particle methods and variational inference. While standard VSMC operates in the offline mode, by re-processing repeatedly a given batch of data, we distribute the approximation of the gradient of the VSMC surrogate ELBO in time using stochastic approximation, allowing for online learning in the presence of streams of data. This results in an algorithm, online VSMC, that is capable of performing efficiently, entirely on-the-fly, both parameter estimation and particle proposal adaptation. In addition, we prov
&lt;/p&gt;</description></item><item><title>Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19887</link><description>&lt;p&gt;
Jamba: &#19968;&#20010;&#28151;&#21512;Transformer-Mamba&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jamba: A Hybrid Transformer-Mamba Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19887
&lt;/p&gt;
&lt;p&gt;
Jamba&#26159;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;Transformer-Mamba&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21333;&#20010;80GB GPU&#19978;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#23545;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#21644;&#38271;&#19978;&#19979;&#25991;&#35780;&#20272;&#20855;&#26377;state-of-the-art&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Jamba&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26032;&#39062;&#30340;&#28151;&#21512;Transformer-Mamba&#28151;&#21512;&#19987;&#23478;(MoE)&#26550;&#26500;&#30340;&#26032;&#22522;&#30784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Jamba&#20132;&#38169;&#20351;&#29992;Transformer&#21644;Mamba&#23618;&#65292;&#20174;&#20004;&#31181;&#27169;&#22411;&#23478;&#26063;&#20013;&#33719;&#30410;&#12290;MoE&#34987;&#28155;&#21152;&#22312;&#20854;&#20013;&#19968;&#20123;&#23618;&#20013;&#65292;&#20197;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27963;&#36291;&#21442;&#25968;&#30340;&#21487;&#25511;&#24615;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#26550;&#26500;&#20801;&#35768;&#29305;&#23450;&#36164;&#28304;&#21644;&#30446;&#26631;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19887v1 Announce Type: new  Abstract: We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transfor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08171</link><description>&lt;p&gt;
&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#21487;&#22788;&#29702;&#30340;&#23616;&#37096;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tractable Local Equilibria in Non-Concave Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21644;&#20854;&#20182;&#26080;&#24724;&#23398;&#20064;&#31243;&#24207;&#21487;&#20197;&#26377;&#25928;&#22320;&#25910;&#25947;&#21040;&#21327;&#35843;&#22343;&#34913;&#65292;&#22312;&#27599;&#20010;Agent&#30340;&#25928;&#29992;&#23545;&#20110;&#20854;&#33258;&#36523;&#31574;&#30053;&#21576;&#20985;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#24403;&#25928;&#29992;&#26159;&#38750;&#20985;&#30340;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20854;&#20013;Agent&#30340;&#31574;&#30053;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#25110;&#32773;Agent&#30340;&#25928;&#29992;&#30001;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#65292;&#25110;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#23454;&#38469;&#19978;&#65292;&#38750;&#20985;&#28216;&#25103;&#23384;&#22312;&#19968;&#31995;&#21015;&#21338;&#24328;&#35770;&#21644;&#20248;&#21270;&#25361;&#25112;&#65306;(i) Nash&#22343;&#34913;&#21487;&#33021;&#19981;&#23384;&#22312;&#65307;(ii) &#23616;&#37096;Nash&#22343;&#34913;&#23384;&#22312;&#20294;&#26159;&#19981;&#21487;&#22788;&#29702;&#65307;(iii) &#28151;&#21512;Nash&#12289;&#21327;&#35843;&#21644;&#31895;&#31961;&#21327;&#35843;&#22343;&#34913;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20855;&#26377;&#26080;&#38480;&#25903;&#25345;&#65292;&#24182;&#19988;&#26159;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#20026;&#20102;&#36991;&#24320;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#31216;&#20026;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#35813;&#27010;&#24565;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#27010;&#25324;&#20102;&#23616;&#37096;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08171v1 Announce Type: cross  Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.03777</link><description>&lt;p&gt;
ENOT&#65306;&#26399;&#26395;&#22238;&#24402;&#29992;&#20110;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#30340;&#24555;&#36895;&#21644;&#20934;&#30830;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ENOT: Expectile Regularization for Fast and Accurate Training of Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03777
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#22238;&#24402;&#27491;&#21017;&#21270;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#20248;&#21270;&#20256;&#36755;&#65288;NOT&#65289;&#35757;&#32451;&#31243;&#24207;&#25193;&#23637;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#20849;&#36717;&#21183;&#27491;&#21017;&#21270;&#33021;&#22815;&#20934;&#30830;&#21644;&#39640;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;&#36755;&#36816;&#26041;&#26696;&#12290;&#29616;&#26377;NOT&#27714;&#35299;&#22120;&#30340;&#20027;&#35201;&#29942;&#39048;&#22312;&#20110;&#25214;&#21040;&#20849;&#36717;&#31639;&#23376;&#65288;&#21363;c-transform&#65289;&#30340;&#25509;&#36817;&#31934;&#30830;&#36817;&#20284;&#30340;&#36807;&#31243;&#65292;&#36825;&#35201;&#20040;&#36890;&#36807;&#20248;&#21270;&#26368;&#23567;-&#26368;&#22823;&#30446;&#26631;&#65292;&#35201;&#20040;&#36890;&#36807;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#23545;&#21021;&#22987;&#36817;&#20284;&#39044;&#27979;&#30340;&#31934;&#32454;&#35843;&#25972;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#12289;&#22312;&#26399;&#26395;&#22238;&#24402;&#24418;&#24335;&#19978;&#24378;&#21046;&#36866;&#24212;&#24615;&#26465;&#20214;&#20110;&#23398;&#20064;&#23545;&#20598;&#21183;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#21270;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#21487;&#33021;&#20849;&#36717;&#21183;&#20998;&#24067;&#30340;&#19978;&#38480;&#20272;&#35745;&#65292;&#24182;&#20351;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#65292;&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#24191;&#27867;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03777v1 Announce Type: cross  Abstract: We present a new extension for Neural Optimal Transport (NOT) training procedure, capable of accurately and efficiently estimating optimal transportation plan via specific regularisation on conjugate potentials. The main bottleneck of existing NOT solvers is associated with the procedure of finding a near-exact approximation of the conjugate operator (i.e., the c-transform), which is done either by optimizing over maximin objectives or by the computationally-intensive fine-tuning of the initial approximated prediction. We resolve both issues by proposing a new, theoretically justified loss in the form of expectile regularization that enforces binding conditions on the learning dual potentials. Such a regularization provides the upper bound estimation over the distribution of possible conjugate potentials and makes the learning stable, eliminating the need for additional extensive finetuning. We formally justify the efficiency of our me
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.00849</link><description>&lt;p&gt;
NeuraLUT: &#22312;Boolean&#21512;&#25104;&#20989;&#25968;&#20013;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#23494;&#24230;
&lt;/p&gt;
&lt;p&gt;
NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00849
&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;&#20102;FPGA&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#20013;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20877;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21152;&#36895;&#22120;&#24050;&#32463;&#35777;&#26126;&#22312;&#22788;&#29702;&#24310;&#36831;&#21644;&#36164;&#28304;&#20851;&#38190;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#26029;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#31070;&#32463;&#32593;&#32476;&#20013;&#35745;&#31639;&#23494;&#38598;&#24230;&#26368;&#39640;&#30340;&#25805;&#20316;&#20043;&#19968;&#26159;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#20043;&#38388;&#30340;&#28857;&#31215;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;FPGA&#21152;&#36895;&#24037;&#20316;&#25552;&#20986;&#23558;&#20855;&#26377;&#37327;&#21270;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#31070;&#32463;&#20803;&#30452;&#25509;&#26144;&#23556;&#21040;&#26597;&#25214;&#34920;&#65288;LUTs&#65289;&#20197;&#36827;&#34892;&#30828;&#20214;&#23454;&#29616;&#12290;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#65292;&#31070;&#32463;&#20803;&#30340;&#36793;&#30028;&#19982;LUTs&#30340;&#36793;&#30028;&#37325;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#25918;&#23485;&#36825;&#20123;&#36793;&#30028;&#65292;&#23558;&#25972;&#20010;&#23376;&#32593;&#32476;&#26144;&#23556;&#21040;&#21333;&#20010;LUT&#12290;&#30001;&#20110;&#23376;&#32593;&#32476;&#34987;&#21560;&#25910;&#21040;LUT&#20013;&#65292;&#20998;&#21306;&#20869;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#21644;&#31934;&#24230;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#30340;&#26597;&#25214;&#34920;&#30340;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#20998;&#21306;&#20869;&#20351;&#29992;&#20855;&#26377;&#28014;&#28857;&#31934;&#24230;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#36825;&#20123;&#23618;&#21463;&#30410;&#20110;&#25104;&#20026;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#39044;&#27979;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32467;&#26500;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.17506</link><description>&lt;p&gt;
&#28909;&#21147;&#23398;&#23548;&#21521;&#31232;&#32570;&#26102;&#38388;&#21160;&#24577;&#25968;&#25454;&#30340;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Thermodynamics-informed super-resolution of scarce temporal dynamics data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#39044;&#27979;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32467;&#26500;&#20445;&#25345;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#21487;&#26377;&#25928;&#35299;&#20915;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#65292;&#24182;&#30830;&#20445;&#28385;&#36275;&#28909;&#21147;&#23398;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20102;&#28909;&#21147;&#23398;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#29289;&#29702;&#31995;&#32479;&#27979;&#37327;&#20998;&#36776;&#29575;&#24182;&#38543;&#21518;&#39044;&#27979;&#20854;&#26102;&#38388;&#28436;&#21270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23558;&#23436;&#25972;&#27169;&#22411;&#30340;&#32500;&#24230;&#38477;&#20302;&#21040;&#19968;&#32452;&#28508;&#21464;&#37327;&#65292;&#36825;&#20123;&#28508;&#21464;&#37327;&#34987;&#24378;&#21046;&#21305;&#37197;&#20808;&#39564;&#65292;&#20363;&#22914;&#27491;&#24577;&#20998;&#24067;&#12290;&#23545;&#25239;&#33258;&#21160;&#32534;&#30721;&#22120;&#34987;&#35270;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35757;&#32451;&#20197;&#20174;&#20302;&#20998;&#36776;&#29575;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#26679;&#26412;&#65292;&#20063;&#23601;&#26159;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#31532;&#20108;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#28508;&#21464;&#37327;&#30340;&#29289;&#29702;&#32467;&#26500;&#24182;&#39044;&#27979;&#20854;&#26102;&#38388;&#28436;&#21270;&#12290;&#36825;&#20010;&#31070;&#32463;&#32593;&#32476;&#34987;&#31216;&#20026;&#20445;&#25345;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#23398;&#20064;&#31995;&#32479;&#30340;metriplectic&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#29289;&#29702;&#20559;&#24046;&#20197;&#30830;&#20445;&#28909;&#21147;&#23398;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#23450;&#24459;&#34987;&#36981;&#23432;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17506v1 Announce Type: cross  Abstract: We present a method to increase the resolution of measurements of a physical system and subsequently predict its time evolution using thermodynamics-aware neural networks. Our method uses adversarial autoencoders, which reduce the dimensionality of the full order model to a set of latent variables that are enforced to match a prior, for example a normal distribution. Adversarial autoencoders are seen as generative models, and they can be trained to generate high-resolution samples from low-resoution inputs, meaning they can address the so-called super-resolution problem. Then, a second neural network is trained to learn the physical structure of the latent variables and predict their temporal evolution. This neural network is known as an structure-preserving neural network. It learns the metriplectic-structure of the system and applies a physical bias to ensure that the first and second principles of thermodynamics are fulfilled. The i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.15171</link><description>&lt;p&gt;
&#29992;&#20110;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#30340;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Covariance-Adaptive Least-Squares Algorithm for Stochastic Combinatorial Semi-Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#26041;&#24046;&#33258;&#36866;&#24212;&#30340;&#26368;&#23567;&#20108;&#20056;&#31639;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#33719;&#24471;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#29305;&#21035;&#22312;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#33021;&#26377;&#25928;&#22320;&#21033;&#29992;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#22312;&#21508;&#31181;&#21442;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#29609;&#23478;&#21487;&#20197;&#20174;&#21253;&#21547;d&#20010;&#22522;&#26412;&#39033;&#30340;P&#20010;&#23376;&#38598;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;CUCB&#12289;ESCB&#12289;OLS-UCB&#65289;&#38656;&#35201;&#23545;&#22870;&#21169;&#20998;&#24067;&#26377;&#20808;&#39564;&#30693;&#35782;&#65292;&#27604;&#22914;&#23376;&#39640;&#26031;&#20195;&#29702;-&#26041;&#24046;&#30340;&#19978;&#30028;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20272;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;OLS-UCB&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#29256;&#26412;&#65292;&#20381;&#36182;&#20110;&#21327;&#26041;&#24046;&#32467;&#26500;&#30340;&#22312;&#32447;&#20272;&#35745;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#20272;&#35745;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31995;&#25968;&#35201;&#23481;&#26131;&#24471;&#22810;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#22522;&#20110;&#20195;&#29702;&#26041;&#24046;&#30340;&#31639;&#27861;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#24403;&#21327;&#26041;&#24046;&#31995;&#25968;&#20840;&#20026;&#38750;&#36127;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#21322;&#33218;&#21453;&#39304;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#32769;&#34382;&#26426;&#21453;&#39304;&#26041;&#27861;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;P&#8811;d&#20197;&#21450;P&#8804;d&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#19968;&#28857;&#24182;&#19981;&#26469;&#33258;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15171v1 Announce Type: new  Abstract: We address the problem of stochastic combinatorial semi-bandits, where a player can select from P subsets of a set containing d base items. Most existing algorithms (e.g. CUCB, ESCB, OLS-UCB) require prior knowledge on the reward distribution, like an upper bound on a sub-Gaussian proxy-variance, which is hard to estimate tightly. In this work, we design a variance-adaptive version of OLS-UCB, relying on an online estimation of the covariance structure. Estimating the coefficients of a covariance matrix is much more manageable in practical settings and results in improved regret upper bounds compared to proxy variance-based algorithms. When covariance coefficients are all non-negative, we show that our approach efficiently leverages the semi-bandit feedback and provably outperforms bandit feedback approaches, not only in exponential regimes where P $\gg$ d but also when P $\le$ d, which is not straightforward from most existing analyses.
&lt;/p&gt;</description></item><item><title>SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.14961</link><description>&lt;p&gt;
&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Elastic Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14961
&lt;/p&gt;
&lt;p&gt;
SEAC&#26159;&#19968;&#31181;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#25511;&#21046;&#39057;&#29575;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#36890;&#24120;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20197;&#20197;&#22266;&#23450;&#25511;&#21046;&#39057;&#29575;&#25191;&#34892;&#21160;&#20316;&#30340;&#25511;&#21046;&#22120;&#12290;&#37492;&#20110;RL&#31639;&#27861;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#23427;&#20204;&#23545;&#25511;&#21046;&#39057;&#29575;&#30340;&#36873;&#25321;&#30340;&#24433;&#21709;&#35270;&#32780;&#19981;&#35265;&#65306;&#25214;&#21040;&#27491;&#30830;&#30340;&#25511;&#21046;&#39057;&#29575;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#38169;&#35823;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#24230;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#29978;&#33267;&#23548;&#33268;&#26080;&#27861;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36719;&#24377;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;SEAC&#65289;, &#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;SEAC&#23454;&#29616;&#20102;&#24377;&#24615;&#26102;&#38388;&#27493;&#38271;&#65292;&#21363;&#20855;&#26377;&#24050;&#30693;&#21464;&#21270;&#25345;&#32493;&#26102;&#38388;&#30340;&#26102;&#38388;&#27493;&#38271;&#65292;&#20801;&#35768;&#20195;&#29702;&#26681;&#25454;&#24773;&#20917;&#25913;&#21464;&#20854;&#25511;&#21046;&#39057;&#29575;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;SEAC&#20165;&#22312;&#24517;&#35201;&#26102;&#24212;&#29992;&#25511;&#21046;&#65292;&#26368;&#23567;&#21270;&#35745;&#31639;&#36164;&#28304;&#21644;&#25968;&#25454;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SEAC&#22312;&#29275;&#39039;&#36816;&#21160;&#23398;&#36855;&#23467;&#23548;&#33322;&#20219;&#21153;&#21644;&#19977;&#32500;&#36187;&#36710;&#35270;&#39057;&#28216;&#25103;Trackmania&#20013;&#30340;&#33021;&#21147;&#12290;SEAC&#22312;&#34920;&#29616;&#19978;&#20248;&#20110;SAC&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14961v1 Announce Type: cross  Abstract: Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate. Given the discrete nature of RL algorithms, they are oblivious to the effects of the choice of control rate: finding the correct control rate can be difficult and mistakes often result in excessive use of computing resources or even lack of convergence.   We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue. SEAC implements elastic time steps, time steps with a known, variable duration, which allow the agent to change its control frequency to adapt to the situation. In practice, SEAC applies control only when necessary, minimizing computational resources and data usage.   We evaluate SEAC's capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;</title><link>https://arxiv.org/abs/2402.13228</link><description>&lt;p&gt;
Smaug&#65306;&#20351;&#29992;DPO-Positive&#20462;&#22797;&#20559;&#22909;&#20248;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13228
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;DPO-Positive&#65288;DPOP&#65289;&#65292;&#20197;&#36991;&#20813;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#20013;&#28508;&#22312;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#26174;&#33879;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#24635;&#32467;&#21644;&#23545;&#40784;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290; DPO&#20351;&#29992;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#25968;&#25454;&#23545;&#27169;&#22411;&#36873;&#25321;&#19968;&#20010;&#21709;&#24212;&#32780;&#19981;&#26159;&#21478;&#19968;&#20010;&#30340;&#8220;&#30456;&#23545;&#8221;&#27010;&#29575;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#34920;&#26126;&#65292;&#21482;&#35201;&#39318;&#36873;&#21644;&#38750;&#39318;&#36873;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#23545;&#27010;&#29575;&#22686;&#21152;&#65292;&#26631;&#20934;DPO&#25439;&#22833;&#23601;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#39318;&#36873;&#31034;&#20363;&#30340;&#21487;&#33021;&#24615;&#38477;&#20302;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#24403;&#22312;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#23436;&#25104;&#20043;&#38388;&#30340;&#32534;&#36753;&#36317;&#31163;&#36739;&#30701;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20250;&#20986;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DPO-Positive&#65288;DPOP&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#22833;&#36133;&#27169;&#24335;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;DPOP&#26126;&#26174;&#20248;&#20110;DPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13228v1 Announce Type: cross  Abstract: Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13210</link><description>&lt;p&gt;
Bayesian Reward Models for LLM Alignment
&lt;/p&gt;
&lt;p&gt;
Bayesian Reward Models for LLM Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22238;&#22797;&#26377;&#30410;&#19988;&#26080;&#27602;&#65292;&#36890;&#24120;&#25105;&#20204;&#20250;&#22312;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#21518;&#25105;&#20204;&#36873;&#25321;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#31574;&#30053;&#22238;&#22797;&#65288;best-of-n&#25277;&#26679;&#65289;&#65292;&#25110;&#32773;&#36827;&#19968;&#27493;&#20248;&#21270;&#31574;&#30053;&#20197;&#29983;&#25104;&#20855;&#26377;&#39640;&#22870;&#21169;&#30340;&#22238;&#22797;&#65288;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#25110;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36873;&#23450;&#30340;&#22238;&#22797;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#32780;&#20855;&#26377;&#39640;&#22870;&#21169;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#20559;&#22909;&#12290;&#36825;&#19968;&#38382;&#39064;&#22312;&#25552;&#31034;&#25110;&#22238;&#22797;&#20559;&#31163;&#35757;&#32451;&#25968;&#25454;&#26102;&#23588;&#20026;&#20005;&#37325;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26102;&#20250;&#20135;&#29983;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Laplace-LoRA&#65288;Yang&#31561;&#20154;&#65292;2024&#65289;&#35757;&#32451;&#20102;&#36125;&#21494;&#26031;&#22870;&#21169;&#27169;&#22411;&#65292;&#21457;&#29616;&#30001;&#27492;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#20197;&#25104;&#21151;&#32531;&#35299;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13210v1 Announce Type: new  Abstract: To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimi
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10086</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Safe and Trustworthy Autonomous Driving: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10086
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;AI&#25216;&#26415;&#23545;&#20110;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#23433;&#20840;&#38382;&#39064;&#21644;&#20449;&#20219;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#26041;&#24335;&#65292;&#20998;&#26512;&#20102;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#22312;&#28385;&#36275;&#33258;&#21160;&#39550;&#39542;&#35201;&#27714;&#26041;&#38754;&#30340;&#20851;&#38190;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#12289;&#36741;&#21161;&#25216;&#26415;&#21644;&#35299;&#37322;&#30340;&#21487;&#35270;&#21270;&#31561;&#20116;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20854;&#22312;&#24863;&#30693;&#21644;&#35268;&#21010;&#20219;&#21153;&#20013;&#30456;&#23545;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#65288;AD&#65289;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38590;&#20197;&#29702;&#35299;&#30340;AI&#31995;&#32479;&#21152;&#21095;&#20102;&#23545;AD&#23433;&#20840;&#20445;&#35777;&#30340;&#29616;&#26377;&#25361;&#25112;&#12290;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20851;&#20110;&#21487;&#35299;&#37322;&#26041;&#27861;&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20840;&#38754;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#22312;AD&#32972;&#26223;&#19979;AI&#30340;&#35201;&#27714;&#65292;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#26426;&#26500;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;XAI&#23545;&#20110;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#20013;&#35299;&#37322;&#30340;&#26469;&#28304;&#65292;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;XAI&#30340;&#20998;&#31867;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;XAI&#22312;&#23433;&#20840;&#21487;&#20449;&#30340;AD&#20013;&#30340;&#20116;&#20010;&#20027;&#35201;&#36129;&#29486;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#35774;&#35745;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#27169;&#22411;&#12289;&#21487;&#35299;&#37322;&#30340;&#30417;&#25511;&#65292;&#36741;&#21161;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10086v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) shows promising applications for the perception and planning tasks in autonomous driving (AD) due to its superior performance compared to conventional methods. However, inscrutable AI systems exacerbate the existing challenge of safety assurance of AD. One way to mitigate this challenge is to utilize explainable AI (XAI) techniques. To this end, we present the first comprehensive systematic literature review of explainable methods for safe and trustworthy AD. We begin by analyzing the requirements for AI in the context of AD, focusing on three key aspects: data, model, and agency. We find that XAI is fundamental to meeting these requirements. Based on this, we explain the sources of explanations in AI and describe a taxonomy of XAI. We then identify five key contributions of XAI for safe and trustworthy AI in AD, which are interpretable design, interpretable surrogate models, interpretable monitoring, auxil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.05369</link><description>&lt;p&gt;
&#20197;&#26174;&#24335;&#22870;&#21169;&#30340;&#22122;&#22768;&#23545;&#27604;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Alignment of Language Models with Explicit Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#36890;&#29992;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#33021;&#22815;&#22788;&#29702;&#26126;&#30830;&#27880;&#37322;&#30340;&#22870;&#21169;&#25968;&#25454;&#65292;&#24182;&#19988;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#24847;&#22270;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#38656;&#35201;&#22312;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26102;&#26368;&#22823;&#21270;&#30340;&#35780;&#20272;&#22870;&#21169;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#22914;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#65288;DPO&#65289;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38544;&#21547;&#23450;&#20041;&#32780;&#38750;&#26126;&#30830;&#32473;&#23450;&#22870;&#21169;&#30340;&#20004;&#20004;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;LM&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#65288;NCE&#65289;&#26469;&#35299;&#20915;&#26126;&#30830;&#27880;&#37322;&#26377;&#26631;&#37327;&#35780;&#20272;&#30340;&#22870;&#21169;&#25968;&#25454;&#22788;&#29702;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#24182;&#34892;&#31639;&#27861;&#65292;NCA&#21644;InfoNCA&#65292;&#20004;&#32773;&#37117;&#33021;&#20174;&#22870;&#21169;&#25968;&#25454;&#21644;&#20559;&#22909;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;LM&#31574;&#30053;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DPO&#25439;&#22833;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;InfoNCA&#30446;&#26631;&#22312;&#20004;&#20004;&#20559;&#22909;&#35774;&#32622;&#19979;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#32780;&#38598;&#25104;&#21644;&#25193;&#23637;&#20102;&#24403;&#21069;&#30340;&#23545;&#40784;&#29702;&#35770;&#12290;&#36890;&#36807;&#23545;&#27604;NCA&#21644;InfoNCA&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;InfoNCA&#21644;DPO&#22914;&#20309;&#22312;&#19981;&#21516;&#21709;&#24212;&#23545;&#20110;&#21333;&#20010;&#25351;&#20196;&#30340;&#30456;&#23545;&#21487;&#33021;&#24615;&#19978;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03214</link><description>&lt;p&gt;
&#26377;&#26426;&#25110;&#25193;&#25955;&#65306;&#25105;&#20204;&#33021;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12289;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#20197;&#21450;&#19987;&#19994;&#33402;&#26415;&#23478;&#30340;&#30693;&#35782;&#12290;&#36825;&#23545;&#38450;&#27490;&#27450;&#35784;&#12289;&#36981;&#23432;&#25919;&#31574;&#20197;&#21450;&#36991;&#20813;&#27169;&#22411;&#23849;&#28291;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22270;&#20687;&#30340;&#20986;&#29616;&#23436;&#20840;&#39072;&#35206;&#20102;&#33402;&#26415;&#30028;&#12290;&#20174;&#20154;&#31867;&#33402;&#26415;&#20013;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20854;&#24433;&#21709;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#19981;&#26029;&#22686;&#21152;&#12290;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#20250;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#32773;&#27450;&#35784;&#37027;&#20123;&#25903;&#20184;&#39640;&#20215;&#36141;&#20080;&#20154;&#31867;&#33402;&#26415;&#21697;&#30340;&#20010;&#20154;&#21644;&#31105;&#27490;&#20351;&#29992;AI&#22270;&#20687;&#30340;&#20844;&#21496;&#12290;&#36825;&#23545;&#20110;&#38656;&#35201;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#20197;&#36991;&#20813;&#28508;&#22312;&#27169;&#22411;&#23849;&#28291;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#32773;&#26469;&#35828;&#20063;&#33267;&#20851;&#37325;&#35201;&#12290;&#21306;&#20998;&#20154;&#31867;&#33402;&#26415;&#21644;AI&#22270;&#20687;&#30340;&#26041;&#27861;&#26377;&#22810;&#31181;&#65292;&#21253;&#25324;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#24037;&#20855;&#65292;&#20197;&#21450;&#36890;&#36807;&#19987;&#19994;&#33402;&#26415;&#23478;&#21033;&#29992;&#20182;&#20204;&#23545;&#33402;&#26415;&#25216;&#24039;&#30340;&#30693;&#35782;&#36827;&#34892;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;7&#31181;&#39118;&#26684;&#30340;&#30495;&#23454;&#20154;&#31867;&#33402;&#26415;&#65292;&#20174;5&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#20102;&#19982;&#20043;&#21305;&#37197;&#30340;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;8&#20010;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors 
&lt;/p&gt;</description></item><item><title>&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01781</link><description>&lt;p&gt;
&#24403;&#22522;&#20934;&#25104;&#20026;&#30446;&#26631;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25490;&#34892;&#27036;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01781
&lt;/p&gt;
&lt;p&gt;
&#20381;&#36182;&#22522;&#20934;&#25490;&#34892;&#27036;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#23384;&#22312;&#36739;&#39640;&#25935;&#24863;&#24615;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#20250;&#23548;&#33268;&#25490;&#21517;&#30340;&#26174;&#33879;&#21464;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#31572;&#26696;&#36873;&#25321;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#20934;&#25490;&#21517;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#25490;&#34892;&#27036;&#32463;&#24120;&#34987;&#29992;&#26469;&#25351;&#23548;&#23454;&#36341;&#32773;&#22312;&#27169;&#22411;&#36873;&#25321;&#20013;&#12290;&#36890;&#24120;&#65292;&#21457;&#24067;&#30340;&#25490;&#34892;&#27036;&#25490;&#21517;&#34987;&#30452;&#25509;&#25509;&#21463; - &#25105;&#20204;&#34920;&#26126;&#36825;&#26159;&#19968;&#20010;&#65288;&#28508;&#22312;&#26114;&#36149;&#30340;&#65289;&#38169;&#35823;&#12290;&#22312;&#29616;&#26377;&#30340;&#25490;&#34892;&#27036;&#19979;&#65292;LLM&#30340;&#30456;&#23545;&#24615;&#33021;&#23545;&#65288;&#36890;&#24120;&#24494;&#23567;&#30340;&#65289;&#32454;&#33410;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#27969;&#34892;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22522;&#20934;&#65288;&#20363;&#22914;MMLU&#65289;&#65292;&#23545;&#22522;&#20934;&#30340;&#24494;&#23567;&#25200;&#21160;&#65292;&#22914;&#25913;&#21464;&#36873;&#39033;&#39034;&#24207;&#25110;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#65292;&#20250;&#23548;&#33268;&#25490;&#21517;&#21464;&#21270;&#36798;&#21040;8&#20010;&#20301;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#25200;&#21160;&#31867;&#21035;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#24182;&#30830;&#23450;&#36825;&#19968;&#34892;&#20026;&#30340;&#26469;&#28304;&#26469;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#20960;&#20010;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#65292;&#21253;&#25324;&#36873;&#25321;&#20248;&#21270;&#30340;&#28151;&#21512;&#35780;&#20998;&#26041;&#27861;&#26469;&#36827;&#34892;&#31572;&#26696;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20381;&#36182;&#31616;&#21333;&#22522;&#20934;&#35780;&#20272;&#30340;&#39118;&#38505;&#65292;&#24182;&#20026;&#26356;&#20581;&#22766;&#30340;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#25351;&#23548;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) leaderboards based on benchmark rankings are regularly used to guide practitioners in model selection. Often, the published leaderboard rankings are taken at face value - we show this is a (potentially costly) mistake. Under existing leaderboards, the relative performance of LLMs is highly sensitive to (often minute) details. We show that for popular multiple choice question benchmarks (e.g. MMLU) minor perturbations to the benchmark, such as changing the order of choices or the method of answer selection, result in changes in rankings up to 8 positions. We explain this phenomenon by conducting systematic experiments over three broad categories of benchmark perturbations and identifying the sources of this behavior. Our analysis results in several best-practice recommendations, including the advantage of a hybrid scoring method for answer selection. Our study highlights the dangers of relying on simple benchmark evaluations and charts the path for more robust
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#19981;&#20844;&#24320;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#20513;&#35758;&#21644;&#39033;&#30446;&#26500;&#24605;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#35299;&#20915;&#38544;&#31169;&#21644;&#27861;&#35268;&#38382;&#39064;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.17653</link><description>&lt;p&gt;
&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A primer on synthetic health data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17653
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#19981;&#20844;&#24320;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#20513;&#35758;&#21644;&#39033;&#30446;&#26500;&#24605;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#35299;&#20915;&#38544;&#31169;&#21644;&#27861;&#35268;&#38382;&#39064;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#21019;&#36896;&#36924;&#30495;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#21512;&#25104;&#25968;&#25454;&#38598;&#26088;&#22312;&#22312;&#19981;&#20844;&#24320;&#30149;&#20154;&#36523;&#20221;&#25110;&#25935;&#24863;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20445;&#30041;&#20174;&#25935;&#24863;&#20581;&#24247;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#30340;&#29305;&#24449;&#12289;&#27169;&#24335;&#21644;&#24635;&#20307;&#31185;&#23398;&#32467;&#35770;&#12290;&#22240;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20419;&#36827;&#23433;&#20840;&#25968;&#25454;&#20849;&#20139;&#65292;&#25903;&#25345;&#19968;&#31995;&#21015;&#20513;&#35758;&#65292;&#21253;&#25324;&#24320;&#21457;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#12289;&#20808;&#36827;&#30340;&#20581;&#24247;IT&#24179;&#21488;&#20197;&#21450;&#19968;&#33324;&#39033;&#30446;&#26500;&#24605;&#21644;&#20551;&#35774;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#38382;&#39064;&#21644;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21253;&#25324;&#22914;&#20309;&#19982;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#33268;&#30340;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#24615;&#21644;&#39044;&#27979;&#25928;&#29992;&#65292;&#20197;&#21450;&#20998;&#20139;&#26102;&#23545;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#39069;&#22806;&#30340;&#27861;&#35268;&#21644;&#27835;&#29702;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#35299;&#20915;&#12290;&#22312;&#36825;&#20010;&#21021;&#25506;&#20013;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#26803;&#29702;&#65292;&#21253;&#25324;&#29983;&#25104;&#21644;&#35780;&#20272;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#21450;&#29616;&#26377;&#37096;&#32626;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep generative models have greatly expanded the potential to create realistic synthetic health datasets. These synthetic datasets aim to preserve the characteristics, patterns, and overall scientific conclusions derived from sensitive health datasets without disclosing patient identity or sensitive information. Thus, synthetic data can facilitate safe data sharing that supports a range of initiatives including the development of new predictive models, advanced health IT platforms, and general project ideation and hypothesis development. However, many questions and challenges remain, including how to consistently evaluate a synthetic dataset's similarity and predictive utility in comparison to the original real dataset and risk to privacy when shared. Additional regulatory and governance issues have not been widely addressed. In this primer, we map the state of synthetic health data, including generation and evaluation methods and tools, existing examples of deployme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.15623</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#29992;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Injecting linguistic knowledge into BERT for Dialogue State Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#23558;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#21040;BERT&#20013;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#19982;&#23545;&#35805;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;DST&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#25512;&#29702;&#36807;&#31243;&#32570;&#20047;&#36879;&#26126;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26694;&#26550;&#25552;&#21462;&#35821;&#35328;&#30693;&#35782;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#22686;&#24378;BERT&#22312;DST&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#30693;&#35782;&#25552;&#21462;&#36807;&#31243;&#35745;&#31639;&#32463;&#27982;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#27880;&#37322;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27880;&#20837;&#25552;&#21462;&#30340;&#30693;&#35782;&#21482;&#38656;&#35201;&#28155;&#21152;&#31616;&#21333;&#30340;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#20984;&#22810;&#38754;&#20307;&#27169;&#22411;(CPM)&#20316;&#20026;DST&#20219;&#21153;&#30340;&#29305;&#24449;&#25552;&#21462;&#24037;&#20855;&#65292;&#24182;&#34920;&#26126;&#25152;&#33719;&#21462;&#30340;&#29305;&#24449;&#19982;&#23545;&#35805;&#20013;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#27169;&#24335;&#30456;&#20851;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#20840;&#38754;&#29702;&#35299;&#24433;&#21709;DST&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#35821;&#35328;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;DST&#20219;&#21153;&#19978;&#23545;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) models often employ intricate neural network architectures, necessitating substantial training data, and their inference processes lack transparency. This paper proposes a method that extracts linguistic knowledge via an unsupervised framework and subsequently utilizes this knowledge to augment BERT's performance and interpretability in DST tasks. The knowledge extraction procedure is computationally economical and does not necessitate annotations or additional training data. The injection of the extracted knowledge necessitates the addition of only simple neural modules. We employ the Convex Polytopic Model (CPM) as a feature extraction tool for DST tasks and illustrate that the acquired features correlate with the syntactic and semantic patterns in the dialogues. This correlation facilitates a comprehensive understanding of the linguistic features influencing the DST model's decision-making process. We benchmark this framework on various DST tasks and ob
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.05661</link><description>&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering a Prompt Engineer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#20219;&#21153;&#23545;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;PE2&#26041;&#27861;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#30340;&#27880;&#20837;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#26159;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23450;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#26816;&#26597;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#20551;&#35774;&#24403;&#21069;&#25552;&#31034;&#20013;&#32570;&#23569;&#25110;&#35823;&#23548;&#20102;&#20160;&#20040;&#65292;&#24182;&#28165;&#26224;&#22320;&#20256;&#36798;&#20219;&#21153;&#65292;&#38656;&#35201;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#25552;&#31034;&#26469;&#25191;&#34892;&#33258;&#21160;&#25552;&#31034;&#24037;&#31243;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#30001;&#20110;&#20803;&#25552;&#31034;&#20013;&#32570;&#20047;&#22797;&#26434;&#25512;&#29702;&#30340;&#20805;&#20998;&#25351;&#23548;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35814;&#32454;&#25551;&#36848;&#12289;&#19978;&#19979;&#25991;&#35268;&#33539;&#21644;&#36880;&#27493;&#25512;&#29702;&#27169;&#26495;&#27880;&#20837;&#21040;&#20803;&#25552;&#31034;&#20013;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#31216;&#20026;PE2&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#20013;&#20986;&#33394;&#30340;&#36866;&#29992;&#24615;&#12290;&#23427;&#25214;&#21040;&#30340;&#25552;&#31034;&#22312;MultiArith&#19978;&#27604;&#8220;&#25353;&#27493;&#39588;&#24605;&#32771;&#8221;&#39640;&#20986;6.3%&#65292;&#22312;GSM8K&#19978;&#39640;&#20986;3.1%&#65292;&#24182;&#22312;&#23545;&#31435;&#20219;&#21153;&#19978;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05661v2 Announce Type: replace-cross  Abstract: Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models on customized tasks. It requires complex reasoning to examine the model's errors, hypothesize what is missing or misleading in the current prompt, and communicate the task with clarity. While recent works indicate that large language models can be meta-prompted to perform automatic prompt engineering, we argue that their potential is limited due to insufficient guidance for complex reasoning in the meta-prompt. We fill this gap by infusing into the meta-prompt three key components: detailed descriptions, context specification, and a step-by-step reasoning template. The resulting method, named PE2, showcases remarkable versatility across diverse language tasks. It finds prompts that outperform "let's think step by step" by 6.3% on MultiArith and 3.1% on GSM8K, and outperforms competitive baselines on counterfactual tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;</title><link>http://arxiv.org/abs/2401.10545</link><description>&lt;p&gt;
&#29702;&#35299;ChatGPT&#22522;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#65306;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#26102;&#38388;&#31283;&#23450;&#24615;&#21644;&#26368;&#26032;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency. (arXiv:2401.10545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10545
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;ChatGPT&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#31995;&#32479;&#35282;&#33394;&#21644;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;GPT-based&#27169;&#22411;&#20542;&#21521;&#20110;&#25512;&#33616;&#26368;&#26032;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;RecLLMs&#65289;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#32454;&#24494;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;ChatGPT&#30340;&#31995;&#32479;&#12290;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#22312;&#30005;&#24433;&#25512;&#33616;&#20013;&#30340;&#24046;&#24322;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20027;&#35201;&#35843;&#26597;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#21450;&#20854;&#23545;&#25512;&#33616;&#36136;&#37327;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#20379;&#24212;&#21830;&#20844;&#24179;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#31283;&#23450;&#24615;&#12289;&#27969;&#34892;&#31867;&#22411;&#21644;&#26102;&#25928;&#24615;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;RecLLMs&#20013;&#24341;&#20837;&#29305;&#23450;&#30340;&#8220;&#31995;&#32479;&#35282;&#33394;&#8221;&#21644;&#8220;&#25552;&#31034;&#31574;&#30053;&#8221;&#26174;&#33879;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22522;&#20110;&#35282;&#33394;&#30340;&#25552;&#31034;&#21487;&#20197;&#22686;&#24378;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20943;&#36731;&#27969;&#34892;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#33021;&#19982;&#20256;&#32479;&#21327;&#21516;&#36807;&#28388;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#21305;&#37197;&#65292;&#20294;&#23427;&#20204;&#20542;&#21521;&#20110;&#25512;&#33616;&#26356;&#26032;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#30005;&#24433;&#27969;&#27966;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-base
&lt;/p&gt;
&lt;p&gt;
This study explores the nuanced capabilities and inherent biases of Recommender Systems using Large Language Models (RecLLMs), with a focus on ChatGPT-based systems. It studies into the contrasting behaviors of generative models and traditional collaborative filtering models in movie recommendations. The research primarily investigates prompt design strategies and their impact on various aspects of recommendation quality, including accuracy, provider fairness, diversity, stability, genre dominance, and temporal freshness (recency).  Our experimental analysis reveals that the introduction of specific 'system roles' and 'prompt strategies' in RecLLMs significantly influences their performance. For instance, role-based prompts enhance fairness and diversity in recommendations, mitigating popularity bias. We find that while GPT-based models do not always match the performance of CF baselines, they exhibit a unique tendency to recommend newer and more diverse movie genres. Notably, GPT-base
&lt;/p&gt;</description></item><item><title>RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.08847</link><description>&lt;p&gt;
RIDGE: &#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and Efficiency Assessment of Medical Image Segmentation Models. (arXiv:2401.08847v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08847
&lt;/p&gt;
&lt;p&gt;
RIDGE&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#24037;&#20316;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#22312;&#31185;&#23398;&#21487;&#38752;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#19978;&#37117;&#20855;&#22791;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#21487;&#37325;&#22797;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#20020;&#24202;&#20013;&#30340;&#24212;&#29992;&#12290;&#22270;&#20687;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#20043;&#19968;&#65292;&#38656;&#35201;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;/&#20307;&#31215;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RIDGE&#28165;&#21333;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#23436;&#25972;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#25928;&#29575;&#30340;&#26694;&#26550;&#12290;&#35813;&#28165;&#21333;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#25552;&#39640;&#20854;&#24037;&#20316;&#30340;&#36136;&#37327;&#21644;&#36879;&#26126;&#24230;&#65292;&#30830;&#20445;&#20998;&#21106;&#27169;&#22411;&#19981;&#20165;&#20855;&#26377;&#31185;&#23398;&#30340;&#21487;&#38752;&#24615;&#65292;&#36824;&#20855;&#26377;&#20020;&#24202;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques, despite their potential, often suffer from a lack of reproducibility and generalizability, impeding their clinical adoption. Image segmentation is one of the critical tasks in medical image analysis, in which one or several regions/volumes of interest should be annotated. This paper introduces the RIDGE checklist, a framework for assessing the Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of deep learning-based medical image segmentation models. The checklist serves as a guide for researchers to enhance the quality and transparency of their work, ensuring that segmentation models are not only scientifically sound but also clinically relevant.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#26469;&#36827;&#34892;&#21327;&#20316;&#25512;&#26029;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#35774;&#22791;&#19978;AI&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#36890;&#36807;&#25552;&#20379;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#21644;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#32479;&#19968;&#34394;&#25311;&#21270;&#35270;&#22270;&#21644;&#36328;&#21160;&#24577;/&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#65292;&#20854;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.08637</link><description>&lt;p&gt;
&#36890;&#36807;MCU&#19978;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#21160;&#24577;&#32452;&#21512;&#23454;&#29616;&#21327;&#20316;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collaborative Inference via Dynamic Composition of Tiny AI Accelerators on MCUs. (arXiv:2401.08637v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#26469;&#36827;&#34892;&#21327;&#20316;&#25512;&#26029;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22312;&#35774;&#22791;&#19978;AI&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#36890;&#36807;&#25552;&#20379;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#21644;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#30340;&#32479;&#19968;&#34394;&#25311;&#21270;&#35270;&#22270;&#21644;&#36328;&#21160;&#24577;/&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#65292;&#20854;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#20986;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26497;&#38480;&#36793;&#32536;&#19978;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#24310;&#36831;&#12289;&#36739;&#20302;&#30340;&#21151;&#32791;&#25104;&#26412;&#21644;&#25913;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36825;&#20123;&#21152;&#36895;&#22120;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22914;&#26377;&#38480;&#30340;&#20869;&#23384;&#21644;&#21333;&#35774;&#22791;&#28966;&#28857;&#65292;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Synergy&#65292;&#19968;&#20010;&#33021;&#22815;&#20026;&#22810;&#31199;&#25143;&#27169;&#22411;&#21160;&#24577;&#32452;&#21512;&#24494;&#22411;AI&#21152;&#36895;&#22120;&#30340;&#31995;&#32479;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#23545;&#20110;&#35774;&#22791;&#19978;AI&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#26102;tinyML&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;Synergy&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#20854;&#25552;&#20379;&#20102;&#34394;&#25311;&#35745;&#31639;&#31354;&#38388;&#65292;&#20026;&#36164;&#28304;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#34394;&#25311;&#21270;&#35270;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#29289;&#29702;&#35774;&#22791;&#30340;&#39640;&#25928;&#20219;&#21153;&#26144;&#23556;&#12290;Synergy&#30340;&#36816;&#34892;&#26102;&#32534;&#25490;&#27169;&#22359;&#30830;&#20445;&#20102;&#36328;&#21160;&#24577;&#21644;&#24322;&#26500;&#21152;&#36895;&#22120;&#30340;&#26368;&#20339;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;Synergy&#30340;&#21534;&#21520;&#37327;&#24179;&#22343;&#25552;&#21319;&#20102;8.0&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of tiny AI accelerators opens opportunities for deep neural network deployment at the extreme edge, offering reduced latency, lower power cost, and improved privacy in on-device ML inference. Despite these advancements, challenges persist due to inherent limitations of these accelerators, such as restricted onboard memory and single-device focus. This paper introduces Synergy, a system that dynamically composes tiny AI accelerators for multi-tenant models, effectively addressing tinyML's critical challenges for the increasing demand for on-device AI. A key feature of Synergy is its virtual computing space, providing a unified, virtualized view of resources and enabling efficient task mapping to physical devices. Synergy's runtime orchestration module ensures optimal inference across dynamic and heterogeneous accelerators. Our evaluations with 7 baselines and 8 models demonstrate that Synergy improves throughput by an average of 8.0X compared to baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDE&#30340;Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#65292;&#24182;&#24314;&#31435;&#20102;&#20174;&#26893;&#34987;PDE&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;</title><link>http://arxiv.org/abs/2312.16762</link><description>&lt;p&gt;
Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDEs
&lt;/p&gt;
&lt;p&gt;
Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;$2\times 2$&#21452;&#26354;PDE&#30340;Backstepping&#31070;&#32463;&#25805;&#20316;&#21592;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#65292;&#24182;&#24314;&#31435;&#20102;&#20174;&#26893;&#34987;PDE&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#38750;&#32447;&#24615;&#25805;&#20316;&#21592;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;DeepONet&#65292;&#22312;&#21333;&#20010;Goursat&#24418;&#24335;&#30340;PDE&#25511;&#21046;&#21333;&#20010;&#21453;&#39304;&#22686;&#30410;&#20989;&#25968;&#30340;PDE&#21453;&#21521;&#35774;&#35745;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#33021;&#21147;&#12290;&#22312;&#32806;&#21512;PDE&#30340;&#36793;&#30028;&#25511;&#21046;&#20013;&#65292;&#32806;&#21512;&#30340;Goursat&#24418;&#24335;PDE&#25511;&#21046;&#20004;&#20010;&#25110;&#22810;&#20010;&#22686;&#30410;&#26680; - &#36825;&#26159;&#36804;&#20170;&#20026;&#27490;DeepONet&#26410;&#35299;&#20915;&#30340;PDE&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#20010;&#31616;&#21333;&#30340;&#36870;&#21521;&#20256;&#25773;$2\times 2$&#32806;&#21512;&#31995;&#32479;&#26469;&#25171;&#24320;&#36229;&#21367;&#31215;$2\times 2$&#26680;PDE&#31995;&#32479;&#30340;&#36817;&#20284;&#20027;&#39064;&#65292;&#20854;&#25511;&#21046;&#20013;&#20986;&#29616;&#20102;Goursat&#24418;&#24335;&#30340;PDE&#31995;&#32479;&#12290;&#24212;&#29992;&#21253;&#25324;&#30707;&#27833;&#38075;&#20117;&#12289;&#27973;&#27700;&#27874;&#30340;Saint-Venant&#27169;&#22411;&#20197;&#21450;&#23494;&#38598;&#20132;&#36890;&#27969;&#20013;&#30340;Aw-Rascle-Zhang&#27169;&#22411;&#30340;&#20572;&#36710;&#21644;&#34892;&#39542;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20174;&#65288;&#24635;&#20849;&#20116;&#20010;&#65289;&#26893;&#34987;PDE&#30340;&#21151;&#33021;&#31995;&#25968;&#21040;&#26680;PDE&#35299;&#30340;&#26144;&#23556;&#30340;&#36830;&#32493;&#24615;&#65292;&#35777;&#26126;&#20102;DeepONet&#36924;&#36817;&#26680;PDE&#35299;&#30340;&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;
Deep neural network approximation of nonlinear operators, commonly referred to as DeepONet, has proven capable of approximating PDE backstepping designs in which a single Goursat-form PDE governs a single feedback gain function. In boundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more gain kernels -- a PDE structure unaddressed thus far with DeepONet. In this note, we open the subject of approximating systems of gain kernel PDEs for hyperbolic PDE plants by considering a simple counter-convecting $2\times 2$ coupled system in whose control a $2\times 2$ kernel PDE systems in Goursat form arises. Applications include oil drilling, Saint-Venant model of shallow water waves, and Aw-Rascle-Zhang model of stop-and-go instability in congested traffic flow. In this paper we establish the continuity of the mapping from (a total of five) plant PDE functional coefficients to the kernel PDE solutions, prove the existence of an arbitrarily close DeepONet approximation to t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;</title><link>http://arxiv.org/abs/2311.00285</link><description>&lt;p&gt;
Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space Detection Approach. (arXiv:2311.00285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Mixture-of-Experts&#29992;&#20110;&#24320;&#25918;&#22495;&#36866;&#24212;&#30340;&#21452;&#31354;&#38388;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#36866;&#24212;&#65288;OSDA&#65289;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#21644;&#26631;&#31614;&#20559;&#31227;&#65292;&#23454;&#29616;&#23545;&#24050;&#30693;&#31867;&#21035;&#30340;&#31934;&#30830;&#20998;&#31867;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#22495;&#20013;&#35782;&#21035;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;OSDA&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#32456;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#33410;&#38408;&#20540;&#65292;&#24182;&#19988;&#21487;&#33021;&#23558;&#26410;&#30693;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;&#24050;&#30693;&#31867;&#21035;&#12290;Mixture-of-Expert&#65288;MoE&#65289;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;MoE&#20013;&#65292;&#19981;&#21516;&#30340;&#19987;&#23478;&#22788;&#29702;&#19981;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#22312;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20013;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#29983;&#25104;&#29420;&#29305;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#20063;&#21487;&#20197;&#26174;&#31034;&#19982;&#24050;&#30693;&#31867;&#21035;&#19981;&#21516;&#30340;&#19987;&#23478;&#36335;&#30001;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31354;&#38388;&#26816;&#27979;&#65292;&#21033;&#29992;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#21644;&#36335;&#30001;&#29305;&#24449;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#26469;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#30340;&#26679;&#26412;&#65292;&#26080;&#38656;&#20219;&#20309;&#38408;&#20540;&#12290;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#22270;&#24418;&#36335;&#30001;&#22120;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#25688;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Set Domain Adaptation (OSDA) aims to cope with the distribution and label shifts between the source and target domains simultaneously, performing accurate classification for known classes while identifying unknown class samples in the target domain. Most existing OSDA approaches, depending on the final image feature space of deep models, require manually-tuned thresholds, and may easily misclassify unknown samples as known classes. Mixture-of-Expert (MoE) could be a remedy. Within an MoE, different experts address different input features, producing unique expert routing patterns for different classes in a routing feature space. As a result, unknown class samples may also display different expert routing patterns to known classes. This paper proposes Dual-Space Detection, which exploits the inconsistencies between the image feature space and the routing feature space to detect unknown class samples without any threshold. Graph Router is further introduced to better make use of the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10649</link><description>&lt;p&gt;
&#29992;&#20110;&#27714;&#35299;Wasserstein Lagrangian&#27969;&#30340;&#35745;&#31639;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#22788;&#29702;&#19981;&#21516;&#30340;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#27169;&#25311;&#36712;&#36857;&#25110;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#30340;&#22522;&#30784;&#20960;&#20309;&#65288;&#21160;&#33021;&#65289;&#21644;&#23494;&#24230;&#36335;&#24452;&#30340;&#27491;&#21017;&#21270;&#65288;&#21183;&#33021;&#65289;&#65292;&#21487;&#20197;&#23545;&#26368;&#20248;&#36755;&#36816;&#30340;&#21160;&#21147;&#23398;&#24418;&#24335;&#36827;&#34892;&#25512;&#24191;&#12290;&#36825;&#20123;&#32452;&#21512;&#20135;&#29983;&#19981;&#21516;&#30340;&#21464;&#20998;&#38382;&#39064;&#65288;Lagrangians&#65289;&#65292;&#28085;&#30422;&#20102;&#35768;&#22810;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22914;Schr&#246;dinger&#26725;&#12289;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21644;&#24102;&#26377;&#29289;&#29702;&#32422;&#26463;&#30340;&#26368;&#20248;&#36755;&#36816;&#31561;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;&#26368;&#20248;&#23494;&#24230;&#36335;&#24452;&#26159;&#26410;&#30693;&#30340;&#65292;&#35299;&#20915;&#36825;&#20123;&#21464;&#20998;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20511;&#21161;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#24418;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#25311;&#25110;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#36712;&#36857;&#65292;&#20063;&#19981;&#38656;&#35201;&#35775;&#38382;&#26368;&#20248;&#32806;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#36890;&#36807;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;R2SL&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#27719;&#24635;&#25968;&#25454;&#26469;&#25429;&#25417;&#21306;&#22495;&#32593;&#32476;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;QoS&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05988</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#26041;&#27861;&#65306;&#21033;&#29992;&#21306;&#22495;&#32593;&#32476;&#30456;&#20284;&#24615;&#36827;&#34892;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Dual Latent State Learning Approach: Exploiting Regional Network Similarities for QoS Prediction. (arXiv:2310.05988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;R2SL&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#27719;&#24635;&#25968;&#25454;&#26469;&#25429;&#25417;&#21306;&#22495;&#32593;&#32476;&#34892;&#20026;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;QoS&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#21306;&#22495;&#20869;&#30340;&#20010;&#20307;&#23545;&#35937;&#65292;&#26080;&#35770;&#26159;&#29992;&#25143;&#36824;&#26159;&#26381;&#21153;&#65292;&#36890;&#24120;&#30001;&#20110;&#23427;&#20204;&#26469;&#33258;&#21516;&#19968;&#22478;&#24066;&#25110;&#33258;&#27835;&#31995;&#32479;&#65288;AS&#65289;&#65292;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#32593;&#32476;&#29366;&#24577;&#12290;&#23613;&#31649;&#23384;&#22312;&#21306;&#22495;&#32593;&#32476;&#30456;&#20284;&#24615;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#24573;&#35270;&#20102;&#20854;&#28508;&#21147;&#65292;&#23548;&#33268;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#32780;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#21452;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#32593;&#32476;&#65288;R2SL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20811;&#26381;&#20256;&#32479;&#22522;&#20110;&#20010;&#20307;&#23545;&#35937;&#30340;QoS&#39044;&#27979;&#25216;&#26415;&#30340;&#32570;&#28857;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;R2SL&#36890;&#36807;&#20174;&#20844;&#20849;&#21306;&#22495;&#27719;&#24635;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21306;&#22495;&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#65306;&#22478;&#24066;&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#21644;AS&#32593;&#32476;&#28508;&#22312;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;R2SL&#37319;&#29992;&#20102;&#22686;&#24378;&#30340;Huber&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual objects, whether users or services, within a specific region often exhibit similar network states due to their shared origin from the same city or autonomous system (AS). Despite this regional network similarity, many existing techniques overlook its potential, resulting in subpar performance arising from challenges such as data sparsity and label imbalance. In this paper, we introduce the regional-based dual latent state learning network(R2SL), a novel deep learning framework designed to overcome the pitfalls of traditional individual object-based prediction techniques in Quality of Service (QoS) prediction. Unlike its predecessors, R2SL captures the nuances of regional network behavior by deriving two distinct regional network latent states: the city-network latent state and the AS-network latent state. These states are constructed utilizing aggregated data from common regions rather than individual object data. Furthermore, R2SL adopts an enhanced Huber loss function that
&lt;/p&gt;</description></item><item><title>LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.04673</link><description>&lt;p&gt;
LauraGPT&#65306;&#20351;&#29992;GPT&#36827;&#34892;&#21548;&#12289;&#20851;&#27880;&#12289;&#29702;&#35299;&#21644;&#20877;&#29983;&#38899;&#39057;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT. (arXiv:2310.04673v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04673
&lt;/p&gt;
&lt;p&gt;
LauraGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558;&#31867;&#20284;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20197;&#21069;&#25552;&#20986;&#30340;&#29992;&#20110;&#38899;&#39057;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#32570;&#20047;&#20805;&#20998;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#35201;&#20040;&#23616;&#38480;&#20110;&#35782;&#21035;&#21644;&#29702;&#35299;&#38899;&#39057;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#35201;&#20040;&#26126;&#26174;&#19981;&#21450;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65288;SOTA&#65289;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LauraGPT&#65292;&#19968;&#20010;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#32479;&#19968;GPT&#27169;&#22411;&#12290;LauraGPT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#38899;&#39057;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#22312;&#20219;&#24847;&#27169;&#24335;&#19979;&#29983;&#25104;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#36827;&#34892;&#19982;&#20869;&#23481;&#12289;&#35821;&#20041;&#12289;&#35821;&#38899;&#23398;&#21644;&#38899;&#39057;&#20449;&#21495;&#20998;&#26512;&#30456;&#20851;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#20854;&#20013;&#19968;&#20123;&#20540;&#24471;&#27880;&#24847;&#30340;&#20219;&#21153;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#22686;&#24378;&#12289;&#33258;&#21160;&#38899;&#39057;&#25429;&#33719;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks. However, there has been limited research on applying similar frameworks to audio tasks. Previously proposed large language models for audio tasks either lack sufficient quantitative evaluations, or are limited to tasks for recognizing and understanding audio content, or significantly underperform existing state-of-the-art (SOTA) models. In this paper, we propose LauraGPT, a unified GPT model for audio recognition, understanding, and generation. LauraGPT is a versatile language model that can process both audio and text inputs and generate outputs in either modalities. It can perform a wide range of tasks related to content, semantics, paralinguistics, and audio-signal analysis. Some of its noteworthy tasks include automatic speech recognition, speech-to-text translation, text-to-speech synthesis, machine translation, speech enhancement, automated audio capt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04218</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#30340;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence Classes with the same Skeleton. (arXiv:2310.04218v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22266;&#23450;&#21442;&#25968;&#21487;&#22788;&#29702;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#25968;&#20855;&#26377;&#30456;&#21516;&#39592;&#26550;&#30340;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;&#20063;&#31216;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#65289;&#26159;&#32534;&#30721;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#22312;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#65292;&#38543;&#26426;&#21464;&#37327;&#34987;&#24314;&#27169;&#20026;&#26377;&#21521;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#19988;&#35268;&#23450;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#22312;&#32473;&#23450;&#20854;&#29238;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#19982;&#20854;&#31062;&#20808;&#33410;&#28857;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21516;&#19968;&#32452;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#20004;&#20010;&#19981;&#21516;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#21487;&#20197;&#20934;&#30830;&#32534;&#30721;&#30456;&#21516;&#30340;&#19968;&#32452;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#65292;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31561;&#20215;&#31867;&#34987;&#31216;&#20026;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#31867;&#65288;MEC&#65289;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#23545;&#20110;MEC&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20123;&#32654;&#20029;&#30340;&#32452;&#21512;&#29305;&#24449;&#65292;&#24182;&#19988;&#24050;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#21516;&#19968;MEC&#20013;&#30340;&#25152;&#26377;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;&#24517;&#39035;&#20855;&#26377;&#30456;&#21516;&#30340;&#8220;&#39592;&#26550;&#8221;&#65288;&#24213;&#23618;&#26080;&#21521;&#22270;&#65289;&#21644;v-&#32467;&#26500;&#65288;&#24418;&#24335;&#20026;$a\rightarrow b \leftarrow c$&#30340;&#35825;&#23548;&#23376;&#22270;&#65289;&#12290;&#36825;&#20123;&#32452;&#21512;&#29305;&#24449;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#33258;&#28982;&#30340;&#31639;&#27861;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal DAGs (also known as Bayesian networks) are a popular tool for encoding conditional dependencies between random variables. In a causal DAG, the random variables are modeled as vertices in the DAG, and it is stipulated that every random variable is independent of its ancestors conditioned on its parents. It is possible, however, for two different causal DAGs on the same set of random variables to encode exactly the same set of conditional dependencies. Such causal DAGs are said to be Markov equivalent, and equivalence classes of Markov equivalent DAGs are known as Markov Equivalent Classes (MECs). Beautiful combinatorial characterizations of MECs have been developed in the past few decades, and it is known, in particular that all DAGs in the same MEC must have the same ''skeleton'' (underlying undirected graph) and v-structures (induced subgraph of the form $a\rightarrow b \leftarrow c$).  These combinatorial characterizations also suggest several natural algorithmic questions. On
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.14507</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#23454;&#29616;&#32467;&#26500;&#21270;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35889;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#35889;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65306;&#23427;&#36890;&#36807;&#23545;&#35266;&#27979;&#36827;&#34892;&#36866;&#24403;&#39044;&#22788;&#29702;&#24471;&#21040;&#30340;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;&#23613;&#31649;&#35889;&#20272;&#35745;&#22120;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#32467;&#26500;&#21270;&#65288;&#21363;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#21644;&#21704;&#23572;&#65289;&#35774;&#35745;&#65292;&#30446;&#21069;&#20165;&#26377;&#23545;&#35889;&#20272;&#35745;&#22120;&#30340;&#20005;&#26684;&#24615;&#33021;&#34920;&#24449;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#22522;&#26412;&#26041;&#27861;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#23454;&#38469;&#30340;&#35774;&#35745;&#30697;&#38453;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#24182;&#19988;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25429;&#25417;&#27979;&#37327;&#30340;&#38750;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#30340;&#30456;&#20851;&#39640;&#26031;&#35774;&#35745;&#65292;&#36890;&#36807;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#36827;&#34892;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#35889;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#36825;&#19968;&#32467;&#26524;&#26469;&#30830;&#23450;&#26368;&#20248;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25152;&#38656;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.13900</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#38469;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation via Marginal Contextual Information. (arXiv:2308.13900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13900
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32622;&#20449;&#24230;&#31934;&#21270;&#26041;&#26696;&#65292;&#22686;&#24378;&#20102;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#20687;&#32032;&#20998;&#32452;&#24182;&#20849;&#21516;&#32771;&#34385;&#23427;&#20204;&#30340;&#20266;&#26631;&#31614;&#65292;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;S4MC&#65292;&#22312;&#20445;&#25345;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#22686;&#21152;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;S4MC&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#38477;&#20302;&#33719;&#24471;&#31264;&#23494;&#26631;&#27880;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#22312;PASCAL VOC 12&#19978;&#20351;&#29992;366&#20010;&#24102;&#27880;&#37322;&#22270;&#20687;&#65292;S4MC&#27604;&#21069;&#19968;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;1.29&#20010;mIoU&#12290;&#26377;&#20851;&#37325;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#21442;&#35265;...
&lt;/p&gt;
&lt;p&gt;
We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a 1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35777;&#26126;&#20102;&#26356;&#24378;&#22823;&#30340;GNNs&#26080;&#27861;&#27867;&#21270;&#21040;&#23567;&#25200;&#21160;&#30340;&#22270;&#32467;&#26500;&#21644;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#12290;</title><link>http://arxiv.org/abs/2308.08173</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness. (arXiv:2308.08173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08173
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;&#20256;&#32479;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#35777;&#26126;&#20102;&#26356;&#24378;&#22823;&#30340;GNNs&#26080;&#27861;&#27867;&#21270;&#21040;&#23567;&#25200;&#21160;&#30340;&#22270;&#32467;&#26500;&#21644;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#30740;&#31350;&#65292;&#35777;&#26126;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#27604;&#20256;&#32479;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26356;&#24378;&#22823;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#20316;&#20026;&#19968;&#31181;&#24037;&#20855;&#26469;&#25581;&#31034;&#23427;&#20204;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#21644;&#32463;&#39564;&#19978;&#23454;&#38469;&#36798;&#21040;&#30340;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20851;&#27880;GNNs&#35745;&#25968;&#29305;&#23450;&#30340;&#23376;&#22270;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#65292;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#36825;&#20010;&#20219;&#21153;&#19978;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#39640;&#25928;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#26469;&#36827;&#34892;&#23376;&#22270;&#35745;&#25968;&#65292;&#24182;&#23637;&#31034;&#26356;&#24378;&#22823;&#30340;GNNs&#21363;&#20351;&#22312;&#23545;&#22270;&#32467;&#26500;&#30340;&#23567;&#25200;&#21160;&#19979;&#20063;&#26080;&#27861;&#27867;&#21270;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#22788;&#29702;&#20998;&#24067;&#19981;&#19968;&#26679;&#30340;&#22270;&#26102;&#20063;&#26080;&#27861;&#35745;&#25968;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform the first adversarial robustness study into Graph Neural Networks (GNNs) that are provably more powerful than traditional Message Passing Neural Networks (MPNNs). In particular, we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power. To do so, we focus on the ability of GNNs to count specific subgraph patterns, which is an established measure of expressivity, and extend the concept of adversarial robustness to this task. Based on this, we develop efficient adversarial attacks for subgraph counting and show that more powerful GNNs fail to generalize even to small perturbations to the graph's structure. Expanding on this, we show that such architectures also fail to count substructures on out-of-distribution graphs.
&lt;/p&gt;</description></item><item><title>VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.10167</link><description>&lt;p&gt;
VITS: &#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10167
&lt;/p&gt;
&lt;p&gt;
VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20256;&#32479;&#30340;TS&#31639;&#27861;&#22312;&#27599;&#36718;&#38656;&#35201;&#20174;&#24403;&#21069;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#24182;&#25552;&#20379;&#25509;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36817;&#20284;&#25216;&#26415;&#35201;&#20040;&#20272;&#35745;&#19981;&#20934;&#30830;&#65288;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65288;MCMC&#26041;&#27861;&#65292;&#38598;&#25104;&#25277;&#26679;...&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#25512;&#29702;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;VITS&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#24182;&#19988;&#23481;&#26131;&#20174;&#20013;&#25277;&#26679;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#26159;TS&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#65292;VITS&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#19982;&#32500;&#24230;&#21644;&#22238;&#21512;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04417</link><description>&lt;p&gt;
&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#20844;&#27491;&#24863;&#30693;&#32852;&#37030;&#26497;&#23567;&#21270;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fairness-aware Federated Minimax Optimization with Convergence Guarantee. (arXiv:2307.04417v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FFALM&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26045;&#21152;&#20844;&#24179;&#32422;&#26463;&#21644;&#35299;&#20915;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#38382;&#39064;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;FFALM&#22312;&#22788;&#29702;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064; (FL) &#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31649;&#29702;&#29992;&#25143;&#25968;&#25454;&#30340;&#33258;&#30001;&#24230;&#19981;&#36275;&#21487;&#33021;&#23548;&#33268;&#32676;&#20307;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#20559;&#21521;&#20110;&#25935;&#24863;&#22240;&#32032;&#35832;&#22914;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;&#24102;&#26377;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#30340;&#20844;&#24179;&#32852;&#37030;&#24179;&#22343;&#27861; (FFALM)&#65292;&#19987;&#38376;&#29992;&#20110;&#35299;&#20915;FL&#20013;&#30340;&#32676;&#20307;&#20844;&#24179;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#35757;&#32451;&#30446;&#26631;&#26045;&#21152;&#20102;&#20844;&#24179;&#32422;&#26463;&#65292;&#24182;&#35299;&#20915;&#20102;&#21463;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;FFALM&#30340;&#25910;&#25947;&#36895;&#29575;&#30340;&#29702;&#35770;&#19978;&#30028;&#12290;&#36890;&#36807;&#22312;CelebA&#21644;UTKFace&#25968;&#25454;&#38598;&#20013;&#20805;&#20998;&#32771;&#34385;&#20005;&#37325;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;FFALM &#22312;&#25552;&#39640;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has garnered considerable attention due to its privacy-preserving feature. Nonetheless, the lack of freedom in managing user data can lead to group fairness issues, where models are biased towards sensitive factors such as race or gender. To tackle this issue, this paper proposes a novel algorithm, fair federated averaging with augmented Lagrangian method (FFALM), designed explicitly to address group fairness issues in FL. Specifically, we impose a fairness constraint on the training objective and solve the minimax reformulation of the constrained optimization problem. Then, we derive the theoretical upper bound for the convergence rate of FFALM. The effectiveness of FFALM in improving fairness is shown empirically on CelebA and UTKFace datasets in the presence of severe statistical heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2307.03034</link><description>&lt;p&gt;
&#24102;&#26377;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
PCL-Indexability and Whittle Index for Restless Bandits with General Observation Models. (arXiv:2307.03034v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#19979;&#30340;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;PCL-&#21487;&#32034;&#24341;&#24615;&#21644;Whittle&#32034;&#24341;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#36817;&#20284;&#36807;&#31243;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#31639;&#27861;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#19968;&#33324;&#35266;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#19981;&#23433;&#23450;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#30001;&#20110;&#36164;&#28304;&#32422;&#26463;&#25110;&#29615;&#22659;&#25110;&#22266;&#26377;&#22122;&#22768;&#65292;&#29609;&#23478;&#25805;&#20316;&#38656;&#35201;&#22522;&#20110;&#26576;&#31181;&#26377;&#35823;&#24046;&#30340;&#21453;&#39304;&#26426;&#21046;&#12290;&#36890;&#36807;&#24314;&#31435;&#21453;&#39304;/&#35266;&#27979;&#21160;&#21147;&#23398;&#30340;&#19968;&#33324;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#20174;&#20219;&#24847;&#21021;&#22987;&#20449;&#24565;&#65288;&#20808;&#39564;&#20449;&#24687;&#65289;&#24320;&#22987;&#30340;&#20855;&#26377;&#21487;&#25968;&#20449;&#24565;&#29366;&#24577;&#31354;&#38388;&#30340;&#19981;&#23433;&#23450;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#37096;&#20998;&#23432;&#24658;&#23450;&#24459;&#65288;PCL&#65289;&#30340;&#21487;&#23454;&#29616;&#21306;&#22495;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26080;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;&#21487;&#32034;&#24341;&#24615;&#21644;&#20248;&#20808;&#32423;&#32034;&#24341;&#65288;Whittle&#32034;&#24341;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#36807;&#31243;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#20197;&#24212;&#29992;Ni&#241;o-Mora&#21644;Bertsimas&#38024;&#23545;&#26377;&#38480;&#29366;&#24577;&#38382;&#39064;&#30340;AG&#31639;&#27861;&#30340;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a general observation model for restless multi-armed bandit problems. The operation of the player needs to be based on certain feedback mechanism that is error-prone due to resource constraints or environmental or intrinsic noises. By establishing a general probabilistic model for dynamics of feedback/observation, we formulate the problem as a restless bandit with a countable belief state space starting from an arbitrary initial belief (a priori information). We apply the achievable region method with partial conservation law (PCL) to the infinite-state problem and analyze its indexability and priority index (Whittle index). Finally, we propose an approximation process to transform the problem into which the AG algorithm of Ni\~no-Mora and Bertsimas for finite-state problems can be applied to. Numerical experiments show that our algorithm has an excellent performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.03235</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#27169;&#22411;&#26550;&#26500;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Information Flow Control in Machine Learning through Modular Model Architecture. (arXiv:2306.03235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;MoE&#26550;&#26500;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20165;&#22522;&#20110;&#35775;&#38382;&#31574;&#30053;&#21551;&#29992;&#23376;&#38598;&#30340;&#19987;&#23478;&#65292;&#23454;&#29616;&#20102;&#23545;&#23433;&#20840;&#35775;&#38382;&#25511;&#21046;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#20309;&#37096;&#20998;&#37117;&#21487;&#20197;&#24433;&#21709;&#20854;&#36755;&#20986;&#12290;&#24403;&#35775;&#38382;&#25511;&#21046;&#21482;&#20801;&#35768;&#20010;&#20154;&#29992;&#25143;&#35775;&#38382;&#25968;&#25454;&#23376;&#38598;&#26102;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#21040;&#27169;&#22411;&#36755;&#20986;&#30340;&#20449;&#24687;&#27969;&#25511;&#21046;&#19981;&#36275;&#25104;&#20026;&#35757;&#32451;&#25935;&#24863;&#25968;&#25454;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#23454;&#29616;&#35775;&#38382;&#25511;&#21046;&#25968;&#25454;&#30340;&#23433;&#20840;&#26426;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20449;&#24687;&#27969;&#25511;&#21046;&#30340;&#27010;&#24565;&#65292;&#24182;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#26550;&#26500;&#24320;&#21457;&#20102;&#19968;&#20010;&#23433;&#20840;Transformer&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#38480;&#21046;&#26469;&#33258;&#27599;&#20010;&#23433;&#20840;&#39046;&#22495;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#21333;&#20010;&#19987;&#23478;&#27169;&#22359;&#30340;&#24433;&#21709;&#65292;&#24182;&#20165;&#22522;&#20110;&#35775;&#38382;&#25511;&#21046;&#31574;&#30053;&#22312;&#25512;&#29702;&#26102;&#21551;&#29992;&#19987;&#23478;&#30340;&#23376;&#38598;&#65292;&#23433;&#20840;MoE&#26550;&#26500;&#25511;&#21046;&#20102;&#20449;&#24687;&#27969;&#12290;&#20351;&#29992;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#35821;&#26009;&#24211;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MoE&#26550;&#26500;&#20855;&#26377;&#26368;&#23567;&#30340;&#24615;&#33021;&#24320;&#38144;&#65288;1.9%&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#65288;&#26368;&#39640;&#21487;&#36798;37%&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#35757;&#32451;&#20934;&#30830;&#21644;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's machine learning (ML) models, any part of the training data can affect its output. This lack of control for information flow from training data to model output is a major obstacle in training models on sensitive data when access control only allows individual users to access a subset of data. To enable secure machine learning for access controlled data, we propose the notion of information flow control for machine learning, and develop a secure Transformer-based language model based on the Mixture-of-Experts (MoE) architecture. The secure MoE architecture controls information flow by limiting the influence of training data from each security domain to a single expert module, and only enabling a subset of experts at inference time based on an access control policy. The evaluation using a large corpus of text data shows that the proposed MoE architecture has minimal (1.9%) performance overhead and can significantly improve model accuracy (up to 37%) by enabling training on acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01237</link><description>&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#20013;&#36125;&#21494;&#26031;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Convex Relaxation Approach to Bayesian Regret Minimization in Offline Bandits. (arXiv:2306.01237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#30452;&#25509;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#19978;&#30028;&#30340;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35770;&#31163;&#32447;&#36951;&#25022;&#30028;&#21644;&#25968;&#20540;&#27169;&#25311;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#36172;&#21338;&#31639;&#27861;&#24517;&#39035;&#20165;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#20248;&#21270;&#20915;&#31574;&#12290;&#31163;&#32447;&#36172;&#21338;&#20013;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#19988;&#36880;&#28176;&#27969;&#34892;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23454;&#29616;&#20302;&#36125;&#21494;&#26031;&#36951;&#25022;&#24182;&#20855;&#26377;&#39640;&#32622;&#20449;&#24230;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#21033;&#29992;&#39640;&#25928;&#30340;&#38181;&#20248;&#21270;&#27714;&#35299;&#22120;&#26469;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#30340;&#19978;&#30028;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#33719;&#24471;&#20102;&#26356;&#20248;&#30340;&#31163;&#32447;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#27969;&#34892;&#30340;LCB&#65288;lower confidence bound&#65289;-style&#31639;&#27861;&#21487;&#33021;&#19981;&#36866;&#21512;&#31163;&#32447;&#36172;&#21338;&#20013;&#26368;&#23567;&#21270;&#36125;&#21494;&#26031;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for offline bandits must optimize decisions in uncertain environments using only offline data. A compelling and increasingly popular objective in offline bandits is to learn a policy which achieves low Bayesian regret with high confidence. An appealing approach to this problem, inspired by recent offline reinforcement learning results, is to maximize a form of lower confidence bound (LCB). This paper proposes a new approach that directly minimizes upper bounds on Bayesian regret using efficient conic optimization solvers. Our bounds build on connections among Bayesian regret, Value-at-Risk (VaR), and chance-constrained optimization. Compared to prior work, our algorithm attains superior theoretical offline regret bounds and better results in numerical simulations. Finally, we provide some evidence that popular LCB-style algorithms may be unsuitable for minimizing Bayesian regret in offline bandits.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#35774;&#32622;&#65292;&#24182;&#21457;&#23637;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#20197;&#20943;&#23569;&#20195;&#29702;&#20043;&#38388;&#30340;&#38598;&#20307;&#36951;&#25022;&#65292;&#22312;&#25968;&#23398;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18784</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24322;&#26500;&#22810;&#33218;&#32769;&#34382;&#26426;&#32763;&#35793;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits. (arXiv:2305.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#35774;&#32622;&#65292;&#24182;&#21457;&#23637;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#20197;&#20943;&#23569;&#20195;&#29702;&#20043;&#38388;&#30340;&#38598;&#20307;&#36951;&#25022;&#65292;&#22312;&#25968;&#23398;&#20998;&#26512;&#20013;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#32769;&#34382;&#26426;&#30340;&#30740;&#31350;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#21512;&#20316;&#35774;&#32622;&#65292;&#20854;&#20013;$N$&#20010;&#26234;&#33021;&#20307;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#27491;&#22312;&#23398;&#20064;$M$&#20010;&#20855;&#26377;&#38543;&#26426;&#24615;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#65292;&#20197;&#20943;&#23569;&#20182;&#20204;&#30340;&#38598;&#20307;&#32047;&#35745;&#36951;&#25022;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#21435;&#20013;&#24515;&#21270;&#31639;&#27861;&#65292;&#20419;&#36827;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#24182;&#38024;&#23545;&#20004;&#31181;&#24773;&#20917;&#36827;&#34892;&#20102;&#24615;&#33021;&#34920;&#24449;&#12290;&#36890;&#36807;&#25512;&#23548;&#27599;&#20010;&#20195;&#29702;&#30340;&#32047;&#31215;&#36951;&#25022;&#21644;&#38598;&#20307;&#36951;&#25022;&#30340;&#19978;&#38480;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#38598;&#20307;&#36951;&#25022;&#30340;&#19979;&#38480;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#36817;&#20046;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17401</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework For Refining Text Classification and Object Recognition from Academic Articles. (arXiv:2305.17401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#28860;&#25991;&#26412;&#20998;&#31867;&#21644;&#23545;&#35937;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#20013;&#25552;&#21462;&#29305;&#23450;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25366;&#25496;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#21160;&#20174;&#22797;&#26434;&#30340;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#25991;&#26723;&#20013;&#25552;&#21462;&#29305;&#23450;&#27169;&#24335;&#12290;&#24403;&#21069;&#30340;&#23398;&#26415;&#35770;&#25991;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#65288;RB&#65289;&#25110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#38656;&#35201;&#32534;&#20889;&#22797;&#26434;&#25490;&#29256;&#35770;&#25991;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#23545;&#25991;&#31456;&#20013;&#22797;&#26434;&#20869;&#23481;&#31867;&#22411;&#36827;&#34892;&#27880;&#37322;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#34987;&#38169;&#35823;&#25552;&#21462;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#20998;&#26512;&#25351;&#23450;&#33879;&#20316;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24067;&#23616;&#21644;&#25490;&#29256;&#35282;&#24230;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified p
&lt;/p&gt;</description></item><item><title>&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.01397</link><description>&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#19981;&#21464;&#27169;&#22411;&#21644;&#34920;&#31034;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01397
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#27169;&#22411;&#32534;&#30721;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#24341;&#21457;&#26377;&#20851;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#23481;&#26131;&#25439;&#22833;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#30340;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#21307;&#23398;&#25104;&#20687;&#27169;&#22411;&#22312;&#20854;&#28508;&#22312;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#26377;&#20851;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65288;&#24180;&#40836;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#65289;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#21487;&#34892;&#21644;&#20540;&#24471;&#35757;&#32451;&#19981;&#32534;&#30721;&#20154;&#21475;&#23646;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#30340;&#19981;&#21464;&#24615;&#65292;&#21363;&#36793;&#38469;&#12289;&#31867;&#26465;&#20214;&#21644;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#24182;&#35828;&#26126;&#23427;&#20204;&#19982;&#31639;&#27861;&#20844;&#24179;&#30340;&#26631;&#20934;&#27010;&#24565;&#30340;&#31561;&#20215;&#24615;&#12290;&#26681;&#25454;&#29616;&#26377;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#38469;&#21644;&#31867;&#26465;&#20214;&#30340;&#19981;&#21464;&#24615;&#21487;&#34987;&#35748;&#20026;&#26159;&#23454;&#29616;&#26576;&#20123;&#20844;&#24179;&#27010;&#24565;&#30340;&#36807;&#24230;&#38480;&#21046;&#26041;&#27861;&#65292;&#23548;&#33268;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#25439;&#22833;&#12290;&#20851;&#20110;&#21453;&#20107;&#23454;&#27169;&#22411;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#23646;&#24615;&#65292;&#23450;&#20041;&#21307;&#23398;&#22270;&#20687;&#21453;&#20107;&#23454;&#23384;&#22312;&#22797;&#26434;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20154;&#21475;&#32479;&#35745;&#23398;&#32534;&#30721;&#29978;&#33267;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.15975</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#20855;&#26377;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;MSc-iNCD&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#30340;&#23398;&#20064;&#20013;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#65292;&#35813;&#26041;&#27861;&#22312;&#22686;&#37327;&#24335;&#26032;&#31867;&#21035;&#21457;&#29616;&#20013;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#38271;&#23398;&#20064;&#32773;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#36830;&#32493;&#22320;&#21457;&#29616;&#26032;&#27010;&#24565;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#26399;&#26395;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#36825;&#31867;&#38382;&#39064;&#22312;&#38750;&#24120;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#37096;&#20998;&#35299;&#20915;&#65292;&#20854;&#20013;&#35201;&#20040;&#20026;&#21457;&#29616;&#26032;&#27010;&#24565;&#25552;&#20379;&#26377;&#26631;&#21495;&#30340;&#25968;&#25454;&#65288;&#20363;&#22914; NCD&#65289;&#65292;&#35201;&#20040;&#23398;&#20064;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#22686;&#37327;&#27493;&#39588;&#20013;&#21457;&#29983;&#65288;&#20363;&#22914;&#31867; iNCD&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026; MSc-iNCD&#65292;&#20854;&#20013;&#23398;&#20064;&#36830;&#32493;&#32780;&#26080;&#20154;&#30417;&#30563;&#65292;&#24182;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#20808;&#39564;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#22312;&#36739;&#38271;&#30340;&#23398;&#20064;&#24773;&#22659;&#19979;&#20855;&#26377;&#24377;&#24615;&#65292;&#32780;&#19988;&#19982;&#22797;&#26434;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20986;&#20046;&#24847;&#26009;&#30340;&#24378;&#22823;&#23454;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#32447;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#21319;&#20102;&#22522;&#20934;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09590</link><description>&lt;p&gt;
&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction. (arXiv:2303.09590v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#21457;&#29616;&#12290;&#21457;&#25496;&#21644;&#29702;&#35299;&#22810;&#21464;&#37327;&#32593;&#32476;&#20013;&#30340;&#20851;&#31995;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#20197;&#25552;&#21462;&#32593;&#32476;&#19981;&#21516;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#20851;&#32852;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#20160;&#20040;&#26159;&#22312;&#31038;&#20132;&#32593;&#32476;&#23494;&#24230;&#26041;&#38754;&#19982;&#19981;&#21516;&#23646;&#24615;&#30340;&#32452;&#21512;&#20851;&#31995;&#65289;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#26681;&#25454;&#25152;&#36873;&#36755;&#20837;&#21644;&#36755;&#20986;&#23646;&#24615;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#20135;&#29983;&#19968;&#20010;&#31616;&#21270;&#30340;&#32467;&#26524;&#38598;&#21512;&#20197;&#20415;&#26816;&#26597;&#65292;&#26368;&#21518;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#38454;&#27573;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#23558;&#30001;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#30452;&#35266;&#35299;&#37322;&#30340;&#32447;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#22411;&#32452;&#32455;&#21592;&#24037;&#20043;&#38388;&#30340;&#30005;&#23376;&#37038;&#20214;&#36890;&#20449;&#25968;&#25454;&#38598;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#24037;&#20316;&#27969;&#31243;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demon
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04032</link><description>&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#32593;&#32476;&#30340;&#31995;&#32479;&#24615;&#33021;&#20998;&#26512;&#65306;&#25171;&#30772;&#36801;&#31227;&#23398;&#20064;&#30340;&#32422;&#23450;
&lt;/p&gt;
&lt;p&gt;
A Systematic Performance Analysis of Deep Perceptual Loss Networks: Breaking Transfer Learning Conventions. (arXiv:2302.04032v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04032
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21450;&#20854;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#65292;&#22312;&#22235;&#20010;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#19978;&#35299;&#20915;&#20102;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#26159;&#19968;&#31181;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#28145;&#24230;&#29305;&#24449;&#26469;&#27169;&#20223;&#20154;&#31867;&#24863;&#30693;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#26377;&#36259;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22270;&#20687;&#25110;&#31867;&#20284;&#22270;&#20687;&#36755;&#20986;&#30340;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#21512;&#25104;&#12289;&#20998;&#21106;&#12289;&#28145;&#24230;&#39044;&#27979;&#31561;&#12290;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#36890;&#24120;&#26159;&#21367;&#31215;&#32593;&#32476;&#65292;&#29992;&#20110;&#25439;&#22833;&#35745;&#31639;&#12290;&#23613;&#31649;&#23545;&#35813;&#26041;&#27861;&#30340;&#20852;&#36259;&#21644;&#24191;&#27867;&#20351;&#29992;&#22686;&#21152;&#20102;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#30340;&#21162;&#21147;&#26469;&#25506;&#32034;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#30340;&#32593;&#32476;&#20197;&#21450;&#20174;&#21738;&#20123;&#23618;&#25552;&#21462;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#31995;&#32479;&#22320;&#35780;&#20272;&#22810;&#31181;&#24120;&#29992;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#21450;&#38024;&#23545;&#22235;&#20010;&#29616;&#26377;&#28145;&#24230;&#24863;&#30693;&#25439;&#22833;&#29992;&#20363;&#30340;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#28857;&#26469;&#32416;&#27491;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep perceptual loss is a type of loss function in computer vision that aims to mimic human perception by using the deep features extracted from neural networks. In recent years, the method has been applied to great effect on a host of interesting computer vision tasks, especially for tasks with image or image-like outputs, such as image synthesis, segmentation, depth prediction, and more. Many applications of the method use pretrained networks, often convolutional networks, for loss calculation. Despite the increased interest and broader use, more effort is needed toward exploring which networks to use for calculating deep perceptual loss and from which layers to extract the features.  This work aims to rectify this by systematically evaluating a host of commonly used and readily available, pretrained networks for a number of different feature extraction points on four existing use cases of deep perceptual loss. The use cases of perceptual similarity, super-resolution, image segmentat
&lt;/p&gt;</description></item><item><title>&#22312;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20849;&#35782;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;eXirt&#65292;&#29992;&#20110;&#35299;&#37322;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25490;&#21517;&#26469;&#35299;&#37322;&#27169;&#22411;&#36755;&#20837;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.09933</link><description>&lt;p&gt;
&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#35299;&#37322;&#65288;eXirt&#65289;&#65306;&#19968;&#31181;&#22312;&#20449;&#20219;&#35270;&#35282;&#19979;&#35299;&#37322;&#26641;&#38598;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#29305;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explanations Based on Item Response Theory (eXirt): A Model-Specific Method to Explain Tree-Ensemble Model in Trust Perspective. (arXiv:2210.09933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09933
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#26041;&#27861;&#20013;&#27809;&#26377;&#20849;&#35782;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#30340;&#26032;&#26041;&#27861;eXirt&#65292;&#29992;&#20110;&#35299;&#37322;&#20351;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25490;&#21517;&#26469;&#35299;&#37322;&#27169;&#22411;&#36755;&#20837;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;XAI&#30740;&#31350;&#20154;&#21592;&#27491;&#35268;&#33539;&#21270;&#25552;&#26696;&#21644;&#24320;&#21457;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#65292;&#20294;&#22312;&#31038;&#21306;&#20013;&#23545;&#20110;&#20351;&#29992;&#21738;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23578;&#26080;&#19968;&#33268;&#24847;&#35265;&#65292;&#32780;&#36825;&#31181;&#36873;&#25321;&#20960;&#20046;&#30452;&#25509;&#19982;&#29305;&#23450;&#26041;&#27861;&#30340;&#27969;&#34892;&#24230;&#30456;&#20851;&#12290;&#35832;&#22914;Ciu&#12289;Dalex&#12289;Eli5&#12289;Lofo&#12289;Shap&#21644;Skater&#31561;&#26041;&#27861;&#36890;&#36807;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#20840;&#23616;&#25490;&#21517;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#19981;&#21516;&#30340;&#26041;&#27861;&#23398;&#29983;&#25104;&#20840;&#23616;&#35299;&#37322;&#65292;&#35828;&#26126;&#27169;&#22411;&#36755;&#20837;&#22914;&#20309;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20102;41&#20010;&#25968;&#25454;&#38598;&#12289;4&#31181;&#26641;&#38598;&#25104;&#31639;&#27861;&#65288;Light Gradient Boosting&#12289;CatBoost&#12289;Random Forest&#21644;Gradient Boosting&#65289;&#21644;6&#31181;XAI&#26041;&#27861;&#26469;&#25903;&#25345;&#25512;&#20986;&#19968;&#31181;&#21517;&#20026;eXirt&#30340;&#26032;&#30340;XAI&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#65292;&#26088;&#22312;&#35299;&#37322;&#20351;&#29992;&#20851;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#34920;&#26684;&#25968;&#25454;&#30340;&#26641;&#38598;&#25104;&#40657;&#30418;&#27169;&#22411;&#12290;&#22312;&#31532;&#19968;&#32452;&#20998;&#26512;&#20013;&#65292;164&#20010;&#20840;&#23616;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
In recent years, XAI researchers have been formalizing proposals and developing new methods to explain black box models, with no general consensus in the community on which method to use to explain these models, with this choice being almost directly linked to the popularity of a specific method. Methods such as Ciu, Dalex, Eli5, Lofo, Shap and Skater emerged with the proposal to explain black box models through global rankings of feature relevance, which based on different methodologies, generate global explanations that indicate how the model's inputs explain its predictions. In this context, 41 datasets, 4 tree-ensemble algorithms (Light Gradient Boosting, CatBoost, Random Forest, and Gradient Boosting), and 6 XAI methods were used to support the launch of a new XAI method, called eXirt, based on Item Response Theory IRT and aimed at tree-ensemble black box models that use tabular data referring to binary classification problems. In the first set of analyses, the 164 global featur
&lt;/p&gt;</description></item></channel></rss>