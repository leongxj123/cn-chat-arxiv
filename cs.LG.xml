<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.</title><link>https://arxiv.org/abs/2404.02040</link><description>&lt;p&gt;
&#21464;&#21387;&#22120;&#20316;&#20026;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers as Transducers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02040
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RASP&#65292;&#25512;&#20986;&#20102;&#26032;&#30340;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#23558;&#21464;&#21387;&#22120;&#19982;&#26377;&#38480;&#20256;&#24863;&#22120;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#36798;&#20196;&#20154;&#24778;&#35766;&#30340;&#22823;&#31867;&#20256;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;RASP&#30340;&#21464;&#20307;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#24110;&#21161;&#20154;&#20204;&#8220;&#20687;&#21464;&#21387;&#22120;&#19968;&#26679;&#24605;&#32771;&#8221;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#20316;&#20026;&#20013;&#38388;&#34920;&#31034;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#24067;&#23572;&#21464;&#20307;B-RASP&#21040;&#24207;&#21015;&#21040;&#24207;&#21015;&#20989;&#25968;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#30830;&#20999;&#35745;&#31639;&#20102;&#19968;&#38454;&#26377;&#29702;&#20989;&#25968;&#65288;&#22914;&#23383;&#31526;&#20018;&#26059;&#36716;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#25193;&#23637;&#12290;B-RASP[pos]&#20801;&#35768;&#22312;&#20301;&#32622;&#19978;&#36827;&#34892;&#35745;&#31639;&#65288;&#22914;&#22797;&#21046;&#23383;&#31526;&#20018;&#30340;&#21069;&#21322;&#37096;&#20998;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#27491;&#21017;&#20989;&#25968;&#12290;S-RASP&#28155;&#21152;&#21069;&#32512;&#21644;&#65292;&#21487;&#20197;&#36827;&#34892;&#39069;&#22806;&#30340;&#31639;&#26415;&#25805;&#20316;&#65288;&#22914;&#23545;&#23383;&#31526;&#20018;&#27714;&#24179;&#26041;&#65289;&#65292;&#24182;&#21253;&#21547;&#25152;&#26377;&#19968;&#38454;&#22810;&#27491;&#21017;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25513;&#30721;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21464;&#21387;&#22120;&#21487;&#20197;&#27169;&#25311;S-RASP&#12290;&#25105;&#20204;&#32467;&#26524;&#30340;&#19968;&#20010;&#25512;&#35770;&#26159;n...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02040v1 Announce Type: cross  Abstract: We study the sequence-to-sequence mapping capacity of transformers by relating them to finite transducers, and find that they can express surprisingly large classes of transductions. We do so using variants of RASP, a programming language designed to help people "think like transformers," as an intermediate representation. We extend the existing Boolean variant B-RASP to sequence-to-sequence functions and show that it computes exactly the first-order rational functions (such as string rotation). Then, we introduce two new extensions. B-RASP[pos] enables calculations on positions (such as copying the first half of a string) and contains all first-order regular functions. S-RASP adds prefix sum, which enables additional arithmetic operations (such as squaring a string) and contains all first-order polyregular functions. Finally, we show that masked average-hard attention transformers can simulate S-RASP. A corollary of our results is a n
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.01340</link><description>&lt;p&gt;
&#20174;&#30456;&#20284;&#21040;&#20248;&#36234;&#65306;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Similarity to Superiority: Channel Clustering for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01340
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36890;&#36947;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36947;&#31574;&#30053;&#65292;&#26377;&#25928;&#24179;&#34913;&#20102;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#21644;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#26368;&#36817;&#20960;&#21313;&#24180;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29420;&#31435;&#36890;&#36947;&#31574;&#30053;&#36890;&#36807;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#36890;&#36947;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#22312;&#26410;&#30693;&#23454;&#20363;&#19978;&#23548;&#33268;&#20102;&#24046;&#21170;&#30340;&#27867;&#21270;&#65292;&#24182;&#24573;&#30053;&#20102;&#36890;&#36947;&#20043;&#38388;&#28508;&#22312;&#30340;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#12290;&#30456;&#21453;&#65292;&#20381;&#36182;&#36890;&#36947;&#31574;&#30053;&#23558;&#25152;&#26377;&#36890;&#36947;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#29978;&#33267;&#21253;&#21547;&#26080;&#20851;&#32039;&#35201;&#21644;&#38543;&#24847;&#30340;&#20449;&#24687;&#65292;&#28982;&#32780;&#36825;&#20250;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#24182;&#38480;&#21046;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#24179;&#34913;&#20010;&#20307;&#36890;&#36947;&#22788;&#29702;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#21448;&#19981;&#24573;&#35270;&#36890;&#36947;&#20043;&#38388;&#24517;&#35201;&#20132;&#20114;&#20316;&#29992;&#30340;&#36890;&#36947;&#31574;&#30053;&#12290;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#32451;&#20064;&#25552;&#39640;&#23545;&#28151;&#21512;&#36890;&#36947;&#30340;&#32467;&#26524;&#19982;&#19968;&#23545;&#36890;&#36947;&#20043;&#38388;&#26412;&#36136;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#32852;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36866;&#24212;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01340v1 Announce Type: cross  Abstract: Time series forecasting has attracted significant attention in recent decades. Previous studies have demonstrated that the Channel-Independent (CI) strategy improves forecasting performance by treating different channels individually, while it leads to poor generalization on unseen instances and ignores potentially necessary interactions between channels. Conversely, the Channel-Dependent (CD) strategy mixes all channels with even irrelevant and indiscriminate information, which, however, results in oversmoothing issues and limits forecasting accuracy. There is a lack of channel strategy that effectively balances individual channel treatment for improved forecasting performance without overlooking essential interactions between channels. Motivated by our observation of a correlation between the time series model's performance boost against channel mixing and the intrinsic similarity on a pair of channels, we developed a novel and adapt
&lt;/p&gt;</description></item><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07447</link><description>&lt;p&gt;
&#38024;&#23545;&#26102;&#38388;&#30456;&#20851;&#22810;&#30005;&#23376;Schr\"odinger&#26041;&#31243;&#30340;&#20174;&#22836;&#21464;&#20998;&#27874;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Ab-initio variational wave functions for the time-dependent many-electron Schr\"odinger equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#65292;&#21487;&#20197;&#35299;&#20915;&#23454;&#26102;&#28436;&#21270;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#25551;&#36848;&#22810;&#30005;&#23376;&#37327;&#23376;&#31995;&#32479;&#21160;&#21147;&#23398;&#23545;&#20110;&#39044;&#27979;&#37327;&#23376;&#21270;&#23398;&#20013;&#30340;&#30005;&#23376;&#32467;&#26500;&#12289;&#20957;&#32858;&#24577;&#31995;&#32479;&#30340;&#24615;&#36136;&#21644;&#22797;&#26434;&#26448;&#26009;&#30340;&#34892;&#20026;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38750;&#24179;&#34913;&#37327;&#23376;&#30005;&#23376;&#31995;&#32479;&#30340;&#23454;&#26102;&#28436;&#21270;&#23545;&#20110;&#29702;&#35770;&#21644;&#35745;&#31639;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#31995;&#32479;&#25506;&#32034;&#20102;&#24191;&#38420;&#30340;&#26500;&#22411;&#31354;&#38388;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36153;&#31859;&#23376;&#26102;&#38388;&#30456;&#20851;&#27874;&#20989;&#25968;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#22810;&#20307;&#30456;&#20851;&#24615;&#36229;&#36234;&#24179;&#22343;&#22330;&#36817;&#20284;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21442;&#25968;&#21270;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24577;&#28436;&#21270;&#30340;&#36817;&#20284;&#12290;&#20026;&#20102;&#32771;&#34385;&#30005;&#23376;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26102;&#38388;&#30456;&#20851;&#30340;Jastrow&#22240;&#23376;&#21644;&#22238;&#27969;&#21464;&#25442;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;&#36825;&#20123;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07447v1 Announce Type: cross  Abstract: Describing the dynamics of many-electron quantum systems is crucial for applications such as predicting electronic structures in quantum chemistry, the properties of condensed matter systems, and the behaviors of complex materials. However, the real-time evolution of non-equilibrium quantum electronic systems poses a significant challenge for theoretical and computational approaches, due to the system's exploration of a vast configuration space. This work introduces a variational approach for fermionic time-dependent wave functions, surpassing mean-field approximations by capturing many-body correlations. The proposed methodology involves parameterizing the time-evolving quantum state, enabling the approximation of the state's evolution. To account for electron correlations, we employ time-dependent Jastrow factors and backflow transformations. We also show that we can incorporate neural networks to parameterize these functions. The ti
&lt;/p&gt;</description></item><item><title>SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.01632</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#27861;&#22686;&#24378;&#25913;&#36827;LLM&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving LLM Code Generation with Grammar Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01632
&lt;/p&gt;
&lt;p&gt;
SynCode&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#21644;DFA mask store&#65292;&#22312;LLMs&#20013;&#29983;&#25104;&#20195;&#30721;&#36807;&#31243;&#20013;&#33719;&#24471;96.07%&#30340;&#21477;&#27861;&#38169;&#35823;&#38477;&#20302;&#65292;&#24182;&#23637;&#29616;&#20986;&#25552;&#39640;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SynCode&#65292;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#21644;&#36890;&#29992;&#22320;&#35299;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#30721;&#30340;&#26032;&#26694;&#26550;&#12290;SynCode&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#30340;&#35821;&#27861;&#65292;&#21033;&#29992;&#31163;&#32447;&#26500;&#24314;&#30340;&#22522;&#20110;&#35821;&#35328;&#35821;&#27861;&#32456;&#32467;&#31526;&#30340;&#39640;&#25928;&#26597;&#25214;&#34920;DFA mask store&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SynCode&#22312;&#32473;&#23450;&#32534;&#31243;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#30340;&#23436;&#22791;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#23637;&#31034;&#20854;&#22312;&#20445;&#30041;&#35821;&#20041;&#19978;&#26377;&#25928;&#20196;&#29260;&#30340;&#21516;&#26102;&#25298;&#32477;&#26080;&#25928;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#19982;&#30001;CFG&#23450;&#20041;&#30340;&#20219;&#20309;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#39564;&#35777;&#20102;&#38024;&#23545;Python&#21644;Go&#30340;CFG&#23454;&#39564;&#12290;&#32467;&#26524;&#31361;&#20986;&#20102;&#24403;SynCode&#19982;&#26368;&#20808;&#36827;&#30340;LLMs&#32467;&#21512;&#26102;&#65292;&#35821;&#27861;&#38169;&#35823;&#20943;&#23569;96.07%&#65292;&#24432;&#26174;&#20102;&#20854;&#23545;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21477;&#27861;&#31934;&#24230;&#30340;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;</title><link>https://arxiv.org/abs/2403.00143</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#30340;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#65306;&#26641;&#24179;&#22343;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Unsupervised Discontinuous Constituency Parsing by Tree Averaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26641;&#24179;&#22343;&#27861;&#26500;&#24314;&#38598;&#25104;&#35299;&#26512;&#22120;&#65292;&#31283;&#23450;&#24182;&#25552;&#21319;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#19981;&#36830;&#32493;&#25104;&#20998;&#21477;&#27861;&#20998;&#26512;&#30340;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#21807;&#19968;&#27169;&#22411;&#30340;&#24615;&#33021;&#23384;&#22312;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23545;&#29616;&#26377;&#19981;&#36830;&#32493;&#35299;&#26512;&#22120;&#30340;&#19981;&#21516;&#36816;&#34892;&#26500;&#24314;&#19968;&#20010;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#24179;&#22343;&#39044;&#27979;&#26641;&#26469;&#31283;&#23450;&#21644;&#25552;&#21319;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#30340;&#20108;&#20803;&#24615;&#21644;&#36830;&#32493;&#24615;&#35774;&#32622;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26641;&#24179;&#22343;&#35745;&#31639;&#22797;&#26434;&#24230;&#20998;&#26512;&#65288;&#20197;P&#21644;NP&#23436;&#20840;&#20026;&#21333;&#20301;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#30830;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#19968;&#20219;&#21153;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#23545;&#25152;&#26377;&#26679;&#26412;&#36816;&#34892;&#26102;&#38388;&#22343;&#21512;&#29702;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00143v1 Announce Type: cross  Abstract: We address unsupervised discontinuous constituency parsing, where we observe a high variance in the performance of the only previous model. We propose to build an ensemble of different runs of the existing discontinuous parser by averaging the predicted trees, to stabilize and boost performance. To begin with, we provide comprehensive computational complexity analysis (in terms of P and NP-complete) for tree averaging under different setups of binarity and continuity. We then develop an efficient exact algorithm to tackle the task, which runs in a reasonable time for all samples in our experiments. Results on three datasets show our method outperforms all baselines in all metrics; we also provide in-depth analyses of our approach.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.18595</link><description>&lt;p&gt;
EncodingNet: &#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;MAC&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
EncodingNet: A Novel Encoding-based MAC Design for Efficient Neural Network Acceleration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#65292;&#36890;&#36807;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#20195;&#26367;&#20056;&#27861;&#22120;&#65292;&#35757;&#32451;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#23454;&#29616;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#65292;&#20174;&#32780;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#30340;&#33021;&#25928;&#21644;&#35745;&#31639;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#36328;  &#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#35832;&#22914;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;DNN&#30340;&#25191;&#34892;&#38656;&#35201;&#22312;&#30828;&#20214;&#19978;&#36827;&#34892;&#22823;&#37327;&#30340;&#20056;-&#32047;&#31215;&#65288;MAC&#65289;&#36816;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#21151;&#32791;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#30340;&#26032;&#22411;&#25968;&#23383;MAC&#35774;&#35745;&#12290;&#22312;&#36825;&#31181;&#26032;&#35774;&#35745;&#20013;&#65292;&#20056;&#27861;&#22120;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#38376;&#25152;&#21462;&#20195;&#65292;&#29992;&#20110;&#23558;&#32467;&#26524;&#25237;&#24433;&#21040;&#23485;&#27604;&#29305;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;&#27604;&#29305;&#25658;&#24102;&#21508;&#33258;&#30340;&#20301;&#32622;&#26435;&#37325;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;&#25512;&#26029;&#31934;&#24230;&#12290;&#26032;&#20056;&#27861;&#22120;&#30340;&#36755;&#20986;&#36890;&#36807;&#36880;&#20301;&#21152;&#26435;&#32047;&#31215;&#36827;&#34892;&#30456;&#21152;&#65292;&#24182;&#19988;&#32047;&#31215;&#32467;&#26524;&#19982;&#29616;&#26377;&#35745;&#31639;&#24179;&#21488;&#20860;&#23481;&#65292;&#21487;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#25110;&#38750;&#32479;&#19968;&#37327;&#21270;&#12290;&#30001;&#20110;&#20056;&#27861;&#20989;&#25968;&#34987;&#31616;&#21333;&#30340;&#36923;&#36753;&#25237;&#24433;&#25152;&#21462;&#20195;&#65292;&#23548;&#33268;&#33021;&#37327;&#25928;&#29575;&#21644;&#35745;&#31639;&#25928;&#26524;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18595v1 Announce Type: cross  Abstract: Deep neural networks (DNNs) have achieved great breakthroughs in many fields such as image classification and natural language processing. However, the execution of DNNs needs to conduct massive numbers of multiply-accumulate (MAC) operations on hardware and thus incurs a large power consumption. To address this challenge, we propose a novel digital MAC design based on encoding. In this new design, the multipliers are replaced by simple logic gates to project the results onto a wide bit representation. These bits carry individual position weights, which can be trained for specific neural networks to enhance inference accuracy. The outputs of the new multipliers are added by bit-wise weighted accumulation and the accumulation results are compatible with existing computing platforms accelerating neural networks with either uniform or non-uniform quantization. Since the multiplication function is replaced by simple logic projection, the c
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#20272;&#25581;&#31034;&#20102;&#36965;&#24863;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#26032;&#22411;&#26041;&#27861;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.13791</link><description>&lt;p&gt;
&#25171;&#24320;&#40657;&#21283;&#23376;&#65306;&#36965;&#24863;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#20272;&#25581;&#31034;&#20102;&#36965;&#24863;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#26032;&#22411;&#26041;&#27861;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#36965;&#24863;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#30340;&#20027;&#23548;&#24314;&#27169;&#33539;&#24335;&#12290;&#23613;&#31649;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#36816;&#34892;&#26426;&#21046;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#36804;&#20170;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#27010;&#36848;&#65292;&#24635;&#32467;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21450;&#20854;&#30446;&#26631;&#12289;&#21457;&#29616;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#31995;&#32479;&#24615;&#35780;&#20272;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#21487;&#35299;&#37322;AI&#22312;&#36965;&#24863;&#20013;&#30340;&#20351;&#29992;&#20027;&#35201;&#36235;&#21183;&#65292;&#25581;&#31034;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#25361;&#25112;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#21644;&#26032;&#20852;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#35299;&#37322;&#35299;&#37322;&#30340;&#24120;&#35265;&#27169;&#24335;&#65292;&#35752;&#35770;&#20102;&#36965;&#24863;&#20013;&#25552;&#21462;&#30340;&#31185;&#23398;&#35265;&#35299;&#65292;&#24182;&#21453;&#24605;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#29616;&#26377;&#25216;&#26415;&#30340;&#23436;&#25972;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13791v1 Announce Type: new  Abstract: In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation. Our review provides a complete summary of the state-of-the-art in the field.
&lt;/p&gt;</description></item><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item><item><title>DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02989</link><description>&lt;p&gt;
DexDiffuser: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;
&lt;/p&gt;
&lt;p&gt;
DexDiffuser: Generating Dexterous Grasps with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02989
&lt;/p&gt;
&lt;p&gt;
DexDiffuser&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28789;&#24039;&#25235;&#21462;&#23039;&#21183;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29289;&#20307;&#28857;&#20113;&#30340;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#25235;&#21462;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DexDiffuser&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28789;&#24039;&#25235;&#21462;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#37096;&#20998;&#29289;&#20307;&#28857;&#20113;&#19978;&#29983;&#25104;&#12289;&#35780;&#20272;&#21644;&#20248;&#21270;&#25235;&#21462;&#23039;&#21183;&#12290;DexDiffuser&#21253;&#25324;&#26465;&#20214;&#25193;&#25955;&#22411;&#25235;&#21462;&#37319;&#26679;&#22120;DexSampler&#21644;&#28789;&#24039;&#25235;&#21462;&#35780;&#20272;&#22120;DexEvaluator&#12290;DexSampler&#36890;&#36807;&#23545;&#38543;&#26426;&#25235;&#21462;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#65292;&#29983;&#25104;&#19982;&#29289;&#20307;&#28857;&#20113;&#26465;&#20214;&#30456;&#20851;&#30340;&#39640;&#36136;&#37327;&#25235;&#21462;&#23039;&#21183;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#31181;&#25235;&#21462;&#20248;&#21270;&#31574;&#30053;&#65306;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#25193;&#25955;(Evaluator-Guided Diffusion&#65292;EGD)&#21644;&#22522;&#20110;&#35780;&#20272;&#22120;&#30340;&#37319;&#26679;&#20248;&#21270;(Evaluator-based Sampling Refinement&#65292;ESR)&#12290;&#25105;&#20204;&#22312;&#34394;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Allegro Hand&#36827;&#34892;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;DexDiffuser&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#22810;&#25351;&#25235;&#21462;&#29983;&#25104;&#26041;&#27861;FFHNet&#65292;&#24179;&#22343;&#25235;&#21462;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;21.71-22.20%&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DexDiffuser, a novel dexterous grasping method that generates, evaluates, and refines grasps on partial object point clouds. DexDiffuser includes the conditional diffusion-based grasp sampler DexSampler and the dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality grasps conditioned on object point clouds by iterative denoising of randomly sampled grasps. We also introduce two grasp refinement strategies: Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR). Our simulation and real-world experiments on the Allegro Hand consistently demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp success rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02304</link><description>&lt;p&gt;
&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep Learning Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21152;&#24378;&#30340;&#39640;&#25928;&#25968;&#20540;&#27874;&#20256;&#25773;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#20540;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#65292;&#20248;&#21270;&#31639;&#27861;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#21644;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;&#36895;&#24230;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20174;&#22320;&#38663;&#24314;&#27169;&#21040;&#21307;&#23398;&#25104;&#20687;&#65292;&#23545;&#20110;&#39640;&#39057;&#27874;&#20256;&#25773;&#30340;&#39640;&#20445;&#30495;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#22312;&#27874;&#20256;&#25773;&#27169;&#22411;&#20013;&#30340;&#19968;&#39033;&#36827;&#23637;&#21033;&#29992;&#36275;&#22815;&#20934;&#30830;&#30340;&#32454;&#27714;&#35299;&#22120;&#36755;&#20986;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#24555;&#36895;&#20294;&#19981;&#20934;&#30830;&#30340;&#31895;&#27714;&#35299;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#31283;&#23450;&#19988;&#24555;&#36895;&#30340;&#27714;&#35299;&#22120;&#36824;&#20801;&#35768;&#20351;&#29992;&#24182;&#34892;&#26102;&#38388;&#31639;&#27861;Parareal&#26469;&#25552;&#21462;&#21644;&#32416;&#27491;&#39640;&#39057;&#27874;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Nguyen&#21644;Tsai&#65288;2023&#65289;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#31995;&#32479;&#65292;&#23558;&#25968;&#20540;&#27714;&#35299;&#22120;&#19982;&#28145;&#24230;&#23398;&#20064;&#32452;&#20214;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#26694;&#26550;&#20013;&#12290;&#22312;&#25552;&#20986;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#21644;Parareal&#26041;&#26696;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#21327;&#35843;&#30340;&#32467;&#26500;&#22312;&#19981;&#29306;&#29298;&#36895;&#24230;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
In a variety of scientific and engineering domains, ranging from seismic modeling to medical imaging, the need for high-fidelity and efficient solutions for high-frequency wave propagation holds great significance. Recent advances in wave modeling use sufficiently accurate fine solver outputs to train neural networks that enhance the accuracy of a fast but inaccurate coarse solver. A stable and fast solver further allows the use of Parareal, a parallel-in-time algorithm to retrieve and correct high-frequency wave components. In this paper we build upon the work of Nguyen and Tsai (2023) and present a novel unified system that integrates a numerical solver with deep learning components into an end-to-end framework. In the proposed setting, we investigate refinements to the neural network architecture, data generation algorithm and Parareal scheme. Our results show that the cohesive structure significantly improves performance without sacrificing speed, and demonstrate the importance of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01810</link><description>&lt;p&gt;
&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#30340;&#38169;&#35823;&#35268;&#33539;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Misspecification uncertainties in near-deterministic regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36817;&#30830;&#23450;&#24615;&#22238;&#24402;&#20013;&#38169;&#35823;&#35268;&#33539;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#21644;&#25511;&#21046;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25439;&#22833;&#26159;&#27169;&#22411;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#21487;&#29992;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;PAC-Bayes&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#25439;&#22833;&#26368;&#23567;&#21270;&#34987;&#35748;&#20026;&#24573;&#30053;&#20102;&#38169;&#35823;&#35268;&#33539;&#21270;&#65292;&#21363;&#27169;&#22411;&#19981;&#33021;&#23436;&#20840;&#22797;&#21046;&#35266;&#27979;&#32467;&#26524;&#12290;&#36825;&#23548;&#33268;&#22823;&#25968;&#25454;&#25110;&#27424;&#21442;&#25968;&#21270;&#26497;&#38480;&#19979;&#23545;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#33879;&#20302;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#36817;&#30830;&#23450;&#24615;&#12289;&#38169;&#35823;&#35268;&#33539;&#21270;&#21644;&#27424;&#21442;&#25968;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36825;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#24191;&#27867;&#30456;&#20851;&#30340;&#19968;&#20010;&#39046;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#21518;&#39564;&#20998;&#24067;&#24517;&#39035;&#35206;&#30422;&#27599;&#20010;&#35757;&#32451;&#28857;&#65292;&#20197;&#36991;&#20813;&#21457;&#25955;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#31526;&#21512;&#36825;&#20010;&#32422;&#26463;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#23545;&#20110;&#32447;&#24615;&#27169;&#22411;&#65292;&#36825;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#39069;&#22806;&#24320;&#38144;&#26368;&#23567;&#12290;&#36825;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#27169;&#22411;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#21407;&#23376;&#23610;&#24230;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expected loss is an upper bound to the model generalization error which admits robust PAC-Bayes bounds for learning. However, loss minimization is known to ignore misspecification, where models cannot exactly reproduce observations. This leads to significant underestimates of parameter uncertainties in the large data, or underparameterized, limit. We analyze the generalization error of near-deterministic, misspecified and underparametrized surrogate models, a regime of broad relevance in science and engineering. We show posterior distributions must cover every training point to avoid a divergent generalization error and derive an ensemble {ansatz} that respects this constraint, which for linear models incurs minimal overhead. The efficient approach is demonstrated on model problems before application to high dimensional datasets in atomistic machine learning. Parameter uncertainties from misspecification survive in the underparametrized limit, giving accurate prediction and boundin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2402.01720</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Amharic Chatbot for FAQs in Universities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38463;&#22982;&#21704;&#25289;&#35821;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#23398;&#29983;&#35299;&#31572;&#24120;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23398;&#29983;&#24120;&#24120;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#21521;&#31649;&#29702;&#21592;&#25110;&#25945;&#24072;&#23547;&#27714;&#24120;&#35265;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#36825;&#23545;&#21452;&#26041;&#26469;&#35828;&#37117;&#24456;&#32321;&#29712;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#38463;&#22982;&#21704;&#25289;&#35821;&#20013;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#65292;&#20316;&#20026;&#34394;&#25311;&#21161;&#25163;&#22788;&#29702;&#38382;&#39064;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31243;&#24207;&#20351;&#29992;&#26631;&#35760;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#21435;&#38500;&#20572;&#29992;&#35789;&#21644;&#35789;&#24178;&#25552;&#21462;&#23545;&#38463;&#22982;&#21704;&#25289;&#35821;&#36755;&#20837;&#21477;&#23376;&#36827;&#34892;&#20998;&#26512;&#21644;&#20998;&#31867;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31639;&#27861;&#26469;&#20998;&#31867;&#26631;&#35760;&#21644;&#26816;&#32034;&#21512;&#36866;&#30340;&#22238;&#31572;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#36890;&#36807;TensorFlow&#12289;Keras&#21644;NLTK&#23454;&#29616;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;
University students often spend a considerable amount of time seeking answers to common questions from administrators or teachers. This can become tedious for both parties, leading to a need for a solution. In response, this paper proposes a chatbot model that utilizes natural language processing and deep learning techniques to answer frequently asked questions (FAQs) in the Amharic language. Chatbots are computer programs that simulate human conversation through the use of artificial intelligence (AI), acting as a virtual assistant to handle questions and other tasks. The proposed chatbot program employs tokenization, normalization, stop word removal, and stemming to analyze and categorize Amharic input sentences. Three machine learning model algorithms were used to classify tokens and retrieve appropriate responses: Support Vector Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented through TensorFlow, Keras, and NLTK. The deep learning model achieved the be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2401.08468</link><description>&lt;p&gt;
&#20445;&#30041;&#36824;&#26159;&#20002;&#24323;&#65311;&#19968;&#31181;&#35780;&#20272;&#26377;&#22122;&#22768;ICA&#35299;&#20915;&#26041;&#26696;&#30340;&#38750;&#21442;&#25968;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#65288;ICA&#65289;&#20110;20&#19990;&#32426;80&#24180;&#20195;&#24341;&#20837;&#65292;&#20316;&#20026;&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#27169;&#22411;&#65292;&#25351;&#30340;&#26159;&#22312;&#23545;&#28151;&#21512;&#20449;&#21495;&#36827;&#34892;&#24674;&#22797;&#26102;&#65292;&#23545;&#28304;&#20449;&#21495;&#25110;&#28151;&#21512;&#36807;&#31243;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#31934;&#23494;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#20294;&#19981;&#21516;&#26041;&#27861;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;ICA&#31639;&#27861;&#21644;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#12290;&#35813;&#20998;&#25968;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#21482;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#26377;&#38480;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#26469;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#23545;&#27604;&#20989;&#25968;&#21644;&#31639;&#27861;&#65292;&#23427;&#20204;&#20855;&#26377;&#19982;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;FASTICA&#21644;JADE&#65289;&#30456;&#21516;&#30340;&#24555;&#36895;&#35745;&#31639;&#24615;&#33021;&#65292;&#20294;&#22312;&#21069;&#32773;&#21487;&#33021;&#22833;&#36133;&#30340;&#39046;&#22495;&#20013;&#24037;&#20316;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20063;&#21487;&#33021;&#23384;&#22312;&#32570;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.08426</link><description>&lt;p&gt;
GD&#26080;&#27861;&#32988;&#20219;&#65306;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#19977;&#31181;&#24433;&#21709;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
GD doesn't make the cut: Three ways that non-differentiability affects neural network training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21487;&#24494;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#25910;&#25947;&#24615;&#24046;&#24322;&#12289;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#20197;&#21450;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#30340;&#19981;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#38750;&#21487;&#24494;&#20989;&#25968;&#65288;NGDMs&#65289;&#21644;&#24212;&#29992;&#20110;&#21487;&#24494;&#20989;&#25968;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#65288;GDs&#65289;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;NGDMs&#30340;&#25910;&#25947;&#24615;&#36136;&#19982;GDs&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#25361;&#25112;&#20102;&#22522;&#20110;$L$-&#20809;&#28369;&#24615;&#30340;&#24191;&#27867;&#31070;&#32463;&#32593;&#32476;&#25910;&#25947;&#25991;&#29486;&#23545;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NGDM&#35299;&#20915;$L_1$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#30683;&#30462;&#24615;&#36136;&#65292;&#34920;&#26126;&#22686;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#20250;&#23548;&#33268;NGDMs&#20013;&#26368;&#20248;&#35299;&#30340;$L_1$&#33539;&#25968;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24191;&#27867;&#37319;&#29992;&#30340;&#22522;&#20110;$L_1$&#24809;&#32602;&#30340;&#32593;&#32476;&#20462;&#21098;&#25216;&#26415;&#24182;&#26410;&#20135;&#29983;&#39044;&#26399;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31283;&#23450;&#36793;&#30028;&#29616;&#35937;&#65288;Edge of Stability&#65289;&#65292;&#25351;&#20986;&#21363;&#20351;&#23545;&#20110;Lipschitz&#36830;&#32493;&#20984;&#21487;&#24494;&#20989;&#25968;&#65292;&#23427;&#20063;&#19981;&#36866;&#29992;&#20110;&#38750;&#20984;&#38750;&#21487;&#24494;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the distinctions between gradient methods applied to non-differentiable functions (NGDMs) and classical gradient descents (GDs) designed for differentiable functions. First, we demonstrate significant differences in the convergence properties of NGDMs compared to GDs, challenging the applicability of the extensive neural network convergence literature based on $L-smoothness$ to non-smooth neural networks. Next, we demonstrate the paradoxical nature of NGDM solutions for $L_{1}$-regularized problems, showing that increasing the regularization penalty leads to an increase in the $L_{1}$ norm of optimal solutions in NGDMs. Consequently, we show that widely adopted $L_{1}$ penalization-based techniques for network pruning do not yield expected results. Finally, we explore the Edge of Stability phenomenon, indicating its inapplicability even to Lipschitz continuous convex differentiable functions, leaving its relevance to non-convex non-differentiable neural networks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.07624</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#29992;&#20110;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A dynamical clipping approach with task feedback for Proximal Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07624
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#21453;&#39304;&#30340;&#21160;&#24577;&#21098;&#35009;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#26469;&#20248;&#21270;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20248;&#21270;&#21644;&#26426;&#22120;&#20154;&#23398;&#20064;&#31561;&#12290;&#28982;&#32780;&#65292;PPO&#21463;&#21040;&#22266;&#23450;&#21098;&#35009;&#36793;&#30028;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30446;&#21069;&#27809;&#26377;&#29702;&#35770;&#35777;&#26126;&#26368;&#20339;&#21098;&#35009;&#36793;&#30028;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#29420;&#29305;&#30340;&#21098;&#35009;&#36793;&#30028;&#25130;&#26029;&#26032;&#26087;&#31574;&#30053;&#30340;&#27604;&#29575;&#65292;&#21487;&#20197;&#30830;&#20445;&#31283;&#23450;&#30340;&#35757;&#32451;&#24182;&#23454;&#29616;&#26368;&#20339;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22266;&#23450;&#30340;&#21098;&#35009;&#36793;&#30028;&#38480;&#21046;&#20102;agent&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#19968;&#31181;&#21160;&#24577;&#21098;&#35009;&#36793;&#30028;&#20197;&#22686;&#24378;PPO&#30340;&#24615;&#33021;&#26159;&#38750;&#24120;&#26377;&#30410;&#30340;&#12290;&#19982;&#20197;&#24448;&#30340;&#21098;&#35009;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#20013;&#22686;&#21152;&#26368;&#22823;&#32047;&#31215;&#22238;&#25253;&#35270;&#20316;RL&#20219;&#21153;&#30340;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07624v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization (PPO) has been broadly applied to various domains, including Large Language Model (LLM) optimization and Robotics learning, etc. However, PPO is limited by a fixed setting for the clipping bound. Specifically, there is no theoretical proof that the optimal clipping bound remains consistent throughout the entire training process. Truncating the ratio of the new and old policies with a unique clipping bound ensures stable training and can achieve the best training performance. Additionally, previous research suggests that a fixed clipping bound limits the agent's exploration. Therefore, researching a dynamical clipping bound to enhance PPO's performance can be highly beneficial. Different from previous clipping approaches, we consider increasing the maximum cumulative Return in reinforcement learning (RL) tasks as the preference of the RL task, and propose a bi-level proximal policy optimization parad
&lt;/p&gt;</description></item><item><title>&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;</title><link>https://arxiv.org/abs/2312.05269</link><description>&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#65306;&#21033;&#29992;LLMs&#22238;&#31572;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#20013;&#30340;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05269
&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#35760;&#24518;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26041;&#24335;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#21033;&#29992;&#38646;-shot&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#65292;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32456;&#36523;&#35760;&#24518;(LifelongMemory)&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#21644;&#26816;&#32034;&#26469;&#35775;&#38382;&#38271;&#31687;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#23384;&#20648;&#12290;&#32456;&#36523;&#35760;&#24518;&#29983;&#25104;&#25668;&#20687;&#26426;&#20329;&#25140;&#32773;&#30340;&#31616;&#27905;&#35270;&#39057;&#27963;&#21160;&#25551;&#36848;&#65292;&#24182;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;-shot&#33021;&#21147;&#26469;&#25512;&#29702;&#38271;&#31687;&#35270;&#39057;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#32456;&#36523;&#35760;&#24518;&#20351;&#29992;&#32622;&#20449;&#24230;&#21644;&#35299;&#37322;&#27169;&#22359;&#26469;&#20135;&#29983;&#33258;&#20449;&#12289;&#39640;&#36136;&#37327;&#21644;&#21487;&#35299;&#37322;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;EgoSchema&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;Ego4D&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;(NLQ)&#25361;&#25112;&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Agentic-Learning-AI-Lab/lifelong-memory &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05269v2 Announce Type: replace-cross  Abstract: In this paper we introduce LifelongMemory, a new framework for accessing long-form egocentric videographic memory through natural language question answering and retrieval. LifelongMemory generates concise video activity descriptions of the camera wearer and leverages the zero-shot capabilities of pretrained large language models to perform reasoning over long-form video context. Furthermore, Lifelong Memory uses a confidence and explanation module to produce confident, high-quality, and interpretable answers. Our approach achieves state-of-the-art performance on the EgoSchema benchmark for question answering and is highly competitive on the natural language query (NLQ) challenge of Ego4D. Code is available at https://github.com/Agentic-Learning-AI-Lab/lifelong-memory.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#29983;&#25104;&#26684;&#32599;&#24067;&#32435;&#22522;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.12904</link><description>&lt;p&gt;
&#23398;&#20064;&#35745;&#31639;&#26684;&#32599;&#24067;&#32435;&#22522;
&lt;/p&gt;
&lt;p&gt;
Learning to Compute Gr\"obner Bases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#65292;&#36890;&#36807;&#35299;&#20915;&#38543;&#26426;&#29983;&#25104;&#26684;&#32599;&#24067;&#32435;&#22522;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22810;&#39033;&#24335;&#31995;&#32479;&#65292;&#25110;&#35745;&#31639;&#30456;&#20851;&#30340;&#26684;&#32599;&#24067;&#32435;&#22522;&#65292;&#19968;&#30452;&#26159;&#35745;&#31639;&#20195;&#25968;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#26114;&#36149;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#26159;&#25351;&#25968;&#32423;&#21452;&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;Transformer&#30340;&#35757;&#32451;&#26469;&#23454;&#29616;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#35745;&#31639;&#12290;&#35757;&#32451;&#38656;&#35201;&#35768;&#22810;&#22810;&#39033;&#24335;&#31995;&#32479;&#21644;&#30456;&#20851;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#37197;&#23545;&#65292;&#24341;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#20195;&#25968;&#38382;&#39064;&#65306;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#38543;&#26426;&#29983;&#25104;&#21644;&#23558;&#20854;&#36716;&#21270;&#20026;&#38750;&#26684;&#32599;&#24067;&#32435;&#22810;&#39033;&#24335;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#26684;&#32599;&#24067;&#32435;&#21453;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#32500;&#26681;&#27491;&#29702;&#24819;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#20123;&#29702;&#24819;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#26041;&#27861;&#27604;&#26420;&#32032;&#26041;&#27861;&#24555;&#19977;&#21040;&#20845;&#20010;&#25968;&#37327;&#32423;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#35745;&#31639;&#26684;&#32599;&#24067;&#32435;&#22522;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving a polynomial system, or computing an associated Gr\"obner basis, has been a fundamental task in computational algebra. However, it is also known for its notoriously expensive computational cost - doubly exponential time complexity in the number of variables in the worst case. In this paper, we achieve for the first time Gr\"obner basis computation through the training of a Transformer. The training requires many pairs of a polynomial system and the associated Gr\"obner basis, raising two novel algebraic problems: random generation of Gr\"obner bases and the transformation of them into non-Gr\"obner polynomial systems, termed as backward Gr\"obner problem. We resolve these problems with zero-dimensional radical ideals, the ideals appearing in various applications. The experiments show that the proposed dataset generation method is three to six orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Gr\"obner bases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#21475;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#25552;&#39640;&#25277;&#26679;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#38598;&#21512;&#30340;&#30456;&#20114;&#20316;&#29992;&#20419;&#36827;&#26410;&#21457;&#29616;&#27169;&#24577;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.15645</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#21512;&#30340;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ensemble-Based Annealed Importance Sampling. (arXiv:2401.15645v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20154;&#21475;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#25552;&#39640;&#25277;&#26679;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#38598;&#21512;&#30340;&#30456;&#20114;&#20316;&#29992;&#20419;&#36827;&#26410;&#21457;&#29616;&#27169;&#24577;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#20998;&#24067;&#20013;&#36827;&#34892;&#25277;&#26679;&#26159;&#35745;&#31639;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#36864;&#28779;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;AIS&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#21512;&#30340;AIS&#29256;&#26412;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#22522;&#20110;&#20154;&#21475;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;&#36890;&#36807;&#36319;&#36394;&#38598;&#21512;&#32780;&#19981;&#26159;&#21333;&#20010;&#31890;&#23376;&#27839;&#36215;&#22987;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#26576;&#20010;&#24310;&#32493;&#36335;&#24452;&#65292;&#25105;&#20204;&#21033;&#29992;&#38598;&#21512;&#20869;&#30340;&#30456;&#20114;&#20316;&#29992;&#26469;&#20419;&#36827;&#26410;&#21457;&#29616;&#27169;&#24577;&#30340;&#25506;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#21033;&#29992;Snooker&#31639;&#27861;&#25110;&#36827;&#21270;&#33945;&#29305;&#21345;&#27931;&#20013;&#20351;&#29992;&#30340;&#36951;&#20256;&#31639;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23454;&#29616;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#21644;&#22343;&#22330;&#26497;&#38480;&#19979;&#25511;&#21046;&#38598;&#21512;&#28436;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling from a multimodal distribution is a fundamental and challenging problem in computational science and statistics. Among various approaches proposed for this task, one popular method is Annealed Importance Sampling (AIS). In this paper, we propose an ensemble-based version of AIS by combining it with population-based Monte Carlo methods to improve its efficiency. By keeping track of an ensemble instead of a single particle along some continuation path between the starting distribution and the target distribution, we take advantage of the interaction within the ensemble to encourage the exploration of undiscovered modes. Specifically, our main idea is to utilize either the snooker algorithm or the genetic algorithm used in Evolutionary Monte Carlo. We discuss how the proposed algorithm can be implemented and derive a partial differential equation governing the evolution of the ensemble under the continuous time and mean-field limit. We also test the efficiency of the proposed alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24120;&#25968;&#30340;&#22833;&#36133;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13971</link><description>&lt;p&gt;
&#36229;&#36807;Lipschitz&#36830;&#32493;&#24615;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity. (arXiv:2401.13971v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#24120;&#25968;&#30340;&#22833;&#36133;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27809;&#26377;&#26631;&#20934;Lipschitz&#36830;&#32493;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#30340;&#38543;&#26426;&#24369;&#20984;&#20248;&#21270;&#12290;&#22522;&#20110;&#26032;&#30340;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#65288;&#27493;&#38271;&#65289;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#65292;&#20855;&#26377;$\mathcal{O} ( 1 / \sqrt{K})$&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#22833;&#36133;&#29575;&#20026;&#24120;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#30456;&#24403;&#24369;&#30340;&#20551;&#35774;&#65306;Lipschitz&#21442;&#25968;&#21487;&#20197;&#36890;&#36807;$\|x\|$&#30340;&#19968;&#33324;&#22686;&#38271;&#20989;&#25968;&#26469;&#38480;&#21046;&#65292;&#25110;&#32773;&#36890;&#36807;&#29420;&#31435;&#38543;&#26426;&#26679;&#26412;&#36827;&#34892;&#23616;&#37096;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers stochastic weakly convex optimization without the standard Lipschitz continuity assumption. Based on new adaptive regularization (stepsize) strategies, we show that a wide class of stochastic algorithms, including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 / \sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on rather weak assumptions: the Lipschitz parameter can be either bounded by a general growth function of $\|x\|$ or locally estimated through independent random samples.
&lt;/p&gt;</description></item><item><title>EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11124</link><description>&lt;p&gt;
EMA-Net: &#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#29992;&#20110;&#31264;&#23494;&#22330;&#26223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EMA-Net: Efficient Multitask Affinity Learning for Dense Scene Predictions. (arXiv:2401.11124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11124
&lt;/p&gt;
&lt;p&gt;
EMA-Net &#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#27169;&#22359;(CTAL)&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22240;&#20854;&#33021;&#22815;&#32852;&#21512;&#39044;&#27979;&#22810;&#20010;&#20219;&#21153;&#65292;&#22312;&#20351;&#29992;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#27599;&#20010;&#20219;&#21153;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#20197;&#35299;&#30721;&#22120;&#20026;&#37325;&#28857;&#30340;&#26550;&#26500;&#36890;&#36807;&#20351;&#29992;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#22810;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#20197;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#24335;&#21516;&#26102;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#20197;&#21450;&#36328;&#20219;&#21153;&#27169;&#24335;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#22810;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#32593;&#32476;&#65288;EMA-Net&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#20219;&#21153;&#25913;&#36827;&#33021;&#21147;&#12290;EMA-Net&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#36328;&#20219;&#21153;&#20851;&#32852;&#23398;&#20064;&#65288;CTAL&#65289;&#27169;&#22359;&#24039;&#22937;&#22320;&#25429;&#25417;&#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#36328;&#20219;&#21153;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;CTAL&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#33021;&#22815;&#20197;&#26368;&#36866;&#21512;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#30340;&#26041;&#24335;&#25805;&#32437;&#20219;&#21153;&#20146;&#21644;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multitask learning (MTL) has gained prominence for its ability to jointly predict multiple tasks, achieving better per-task performance while using fewer per-task model parameters than single-task learning. More recently, decoder-focused architectures have considerably improved multitask performance by refining task predictions using the features of other related tasks. However, most of these refinement methods fail to simultaneously capture local and global task-specific representations, as well as cross-task patterns in a parameter-efficient manner. In this paper, we introduce the Efficient Multitask Affinity Learning Network (EMA-Net), which is a lightweight framework that enhances the task refinement capabilities of multitask networks. EMA-Net adeptly captures local, global, and cross-task interactions using our novel Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in its ability to manipulate task affinity matrices in a manner that is optimally suited t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09452</link><description>&lt;p&gt;
&#20351;&#29992;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#23398;&#20064;&#39134;&#26426;&#26426;&#32764;&#19978;&#30340;&#21387;&#21147;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#24212;&#29992;&#20110;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#27668;&#21160;&#31995;&#25968;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#26426;&#30340;&#27668;&#21160;&#31995;&#25968;&#21463;&#20854;&#20960;&#20309;&#24418;&#29366;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#25915;&#35282;&#36739;&#22823;&#26102;&#12290;&#22312;&#31354;&#27668;&#21160;&#21147;&#23398;&#39046;&#22495;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#21442;&#25968;&#26469;&#25551;&#36848;&#32764;&#22411;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32764;&#30340;&#19977;&#32500;&#20960;&#20309;&#24418;&#29366;&#27604;&#20108;&#32500;&#32764;&#22411;&#22797;&#26434;&#65292;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#38590;&#20197;&#20934;&#30830;&#34920;&#31034;&#32764;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#30340;&#25972;&#20307;&#24418;&#29366;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#29992;&#20110;&#25551;&#36848;&#20108;&#32500;&#32764;&#22411;&#25110;&#32764;&#25130;&#38754;&#24418;&#29366;&#30340;&#22823;&#37327;&#28508;&#22312;&#31070;&#32463;&#34920;&#31034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#20960;&#20309;&#29305;&#24449;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#27668;&#21160;&#31995;&#25968;&#30340;&#20934;&#30830;&#24615;&#12290;&#21463;&#20960;&#20309;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#40654;&#26364;&#20960;&#20309;&#29305;&#24449;&#32435;&#20837;&#23398;&#20064;&#32764;&#38754;&#21387;&#21147;&#31995;&#25968;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#20960;&#20309;&#29305;&#24449;&#65288;&#40654;&#26364;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aerodynamic coefficients of aircrafts are significantly impacted by its geometry, especially when the angle of attack (AoA) is large. In the field of aerodynamics, traditional polynomial-based parameterization uses as few parameters as possible to describe the geometry of an airfoil. However, because the 3D geometry of a wing is more complicated than the 2D airfoil, polynomial-based parameterizations have difficulty in accurately representing the entire shape of a wing in 3D space. Existing deep learning-based methods can extract massive latent neural representations for the shape of 2D airfoils or 2D slices of wings. Recent studies highlight that directly taking geometric features as inputs to the neural networks can improve the accuracy of predicted aerodynamic coefficients. Motivated by geometry theory, we propose to incorporate Riemannian geometric features for learning Coefficient of Pressure (CP) distributions on wing surfaces. Our method calculates geometric features (Rieman
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.10656</link><description>&lt;p&gt;
&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65306;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20154;&#31867;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Virtual Human Generative Model: Masked Modeling Approach for Learning Human Characteristics. (arXiv:2306.10656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VHGM&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22522;&#20110;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20581;&#24247;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#20363;&#22914;&#29992;&#20110;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21307;&#30103;&#23646;&#24615;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20154;&#26684;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#21892;&#36523;&#20307;&#21644;&#31934;&#31070;&#29366;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34394;&#25311;&#20154;&#31867;&#29983;&#25104;&#27169;&#22411;&#65288;VHGM&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20272;&#35745;&#26377;&#20851;&#21307;&#30103;&#20445;&#20581;&#12289;&#29983;&#27963;&#26041;&#24335;&#21644;&#20010;&#24615;&#30340;&#23646;&#24615;&#12290;VHGM&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#30721;&#24314;&#27169;&#35757;&#32451;&#65292;&#22312;&#24050;&#30693;&#23646;&#24615;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#23646;&#24615;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#21033;&#29992;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;VHGM&#39640;&#25928;&#22320;&#23398;&#20064;&#20102;&#36229;&#36807;1,800&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#25968;&#20540;&#35780;&#20272;&#20102;VHGM&#21450;&#20854;&#35757;&#32451;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;VHGM&#30340;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#24212;&#29992;&#31243;&#24207;&#65292;&#28436;&#31034;&#20102;&#29992;&#25143;&#24773;&#22659;&#65292;&#20363;&#22914;&#21307;&#30103;&#23646;&#24615;&#30340;&#34394;&#25311;&#27979;&#37327;&#21644;&#29983;&#27963;&#26041;&#24335;&#30340;&#20551;&#35774;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the relationship between healthcare attributes, lifestyles, and personality is vital for understanding and improving physical and mental conditions. Machine learning approaches are promising for modeling their relationships and offering actionable suggestions. In this paper, we propose Virtual Human Generative Model (VHGM), a machine learning model for estimating attributes about healthcare, lifestyles, and personalities. VHGM is a deep generative model trained with masked modeling to learn the joint distribution of attributes conditioned on known ones. Using heterogeneous tabular datasets, VHGM learns more than 1,800 attributes efficiently. We numerically evaluate the performance of VHGM and its training techniques. As a proof-of-concept of VHGM, we present several applications demonstrating user scenarios, such as virtual measurements of healthcare attributes and hypothesis verifications of lifestyles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.03202</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Distributionally Robust Optimization. (arXiv:2306.03202v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807; Gateaux Derivative &#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#19968;&#31867;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#22312;&#20998;&#24067;&#19978;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#25991;&#29486;&#26377;&#25152;&#19981;&#21516;&#12290;&#20026;&#35299;&#20915;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#20248;&#21270;&#38750;&#32447;&#24615;&#20989;&#25968;&#38754;&#20020;&#30340;&#29702;&#35770;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Derivative&#21644;&#30456;&#24212;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#65292;&#22522;&#20110;Gateaux Derivative&#26469;&#22788;&#29702;&#19968;&#33324;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;Var&#12289;entropic risk&#21644;&#26377;&#38480;&#25903;&#25345;&#38598;&#19978;&#30340;&#19977;&#20010;&#36816;&#34892;&#39118;&#38505;&#24230;&#37327;&#31034;&#20363;&#26469;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20026;&#27010;&#29575;&#31354;&#38388;&#20013;&#19968;&#33324;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;G-derivative&#30340;Frank-Wolfe&#65288;FW&#65289;&#31639;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#29420;&#31435;&#20110;&#33539;&#25968;&#30340;&#26041;&#24335;&#25512;&#23548;&#20986;&#20854;&#25910;&#25947;&#24615;&#22312;&#25552;&#20986;&#30340;&#24179;&#28369;&#24230;&#27010;&#24565;&#19979;&#12290;&#25105;&#20204;&#21033;&#29992;FW&#31639;&#27861;&#30340;&#35774;&#32622;&#26469;&#35774;&#35745;&#19968;&#31181;&#35745;&#31639;&#38750;&#32447;&#24615;DRO&#38382;&#39064;&#38797;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22788;&#29702;&#20998;&#24067;&#30340;&#38750;&#32447;&#24615;&#30446;&#26631;&#20989;&#25968;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-lin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15612</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#39640;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#34987;&#24191;&#27867;&#29992;&#20316;&#26367;&#20195;&#20989;&#25968;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#32473;&#23450;&#36755;&#20837;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20989;&#25968;&#35780;&#20272;&#30340;&#26174;&#24335;&#20998;&#24067;&#12290;&#38500;&#20102;&#22522;&#20110;&#27010;&#29575;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#25552;&#20986;&#26469;&#20272;&#35745;&#30456;&#23545;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#30456;&#23545;&#25509;&#36817;&#21644;&#30456;&#23545;&#36828;&#31163;&#30340;&#20004;&#32452;&#23494;&#24230;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#36825;&#20004;&#32452;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23494;&#24230;&#27604;&#12290;&#28982;&#32780;&#65292;&#27492;&#31574;&#30053;&#20013;&#20351;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14077</link><description>&lt;p&gt;
&#35686;&#24789;&#23574;&#23792;&#65306;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Mind the spikes: Benign overfitting of kernels and neural networks in fixed dimension. (arXiv:2305.14077v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22266;&#23450;&#32500;&#24230;&#19979;&#20869;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#21457;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#20851;&#38190;&#22312;&#20110;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#32780;&#19981;&#26159;&#32500;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36798;&#21040;&#25509;&#36817;&#38646;&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#21363;&#20351;&#20272;&#35745;&#22120;&#25554;&#20540;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#36824;&#26159;&#20855;&#26377;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#23613;&#31649;&#26576;&#20123;&#23398;&#20064;&#26041;&#27861;&#30340;&#22266;&#23450;&#32500;&#24230;&#19979;&#24050;&#32463;&#30830;&#23450;&#20102;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#23545;&#20110;&#20856;&#22411;&#20869;&#26680;&#26041;&#27861;&#21644;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#38656;&#35201;&#39640;&#32500;&#24230;&#35774;&#32622;&#65292;&#20854;&#20013;&#32500;&#25968;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20272;&#35745;&#22120;&#30340;&#24179;&#28369;&#24230;&#26159;&#20851;&#38190;&#65292;&#32780;&#19981;&#26159;&#32500;&#25968;&#65306;&#21482;&#26377;&#24403;&#20272;&#35745;&#22120;&#30340;&#23548;&#25968;&#36275;&#22815;&#22823;&#26102;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#25165;&#21487;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#25554;&#20540;&#27169;&#22411;&#21644;&#26356;&#22810;&#20869;&#26680;&#65292;&#20197;&#34920;&#26126;&#22312;&#22266;&#23450;&#32500;&#24230;&#19979;&#20013;&#24230;&#23548;&#25968;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#29992;&#24207;&#21015;&#26680;&#36827;&#34892;&#22238;&#24402;&#26159;&#21487;&#33021;&#20986;&#29616;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of over-parameterized neural networks trained to near-zero training error has caused great interest in the phenomenon of benign overfitting, where estimators are statistically consistent even though they interpolate noisy training data. While benign overfitting in fixed dimension has been established for some learning methods, current literature suggests that for regression with typical kernel methods and wide neural networks, benign overfitting requires a high-dimensional setting where the dimension grows with the sample size. In this paper, we show that the smoothness of the estimators, and not the dimension, is the key: benign overfitting is possible if and only if the estimator's derivatives are large enough. We generalize existing inconsistency results to non-interpolating models and more kernels to show that benign overfitting with moderate derivatives is impossible in fixed dimension. Conversely, we show that benign overfitting is possible for regression with a seque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2302.02420</link><description>&lt;p&gt;
&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Uncertainty Quantification. (arXiv:2302.02420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#65288;DirectUQ&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#30452;&#25509;&#36755;&#20986;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#22120;&#21644;&#39118;&#38505;&#36793;&#30028;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26131;&#20110;&#35757;&#32451;&#65292;&#20294;&#20250;&#20135;&#29983;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#65307;&#32780;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#37327;&#21270;&#65292;&#20294;&#20248;&#21270;&#23427;&#20204;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#8220;&#30452;&#25509;&#19981;&#30830;&#23450;&#37327;&#21270;&#8221;&#65288;DirectUQ&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#36755;&#20986;&#26368;&#21518;&#19968;&#23618;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#12290;DirectUQ&#21487;&#20197;&#23548;&#20986;&#20026;&#19968;&#20010;&#26367;&#20195;&#30340;&#21464;&#20998;&#19979;&#30028;&#65292;&#22240;&#27492;&#20174;&#33853;&#21333;&#21464;&#20998;&#25512;&#29702;&#20013;&#33719;&#30410;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;&#38750;&#27010;&#29575;&#27169;&#22411;&#19968;&#26679;&#65292;DirectUQ&#20855;&#26377;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;Rademacher&#22797;&#26434;&#24615;&#20026;&#27169;&#22411;&#25552;&#20379;&#39118;&#38505;&#36793;&#30028;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DirectUQ&#21644;DirectUQ&#38598;&#25104;&#25552;&#20379;&#20102;&#26102;&#38388;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#38754;&#30340;&#33391;&#22909;&#24179;&#34913;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional neural networks are simple to train but they produce overconfident predictions, while Bayesian neural networks provide good uncertainty quantification but optimizing them is time consuming. This paper introduces a new approach, direct uncertainty quantification (DirectUQ), that combines their advantages where the neural network directly outputs the mean and variance of the last layer. DirectUQ can be derived as an alternative variational lower bound, and hence benefits from collapsed variational inference that provides improved regularizers. On the other hand, like non-probabilistic models, DirectUQ enjoys simple training and one can use Rademacher complexity to provide risk bounds for the model. Experiments show that DirectUQ and ensembles of DirectUQ provide a good tradeoff in terms of run time and uncertainty quantification, especially for out of distribution data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#35813;&#20195;&#30721;&#21487;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#29992;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20998;&#20139;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;GLCC&#20195;&#30721;&#26159;&#29616;&#26377;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#30340;&#19968;&#31181;&#29305;&#20363;&#65292;&#20855;&#26377;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2204.11168</link><description>&lt;p&gt;
&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65306;&#29992;&#20110;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35745;&#31639;&#36890;&#20449;&#25240;&#34935;&#30340;&#28789;&#27963;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation. (arXiv:2204.11168v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.11168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#35813;&#20195;&#30721;&#21487;&#20197;&#25552;&#20379;&#40065;&#26834;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#20351;&#29992;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#20998;&#20139;&#32534;&#30721;&#25968;&#25454;&#28857;&#65292;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;GLCC&#20195;&#30721;&#26159;&#29616;&#26377;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#30340;&#19968;&#31181;&#29305;&#20363;&#65292;&#20855;&#26377;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#22810;&#20010;&#36755;&#20837;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20219;&#24847;&#22810;&#20803;&#22810;&#39033;&#24335;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#20027;&#33410;&#28857;&#21644;&#22810;&#20010;&#24037;&#20316;&#33410;&#28857;&#30340;&#20998;&#24067;&#24335;&#35745;&#31639;&#31995;&#32479;&#12290;&#25552;&#20986;&#20102;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;GLCC&#65289;&#20195;&#30721;&#65292;&#21516;&#26102;&#25552;&#20379;&#38024;&#23545;&#19981;&#21450;&#26102;&#36820;&#22238;&#35745;&#31639;&#32467;&#26524;&#30340;&#33853;&#21518;&#32773;&#30340;&#40065;&#26834;&#24615;&#65292;&#38024;&#23545;&#24694;&#24847;&#24037;&#20154;&#25925;&#24847;&#20462;&#25913;&#32467;&#26524;&#20197;&#33719;&#21462;&#22909;&#22788;&#30340;&#23433;&#20840;&#24615;&#65292;&#20197;&#21450;&#22312;&#24037;&#20154;&#21487;&#33021;&#20849;&#35851;&#30340;&#24773;&#20917;&#19979;&#32500;&#25252;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#12290;GLCC&#20195;&#30721;&#39318;&#20808;&#23558;&#25968;&#25454;&#38598;&#20998;&#25104;&#22810;&#20010;&#32452;&#65292;&#28982;&#21518;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25554;&#20540;&#22810;&#39033;&#24335;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#23558;&#22810;&#20010;&#32534;&#30721;&#25968;&#25454;&#28857;&#20998;&#20139;&#32473;&#27599;&#20010;&#24037;&#20316;&#22120;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#20027;&#33410;&#28857;&#19978;&#28040;&#38500;&#36328;&#32452;&#30340;&#24178;&#25200;&#35745;&#31639;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;GLCC&#20195;&#30721;&#21253;&#25324;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25289;&#26684;&#26391;&#26085;&#32534;&#30721;&#35745;&#31639;&#65288;LCC&#65289;&#20195;&#30721;&#20316;&#20026;&#19968;&#31181;&#29305;&#20363;&#65292;&#26174;&#31034;&#20986;&#26356;&#28789;&#27963;&#30340;&#25240;&#34935;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of evaluating arbitrary multivariate polynomials over a massive dataset containing multiple inputs, on a distributed computing system with a master node and multiple worker nodes. Generalized Lagrange Coded Computing (GLCC) codes are proposed to simultaneously provide resiliency against stragglers who do not return computation results in time, security against adversarial workers who deliberately modify results for their benefit, and information-theoretic privacy of the dataset amidst possible collusion of workers. GLCC codes are constructed by first partitioning the dataset into multiple groups, then encoding the dataset using carefully designed interpolation polynomials, and sharing multiple encoded data points to each worker, such that interference computation results across groups can be eliminated at the master. Particularly, GLCC codes include the state-of-the-art Lagrange Coded Computing (LCC) codes as a special case, and exhibit a more flexible tradeoff 
&lt;/p&gt;</description></item></channel></rss>