<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15654</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#23545;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#22522;&#26412;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;Decentralized Gradient Tracking (DGT) &#21644; Decentralized Gradient Descent (DGD)&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#22659;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21152;&#20837; $K &gt; 1$ &#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#33021;&#22815;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110; $\mu$-&#24378;&#20984;&#21644; $L$-&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26412;&#22320; DGT &#26041;&#27861;&#23454;&#29616;&#20102;&#36890;&#20449;&#22797;&#26434;&#24230;&#20026; $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$&#65292;&#20854;&#20013; $\rho$ &#34913;&#37327;&#32593;&#32476;&#36830;&#36890;&#24615;&#65292;$\delta$ &#34920;&#31034;&#26412;&#22320;&#25439;&#22833;&#30340;&#20108;&#38454;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#65292;&#22686;&#21152; $K$ &#33021;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K &gt; 1$ local update steps can reduce communication complexity. Specifically, for $\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$, where $\rho$ measures the network connectivity and $\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18334</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Instruction Tuning Datasets for Zero-Shot Task Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18334
&lt;/p&gt;
&lt;p&gt;
Bonito&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25351;&#20196;&#35843;&#20248;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29992;&#25143;&#19987;&#23646;&#25968;&#25454;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bonito&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#27169;&#22411;&#65292;&#29992;&#20110;&#26465;&#20214;&#20219;&#21153;&#29983;&#25104;&#65306;&#23558;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;&#29992;&#20110;&#25351;&#20196;&#35843;&#20248;&#30340;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#29992;&#25143;&#19987;&#38376;&#30340;&#31169;&#20154;&#25968;&#25454;&#19978;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;shot&#20219;&#21153;&#36866;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;1.65M&#20010;&#31034;&#20363;&#30340;&#26032;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#35757;&#32451;Bonito&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#37325;&#26032;&#28151;&#21512;&#25104;&#20803;&#27169;&#26495;&#32780;&#21019;&#24314;&#30340;&#12290;&#25968;&#25454;&#38598;&#30340;&#20803;&#27169;&#26495;&#20135;&#29983;&#35757;&#32451;&#31034;&#20363;&#65292;&#20854;&#20013;&#36755;&#20837;&#26159;&#26410;&#27880;&#37322;&#30340;&#25991;&#26412;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#36755;&#20986;&#21253;&#25324;&#25351;&#20196;&#21644;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;Bonito&#20026;&#19971;&#20010;&#19987;&#19994;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#20219;&#21153;&#65292;&#36328;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411; -- &#26159;&#38750;&#38382;&#31572;&#12289;&#25277;&#21462;&#24335;&#38382;&#31572;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702; -- &#24182;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Bonito&#26174;&#33879;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18334v1 Announce Type: new  Abstract: We introduce Bonito, an open-source model for conditional task generation: the task of converting unannotated text into task-specific training datasets for instruction tuning. Our goal is to enable zero-shot task adaptation of large language models on users' specialized, private data. We train Bonito on a new large-scale dataset with 1.65M examples created by remixing existing instruction tuning datasets into meta-templates. The meta-templates for a dataset produce training examples where the input is the unannotated text and the task attribute and the output consists of the instruction and the response. We use Bonito to generate synthetic tasks for seven datasets from specialized domains across three task types -- yes-no question answering, extractive question answering, and natural language inference -- and adapt language models. We show that Bonito significantly improves the average performance of pretrained and instruction tuned mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#23545;APT&#25915;&#20987;&#27963;&#21160;&#20013;&#25112;&#26415;/&#25216;&#26415;&#30340;&#35782;&#21035;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#32454;&#31890;&#24230;APT&#25216;&#26415;&#21644;&#21464;&#31181;APT&#25915;&#20987;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.15147</link><description>&lt;p&gt;
TREC: &#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;APT&#25112;&#26415;/&#25216;&#26415;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TREC: APT Tactic / Technique Recognition via Few-Shot Provenance Subgraph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#23569;&#26679;&#26412;&#36861;&#28335;&#23376;&#22270;&#23398;&#20064;&#23454;&#29616;&#23545;APT&#25915;&#20987;&#27963;&#21160;&#20013;&#25112;&#26415;/&#25216;&#26415;&#30340;&#35782;&#21035;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26080;&#27861;&#35782;&#21035;&#32454;&#31890;&#24230;APT&#25216;&#26415;&#21644;&#21464;&#31181;APT&#25915;&#20987;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
APT&#65288;&#39640;&#32423;&#25345;&#32493;&#24615;&#23041;&#32961;&#65289;&#20855;&#26377;&#25345;&#20037;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#29305;&#24449;&#65292;&#26159;&#23545;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#30340;&#26368;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290; &#20026;&#24212;&#23545;&#27492;&#23041;&#32961;&#65292;&#29616;&#26377;&#30740;&#31350;&#21033;&#29992;&#28335;&#28304;&#22270;&#26469;&#25429;&#25417;&#20027;&#26426;&#20013;&#31995;&#32479;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;APT&#26816;&#27979;&#12290; &#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#21482;&#26816;&#27979;&#21333;&#20010;&#25915;&#20987;&#20107;&#20214;&#19981;&#21516;&#65292;&#29702;&#35299;&#32452;&#32455;&#21644;&#23436;&#25104;APT&#25915;&#20987;&#27963;&#21160;&#25152;&#24212;&#29992;&#30340;&#25112;&#26415;/&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;Kill-Chain&#12289;ATT&amp;CK&#65289;&#23545;&#23433;&#20840;&#36816;&#33829;&#26356;&#20026;&#37325;&#35201;&#12290; &#29616;&#26377;&#30740;&#31350;&#23581;&#35797;&#25163;&#21160;&#35774;&#35745;&#19968;&#32452;&#35268;&#21017;&#65292;&#23558;&#20302;&#32423;&#31995;&#32479;&#20107;&#20214;&#26144;&#23556;&#21040;&#39640;&#32423;APT&#25112;&#26415;/&#25216;&#26415;&#12290; &#20294;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#31895;&#31890;&#24230;&#19988;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#22240;&#27492;&#21482;&#33021;&#35782;&#21035;APT&#25112;&#26415;&#65292;&#26080;&#27861;&#36776;&#35782;&#32454;&#31890;&#24230;&#30340;APT&#25216;&#26415;&#21644;&#31361;&#21464;&#30340;APT&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;TREC&#65292;&#36825;&#26159;&#39318;&#20010;&#23581;&#35797;&#35748;&#30693;AP
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15147v1 Announce Type: cross  Abstract: APT (Advanced Persistent Threat) with the characteristics of persistence, stealth, and diversity is one of the greatest threats against cyber-infrastructure. As a countermeasure, existing studies leverage provenance graphs to capture the complex relations between system entities in a host for effective APT detection. In addition to detecting single attack events as most existing work does, understanding the tactics / techniques (e.g., Kill-Chain, ATT&amp;CK) applied to organize and accomplish the APT attack campaign is more important for security operations. Existing studies try to manually design a set of rules to map low-level system events to high-level APT tactics / techniques. However, the rule based methods are coarse-grained and lack generalization ability, thus they can only recognize APT tactics and cannot identify fine-grained APT techniques and mutant APT attacks. In this paper, we propose TREC, the first attempt to recognize AP
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;Diff-RNTraj&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#21463;&#21040;&#32422;&#26463;&#30340;&#36712;&#36857;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20445;&#35777;&#36712;&#36857;&#32422;&#26463;&#22312;&#36947;&#36335;&#19978;&#12289;&#32570;&#20047;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07369</link><description>&lt;p&gt;
Diff-RNTraj: &#19968;&#31181;&#38754;&#21521;&#36947;&#36335;&#32593;&#32476;&#32422;&#26463;&#30340;&#36712;&#36857;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07369
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;Diff-RNTraj&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#21463;&#21040;&#32422;&#26463;&#30340;&#36712;&#36857;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20445;&#35777;&#36712;&#36857;&#32422;&#26463;&#22312;&#36947;&#36335;&#19978;&#12289;&#32570;&#20047;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#35760;&#24405;&#20102;&#36710;&#36742;&#30340;&#31227;&#21160;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#36712;&#36857;&#25968;&#25454;&#25366;&#25496;&#21644;&#22522;&#20110;&#36712;&#36857;&#30340;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#30340;&#26041;&#27861;&#26469;&#25193;&#22823;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#22320;&#29702;&#22352;&#26631;&#31995;&#32479;&#20013;&#29983;&#25104;&#36712;&#36857;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#19981;&#33021;&#30830;&#20445;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#36947;&#36335;&#19978;&#21463;&#21040;&#32422;&#26463;&#12290;2&#65289;&#32570;&#20047;&#19982;&#36947;&#36335;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#38754;&#21521;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#30340;&#36947;&#36335;&#32593;&#32476;&#32422;&#26463;&#36712;&#36857;&#65288;RNTraj&#65289;&#29983;&#25104;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#24102;&#26377;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#36712;&#36857;&#12290;RNTraj&#26159;&#19968;&#31181;&#28151;&#21512;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#22312;&#27599;&#20010;poi&#19978;&#37117;&#19982;&#26102;&#38388;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory data is essential for various applications as it records the movement of vehicles. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications. To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset. However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road. 2) the lack of road-related information. In this paper, we propose a new problem to meet the practical application need, \emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. RNTraj is a hybrid type of data, in which each poi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05930</link><description>&lt;p&gt;
WebLINX: &#22810;&#36718;&#23545;&#35805;&#19979;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010; WEBLINX &#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#37327;&#20449;&#24687;&#30340;&#22788;&#29702;&#29942;&#39048;&#65292;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#31181;&#22330;&#26223;&#19979;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#23383;&#20195;&#29702;&#25511;&#21046;&#30528;&#19968;&#20010;&#32593;&#39029;&#27983;&#35272;&#22120;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#20197;&#22810;&#36718;&#23545;&#35805;&#30340;&#26041;&#24335;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; WEBLINX - &#19968;&#20010;100K&#20132;&#20114;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;2300&#20010;&#19987;&#23478;&#28436;&#31034;&#20013;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#32593;&#31449;&#23548;&#33322;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#28085;&#30422;&#20102;150&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#32593;&#31449;&#19978;&#30340;&#24191;&#27867;&#27169;&#24335;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#29702;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#20449;&#24687;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#23454;&#26102;&#22788;&#29702;&#25972;&#20010;&#32593;&#39029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#29942;&#39048;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#26816;&#32034;&#21551;&#21457;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#21517;&#30456;&#20851;&#20803;&#32032;&#26469;&#39640;&#25928;&#22320;&#20462;&#21098; HTML &#39029;&#38754;&#12290;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#20803;&#32032;&#65292;&#20197;&#21450;&#23631;&#24149;&#25130;&#22270;&#21644;&#25805;&#20316;&#21382;&#21490;&#35760;&#24405;&#65292;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#22312;&#23548;&#33322;&#32593;&#39029;&#26102;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20174;&#23567;&#22411;&#32431;&#25991;&#26412;&#27169;&#22411;&#21040;&#19987;&#26377;&#30340;&#22810;&#27169;&#24577; LLMs &#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We fi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00711</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Explaining Text Classifiers with Counterfactual Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#20107;&#23454;&#34920;&#31034;&#35299;&#37322;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24178;&#39044;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#30340;&#35299;&#37322;&#26041;&#27861;&#21487;&#20197;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#21453;&#20107;&#23454;&#26159;&#25351;&#38500;&#20102;&#19968;&#20010;&#20998;&#31867;&#29305;&#24449;&#20043;&#22806;&#65292;&#19982;&#30495;&#23454;&#35266;&#23519;&#23436;&#20840;&#30456;&#21516;&#30340;&#20551;&#35774;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#26412;&#39046;&#22495;&#26500;&#24314;&#36825;&#31181;&#21453;&#20107;&#23454;&#23384;&#22312;&#29305;&#23450;&#25361;&#25112;&#65292;&#22240;&#20026;&#26576;&#20123;&#23646;&#24615;&#20540;&#21487;&#33021;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#24178;&#39044;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#24178;&#39044;&#26041;&#27861;&#26159;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;Pearl&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20013;&#23450;&#20041;&#30340;&#21453;&#20107;&#23454;&#26159;&#19968;&#33268;&#30340;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#22522;&#20110;&#30495;&#23454;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#26126;&#30830;&#30340;&#25991;&#26412;&#24178;&#39044;&#33719;&#24471;&#65289;&#21644;&#25105;&#20204;&#30340;&#21453;&#20107;&#23454;&#65288;&#36890;&#36807;&#23545;&#25991;&#26412;&#34920;&#31034;&#30340;&#24178;&#39044;&#24471;&#21040;&#65289;&#30340;&#20998;&#31867;&#22120;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2311.12573</link><description>&lt;p&gt;
&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;: AI&#20013;&#20171;&#24179;&#21488;&#30340;&#24179;&#21488;&#27835;&#29702;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
Moderating Model Marketplaces: Platform Governance Puzzles for AI Intermediaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#24066;&#22330;&#30340;&#35843;&#33410;&#38382;&#39064;&#65292;&#20998;&#26512;&#20102;AI&#20013;&#20171;&#24179;&#21488;&#38754;&#20020;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#19994;&#30028;&#30340;&#30456;&#20851;&#23454;&#36341;&#65292;&#21253;&#25324;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv: 2311.12573v2 &#20844;&#21578;&#31867;&#22411;: replace-cross &#25688;&#35201;: AI&#24320;&#21457;&#31038;&#21306;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#25176;&#31649;&#20013;&#20171;&#24179;&#21488;&#65292;&#22914;Hugging Face&#65292;&#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#20415;&#25463;&#35775;&#38382;&#12290;&#36825;&#20123;&#27169;&#22411;&#24066;&#22330;&#38477;&#20302;&#20102;&#25104;&#21315;&#19978;&#19975;&#29992;&#25143;&#30340;&#25216;&#26415;&#37096;&#32626;&#38376;&#27099;&#65292;&#20294;&#20063;&#21487;&#33021;&#34987;&#29992;&#20110;&#35768;&#22810;&#28508;&#22312;&#26377;&#23475;&#21644;&#38750;&#27861;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;AI&#31995;&#32479;&#22914;&#20309;&#26082;&#33021;&#8220;&#21253;&#21547;&#8221;&#20869;&#23481;&#21448;&#33021;&#26159;&#24320;&#25918;&#24335;&#24037;&#20855;&#65292;&#20174;&#32780;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#26368;&#26840;&#25163;&#30340;&#24179;&#21488;&#27835;&#29702;&#25361;&#25112;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#20998;&#26512;&#27169;&#22411;&#24066;&#22330;&#22914;&#20309;&#31649;&#29702;&#27169;&#22411;&#65292;&#36825;&#20123;&#26696;&#20363;&#36328;&#36234;&#20102;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#24179;&#21488;&#65292;&#21363;Hugging Face&#12289;GitHub&#21644;Civitai&#12290;&#22522;&#20110;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19994;&#30028;&#27491;&#22312;&#21046;&#23450;&#30340;&#37325;&#35201;&#65288;&#20294;&#20173;&#28982;&#26377;&#38480;&#65289;&#24212;&#23545;&#35843;&#33410;&#38656;&#27714;&#30340;&#20570;&#27861;&#65306;&#35768;&#21487;&#12289;&#35775;&#38382;&#21644;&#20351;&#29992;&#38480;&#21046;&#12289;&#33258;&#21160;&#20869;&#23481;&#35843;&#33410;&#20197;&#21450;&#20844;&#24320;&#25919;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12573v2 Announce Type: replace-cross  Abstract: The AI development community is increasingly making use of hosting intermediaries such as Hugging Face provide easy access to user-uploaded models and training data. These model marketplaces lower technical deployment barriers for hundreds of thousands of users, yet can be used in numerous potentially harmful and illegal ways. In this article, we explain ways in which AI systems, which can both `contain' content and be open-ended tools, present one of the trickiest platform governance challenges seen to date. We provide case studies of several incidents across three illustrative platforms -- Hugging Face, GitHub and Civitai -- to examine how model marketplaces moderate models. Building on this analysis, we outline important (and yet nevertheless limited) practices that industry has been developing to respond to moderation demands: licensing, access and use restrictions, automated content moderation, and open policy development.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2312.10813</link><description>&lt;p&gt;
&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65306;&#22312;0.5K&#21442;&#25968;&#20869;&#25512;&#24191;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters. (arXiv:2312.10813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#24212;&#36807;&#31243;&#20013;&#23454;&#29616;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#31034;&#35843;&#20248;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#20923;&#32467;&#39592;&#24178;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#21482;&#35774;&#35745;&#21644;&#35843;&#25972;&#25552;&#31034;&#12290;&#19968;&#26041;&#38754;&#65292;&#25552;&#31034;&#35843;&#20248;&#30340;&#31934;&#24515;&#35774;&#35745;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#26356;&#26032;&#35268;&#21017;&#22823;&#22823;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#21463;&#21040;&#35266;&#23519;&#21040;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#28436;&#21464;&#27169;&#24335;&#19982;&#36866;&#24212;&#36807;&#31243;&#20013;&#25552;&#31034;&#30697;&#38453;&#31209;&#21464;&#21270;&#36235;&#21183;&#30340;&#35843;&#21644;&#19968;&#33268;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#20302;&#31209;&#25552;&#31034;&#65288;RLP&#65289;&#65292;&#29992;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22823;&#22823;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#21644;&#23384;&#20648;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large pre-trained vision-language models, how to effectively transfer the knowledge of such foundational models to downstream tasks becomes a hot topic, especially in a data-deficient scenario. Recently, prompt tuning has become a popular solution. When adapting the vision-language models, researchers freeze the parameters in the backbone and only design and tune the prompts. On the one hand, the delicate design of prompt tuning exhibits strong performance. On the other hand, complicated structures and update rules largely increase the computation and storage cost. Motivated by the observation that the evolution pattern of the generalization capability in visual-language models aligns harmoniously with the trend of rank variations in the prompt matrix during adaptation, we design a new type of prompt, Re-parameterized Low-rank Prompt (RLP), for both efficient and effective adaptation. Our method could largely reduce the number of tunable parameters and storage s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#26102;&#38388;&#27010;&#29575;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#27169;&#25311;&#38382;&#39064;&#35270;&#20026;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#24182;&#34892;&#22788;&#29702;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#30340;&#21516;&#26102;&#23558;&#26102;&#38388;&#24320;&#38144;&#38477;&#20302;&#21040;&#23545;&#25968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.01145</link><description>&lt;p&gt;
&#24182;&#34892;&#26102;&#38388;&#27010;&#29575;&#25968;&#20540;ODE&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Parallel-in-Time Probabilistic Numerical ODE Solvers. (arXiv:2310.01145v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#26102;&#38388;&#27010;&#29575;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#27169;&#25311;&#38382;&#39064;&#35270;&#20026;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#36125;&#21494;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#24182;&#34892;&#22788;&#29702;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#30340;&#21516;&#26102;&#23558;&#26102;&#38388;&#24320;&#38144;&#38477;&#20302;&#21040;&#23545;&#25968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#30340;&#27010;&#29575;&#25968;&#20540;&#27714;&#35299;&#22120;&#23558;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#20540;&#20223;&#30495;&#38382;&#39064;&#35270;&#20026;&#36125;&#21494;&#26031;&#29366;&#24577;&#20272;&#35745;&#38382;&#39064;&#12290;&#38500;&#20102;&#29983;&#25104;ODE&#35299;&#30340;&#21518;&#39564;&#20998;&#24067;&#24182;&#22240;&#27492;&#37327;&#21270;&#26041;&#27861;&#26412;&#36523;&#30340;&#25968;&#20540;&#36924;&#36817;&#35823;&#24046;&#20043;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#30340;&#19968;&#20010;&#19981;&#24120;&#34987;&#27880;&#24847;&#21040;&#30340;&#20248;&#21183;&#26159;&#36890;&#36807;&#22312;&#36125;&#21494;&#26031;&#28388;&#27874;&#21644;&#24179;&#28369;&#30340;&#26694;&#26550;&#20013;&#36827;&#34892;&#25968;&#20540;&#27169;&#25311;&#32780;&#33719;&#24471;&#30340;&#31639;&#27861;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#28789;&#27963;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#24182;&#34892;&#36845;&#20195;&#25193;&#23637;&#21345;&#23572;&#26364;&#24179;&#28369;&#22120;&#30340;&#20844;&#24335;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#26102;&#38388;&#27010;&#29575;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#12290;&#19982;&#24403;&#21069;&#30340;&#27010;&#29575;&#27714;&#35299;&#22120;&#20381;&#27425;&#25353;&#26102;&#38388;&#39034;&#24207;&#27169;&#25311;&#21160;&#21147;&#31995;&#32479;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#24182;&#34892;&#26041;&#24335;&#22788;&#29702;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#23558;&#26102;&#38388;&#24320;&#38144;&#20174;&#32447;&#24615;&#38477;&#20302;&#21040;&#23545;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#27493;&#39588;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#31181;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic numerical solvers for ordinary differential equations (ODEs) treat the numerical simulation of dynamical systems as problems of Bayesian state estimation. Aside from producing posterior distributions over ODE solutions and thereby quantifying the numerical approximation error of the method itself, one less-often noted advantage of this formalism is the algorithmic flexibility gained by formulating numerical simulation in the framework of Bayesian filtering and smoothing. In this paper, we leverage this flexibility and build on the time-parallel formulation of iterated extended Kalman smoothers to formulate a parallel-in-time probabilistic numerical ODE solver. Instead of simulating the dynamical system sequentially in time, as done by current probabilistic solvers, the proposed method processes all time steps in parallel and thereby reduces the span cost from linear to logarithmic in the number of time steps. We demonstrate the effectiveness of our approach on a variety o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.14960</link><description>&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#30340;&#26631;&#31614;&#23545;&#40784;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Label Alignment Regularization for Distribution Shift. (arXiv:2211.14960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14960
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#26469;&#23454;&#29616;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20010;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#27491;&#21017;&#21270;&#34920;&#31034;&#12290;&#36890;&#36807;&#28040;&#38500;&#23545;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#35813;&#26041;&#27861;&#23637;&#31034;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#23545;&#40784;&#23646;&#24615;&#65288;LAP&#65289;&#65292;&#21363;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#26631;&#31614;&#30340;&#21521;&#37327;&#22823;&#37096;&#20998;&#22312;&#25968;&#25454;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#40723;&#21169;&#30446;&#26631;&#22495;&#20013;&#30340;&#39044;&#27979;&#19982;&#20854;&#21069;&#20960;&#20010;&#22855;&#24322;&#21521;&#37327;&#23545;&#40784;&#12290;&#19982;&#20256;&#32479;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#19987;&#27880;&#20110;&#27491;&#21017;&#21270;&#34920;&#31034;&#19981;&#21516;&#65292;&#25105;&#20204;&#30456;&#21453;&#65292;&#36890;&#36807;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#20351;&#29992;LAP&#65292;&#29992;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#19982;&#26080;&#30417;&#30563;&#30446;&#26631;&#25968;&#25454;&#23545;&#40784;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#21069;&#20960;&#20010;&#21491;&#22855;&#24322;&#21521;&#37327;&#30340;&#24352;&#25104;&#31354;&#38388;&#20869;&#65292;&#24182;&#19982;&#26368;&#20248;&#35299;&#23545;&#40784;&#12290;&#36890;&#36807;&#28040;&#38500;&#32463;&#20856;&#39046;&#22495;&#36866;&#24212;&#29702;&#35770;&#20013;&#24120;&#35265;&#30340;&#26368;&#20248;&#32852;&#21512;&#39118;&#38505;&#20551;&#35774;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has highlighted the label alignment property (LAP) in supervised learning, where the vector of all labels in the dataset is mostly in the span of the top few singular vectors of the data matrix. Drawing inspiration from this observation, we propose a regularization method for unsupervised domain adaptation that encourages alignment between the predictions in the target domain and its top singular vectors. Unlike conventional domain adaptation approaches that focus on regularizing representations, we instead regularize the classifier to align with the unsupervised target data, guided by the LAP in both the source and target domains. Theoretical analysis demonstrates that, under certain assumptions, our solution resides within the span of the top right singular vectors of the target domain data and aligns with the optimal solution. By removing the reliance on the commonly used optimal joint risk assumption found in classic domain adaptation theory, we showcase the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;NIE&#65289;&#21644;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;ANIE&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#31215;&#20998;&#31639;&#23376;&#36827;&#34892;&#27169;&#22411;&#24314;&#31435;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15190</link><description>&lt;p&gt;
&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Integral Equations. (arXiv:2209.15190v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;NIE&#65289;&#21644;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243;&#65288;ANIE&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#31215;&#20998;&#31639;&#23376;&#36827;&#34892;&#27169;&#22411;&#24314;&#31435;&#65292;&#24182;&#19988;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#20998;&#26041;&#31243; (IEs) &#26159;&#29992;&#20110;&#24314;&#27169;&#20855;&#26377;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26102;&#31354;&#31995;&#32479;&#30340;&#26041;&#31243;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#29702;&#35770;&#21644;&#24212;&#29992;&#31185;&#23398;&#20013;&#25214;&#21040;&#20102;&#37325;&#35201;&#24212;&#29992;&#65292;&#21253;&#25324;&#29289;&#29702;&#23398;&#12289;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#24037;&#31243;&#23398;&#12290;&#34429;&#28982;&#23384;&#22312;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#32473;&#23450;&#30340;IEs&#65292;&#20294;&#19981;&#23384;&#22312;&#21487;&#20197;&#20165;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;IE&#21644;&#20854;&#30456;&#20851;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243; (NIE)&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;IE&#27714;&#35299;&#22120;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#30340;&#31215;&#20998;&#31639;&#23376;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#33258;&#27880;&#24847;&#31070;&#32463;&#31215;&#20998;&#26041;&#31243; (ANIE)&#65292;&#20854;&#20013;&#31215;&#20998;&#34987;&#33258;&#27880;&#24847;&#21147;&#26367;&#25442;&#65292;&#36825;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#12289;&#23481;&#37327;&#65292;&#24182;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;(A)NIE&#22312;ODE&#12289;PDE&#21644;IE&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integral equations (IEs) are equations that model spatiotemporal systems with non-local interactions. They have found important applications throughout theoretical and applied sciences, including in physics, chemistry, biology, and engineering. While efficient algorithms exist for solving given IEs, no method exists that can learn an IE and its associated dynamics from data alone. In this paper, we introduce Neural Integral Equations (NIE), a method that learns an unknown integral operator from data through an IE solver. We also introduce Attentional Neural Integral Equations (ANIE), where the integral is replaced by self-attention, which improves scalability, capacity, and results in an interpretable model. We demonstrate that (A)NIE outperforms other methods in both speed and accuracy on several benchmark tasks in ODE, PDE, and IE systems of synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#36817;&#20284;&#20110;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#21644;&#24067;&#23572;&#20989;&#25968;&#65292;&#19988;&#22312;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#35823;&#24046;&#19982;&#26368;&#20248;&#35823;&#24046;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2108.06339</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#20998;&#31867;&#30340;&#26368;&#20248;&#24615;&#21644;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimality and complexity of classification by random projection. (arXiv:2108.06339v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#36817;&#20284;&#20110;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#21644;&#24067;&#23572;&#20989;&#25968;&#65292;&#19988;&#22312;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#35823;&#24046;&#19982;&#26368;&#20248;&#35823;&#24046;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#36873;&#25321;&#20998;&#31867;&#22120;&#30340;&#20989;&#25968;&#38598;&#30340;&#22797;&#26434;&#24230;&#26377;&#20851;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#36890;&#36807;&#38543;&#26426;&#19968;&#32500;&#29305;&#24449;&#20570;&#38408;&#20540;&#22788;&#29702;&#12290;&#35813;&#29305;&#24449;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#30001;&#39640;&#27425;&#21333;&#39033;&#24335;&#21442;&#25968;&#21270;&#30340;&#26356;&#39640;&#32500;&#31354;&#38388;&#20013;&#21518;&#22312;&#38543;&#26426;&#30452;&#32447;&#19978;&#36827;&#34892;&#25237;&#24433;&#32780;&#24471;&#21040;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25193;&#23637;&#30340;&#25968;&#25454;&#34987;&#25237;&#24433;n&#27425;&#65292;&#24182;&#20174;&#36825;n&#20010;&#20013;&#36873;&#20986;&#34920;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#31867;&#22120;&#26159;&#26497;&#20854;&#28789;&#27963;&#30340;&#65292;&#22240;&#20026;&#23427;&#26377;&#21487;&#33021;&#36817;&#20284;&#20110;&#20219;&#20309;&#22312;&#32039;&#33268;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#21450;&#23558;&#25903;&#25745;&#38598;&#25286;&#20998;&#20026;&#21487;&#27979;&#23376;&#38598;&#30340;&#20219;&#20309;&#24067;&#23572;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#23436;&#20840;&#30693;&#35782;&#65292;&#21017;&#36825;&#20123;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#30340;&#35823;&#24046;&#23558;&#22312;k&#21644;n&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;&#26368;&#20248;&#65288;&#36125;&#21494;&#26031;&#65289;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error of a classifier is related to the complexity of the set of functions among which the classifier is chosen. We study a family of low-complexity classifiers consisting of thresholding a random one-dimensional feature. The feature is obtained by projecting the data on a random line after embedding it into a higher-dimensional space parametrized by monomials of order up to k. More specifically, the extended data is projected n-times and the best classifier among those n, based on its performance on training data, is chosen. We show that this type of classifier is extremely flexible, as it is likely to approximate, to an arbitrary precision, any continuous function on a compact set as well as any boolean function on a compact set that splits the support into measurable subsets. In particular, given full knowledge of the class conditional densities, the error of these low-complexity classifiers would converge to the optimal (Bayes) error as k and n go to infinity. On
&lt;/p&gt;</description></item></channel></rss>