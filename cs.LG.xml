<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17329</link><description>&lt;p&gt;
&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Support Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;&#20854;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#36825;&#31181;&#20851;&#31995;&#30340;&#23454;&#38469;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#39046;&#22495;&#24320;&#23637;&#20102;&#19968;&#39033;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeepKKT&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#30340;&#35843;&#25972;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#30340;&#20999;&#23454;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#65292;&#31867;&#20284;&#20110;SVM&#20013;&#30340;&#36807;&#31243;&#12290;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;EEG&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#22312;&#33041;&#26426;&#25509;&#21475;&#20219;&#21153;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15415</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#24322;&#26500;&#33041;&#30005;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Physics-informed and Unsupervised Riemannian Domain Adaptation for Machine Learning on Heterogeneous EEG Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15415
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#40654;&#26364;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#19981;&#21516;EEG&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#22312;&#33041;&#26426;&#25509;&#21475;&#20219;&#21153;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20250;&#35805;&#12289;&#21463;&#35797;&#32773;&#21644;&#35774;&#22791;&#30340;&#21464;&#24322;&#24615;&#65292;&#23558;&#33041;&#30005;&#22270; (EEG) &#25968;&#25454;&#38598;&#29992;&#20110;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064; (ML) &#20855;&#26377;&#25361;&#25112;&#24615;&#12290;ML&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#65292;&#30001;&#20110;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#20301;&#32622;&#30340;&#21464;&#21270;&#32780;&#22797;&#26434;&#21270;&#20102;&#20998;&#26512;&#12290;&#31616;&#21333;&#30340;&#36890;&#36947;&#36873;&#25321;&#20250;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22312;&#20849;&#20139;&#23569;&#37327;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#26356;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;EEG&#20449;&#21495;&#29289;&#29702;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22330;&#25554;&#20540;&#23558;EEG&#36890;&#36947;&#26144;&#23556;&#21040;&#22266;&#23450;&#20301;&#32622;&#65292;&#20419;&#36827;&#26080;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#21033;&#29992;&#40654;&#26364;&#20960;&#20309;&#20998;&#31867;&#27969;&#31243;&#21644;&#36801;&#31227;&#23398;&#20064;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33041;&#26426;&#25509;&#21475; (BCI) &#20219;&#21153;&#21644;&#28508;&#22312;&#29983;&#29289;&#26631;&#24535;&#29289;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#19982;&#19968;&#31181;&#31216;&#20026;&#36229;&#36234;&#32500;&#24230;&#30340;&#22522;&#20110;&#32479;&#35745;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#30340;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15415v1 Announce Type: cross  Abstract: Combining electroencephalogram (EEG) datasets for supervised machine learning (ML) is challenging due to session, subject, and device variability. ML algorithms typically require identical features at train and test time, complicating analysis due to varying sensor numbers and positions across datasets. Simple channel selection discards valuable data, leading to poorer performance, especially with datasets sharing few channels. To address this, we propose an unsupervised approach leveraging EEG signal physics. We map EEG channels to fixed positions using field interpolation, facilitating source-free domain adaptation. Leveraging Riemannian geometry classification pipelines and transfer learning steps, our method demonstrates robust performance in brain-computer interface (BCI) tasks and potential biomarker applications. Comparative analysis against a statistical-based approach known as Dimensionality Transcending, a signal-based imputa
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13040</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Neural Networks for Intraventricular Vector Flow Mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;nnU-Net&#30340;&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#24515;&#23460;&#21521;&#37327;&#27969;&#21160;&#26144;&#23556;&#65292;&#25928;&#26524;&#19982;&#20256;&#32479;&#31639;&#27861;&#30456;&#24403;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Intraventricular vector flow mapping (iVFM)&#26088;&#22312;&#22686;&#24378;&#21644;&#37327;&#21270;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#22522;&#20110;&#29289;&#29702;&#24341;&#23548;&#30340;nnU-Net&#30417;&#30563;&#26041;&#27861;&#26469;&#20248;&#21270;&#20256;&#32479;&#30340;iVFM&#20248;&#21270;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#22522;&#20110;&#24739;&#32773;&#29305;&#23450;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#22411;&#20135;&#29983;&#30340;&#27169;&#25311;&#24425;&#33394;&#22810;&#26222;&#21202;&#22270;&#20687;&#21644;&#20307;&#20869;&#22810;&#26222;&#21202;&#37319;&#38598;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#20004;&#31181;&#26041;&#27861;&#22343;&#23637;&#29616;&#20986;&#19982;&#21407;&#22987;iVFM&#31639;&#27861;&#30456;&#24403;&#30340;&#37325;&#24314;&#24615;&#33021;&#12290; PINNs&#30340;&#25928;&#29575;&#36890;&#36807;&#21452;&#38454;&#27573;&#20248;&#21270;&#21644;&#39044;&#20248;&#21270;&#26435;&#37325;&#24471;&#21040;&#25552;&#21319;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;nnU-Net&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#23454;&#26102;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;nnU-Net&#22312;&#31232;&#30095;&#21644;&#25130;&#26029;&#22810;&#26222;&#21202;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#29420;&#31435;&#20110;&#26126;&#30830;&#30340;&#36793;&#30028;&#26465;&#20214;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13040v1 Announce Type: cross  Abstract: Intraventricular vector flow mapping (iVFM) seeks to enhance and quantify color Doppler in cardiac imaging. In this study, we propose novel alternatives to the traditional iVFM optimization scheme by utilizing physics-informed neural networks (PINNs) and a physics-guided nnU-Net-based supervised approach. Through rigorous evaluation on simulated color Doppler images derived from a patient-specific computational fluid dynamics model and in vivo Doppler acquisitions, both approaches demonstrate comparable reconstruction performance to the original iVFM algorithm. The efficiency of PINNs is boosted through dual-stage optimization and pre-optimized weights. On the other hand, the nnU-Net method excels in generalizability and real time capabilities. Notably, nnU-Net shows superior robustness on sparse and truncated Doppler data while maintaining independence from explicit boundary conditions. Overall, our results highlight the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#24182;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#33021;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12946</link><description>&lt;p&gt;
&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31163;&#32447;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Offline Distributionally Robust Linear Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#24182;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#33021;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#25552;&#39640;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#65292;&#32570;&#20047;&#31215;&#26497;&#25506;&#32034;&#38656;&#35201;&#20851;&#27880;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#35299;&#20915;&#27169;&#25311;&#21644;&#37096;&#32626;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20854;&#20013;&#27169;&#25311;&#21644;&#23454;&#38469;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#23398;&#20064;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#20197;&#26679;&#26412;&#39640;&#25928;&#30340;&#26041;&#24335;&#36171;&#20104;&#23398;&#20064;&#31574;&#30053;&#22312;&#39640;&#32500;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#36890;&#36807;&#24635;&#21464;&#24046;&#36317;&#31163;&#34920;&#24449;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20998;&#24067;&#40065;&#26834;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24754;&#35266;&#27169;&#22411;&#31639;&#27861;&#65292;&#24182;&#22312;&#26368;&#23567;&#25968;&#25454;&#35206;&#30422;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#20854;&#26679;&#26412;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#20854;&#24615;&#33021;&#33267;&#23569;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#20248;&#20110;$\tilde{O}(d)$&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12946v1 Announce Type: new  Abstract: In offline reinforcement learning (RL), the absence of active exploration calls for attention on the model robustness to tackle the sim-to-real gap, where the discrepancy between the simulated and deployed environments can significantly undermine the performance of the learned policy. To endow the learned policy with robustness in a sample-efficient manner in the presence of high-dimensional state-action space, this paper considers the sample complexity of distributionally robust linear Markov decision processes (MDPs) with an uncertainty set characterized by the total variation distance using offline data. We develop a pessimistic model-based algorithm and establish its sample complexity bound under minimal data coverage assumptions, which outperforms prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We further improve the performance guarantee of the proposed algorithm by incorporating a carefully-designed varia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11565</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20855;&#26377;&#38750;&#20984;&#21644;&#38750;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#21435;&#20013;&#24515;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#20851;&#27880;&#38750;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DSM&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#25104;&#24207;&#21015;&#28176;&#36817;&#36924;&#36817;&#20854;&#20851;&#32852;&#24494;&#20998;&#21253;&#21547;&#30340;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;DSGD&#65289;&#65292;&#20855;&#26377;&#26799;&#24230;&#36319;&#36394;&#25216;&#26415;&#30340;DSGD&#65288;DSGD-T&#65289;&#21644;&#24102;&#21160;&#37327;&#30340;DSGD&#65288;DSGDm&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;SignSGD&#65292;&#37319;&#29992;&#31526;&#21495;&#26144;&#23556;&#26469;&#27491;&#21017;&#21270;DSGDm&#20013;&#30340;&#26356;&#26032;&#26041;&#21521;&#65292;&#24182;&#34920;&#26126;&#23427;&#34987;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos
&lt;/p&gt;</description></item><item><title>JAXbind&#26088;&#22312;&#22823;&#24133;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#65292;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#23450;&#20041;&#33258;&#23450;&#20041;JAX&#21407;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.08847</link><description>&lt;p&gt;
JAXbind: &#23558;&#20219;&#20309;&#20989;&#25968;&#32465;&#23450;&#21040;JAX
&lt;/p&gt;
&lt;p&gt;
JAXbind: Bind any function to JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08847
&lt;/p&gt;
&lt;p&gt;
JAXbind&#26088;&#22312;&#22823;&#24133;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#65292;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#23450;&#20041;&#33258;&#23450;&#20041;JAX&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
JAX&#34987;&#24191;&#27867;&#24212;&#29992;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#21518;&#32773;&#32463;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#39640;&#24615;&#33021;&#20195;&#30721;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#23558;&#20854;&#32435;&#20837;JAX&#20013;&#12290;&#37325;&#26032;&#22312;JAX&#20013;&#23454;&#29616;&#29616;&#26377;&#20195;&#30721;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#32780;JAX&#20013;&#29992;&#20110;&#32465;&#23450;&#33258;&#23450;&#20041;&#20195;&#30721;&#30340;&#29616;&#26377;&#25509;&#21475;&#38656;&#35201;&#23545;JAX&#21450;&#20854;C++&#21518;&#31471;&#26377;&#28145;&#20837;&#20102;&#35299;&#12290;JAXbind&#30340;&#30446;&#26631;&#26159;&#22823;&#22823;&#20943;&#23569;&#23558;&#20854;&#20182;&#32534;&#31243;&#35821;&#35328;&#20013;&#23454;&#29616;&#30340;&#33258;&#23450;&#20041;&#20989;&#25968;&#32465;&#23450;&#21040;JAX&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;JAXbind&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#65292;&#29992;&#20110;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;JAX&#36716;&#25442;&#30340;&#33258;&#23450;&#20041;&#25152;&#35859;JAX&#21407;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08847v1 Announce Type: cross  Abstract: JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code requires deep knowledge of JAX and its C++ backend. The goal of JAXbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom so-called JAX primitives that support arbitrary JAX transformations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08819</link><description>&lt;p&gt;
&#28201;&#24230;&#35745;&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Thermometer: Towards Universal Calibration for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#20934;&#30830;&#24615;&#20445;&#25345;&#24182;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#21709;&#24212;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#26657;&#20934;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#22914;&#25351;&#20196;&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#26657;&#20934;&#19981;&#20339;&#30340;LLMs&#12290;&#23613;&#31649;&#26657;&#20934;&#22312;&#20256;&#32479;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#35752;&#65292;&#20294;&#23545;LLMs&#36827;&#34892;&#26657;&#20934;&#20855;&#26377;&#29420;&#29305;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#26469;&#33258;LLMs&#30340;&#20005;&#26684;&#35745;&#31639;&#35201;&#27714;&#65292;&#20063;&#26469;&#33258;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20351;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#26657;&#20934;&#26041;&#27861;THERMOMETER&#12290;THERMOMETER&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#36741;&#21161;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#12290;&#23427;&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#39640;&#65292;&#20445;&#25345;&#20102;LLM&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20026;&#26032;&#20219;&#21153;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#26657;&#20934;&#21709;&#24212;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#36873;&#25321;&#36866;&#24403;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07218</link><description>&lt;p&gt;
SoK&#65306;&#36712;&#36857;&#29983;&#25104;&#26159;&#21542;&#33021;&#22815;&#20860;&#39038;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
SoK: Can Trajectory Generation Combine Privacy and Utility?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35774;&#35745;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#36873;&#25321;&#36866;&#24403;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20301;&#32622;&#36712;&#36857;&#20195;&#34920;&#30528;&#20379;&#21508;&#31181;&#20998;&#26512;&#21644;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#30340;&#23453;&#36149;&#25968;&#25454;&#26469;&#28304;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#65292;&#22914;&#25919;&#27835;&#21644;&#23447;&#25945;&#20559;&#22909;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;ially private&#21457;&#24067;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20445;&#25252;&#26041;&#26696;&#23384;&#22312;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#30340;&#26435;&#34913;&#38480;&#21046;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#30456;&#20851;&#24615;&#21644;&#37325;&#26500;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#21512;&#25104;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#21644;&#21457;&#24067;&#20195;&#34920;&#20102;&#20445;&#25252;&#31639;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#34429;&#28982;&#26368;&#21021;&#30340;&#25552;&#35758;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#29992;&#24615;&#65292;&#20294;&#26410;&#33021;&#25552;&#20379;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23450;&#20041;&#20116;&#20010;&#35774;&#35745;&#30446;&#26631;&#65292;&#29305;&#21035;&#24378;&#35843;&#36873;&#25321;&#36866;&#24403;&#30340;&#38544;&#31169;&#21333;&#20301;&#30340;&#37325;&#35201;&#24615;&#65292;&#26469;&#35774;&#35745;&#19968;&#20010;&#20445;&#25252;&#38544;&#31169;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#36712;&#36857;&#21457;&#24067;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07218v1 Announce Type: cross  Abstract: While location trajectories represent a valuable data source for analyses and location-based services, they can reveal sensitive information, such as political and religious preferences. Differentially private publication mechanisms have been proposed to allow for analyses under rigorous privacy guarantees. However, the traditional protection schemes suffer from a limiting privacy-utility trade-off and are vulnerable to correlation and reconstruction attacks. Synthetic trajectory data generation and release represent a promising alternative to protection algorithms. While initial proposals achieve remarkable utility, they fail to provide rigorous privacy guarantees. This paper proposes a framework for designing a privacy-preserving trajectory publication approach by defining five design goals, particularly stressing the importance of choosing an appropriate Unit of Privacy. Based on this framework, we briefly discuss the existing traje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.06748</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#25463;&#24452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shortcut Learning in Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#29616;&#35937;&#25193;&#23637;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#21457;&#29616;&#20020;&#24202;&#27880;&#37322;&#21644;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#26041;&#24335;&#21487;&#33021;&#35823;&#23548;&#27169;&#22411;&#24182;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25463;&#24452;&#23398;&#20064;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20248;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#28508;&#22312;&#35823;&#23548;&#30340;&#25968;&#25454;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#24456;&#38590;&#27867;&#21270;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#25506;&#35752;&#36825;&#19968;&#29616;&#35937;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#65292;&#32780;&#36825;&#39033;&#30740;&#31350;&#23558;&#25463;&#24452;&#23398;&#20064;&#25506;&#32034;&#24310;&#20280;&#21040;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20020;&#24202;&#27880;&#37322;&#22914;&#21345;&#23610;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#38646;&#22635;&#20805;&#21367;&#31215;&#21644;&#20013;&#24515;&#35009;&#21098;&#30340;&#32452;&#21512;&#21487;&#20197;&#26080;&#24847;&#20013;&#20316;&#20026;&#25463;&#24452;&#65292;&#24433;&#21709;&#20998;&#21106;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#20294;&#24120;&#35265;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#20102;&#25463;&#24452;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#24433;&#21709;&#12289;&#25552;&#39640;&#20998;&#21106;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#36890;&#36807;&#25581;&#31034;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25463;&#24452;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06748v1 Announce Type: cross  Abstract: Shortcut learning is a phenomenon where machine learning models prioritize learning simple, potentially misleading cues from data that do not generalize well beyond the training set. While existing research primarily investigates this in the realm of image classification, this study extends the exploration of shortcut learning into medical image segmentation. We demonstrate that clinical annotations such as calipers, and the combination of zero-padded convolutions and center-cropped training sets in the dataset can inadvertently serve as shortcuts, impacting segmentation accuracy. We identify and evaluate the shortcut learning on two different but common medical image segmentation tasks. In addition, we suggest strategies to mitigate the influence of shortcut learning and improve the generalizability of the segmentation models. By uncovering the presence and implications of shortcuts in medical image segmentation, we provide insights a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>&#32570;&#22833;&#25968;&#25454;&#22686;&#21152;&#20102;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#8212;&#8212;&#22522;&#20110;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03069</link><description>&lt;p&gt;
&#29992;&#28151;&#21512;&#21464;&#20998;&#23478;&#26063;&#25913;&#36827;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03069
&lt;/p&gt;
&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#22686;&#21152;&#20102;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#8212;&#8212;&#22522;&#20110;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32570;&#22833;&#25968;&#25454;&#20250;&#22686;&#21152;&#27169;&#22411;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#19982;&#23436;&#20840;&#35266;&#27979;&#30340;&#24773;&#20917;&#30456;&#27604;&#12290;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#21487;&#33021;&#20250;&#30001;&#20110;&#21464;&#20998;&#20998;&#24067;&#21644;&#27169;&#22411;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#32780;&#23545;&#27169;&#22411;&#25311;&#21512;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#22522;&#20110;&#65288;i&#65289;&#26377;&#38480;&#21464;&#20998;&#28151;&#21512;&#21644;&#65288;ii&#65289;&#22522;&#20110;&#22635;&#34917;&#30340;&#21464;&#20998;&#28151;&#21512;&#20998;&#24067;&#30340;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#22686;&#21152;&#30340;&#21518;&#39564;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#23545;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#34920;&#26126;&#21464;&#20998;&#28151;&#21512;&#22312;&#25913;&#36827;&#20174;&#19981;&#23436;&#25972;&#25968;&#25454;&#20272;&#35745;VAE&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03069v1 Announce Type: new  Abstract: We consider the task of estimating variational autoencoders (VAEs) when the training data is incomplete. We show that missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case. The increased complexity may adversely affect the fit of the model due to a mismatch between the variational and model posterior distributions. We introduce two strategies based on (i) finite variational-mixture and (ii) imputation-based variational-mixture distributions to address the increased posterior complexity. Through a comprehensive evaluation of the proposed approaches, we show that variational mixtures are effective at improving the accuracy of VAE estimation from incomplete data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;, &#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#65292;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#26080;&#38656;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.15411</link><description>&lt;p&gt;
&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Optimisic Information Directed Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;, &#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#65292;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#26080;&#38656;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#19978;&#19979;&#25991;&#24378;&#30423;&#38382;&#39064;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#25439;&#22833;&#20989;&#25968;&#34987;&#20551;&#23450;&#23646;&#20110;&#24050;&#30693;&#30340;&#21442;&#25968;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#23427;&#22312;&#36125;&#21494;&#26031;&#29702;&#35770;&#21644;&#22522;&#20110;&#20915;&#31574;&#20272;&#35745;&#31995;&#25968;&#30340;&#26368;&#22351;&#24773;&#24418;&#29702;&#35770;&#20043;&#38388;&#26550;&#36215;&#20102;&#26725;&#26753;&#12290;&#27762;&#21462;&#36825;&#20004;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20048;&#35266;&#20449;&#24687;&#23548;&#21521;&#37319;&#26679;&#30340;&#31639;&#27861;&#27169;&#26495;&#65292;&#24182;&#23637;&#31034;&#23427;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#20110;&#32463;&#20856;&#36125;&#21494;&#26031;IDS&#26041;&#27861;&#21487;&#23454;&#29616;&#30340;&#23454;&#20363;&#30456;&#20851;&#36951;&#25022;&#20445;&#35777;&#65292;&#20294;&#21364;&#26080;&#38656;&#20219;&#20309;&#36125;&#21494;&#26031;&#20551;&#35774;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010;&#20048;&#35266;&#30340;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;&#36951;&#25022;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23450;&#20041;&#19968;&#20010;&#22522;&#20110;&#39057;&#29575;&#30340;&#22522;&#20110;Russo&#21644;Van Roy (2018)&#30340;&#20449;&#24687;&#27604;&#65292;&#20197;&#21450;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15411v1 Announce Type: new  Abstract: We study the problem of online learning in contextual bandit problems where the loss function is assumed to belong to a known parametric function class. We propose a new analytic framework for this setting that bridges the Bayesian theory of information-directed sampling due to Russo and Van Roy (2018) and the worst-case theory of Foster, Kakade, Qian, and Rakhlin (2021) based on the decision-estimation coefficient. Drawing from both lines of work, we propose a algorithmic template called Optimistic Information-Directed Sampling and show that it can achieve instance-dependent regret guarantees similar to the ones achievable by the classic Bayesian IDS method, but with the major advantage of not requiring any Bayesian assumptions. The key technical innovation of our analysis is introducing an optimistic surrogate model for the regret and using it to define a frequentist version of the Information Ratio of Russo and Van Roy (2018), and a l
&lt;/p&gt;</description></item><item><title>MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14905</link><description>&lt;p&gt;
MobileLLM&#65306;&#20248;&#21270;&#20122;&#21313;&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#20197;&#29992;&#20110;&#35774;&#22791;&#31471;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14905
&lt;/p&gt;
&lt;p&gt;
MobileLLM&#36890;&#36807;&#20248;&#21270;&#27169;&#22411;&#26550;&#26500;&#65292;&#37319;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#12289;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#19988;&#20165;&#26377;&#26497;&#23567;&#24310;&#36831;&#24320;&#38144;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36843;&#20999;&#38656;&#27714;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20113;&#25104;&#26412;&#21644;&#24310;&#36831;&#38382;&#39064;&#19981;&#26029;&#22686;&#21152;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#19981;&#21040;&#21313;&#20159;&#21442;&#25968;&#30340;&#39030;&#32423;LLMs&#65292;&#36825;&#26159;&#31227;&#21160;&#37096;&#32626;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#24378;&#35843;&#25968;&#25454;&#21644;&#21442;&#25968;&#25968;&#37327;&#22312;&#30830;&#23450;&#27169;&#22411;&#36136;&#37327;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20122;&#21313;&#20159;&#35268;&#27169;LLMs&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#28145;&#24230;&#21644;&#30246;&#36523;&#32467;&#26500;&#65292;&#20877;&#21152;&#19978;&#23884;&#20837;&#20849;&#20139;&#21644;&#20998;&#32452;&#26597;&#35810;&#27880;&#24847;&#26426;&#21046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32593;&#32476;&#65292;&#31216;&#20026;MobileLLM&#65292;&#20854;&#22312;&#23558;&#36817;125M/350M&#20808;&#36827;&#27169;&#22411;&#19978;&#20998;&#21035;&#33719;&#24471;&#20102;&#24778;&#20154;&#30340;2.7%/4.3%&#30340;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31435;&#21363;&#30340;&#22359;&#29366;&#26435;&#37325;&#20849;&#20139;&#26041;&#27861;&#65292;&#19981;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#65292;&#19988;&#20165;&#20855;&#26377;&#26497;&#23567;&#30340;&#24310;&#36831;&#24320;&#38144;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;MobileLLM-L
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14905v1 Announce Type: cross  Abstract: This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.11740</link><description>&lt;p&gt;
&#21033;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#24182;&#36827;&#34892;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Extraction of nonlinearity in neural networks and model compression with Koopman operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;Koopman&#31639;&#23376;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#65292;&#35777;&#26126;&#20102;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#36275;&#22815;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#33021;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#20851;&#38190;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;Koopman&#31639;&#23376;&#12289;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#21644;&#24352;&#37327;&#21015;&#36710;&#26684;&#24335;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21463;&#38480;&#38750;&#32447;&#24615;&#24050;&#32463;&#36275;&#20197;&#36827;&#34892;&#25163;&#20889;&#25968;&#23383;&#30340;&#20998;&#31867;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#22788;&#29702;&#22823;&#22411;&#32593;&#32476;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#21033;&#29992;Koopman&#31639;&#23376;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#22788;&#29702;&#20013;&#20351;&#29992;&#32447;&#24615;&#20195;&#25968;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39640;&#24230;&#21387;&#32553;&#27169;&#22411;&#35774;&#32622;&#19979;&#22312;&#25163;&#20889;&#25968;&#23383;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#35201;&#20040;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#24403;&#65292;&#35201;&#20040;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11740v1 Announce Type: new  Abstract: Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.10898</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
The Price of Adaptivity in Stochastic Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;&#32473;&#23450;&#19968;&#32452;&#25105;&#20204;&#24076;&#26395;&#36866;&#24212;&#30340;&#38382;&#39064;&#21442;&#25968;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#8220;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#8221;&#65288;PoA&#65289;&#65292;&#31895;&#30053;&#22320;&#35828;&#65292;&#23427;&#34913;&#37327;&#20102;&#30001;&#20110;&#36825;&#20123;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#30340;&#20056;&#27861;&#22686;&#21152;&#12290;&#24403;&#21021;&#22987;&#36317;&#31163;&#26368;&#20248;&#35299;&#26410;&#30693;&#20294;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;PoA&#33267;&#23569;&#23545;&#20110;&#26399;&#26395;&#27425;&#20248;&#24615;&#26159;&#23545;&#25968;&#32423;&#21035;&#65292;&#23545;&#20110;&#20013;&#20301;&#25968;&#27425;&#20248;&#24615;&#26159;&#21452;&#23545;&#25968;&#32423;&#21035;&#12290;&#24403;&#36317;&#31163;&#21644;&#26799;&#24230;&#33539;&#25968;&#37117;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;PoA&#24517;&#39035;&#26159;&#19982;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#22810;&#39033;&#24335;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#20960;&#20046;&#19982;&#29616;&#26377;&#30340;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#30830;&#23450;&#20102;&#27809;&#26377;&#26080;&#21442;&#25968;&#21320;&#39184;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09478</link><description>&lt;p&gt;
&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Data Reconstruction Attacks and Defenses: A Systematic Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#25915;&#20987;&#21644;&#38450;&#24481;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#32463;&#39564;&#35266;&#23519;&#19978;&#65292;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#19988;&#26080;&#27861;&#21306;&#20998;&#38450;&#24481;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#19982;&#25915;&#20987;&#26041;&#27861;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#12290;&#35813;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19982;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#36825;&#31181;&#26356;&#24378;&#30340;&#25915;&#20987;&#19979;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;&#20102;&#26368;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#22914;&#26799;&#24230;&#21098;&#36753;&#12289;dropout&#12289;&#28155;&#21152;&#22122;&#38899;&#12289;&#23616;&#37096;&#32858;&#21512;&#31561;&#31561;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09478v1 Announce Type: cross  Abstract: Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical groundings, and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of federated learning. The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient pruning emerges as the most effective strategy to defend against state-of-the-art attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06530</link><description>&lt;p&gt;
&#25913;&#36827;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#22312;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite Kernel Strategy in One-Class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#22797;&#21512;&#26680;&#31574;&#30053;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#31639;&#27861;&#26469;&#36827;&#34892;&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#25237;&#24433;&#30697;&#38453;&#21644;&#29305;&#24449;&#36716;&#21270;&#65292;&#25552;&#39640;&#20102;&#24515;&#32908;&#26775;&#27515;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24515;&#32908;&#26775;&#27515;&#65288;MI&#65289;&#30340;&#26816;&#27979;&#23545;&#20110;&#39044;&#38450;&#36827;&#19968;&#27493;&#24515;&#32908;&#25439;&#20260;&#38750;&#24120;&#37325;&#35201;&#65292;MI&#26159;&#30001;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#24341;&#36215;&#30340;&#19968;&#31181;&#20005;&#37325;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#21333;&#19968;&#31867;&#21035;&#20998;&#31867;&#65288;OCC&#65289;&#31639;&#27861;&#36827;&#34892;&#26089;&#26399;MI&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#22810;&#27169;&#24577;&#23376;&#31354;&#38388;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#20811;&#26381;&#20102;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;&#25216;&#26415;&#28041;&#21450;&#19968;&#31181;&#29305;&#27530;&#30340;MI&#26816;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;&#22797;&#21512;&#26680;&#22312;&#38750;&#32447;&#24615;&#25237;&#24433;&#25216;&#24039;&#20013;&#34701;&#21512;&#39640;&#26031;&#21644;&#25289;&#26222;&#25289;&#26031;sigmoid&#20989;&#25968;&#65292;&#23558;&#22810;&#35270;&#22270;&#36229;&#22768;&#24515;&#21160;&#22270;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#35843;&#25972;&#25237;&#24433;&#30697;&#38453;&#30340;&#26368;&#22823;&#21270;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#25237;&#24433;&#30697;&#38453;&#26356;&#26032;&#31574;&#30053;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#20174;&#36229;&#22768;&#24515;&#21160;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#20248;&#21270;&#30340;&#20302;&#32500;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;MI&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of myocardial infarction (MI), a critical condition arising from coronary artery disease (CAD), is vital to prevent further myocardial damage. This study introduces a novel method for early MI detection using a one-class classification (OCC) algorithm in echocardiography. Our study overcomes the challenge of limited echocardiography data availability by adopting a novel approach based on Multi-modal Subspace Support Vector Data Description. The proposed technique involves a specialized MI detection framework employing multi-view echocardiography incorporating a composite kernel in the non-linear projection trick, fusing Gaussian and Laplacian sigmoid functions. Additionally, we enhance the update strategy of the projection matrices by adapting maximization for both or one of the modalities in the optimization process. Our method boosts MI detection capability by efficiently transforming features extracted from echocardiography data into an optimized lower-dimensional su
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.04929</link><description>&lt;p&gt;
&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#25193;&#25955;&#24341;&#23548;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Source-Free Domain Adaptation with Diffusion-Guided Source Data Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#22270;&#20687;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;DM-SFDA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DM-SFDA&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20351;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#29305;&#24449;&#26469;&#25351;&#23548;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#28304;&#22495;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#34987;&#24494;&#35843;&#20197;&#29983;&#25104;&#26368;&#23567;&#21270;&#29109;&#24182;&#26368;&#22823;&#21270;&#39044;&#35757;&#32451;&#28304;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#28304;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24050;&#24314;&#31435;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#23558;&#29983;&#25104;&#30340;&#28304;&#22270;&#20687;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;Office-31&#12289;Office-Home&#21644;VisDA&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#12289;&#39046;&#22495;&#29305;&#23450;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to leverage the generalizability capability of Diffusion Models for Source-Free Domain Adaptation (DM-SFDA). Our proposed DM-SFDA method involves fine-tuning a pre-trained text-to-image diffusion model to generate source domain images using features from the target images to guide the diffusion process. Specifically, the pre-trained diffusion model is fine-tuned to generate source samples that minimize entropy and maximize confidence for the pre-trained source model. We then apply established unsupervised domain adaptation techniques to align the generated source images with target domain data. We validate our approach through comprehensive experiments across a range of datasets, including Office-31, Office-Home, and VisDA. The results highlight significant improvements in SFDA performance, showcasing the potential of diffusion models in generating contextually relevant, domain-specific images.
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02021</link><description>&lt;p&gt;
ECG&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#26377;&#25928;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in ECG Diagnosis: Is It Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23545;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#21457;&#29616;&#24494;&#35843;&#23545;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#26159;&#36739;&#22909;&#30340;&#36873;&#25321;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21516;&#26102;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#24515;&#30005;&#22270;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#21463;&#21040;&#22823;&#35268;&#27169;&#12289;&#26631;&#35760;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#21033;&#29992;&#20174;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36801;&#31227;&#23398;&#20064;&#22987;&#32456;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#26222;&#36941;&#20551;&#35774;&#20174;&#26410;&#34987;&#31995;&#32479;&#39564;&#35777;&#36807;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#26631;&#31614;&#24515;&#30005;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#24494;&#35843;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#24191;&#27867;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#26469;&#39564;&#35777;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#23454;&#65292;&#23545;&#20110;&#23567;&#22411;&#30340;&#19979;&#28216;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#24494;&#35843;&#26159;&#26356;&#22909;&#30340;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#38598;&#36275;&#22815;&#22823;&#26102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#24615;&#33021;&#65292;&#23613;&#31649;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#26469;&#36814;&#22836;&#36214;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36801;&#31227;&#23398;&#20064;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of deep learning in ECG diagnosis is often hindered by the scarcity of large, well-labeled datasets in real-world scenarios, leading to the use of transfer learning to leverage features learned from larger datasets. Yet the prevailing assumption that transfer learning consistently outperforms training from scratch has never been systematically validated. In this study, we conduct the first extensive empirical study on the effectiveness of transfer learning in multi-label ECG classification, by investigating comparing the fine-tuning performance with that of training from scratch, covering a variety of ECG datasets and deep neural networks. We confirm that fine-tuning is the preferable choice for small downstream datasets; however, when the dataset is sufficiently large, training from scratch can achieve comparable performance, albeit requiring a longer training time to catch up. Furthermore, we find that transfer learning exhibits better compatibility with convolutional ne
&lt;/p&gt;</description></item><item><title>$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.17293</link><description>&lt;p&gt;
$\mu$GUIDE:&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#20351;&#29992;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#25512;&#26029;&#26469;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17293
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mu$GUIDE:&#19968;&#31181;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#25110;MRI&#20449;&#21495;&#34920;&#31034;&#20013;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36890;&#36807;&#25193;&#25955;&#21152;&#26435;MRI&#30340;&#31034;&#20363;&#28436;&#31034;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#20449;&#21495;&#29305;&#24449;&#36873;&#25321;&#65292;&#32467;&#21512;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#21644;&#21518;&#39564;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;$\mu$GUIDE&#32469;&#36807;&#20102;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#37319;&#38598;&#32422;&#26463;&#26469;&#23450;&#20041;&#27169;&#22411;&#29305;&#23450;&#30340;&#25688;&#35201;&#32479;&#35745;&#37327;&#12290;&#33719;&#24471;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#31361;&#20986;&#26174;&#31034;&#27169;&#22411;&#23450;&#20041;&#20013;&#23384;&#22312;&#30340;&#36864;&#21270;&#24615;&#65292;&#24182;&#37327;&#21270;&#25152;&#20272;&#35745;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2311.04698</link><description>&lt;p&gt;
&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#25361;&#25112;&#24120;&#35265;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Challenging Common Paradigms in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04698
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25361;&#25112;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24120;&#35265;&#33539;&#24335;&#65292;&#36890;&#36807;&#30740;&#31350;&#22312;&#21333;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#20248;&#21270;&#22120;&#36873;&#25321;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#29702;&#35770;&#25512;&#23548;&#20986;&#20102;&#26799;&#24230;&#20914;&#31361;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#20294;&#20854;&#22522;&#26412;&#26426;&#21046;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#24182;&#26410;&#24102;&#26469;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#65288;STL&#65289;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#26356;&#28145;&#20837;&#20102;&#35299;MTL&#29305;&#23450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;MTL&#20013;&#30340;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20960;&#28857;&#20851;&#20110;STL&#30340;&#37325;&#35201;&#24433;&#21709;&#65306;&#39318;&#20808;&#65292;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#23545;MTL&#30340;&#24433;&#21709;&#21482;&#21463;&#21040;&#20102;&#36731;&#24494;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#30340;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;&#24120;&#35265;STL&#24037;&#20855;&#65288;&#20363;&#22914;Adam&#20248;&#21270;&#22120;&#65289;&#22312;MTL&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;Adam&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#19979;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#37096;&#20998;&#25439;&#22833;&#23610;&#24230;&#19981;&#21464;&#24615;&#12290;&#20854;&#27425;&#65292;&#26799;&#24230;&#20914;&#31361;&#30340;&#27010;&#24565;&#32463;&#24120;&#34987;&#25551;&#36848;&#20026;MTL&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#26799;&#24230;&#20914;&#31361;&#22312;MTL&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;STL&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35282;&#24230;&#26799;&#24230;&#23545;&#40784;&#26041;&#38754;&#65292;&#25105;&#20204;&#27809;&#26377;&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04698v3 Announce Type: replace-cross  Abstract: While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we challenge paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We show the pivotal role of common STL tools such as the Adam optimizer in MTL empirically in various experiments. To further investigate Adam's effectiveness, we theoretical derive a partial loss-scale invariance under mild assumptions. Second, the notion of gradient conflicts has often been phrased as a specific problem in MTL. We delve into the role of gradient conflicts in MTL and compare it to STL. For angular gradient alignment we find no 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2401.12476</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#38477;&#38454;&#24314;&#27169;&#36827;&#34892;&#36125;&#21494;&#26031;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#35782;&#21035;&#21644;&#22810;&#39033;&#24335;&#22122;&#22768; (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32479;&#35745;&#30456;&#20851;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#26694;&#26550;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#39640;&#32500;&#31995;&#32479;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20351;&#29992;&#38543;&#26426;&#21160;&#21147;&#27169;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#27979;&#37327;&#22122;&#22768;&#12290;&#35813;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#32452;&#25104;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#30340;&#20284;&#28982;&#20989;&#25968;&#25152;&#38656;&#30340;&#32479;&#35745;&#30456;&#20851;&#30340;&#65292;&#30690;&#37327;&#20540;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#22122;&#22768;&#27169;&#22411;&#30340;&#39640;&#26031;&#28388;&#27874;&#22120;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#23545;&#39640;&#32500;&#31995;&#32479;&#36827;&#34892;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#31995;&#32479;&#35782;&#21035;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#32435;&#20837;&#25152;&#25552;&#35758;&#30340;&#26694;&#26550;&#20013;&#65292;&#20351;&#29992;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#31995;&#32479;&#20316;&#20026;&#19968;&#20010;&#20030;&#20363;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#25105;&#20204;&#23558;&#36125;&#21494;&#26031;&#26041;&#27861;&#19982;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19968;&#20010;&#20856;&#22411;&#30340;&#38750;&#20998;&#31163;&#21704;&#23494;&#39039;&#27169;&#22411;&#21644;&#24102;&#26377;&#23567;&#22411;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#28151;&#27788;&#21452;&#25670;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that us
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09181</link><description>&lt;p&gt;
&#36229;&#36234;&#21453;&#36951;&#24536;: &#24102;&#26377;&#27491;&#21521;&#20256;&#36882;&#30340;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer. (arXiv:2401.09181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fwd-Prompt&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#22312;&#27531;&#24046;&#31354;&#38388;&#21644;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#26799;&#24230;&#25237;&#24433;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#36830;&#32493;&#25351;&#23548;&#35843;&#20248;&#65288;MCIT&#65289;&#20351;&#24471;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#28385;&#36275;&#19981;&#26029;&#20986;&#29616;&#30340;&#38656;&#27714;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;MCIT&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#65306;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;&#26087;&#30693;&#35782;&#34987;&#36951;&#24536;&#65289;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#65288;&#26410;&#26469;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#65289;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22823;&#22823;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20294;&#20173;&#28982;&#36973;&#21463;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#36890;&#36807;&#23545;&#36755;&#20837;&#23884;&#20837;&#36827;&#34892;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#36755;&#20837;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#24322;&#12290;&#36825;&#31181;&#24046;&#24322;&#23548;&#33268;&#27169;&#22411;&#23398;&#20064;&#19982;&#26087;&#30340;&#21644;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36127;&#38754;&#30340;&#27491;&#21521;&#20256;&#36882;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fwd-Prompt&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#26799;&#24230;&#25237;&#24433;&#21040;&#27531;&#24046;&#31354;&#38388;&#20013;&#65292;&#20197;&#20943;&#23567;&#20219;&#21153;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#24182;&#25237;&#24433;&#21040;&#39044;&#35757;&#32451;&#23376;&#31354;&#38388;&#20013;&#20197;&#37325;&#29992;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large Language Models (MLLMs) to meet continuously emerging requirements without expensive retraining. MCIT faces two major obstacles: catastrophic forgetting (where old knowledge is forgotten) and negative forward transfer (where the performance of future tasks is degraded). Although existing methods have greatly alleviated catastrophic forgetting, they still suffer from negative forward transfer. By performing singular value decomposition (SVD) on input embeddings, we discover a large discrepancy in different input embeddings. The discrepancy results in the model learning irrelevant information for old and pre-trained tasks, which leads to catastrophic forgetting and negative forward transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method projecting prompt gradient to the residual space to minimize the interference between tasks and to the pre-trained subspace for reusing pre-trained knowledge. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.07202</link><description>&lt;p&gt;
&#36755;&#20837;&#20984;LSTM&#65306;&#19968;&#31181;&#24555;&#36895;&#22522;&#20110;Lyapunov&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model Predictive Control. (arXiv:2311.07202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#21644;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#26469;&#25913;&#21892;MPC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;ICNN&#65289;&#65292;&#22522;&#20110;ICNN&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#36890;&#36807;&#22312;MPC&#26694;&#26550;&#20013;&#20445;&#25345;&#20984;&#24615;&#25104;&#21151;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ICNN&#26550;&#26500;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#22797;&#26434;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;MPC&#21644;&#22522;&#20110;ICNN&#30340;MPC&#65292;&#19982;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#27169;&#22411;&#30340;MPC&#30456;&#27604;&#38754;&#20020;&#36739;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;ICNN&#30340;&#21407;&#29702;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20984;LSTM&#30340;&#22522;&#20110;Lyapunov&#30340;MPC&#65292;&#26088;&#22312;&#20943;&#23569;&#25910;&#25947;&#26102;&#38388;&#12289;&#32531;&#35299;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#24182;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#38750;&#32447;&#24615;&#21270;&#23398;&#21453;&#24212;&#22120;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#38382;&#39064;&#30340;&#32531;&#35299;&#21644;&#25910;&#25947;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#25910;&#25947;&#26102;&#38388;&#24179;&#22343;&#38477;&#20302;&#20102;&#19968;&#23450;&#30340;&#30334;&#20998;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging Input Convex Neural Networks (ICNNs), ICNN-based Model Predictive Control (MPC) successfully attains globally optimal solutions by upholding convexity within the MPC framework. However, current ICNN architectures encounter the issue of vanishing/exploding gradients, which limits their ability to serve as deep neural networks for complex tasks. Additionally, the current neural network-based MPC, including conventional neural network-based MPC and ICNN-based MPC, faces slower convergence speed when compared to MPC based on first-principles models. In this study, we leverage the principles of ICNNs to propose a novel Input Convex LSTM for Lyapunov-based MPC, with the specific goal of reducing convergence time and mitigating the vanishing/exploding gradient problem while ensuring closed-loop stability. From a simulation study of a nonlinear chemical reactor, we observed a mitigation of vanishing/exploding gradient problem and a reduction in convergence time, with a percentage de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>MixerFlow&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#21644;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.16777</link><description>&lt;p&gt;
&#22270;&#20687;&#24314;&#27169;&#30340;MixerFlow
&lt;/p&gt;
&lt;p&gt;
MixerFlow for Image Modelling. (arXiv:2310.16777v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16777
&lt;/p&gt;
&lt;p&gt;
MixerFlow&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#27491;&#21017;&#21270;&#27969;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#21644;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#27969;&#26159;&#19968;&#31181;&#32479;&#35745;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#23556;&#21464;&#25442;&#23558;&#22797;&#26434;&#23494;&#24230;&#36716;&#25442;&#20026;&#31616;&#21333;&#23494;&#24230;&#65292;&#23454;&#29616;&#20102;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#21333;&#20010;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#30340;&#21151;&#33021;&#12290;&#22312;&#22270;&#20687;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#20027;&#35201;&#36873;&#25321;&#30340;&#26159;&#22522;&#20110;Glow&#30340;&#26550;&#26500;&#65292;&#32780;&#20854;&#20182;&#26550;&#26500;&#22312;&#30740;&#31350;&#30028;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLP-Mixer&#26550;&#26500;&#30340;&#26032;&#22411;&#26550;&#26500;MixerFlow&#65292;&#36827;&#19968;&#27493;&#32479;&#19968;&#20102;&#29983;&#25104;&#24615;&#21644;&#21028;&#21035;&#24615;&#24314;&#27169;&#26550;&#26500;&#12290;MixerFlow&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26435;&#37325;&#20849;&#20139;&#26426;&#21046;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22266;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#65292;MixerFlow&#22312;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#23494;&#24230;&#20272;&#35745;&#24615;&#33021;&#65292;&#24182;&#19988;&#38543;&#30528;&#22270;&#20687;&#20998;&#36776;&#29575;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#20063;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#65292;&#20351;&#24471;MixerFlow&#25104;&#20026;Glow-based&#26550;&#26500;&#30340;&#19968;&#20010;&#24378;&#22823;&#32780;&#31616;&#21333;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MixerFlow&#25552;&#20379;&#20102;&#27604;Glow-based&#26550;&#26500;&#26356;&#20016;&#23500;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising flows are statistical models that transform a complex density into a simpler density through the use of bijective transformations enabling both density estimation and data generation from a single model. In the context of image modelling, the predominant choice has been the Glow-based architecture, whereas alternative architectures remain largely unexplored in the research community. In this work, we propose a novel architecture called MixerFlow, based on the MLP-Mixer architecture, further unifying the generative and discriminative modelling architectures. MixerFlow offers an effective mechanism for weight sharing for flow-based models. Our results demonstrate better density estimation on image datasets under a fixed computational budget and scales well as the image resolution increases, making MixeFlow a powerful yet simple alternative to the Glow-based architectures. We also show that MixerFlow provides more informative embeddings than Glow-based architectures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11009</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#32463;&#20856;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#21551;&#21457;&#24335;&#24230;&#37327;&#34987;&#36873;&#25321;&#20026;&#22312;&#19982;&#38142;&#36335;&#24418;&#25104;&#30456;&#20851;&#30340;&#22522;&#26412;&#22240;&#32032;&#19978;&#19982;&#20043;&#30456;&#20851;&#33391;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#20248;&#21183;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#20197;&#21450;&#25429;&#25417;&#20505;&#36873;&#38142;&#36335;&#20013;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#8220;&#23545;&#21521;&#32534;&#30721;&#8221;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#21521;&#32534;&#30721;&#24448;&#24448;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#26412;&#22240;&#32032;&#26469;&#20998;&#31867;&#25152;&#26377;&#38142;&#36335;&#12290;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#27491;&#30830;&#20998;&#31867;&#21487;&#33021;&#30001;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#21508;&#31181;&#19981;&#21516;&#38142;&#36335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27010;&#24565;&#21457;&#29616;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#25991;&#26412;&#27169;&#22411;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#27010;&#24565;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.02116</link><description>&lt;p&gt;
&#20998;&#23618;&#27010;&#24565;&#21457;&#29616;&#27169;&#22411;&#65306;&#19968;&#20010;&#27010;&#24565;&#37329;&#23383;&#22612;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Concept Discovery Models: A Concept Pyramid Scheme. (arXiv:2310.02116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27010;&#24565;&#21457;&#29616;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;&#25991;&#26412;&#27169;&#22411;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#20154;&#31867;&#21487;&#29702;&#35299;&#27010;&#24565;&#30340;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#22823;&#37327;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#19981;&#21487;&#35299;&#37322;&#30340;&#25805;&#20316;&#26041;&#24335;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#33258;&#20449;&#37096;&#32626;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#30340;&#26159;ante hoc&#21487;&#35299;&#37322;&#24615;&#65292;&#20855;&#20307; &#35828;&#26159;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#22810;&#20010;&#23618;&#27425;&#31890;&#24230;&#19978;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#27010;&#24565;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#21033;&#29992;&#65306;&#65288;i&#65289;&#22270;&#20687;&#25991;&#26412;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#21644;&#31232;&#30095;&#35825;&#23548;&#30340;&#36125;&#21494;&#26031;&#21442;&#25968;&#36827;&#34892;&#22810;&#23618;&#27010;&#24565;&#36873;&#25321;&#30340;&#21019;&#26032;&#20844;&#24335;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#27010;&#24565;&#20449;&#24687;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#25972;&#20307;&#22270;&#20687;&#19982;&#19968;&#33324;&#38750;&#32467;&#26500;&#21270;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#24565;&#23618;&#27425;&#30340;&#27010;&#24565;&#65292;&#20197;&#25581;&#31034;&#21644;&#21033;&#29992;&#26356;&#22810;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning algorithms have recently gained significant attention due to their impressive performance. However, their high complexity and un-interpretable mode of operation hinders their confident deployment in real-world safety-critical tasks. This work targets ante hoc interpretability, and specifically Concept Bottleneck Models (CBMs). Our goal is to design a framework that admits a highly interpretable decision making process with respect to human understandable concepts, on multiple levels of granularity. To this end, we propose a novel hierarchical concept discovery formulation leveraging: (i) recent advances in image-text models, and (ii) an innovative formulation for multi-level concept selection via data-driven and sparsity inducing Bayesian arguments. Within this framework, concept information does not solely rely on the similarity between the whole image and general unstructured concepts; instead, we introduce the notion of concept hierarchy to uncover and exploit more gra
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06212</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06212
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#19994;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#23545;&#20110;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38271;&#26399;&#20915;&#31574;&#65292;&#25552;&#21069;&#19968;&#24180;&#36827;&#34892;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#20869;&#21508;&#31181;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#39044;&#27979;&#36825;&#19968;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#26681;&#25454;Palmer&#24178;&#26097;&#20005;&#37325;&#25351;&#25968;&#65288;PDSI&#65289;&#39044;&#27979;&#24863;&#20852;&#36259;&#20122;&#21306;&#30340;&#24178;&#26097;&#24378;&#24230;&#65292;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#30340;&#20869;&#22312;&#22240;&#32032;&#21644;&#35265;&#35299;&#26469;&#25552;&#39640;&#24178;&#26097;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26799;&#24230;&#25552;&#21319;&#21644;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#21069;&#20004;&#31181;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;ROC AUC&#20998;&#25968;&#65292;&#39640;&#36798;0.90
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.11933</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
System Identification for Continuous-time Linear Dynamical Systems. (arXiv:2308.11933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#19979;&#35266;&#27979;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27966;&#29983;&#33719;&#24471;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#35745;&#31639;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kalman&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#22312;&#23398;&#20064;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#21442;&#25968;&#26102;&#65292;&#36890;&#24120;&#20551;&#35774;&#35266;&#27979;&#20540;&#22312;&#31561;&#38388;&#38548;&#30340;&#26102;&#38388;&#28857;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#36830;&#32493;&#31163;&#25955;&#28388;&#27874;&#22120;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#36890;&#36807;&#27714;&#35299;&#36830;&#32493;&#26102;&#38388;Ito&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#25512;&#24191;Kalman&#28388;&#27874;&#22120;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20004;&#28388;&#27874;&#22120;&#65292;&#20855;&#26377;&#36125;&#21494;&#26031;&#27966;&#29983;&#30340;&#35299;&#26512;&#24418;&#24335;&#30340;&#21518;&#39564;&#65292;&#36825;&#26679;&#21487;&#20197;&#24471;&#21040;&#19981;&#38656;&#35201;&#39044;&#20808;&#35745;&#31639;&#30340;&#27491;&#21521;&#20256;&#36882;&#30340;&#35299;&#26512;&#26356;&#26032;&#12290;&#21033;&#29992;&#36825;&#31181;&#35299;&#26512;&#30340;&#39640;&#25928;&#35745;&#31639;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;EM&#36807;&#31243;&#65292;&#29992;&#20110;&#20272;&#35745;SDE&#30340;&#21442;&#25968;&#65292;&#33258;&#28982;&#22320;&#32435;&#20837;&#20102;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of system identification for the Kalman filter, relying on the expectation-maximization (EM) procedure to learn the underlying parameters of a dynamical system, has largely been studied assuming that observations are sampled at equally-spaced time points. However, in many applications this is a restrictive and unrealistic assumption. This paper addresses system identification for the continuous-discrete filter, with the aim of generalizing learning for the Kalman filter by relying on a solution to a continuous-time It\^o stochastic differential equation (SDE) for the latent state and covariance dynamics. We introduce a novel two-filter, analytical form for the posterior with a Bayesian derivation, which yields analytical updates which do not require the forward-pass to be pre-computed. Using this analytical and efficient computation of the posterior, we provide an EM procedure which estimates the parameters of the SDE, naturally incorporating irregularly sampled measurement
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05239</link><description>&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#36719;&#20214;&#12289;&#31995;&#32479;&#21644;&#20225;&#19994;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#23427;&#20204;&#35782;&#21035;&#20102;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#23450;&#20041;&#20102;&#26550;&#26500;&#30340;&#35266;&#28857;&#21644;&#35270;&#22270;&#65292;&#20197;&#26694;&#26550;&#21644;&#35299;&#20915;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#23578;&#26410;&#21253;&#25324;&#19982;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#25454;&#24037;&#31243;&#24072;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#35299;&#20915;&#21709;&#24212;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#20851;&#27880;&#30340;&#26550;&#26500;&#35270;&#28857;&#21644;&#35270;&#22270;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#36866;&#29992;&#20110;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#30340;&#26550;&#26500;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#26222;&#36941;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#32452;&#36866;&#24212;CPS&#39640;&#25928;&#24320;&#21457;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#20248;&#28857;&#26631;&#20934;&#65292;&#21363;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;CPS&#30340;&#26631;&#20934;&#65292;
&lt;/p&gt;
&lt;p&gt;
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14938</link><description>&lt;p&gt;
&#39640;&#25928;&#20114;&#21160;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#21453;&#39304;&#29615;&#30340;&#21306;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Interaction-Aware Interval Analysis of Neural Network Feedback Loops. (arXiv:2307.14938v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#20989;&#25968;&#21644;&#26500;&#24314;&#23884;&#20837;&#31995;&#32479;&#26469;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#21306;&#38388;&#21487;&#36798;&#24615;&#20998;&#26512;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21644;&#24320;&#29615;&#31995;&#32479;&#30340;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#22120;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21253;&#21547;&#20989;&#25968;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#38597;&#21487;&#27604;&#36793;&#30028;&#30340;&#24320;&#29615;&#21160;&#21147;&#23398;&#21253;&#21547;&#20989;&#25968;&#30340;&#26032;&#31867;&#21035;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25429;&#25417;&#31995;&#32479;&#21644;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#23545;&#20110;&#20219;&#24847;&#21160;&#21147;&#31995;&#32479;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#21547;&#20989;&#25968;&#26500;&#24314;&#19968;&#20010;&#29366;&#24577;&#25968;&#26159;&#21407;&#31995;&#32479;&#20004;&#20493;&#30340;&#23884;&#20837;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#23884;&#20837;&#31995;&#32479;&#30340;&#21333;&#20010;&#36712;&#36857;&#21487;&#20197;&#25552;&#20379;&#21487;&#36798;&#38598;&#30340;&#36229;&#30697;&#24418;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#21160;&#21147;&#31995;&#32479;&#30340;&#38381;&#29615;&#23884;&#20837;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#32771;&#34385;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a computationally efficient framework for interval reachability of neural network controlled systems. Our approach builds upon inclusion functions for the neural network controller and the open-loop system. We observe that many state-of-the-art neural network verifiers can produce inclusion functions for neural networks. We introduce and analyze a new class of inclusion functions for the open-loop dynamics based on bounds of the function Jacobian that is particularly suitable for capturing the interactions between systems and neural network controllers. Next, for any dynamical system, we use inclusion functions to construct an embedding system with twice the number of states as the original system. We show that a single trajectory of this embedding system provides hyper-rectangular over-approximations of reachable sets. We then propose two approaches for constructing a closed-loop embedding system for a neural network controlled dynamical system that accounts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.14839</link><description>&lt;p&gt;
&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Kernelised Normalising Flows. (arXiv:2307.14839v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#24402;&#19968;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#20135;&#29983;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#20197;&#20854;&#21487;&#36870;&#30340;&#26550;&#26500;&#32780;&#34987;&#25551;&#36848;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21487;&#36870;&#24615;&#35201;&#27714;&#23545;&#20854;&#34920;&#36798;&#33021;&#21147;&#26045;&#21152;&#38480;&#21046;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#21019;&#26032;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#36798;&#21040;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#26367;&#20195;&#30340;&#36716;&#25442;&#26041;&#27861;&#21364;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26680;&#21270;&#24402;&#19968;&#21270;&#27969;&#33539;&#24335;&#65292;&#31216;&#20026;Ferumal&#27969;&#65292;&#23427;&#23558;&#26680;&#20989;&#25968;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27969;&#65292;&#26680;&#21270;&#27969;&#21487;&#20197;&#20135;&#29983;&#26377;&#31454;&#20105;&#21147;&#25110;&#20248;&#36234;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#12290;&#26680;&#21270;&#27969;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalising Flows are generative models characterised by their invertible architecture. However, the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve satisfactory outcomes. Whilst flow-based models predominantly rely on neural-network-based transformations for expressive designs, alternative transformation methods have received limited attention. In this work, we present Ferumal flow, a novel kernelised normalising flow paradigm that integrates kernels into the framework. Our results demonstrate that a kernelised flow can yield competitive or superior results compared to neural network-based flows whilst maintaining parameter efficiency. Kernelised flows excel especially in the low-data regime, enabling flexible non-parametric density estimation in applications with sparse data availability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01843</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Maximum Likelihood Training of Autoencoders. (arXiv:2306.01843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25104;&#21151;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#29983;&#25104;&#24314;&#27169;&#30340;&#20248;&#24322;&#24615;&#36136;&#19982;&#39640;&#25928;&#33258;&#32534;&#30721;&#22120;&#30456;&#32467;&#21512;&#12290;&#20316;&#32773;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#35774;&#35745;&#20102;&#28040;&#38500;&#36845;&#20195;&#30340;&#20272;&#35745;&#22120;&#24182;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#20855;&#26377;&#20248;&#24322;&#30340;&#32479;&#35745;&#24615;&#36136;&#65292;&#23588;&#20854;&#26159;&#22312;&#24402;&#19968;&#21270;&#27969;&#27169;&#22411;&#20013;&#38750;&#24120;&#27969;&#34892;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#29983;&#25104;&#33258;&#32534;&#30721;&#22120;&#26377;&#26395;&#27604;&#24402;&#19968;&#21270;&#27969;&#26356;&#39640;&#25928;&#12290;&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#38750;&#32422;&#26463;&#33258;&#32534;&#30721;&#22120;&#30340;&#25104;&#21151;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#65292;&#23558;&#36825;&#20004;&#31181;&#33539;&#24335;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35782;&#21035;&#24182;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#29616;&#26377;&#30340;&#33258;&#30001;&#26684;&#24335;&#32593;&#32476;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#36807;&#20110;&#32531;&#24930;&#65292;&#20381;&#36182;&#20110;&#36845;&#20195;&#26041;&#26696;&#65292;&#20854;&#25104;&#26412;&#38543;&#28508;&#22312;&#32500;&#24230;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#36845;&#20195;&#65292;&#20174;&#32780;&#20351;&#25104;&#26412;&#20445;&#25345;&#19981;&#21464;&#65288;&#27599;&#20010;&#25209;&#27425;&#30340;&#36816;&#34892;&#26102;&#38388;&#22823;&#32422;&#26159;&#26222;&#36890;&#33258;&#32534;&#30721;&#22120;&#30340;&#20004;&#20493;&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35777;&#26126;&#26420;&#32032;&#22320;&#23558;&#26368;&#22823;&#20284;&#28982;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#21487;&#33021;&#23548;&#33268;&#21457;&#25955;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#24819;&#27861;&#26469;&#25512;&#21160;&#31283;&#23450;&#30340;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#35757;&#32451;&#19968;&#31995;&#21015;&#38750;&#32422;&#26463;&#24615;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#29983;&#25104;&#22270;&#20687;&#12289;&#25554;&#20540;&#21644;&#21464;&#25442;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood training has favorable statistical properties and is popular for generative modeling, especially with normalizing flows. On the other hand, generative autoencoders promise to be more efficient than normalizing flows due to the manifold hypothesis. In this work, we introduce successful maximum likelihood training of unconstrained autoencoders for the first time, bringing the two paradigms together. To do so, we identify and overcome two challenges: Firstly, existing maximum likelihood estimators for free-form networks are unacceptably slow, relying on iteration schemes whose cost scales linearly with latent dimension. We introduce an improved estimator which eliminates iteration, resulting in constant cost (roughly double the runtime per batch of a vanilla autoencoder). Secondly, we demonstrate that naively applying maximum likelihood to autoencoders can lead to divergent solutions and use this insight to motivate a stable maximum likelihood training objective. We per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17323</link><description>&lt;p&gt;
&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24378;&#20984;&#20294;&#28508;&#22312;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20248;&#21270;&#30340;&#65288;&#38543;&#26426;&#65289;&#27425;&#26799;&#24230;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31561;&#20215;&#23545;&#20598;&#25551;&#36848;&#65288;&#31867;&#20284;&#20110;&#23545;&#20598;&#24179;&#22343;&#65289;&#26469;&#25551;&#36848;&#32463;&#20856;&#30340;&#27425;&#26799;&#24230;&#27861;&#65292;&#36817;&#31471;&#27425;&#26799;&#24230;&#27861;&#21644;&#20999;&#25442;&#27425;&#26799;&#24230;&#27861;&#12290;&#36825;&#20123;&#31561;&#20215;&#24615;&#33021;&#22815;&#20197; $O(1/T)$ &#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#20998;&#21035;&#36824;&#25552;&#20379;&#20102;&#32463;&#20856;&#21407;&#22987;&#38388;&#38553;&#21644;&#21069;&#20154;&#26410;&#26366;&#20998;&#26512;&#30340;&#23545;&#20598;&#38388;&#38553;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#36825;&#20123;&#32463;&#20856;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#36817;&#20046;&#25152;&#26377;&#30340;&#27493;&#38271;&#36873;&#25321;&#21644;&#19968;&#31995;&#21015;&#30340;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#23545;&#20110;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27425;&#26799;&#24230;&#27861;&#30340;&#26089;&#26399;&#36845;&#20195;&#21487;&#33021;&#20250;&#20986;&#29616;&#25351;&#25968;&#32423;&#30340;&#21457;&#25955;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#22788;&#29702;&#36807;&#36825;&#31181;&#38382;&#39064;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#19981;&#33391;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20173;&#28982;&#30830;&#20445;&#21644; bounds &#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.11957</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;&#25506;&#32034;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards understanding neural collapse in supervised contrastive learning with the information bottleneck method. (arXiv:2305.11957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#25351;&#22312;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#21518;&#19968;&#23618;&#28608;&#27963;&#30340;&#20960;&#20309;&#23398;&#34920;&#29616;&#12290;&#30446;&#21069;&#23384;&#22312;&#30340;&#38382;&#39064;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#26159;&#21542;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#65292;&#22914;&#26524;&#26159;&#65292;&#36229;&#20986;&#24615;&#33021;&#24179;&#21488;&#30340;&#35757;&#32451;&#22914;&#20309;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#12290;&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#24314;&#27169;&#20026;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#20854;&#19982;&#27867;&#21270;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23849;&#28291;&#23548;&#33268;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#25509;&#36817;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#20449;&#24687;&#29942;&#39048;&#35299;&#26102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#23545;&#27604;&#25439;&#22833;&#30446;&#26631;&#29420;&#31435;&#35757;&#32451;&#30340;&#20004;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#32447;&#24615;&#21487;&#35782;&#21035;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#24471;&#21040;&#30340;&#34920;&#31034;&#31561;&#25928;&#20110;&#30697;&#38453;&#21464;&#25442;&#12290;&#25105;&#20204;&#21033;&#29992;&#32447;&#24615;&#21487;&#35782;&#21035;&#24615;&#26469;&#36817;&#20284;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#12290;&#36825;&#20010;&#36817;&#20284;&#34920;&#26126;&#65292;&#24403;&#31867;&#24179;&#22343;&#20540;&#30456;&#31561;&#26102;&#65292;&#26368;&#20248;&#35299;&#38750;&#24120;&#25509;&#36817;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural collapse describes the geometry of activation in the final layer of a deep neural network when it is trained beyond performance plateaus. Open questions include whether neural collapse leads to better generalization and, if so, why and how training beyond the plateau helps. We model neural collapse as an information bottleneck (IB) problem in order to investigate whether such a compact representation exists and discover its connection to generalization. We demonstrate that neural collapse leads to good generalization specifically when it approaches an optimal IB solution of the classification problem. Recent research has shown that two deep neural networks independently trained with the same contrastive loss objective are linearly identifiable, meaning that the resulting representations are equivalent up to a matrix transformation. We leverage linear identifiability to approximate an analytical solution of the IB problem. This approximation demonstrates that when class means exh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09605</link><description>&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Expressiveness Remarks for Denoising Diffusion Models and Samplers. (arXiv:2305.09605v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26041;&#38754;&#36827;&#34892;&#20102;&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;&#24050;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#26368;&#36817;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#28459;&#25193;&#36807;&#31243;&#36880;&#28176;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#22122;&#22768;&#65292;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#27169;&#25311;&#35813;&#28459;&#25193;&#30340;&#26102;&#38388;&#21453;&#28436;&#30340;&#36924;&#36817;&#26469;&#33719;&#21462;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#65292;&#21018;&#24320;&#22987;&#36825;&#20010;&#28459;&#25193;&#27169;&#25311;&#30340;&#21021;&#22987;&#20540;&#26159;&#39640;&#26031;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#28459;&#25193;&#27169;&#22411;&#36866;&#24212;&#20110;&#37319;&#26679;&#21644;&#25512;&#26029;&#20219;&#21153;&#12290;&#26412;&#25991;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#19982;F\"ollmer&#28418;&#31227;&#31867;&#20284;&#30340;&#38543;&#26426;&#25511;&#21046;&#32852;&#31995;&#65292;&#23558;&#38024;&#23545;F\"ollmer&#28418;&#31227;&#30340;&#24050;&#30693;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#32467;&#26524;&#25193;&#23637;&#21040;&#28459;&#25193;&#25193;&#25955;&#27169;&#22411;&#21644;&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models are a class of generative models which have recently achieved state-of-the-art results across many domains. Gradual noise is added to the data using a diffusion process, which transforms the data distribution into a Gaussian. Samples from the generative model are then obtained by simulating an approximation of the time reversal of this diffusion initialized by Gaussian samples. Recent research has explored adapting diffusion models for sampling and inference tasks. In this paper, we leverage known connections to stochastic control akin to the F\"ollmer drift to extend established neural network approximation results for the F\"ollmer drift to denoising diffusion models and samplers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2303.13775</link><description>&lt;p&gt;
GSplit: &#36890;&#36807;&#20998;&#35010;&#24182;&#34892;&#23454;&#29616;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism. (arXiv:2303.13775v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#34892;&#19994;&#12289;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#22270;&#20998;&#26512;&#12289;&#30693;&#35782;&#24211;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#29983;&#29289;&#23398;&#65289;&#20013;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#36793;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#20110;&#22312;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#26469;&#23398;&#20064;&#36825;&#20123;&#22270;&#24418;&#12290;&#22312;&#22823;&#22411;&#22270;&#24418;&#19978;&#35757;&#32451;&#36890;&#24120;&#37319;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#65292;&#24182;&#19988;&#25968;&#25454;&#24182;&#34892;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#25193;&#23637;&#21040;&#22810;&#20010; GPU &#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;GNN &#35757;&#32451;&#31995;&#32479;&#30340;&#20960;&#20010;&#22522;&#26412;&#24615;&#33021;&#29942;&#39048;&#19982;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#26377;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#33539;&#24335;- &#20998;&#35010;&#24182;&#34892;&#65292;&#24182;&#23558;&#20854;&#23454;&#29616;&#22312;&#19968;&#20010;&#21517;&#20026;gsplit&#30340;&#26032;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;gsplit &#30340;&#24615;&#33021;&#20248;&#20110;DGL&#12289;Quiver&#21644;PaGraph&#31561;&#29616;&#26377;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.
&lt;/p&gt;</description></item><item><title>&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.11695</link><description>&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning. (arXiv:2211.11695v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11695
&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#22240;&#32032;&#65292;&#20174;&#32780;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#23427;&#22312;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#32544;&#21487;&#35266;&#27979;&#25968;&#25454;&#20013;&#38544;&#34255;&#22240;&#32032;&#30340;&#27169;&#22411;&#12290;&#23558;&#21464;&#21270;&#30340;&#28508;&#22312;&#35201;&#32032;&#20998;&#31163;&#25104;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#30340;&#21464;&#37327;&#30340;&#36807;&#31243;&#26377;&#21161;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#27169;&#20223;&#20154;&#31867;&#35266;&#23519;&#23545;&#35937;&#25110;&#20851;&#31995;&#26102;&#30340;&#26377;&#24847;&#20041;&#29702;&#35299;&#36807;&#31243;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;DRL&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#21487;&#25511;&#24615;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25968;&#25454;&#25366;&#25496;&#31561;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;DRL&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#23450;&#20041;&#12289;&#26041;&#27861;&#35770;&#12289;&#35780;&#20272;&#12289;&#24212;&#29992;&#21644;&#27169;&#22411;&#35774;&#35745;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;&#20004;&#20010;&#20844;&#35748;&#23450;&#20041;&#65288;&#30452;&#35266;&#23450;&#20041;&#21644;&#32676;&#35770;&#23450;&#20041;&#65289;&#30340;DRL&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;DRL&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further ca
&lt;/p&gt;</description></item></channel></rss>