<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;</title><link>https://arxiv.org/abs/2404.02180</link><description>&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#22320;&#36136;&#21046;&#22270;&#30340;&#36965;&#24863;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Remote sensing framework for geological mapping via stacked autoencoders and clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02180
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#23454;&#29616;&#36965;&#24863;&#25968;&#25454;&#22320;&#36136;&#21046;&#22270;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#36965;&#24863;&#22320;&#36136;&#21046;&#22270;&#20013;&#38754;&#20020;&#30528;&#30001;&#20110;&#20934;&#30830;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#32780;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25581;&#31034;&#36965;&#24863;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#38477;&#32500;&#26041;&#27861;&#20855;&#26377;&#22312;&#25552;&#39640;&#22320;&#36136;&#22270;&#20934;&#30830;&#24615;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#38477;&#32500;&#26041;&#27861;&#21487;&#33021;&#22312;&#38750;&#32447;&#24615;&#25968;&#25454;&#19978;&#36935;&#21040;&#22256;&#38590;&#65292;&#20294;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#27169;&#25311;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#23618;&#65292;&#29992;&#20110;&#25429;&#33719;&#23545;&#36965;&#24863;&#25968;&#25454;&#26377;&#29992;&#30340;&#20998;&#23618;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22534;&#21472;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#22788;&#29702;&#36965;&#24863;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02180v1 Announce Type: cross  Abstract: Supervised learning methods for geological mapping via remote sensing face limitations due to the scarcity of accurately labelled training data. In contrast, unsupervised learning methods, such as dimensionality reduction and clustering have the ability to uncover patterns and structures in remote sensing data without relying on predefined labels. Dimensionality reduction methods have the potential to play a crucial role in improving the accuracy of geological maps. Although conventional dimensionality reduction methods may struggle with nonlinear data, unsupervised deep learning models such as autoencoders have the ability to model nonlinear relationship in data. Stacked autoencoders feature multiple interconnected layers to capture hierarchical data representations that can be useful for remote sensing data. In this study, we present an unsupervised machine learning framework for processing remote sensing data by utilizing stacked au
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2404.01273</link><description>&lt;p&gt;
TWIN-GPT: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#23383;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#34394;&#25311;&#20020;&#24202;&#35797;&#39564;&#20135;&#29983;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#65292;&#36825;&#20123;&#35797;&#39564;&#27169;&#25311;&#20102;&#29616;&#23454;&#19990;&#30028;&#24773;&#22659;&#65292;&#26377;&#26395;&#26174;&#33879;&#22686;&#24378;&#24739;&#32773;&#23433;&#20840;&#24615;&#65292;&#21152;&#24555;&#24320;&#21457;&#36895;&#24230;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#24182;&#20026;&#21307;&#30103;&#39046;&#22495;&#30340;&#26356;&#24191;&#27867;&#31185;&#23398;&#30693;&#35782;&#36129;&#29486;&#21147;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;TWIN-GPT&#65292;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01273v1 Announce Type: cross  Abstract: Recently, there has been a burgeoning interest in virtual clinical trials, which simulate real-world scenarios and hold the potential to significantly enhance patient safety, expedite development, reduce costs, and contribute to the broader scientific knowledge in healthcare. Existing research often focuses on leveraging electronic health records (EHRs) to support clinical trial outcome prediction. Yet, trained with limited clinical trial outcome data, existing approaches frequently struggle to perform accurate predictions. Some research has attempted to generate EHRs to augment model development but has fallen short in personalizing the generation for individual patient profiles. Recently, the emergence of large language models has illuminated new possibilities, as their embedded comprehensive clinical knowledge has proven beneficial in addressing medical issues. In this paper, we propose a large language model-based digital twin crea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; RECO-SLIP&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#22270;&#20013;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#33410;&#28857;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2404.01216</link><description>&lt;p&gt;
&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#26032;&#39062;&#33410;&#28857;&#31867;&#21035;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Novel Node Category Detection Under Subpopulation Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01216
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; RECO-SLIP&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#22270;&#20013;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#30340;&#33410;&#28857;&#26816;&#27979;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#65292;&#20998;&#24067;&#36716;&#31227;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#26041;&#24335;&#34920;&#29616;&#65292;&#20363;&#22914;&#26032;&#31867;&#21035;&#30340;&#20986;&#29616;&#21644;&#29616;&#26377;&#31867;&#21035;&#30456;&#23545;&#27604;&#20363;&#30340;&#21464;&#21270;&#12290;&#22312;&#36825;&#31181;&#20998;&#24067;&#36716;&#31227;&#19979;&#65292;&#26816;&#27979;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#23545;&#20110;&#23433;&#20840;&#25110;&#27934;&#23519;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20855;&#26377;&#36873;&#25321;&#24615;&#38142;&#36335;&#39044;&#27979;&#30340;&#21484;&#22238;&#32422;&#26463;&#20248;&#21270;&#65288;RECO-SLIP&#65289;&#65292;&#29992;&#20110;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#26816;&#27979;&#23646;&#24615;&#22270;&#20013;&#23646;&#20110;&#26032;&#31867;&#21035;&#30340;&#33410;&#28857;&#12290;&#36890;&#36807;&#23558;&#21484;&#22238;&#32422;&#26463;&#23398;&#20064;&#26694;&#26550;&#19982;&#39640;&#25928;&#26679;&#26412;&#39044;&#27979;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;RECO-SLIP&#35299;&#20915;&#20102;&#25269;&#25239;&#23376;&#32676;&#20307;&#36716;&#31227;&#21644;&#26377;&#25928;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#21452;&#37325;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;RECO-SLIP&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01216v1 Announce Type: new  Abstract: In real-world graph data, distribution shifts can manifest in various ways, such as the emergence of new categories and changes in the relative proportions of existing categories. It is often important to detect nodes of novel categories under such distribution shifts for safety or insight discovery purposes. We introduce a new approach, Recall-Constrained Optimization with Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel categories in attributed graphs under subpopulation shifts. By integrating a recall-constrained learning framework with a sample-efficient link prediction mechanism, RECO-SLIP addresses the dual challenges of resilience against subpopulation shifts and the effective exploitation of graph structure. Our extensive empirical evaluation across multiple graph datasets demonstrates the superior performance of RECO-SLIP over existing methods.
&lt;/p&gt;</description></item><item><title>AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.19708</link><description>&lt;p&gt;
AttentionStore: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#23454;&#29616;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#25104;&#26412;&#25928;&#30410;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
AttentionStore: Cost-effective Attention Reuse across Multi-turn Conversations in Large Language Model Serving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19708
&lt;/p&gt;
&lt;p&gt;
AttentionStore&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#23454;&#29616;KV&#32531;&#23384;&#30340;&#22797;&#29992;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#37325;&#22797;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#19982;&#20154;&#31867;&#36827;&#34892;&#20132;&#20114;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#37325;&#22797;&#35745;&#31639;&#21382;&#21490;&#35760;&#21495;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#23548;&#33268;&#29616;&#26377;&#29992;&#20110;&#25191;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;LLM&#26381;&#21153;&#24341;&#25806;&#25928;&#29575;&#20302;&#19979;&#65292;&#20135;&#29983;&#39640;&#26114;&#30340;&#26381;&#21153;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AttentionStore&#65292;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36328;&#22810;&#36718;&#23545;&#35805;&#30340;KV&#32531;&#23384;&#22797;&#29992;&#65288;&#21363; &#27880;&#24847;&#21147;&#22797;&#29992;&#65289;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#37325;&#22797;&#35745;&#31639;&#24320;&#38144;&#12290;AttentionStore&#32500;&#25252;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;KV&#32531;&#23384;&#31995;&#32479;&#65292;&#21033;&#29992;&#25104;&#26412;&#25928;&#30410;&#30340;&#20869;&#23384;/&#23384;&#20648;&#20171;&#36136;&#20026;&#25152;&#26377;&#35831;&#27714;&#20445;&#23384;KV&#32531;&#23384;&#12290;&#20026;&#20102;&#20943;&#23569;&#24930;&#36895;&#20171;&#36136;&#30340;KV&#32531;&#23384;&#35775;&#38382;&#24320;&#38144;&#65292;AttentionStore&#37319;&#29992;&#36880;&#23618;&#39044;&#21152;&#36733;&#21644;&#24322;&#27493;&#20445;&#23384;&#26041;&#26696;&#65292;&#23558;KV&#32531;&#23384;&#35775;&#38382;&#19982;GPU&#35745;&#31639;&#37325;&#21472;&#12290;&#20026;&#30830;&#20445;&#35201;&#35775;&#38382;&#30340;KV&#32531;&#23384;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19708v1 Announce Type: new  Abstract: Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines for executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes AttentionStore, a new attention mechanism that enables the reuse of KV caches (i.e., attention reuse) across multi-turn conversations, significantly reducing the repetitive computation overheads. AttentionStore maintains a hierarchical KV caching system that leverages cost-effective memory/storage mediums to save KV caches for all requests. To reduce KV cache access overheads from slow mediums, AttentionStore employs layer-wise pre-loading and asynchronous saving schemes to overlap the KV cache access with the GPU computation. To ensure that the KV caches to be accessed are pl
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#38463;&#26681;&#24311;&#22320;&#21306;&#30340;&#24773;&#20917;&#65292;&#21457;&#23637;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#20004;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18631</link><description>&lt;p&gt;
&#39318;&#27425;&#22312;&#38463;&#26681;&#24311;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#31958;&#23615;&#30149;&#39118;&#38505;&#20154;&#32676;&#30340;&#21021;&#27493;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
First Experiences with the Identification of People at Risk for Diabetes in Argentina using Machine Learning Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18631
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#38024;&#23545;&#38463;&#26681;&#24311;&#22320;&#21306;&#30340;&#24773;&#20917;&#65292;&#21457;&#23637;&#21644;&#35780;&#20272;&#20102;&#29992;&#20110;&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#20004;&#20010;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;2&#22411;&#31958;&#23615;&#30149;&#65288;T2D&#65289;&#21644;&#31958;&#23615;&#30149;&#21069;&#26399;&#65288;PD&#65289;&#23545;&#21307;&#23398;&#26159;&#19968;&#20010;&#30495;&#27491;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#30149;&#21407;&#24615;&#30151;&#29366;&#21644;&#24050;&#30693;&#30456;&#20851;&#21361;&#38505;&#22240;&#32032;&#12290;&#23613;&#31649;&#26377;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25552;&#35758;&#20351;&#24471;&#35782;&#21035;&#24739;&#30149;&#39118;&#38505;&#30340;&#20154;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#36825;&#31181;&#30149;&#30151;&#30340;&#24615;&#36136;&#20351;&#24471;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#31181;&#20154;&#32676;&#30340;&#27169;&#22411;&#26410;&#24517;&#36866;&#29992;&#20110;&#21478;&#19968;&#31181;&#20154;&#32676;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#38463;&#26681;&#24311;&#29305;&#21035;&#21457;&#23637;&#21644;&#35780;&#20272;&#29992;&#20110;&#35782;&#21035;T2D&#21644;PD&#39118;&#38505;&#20154;&#32676;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25968;&#25454;&#24211;&#32463;&#36807;&#24443;&#24213;&#39044;&#22788;&#29702;&#65292;&#29983;&#25104;&#20102;&#19977;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#21040;&#35760;&#24405;&#25968;&#21644;&#21487;&#29992;&#21464;&#37327;&#30340;&#26435;&#34913;&#12290;&#24212;&#29992;&#20102;5&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#27169;&#22411;&#21518;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#22320;&#65292;RF&#12289;DT&#21644;ANN&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18631v1 Announce Type: new  Abstract: Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for medicine due to the absence of pathogenic symptoms and the lack of known associated risk factors. Even though some proposals for machine learning models enable the identification of people at risk, the nature of the condition makes it so that a model suitable for one population may not necessarily be suitable for another. In this article, the development and assessment of predictive models to identify people at risk for T2D and PD specifically in Argentina are discussed. First, the database was thoroughly preprocessed and three specific datasets were generated considering a compromise between the number of records and the amount of available variables. After applying 5 different classification models, the results obtained show that a very good performance was observed for two datasets with some of these models. In particular, RF, DT, and ANN demonstrated great c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Watermarking LLM-Generated Code Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#20316;&#21697;&#34920;&#26126;&#27700;&#21360;&#25216;&#26415;&#23545;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#24456;&#23481;&#26131;&#31227;&#38500;&#20195;&#30721;&#19978;&#30340;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17852</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;&#20559;&#35265;&#27491;&#20132;&#30340;&#26041;&#24335;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness through Transforming Data Orthogonal to Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20915;&#31574;&#65292;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24453;&#36935;&#19981;&#24179;&#31561;&#12290;&#23613;&#31649;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#22810;&#20803;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#23545;&#20915;&#31574;&#32467;&#26524;&#30340;&#24494;&#22937;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#21363;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#26088;&#22312;&#28040;&#38500;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20013;&#32852;&#21512;&#27491;&#24577;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#21363;&#21487;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;OB&#31639;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#26368;&#23567;&#21270;&#20316;&#19994;&#24310;&#36831;&#65292;&#24341;&#20837;&#20102;&#20999;&#25442;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17480</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#23384;&#21644;&#20999;&#25442;&#25104;&#26412;&#30340;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#23481;&#37327;&#35843;&#37197;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17480
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#26368;&#23567;&#21270;&#20316;&#19994;&#24310;&#36831;&#65292;&#24341;&#20837;&#20102;&#20999;&#25442;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20102;&#19968;&#31181;&#22312;&#32447;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#35843;&#33410;&#27963;&#21160;&#26381;&#21153;&#22120;&#30340;&#25968;&#37327;&#26469;&#26368;&#23567;&#21270;&#19968;&#32452;&#20316;&#19994;&#30340;&#27969;&#37327;&#26102;&#38388;&#65288;&#24635;&#24310;&#36831;&#65289;&#65292;&#20294;&#22312;&#26102;&#38388;&#21464;&#21270;&#26102;&#25913;&#21464;&#27963;&#21160;&#26381;&#21153;&#22120;&#25968;&#37327;&#20250;&#20135;&#29983;&#20999;&#25442;&#25104;&#26412;&#12290;&#27599;&#20010;&#20316;&#19994;&#22312;&#20219;&#20309;&#26102;&#38388;&#20869;&#26368;&#22810;&#21487;&#20197;&#30001;&#19968;&#20010;&#22266;&#23450;&#36895;&#24230;&#30340;&#26381;&#21153;&#22120;&#22788;&#29702;&#12290;&#19982;&#36890;&#24120;&#20855;&#26377;&#20999;&#25442;&#25104;&#26412;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#38382;&#39064;&#30456;&#27604;&#65292;&#25152;&#32771;&#34385;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;&#38750;&#20984;&#30340;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#65292;&#23427;&#21462;&#20915;&#20110;&#25152;&#26377;&#36807;&#21435;&#30340;&#20915;&#31574;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24403;&#21069;&#30340;&#20915;&#31574;&#12290;&#32771;&#34385;&#20102;&#26368;&#22351;&#24773;&#20917;&#21644;&#38543;&#26426;&#36755;&#20837;&#65307;&#23545;&#20110;&#36825;&#20004;&#31181;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#31454;&#20105;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17480v1 Announce Type: cross  Abstract: An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.16986</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#24577;&#30456;&#23545;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relative Representations for Goal-Oriented Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#36890;&#20449;&#30340;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23558;&#21457;&#25381;&#22522;&#30784;&#20316;&#29992;&#65292;&#23558;&#21547;&#20041;&#21644;&#30456;&#20851;&#24615;&#32435;&#20837;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#36923;&#36753;&#25110;&#20869;&#37096;&#34920;&#31034;&#26102;&#65292;&#20250;&#20986;&#29616;&#38556;&#30861;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#21487;&#33021;&#21361;&#21450;&#29702;&#35299;&#12290;&#22312;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#20013;&#65292;&#36825;&#19968;&#25361;&#25112;&#34920;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#26102;&#39640;&#32500;&#34920;&#31034;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26469;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#35843;&#25972;&#30456;&#23545;&#34920;&#31034;&#12289;&#36890;&#20449;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32531;&#35299;&#20013;&#36215;&#20316;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
&lt;/p&gt;</description></item><item><title>CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13583</link><description>&lt;p&gt;
CONLINE: &#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#19982;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#30340;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13583
&lt;/p&gt;
&lt;p&gt;
CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#29983;&#25104;&#22797;&#26434;&#20195;&#30721;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#12289;&#24494;&#22937;&#30340;&#38169;&#35823;&#12289;&#23545;&#39640;&#32423;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#20197;&#21450;&#32570;&#23569;&#36741;&#21161;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CONLINE&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#21010;&#30340;&#22312;&#32447;&#25628;&#32034;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#21160;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#65292;&#36827;&#34892;&#36845;&#20195;&#31934;&#28860;&#12290;CONLINE&#36824;&#20018;&#34892;&#21270;&#20102;&#22797;&#26434;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20197;&#25913;&#21892;&#29702;&#35299;&#65292;&#24182;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#65292;&#30830;&#20445;&#26694;&#26550;&#36866;&#29992;&#20110;&#29616;&#23454;&#24212;&#29992;&#12290;CONLINE&#36890;&#36807;&#23545;DS-1000&#21644;ClassEval&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CONLINE&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#31361;&#26174;&#20102;&#20854;&#25552;&#21319;&#23454;&#36341;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13583v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the pra
&lt;/p&gt;</description></item><item><title>&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.12975</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#20123;&#29702;&#35770;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Training morphological neural networks with gradient descent: some theoretical insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12975
&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23384;&#22312;&#25361;&#25112;&#65292;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#23545;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#30340;&#29702;&#35770;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#24577;&#31070;&#32463;&#32593;&#32476;&#25110;&#23618;&#21487;&#20197;&#25104;&#20026;&#25552;&#21319;&#25968;&#23398;&#24418;&#24577;&#23398;&#36827;&#23637;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#23436;&#25972;&#26684;&#31639;&#23376;&#30340;&#34920;&#31034;&#65292;&#36824;&#26159;&#22312;&#22270;&#20687;&#22788;&#29702;&#27969;&#31243;&#30340;&#24320;&#21457;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26550;&#26500;&#21253;&#21547;&#22810;&#23618;&#24418;&#24577;&#23398;&#26102;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#19979;&#38477;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#27969;&#34892;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#36825;&#20123;&#32593;&#32476;&#24456;&#38590;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#24494;&#20998;&#26041;&#27861;&#21644;&#21453;&#21521;&#20256;&#25773;&#24212;&#29992;&#20110;&#24418;&#24577;&#32593;&#32476;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#32771;&#34385;&#21040;Bouligand&#23548;&#25968;&#30340;&#38750;&#20809;&#28369;&#20248;&#21270;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#39318;&#20010;&#29702;&#35770;&#25351;&#21335;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#21021;&#22987;&#21270;&#21644;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12975v1 Announce Type: cross  Abstract: Morphological neural networks, or layers, can be a powerful tool to boost the progress in mathematical morphology, either on theoretical aspects such as the representation of complete lattice operators, or in the development of image processing pipelines. However, these architectures turn out to be difficult to train when they count more than a few morphological layers, at least within popular machine learning frameworks which use gradient descent based optimization algorithms. In this paper we investigate the potential and limitations of differentiation based approaches and back-propagation applied to morphological networks, in light of the non-smooth optimization concept of Bouligand derivative. We provide insights and first theoretical guidelines, in particular regarding initialization and learning rates.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2403.10123</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Regularization-Based Efficient Continual Learning in Deep State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#36827;&#34892;&#26377;&#25928;&#26356;&#26032;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#23545;&#21160;&#24577;&#31995;&#32479;&#20855;&#26377;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DSSMs&#65289;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DSSM&#24037;&#20316;&#23616;&#38480;&#20110;&#21333;&#20219;&#21153;&#24314;&#27169;&#65292;&#36825;&#38656;&#35201;&#22312;&#37325;&#26032;&#35775;&#38382;&#20043;&#21069;&#30340;&#20219;&#21153;&#26102;&#21033;&#29992;&#21382;&#21490;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25345;&#32493;&#23398;&#20064;DSSMs&#65288;CLDSSMs&#65289;&#65292;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#20219;&#21153;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;CLDSSMs&#38598;&#25104;&#20102;&#20027;&#27969;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#65292;&#30830;&#20445;&#22312;&#23545;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#24314;&#27169;&#26102;&#39640;&#25928;&#26356;&#26032;&#65292;&#20445;&#25345;&#19981;&#21464;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#25105;&#20204;&#36824;&#23545;&#24212;&#29992;&#20110;&#21508;&#33258;CLDSSMs&#30340;&#27599;&#31181;CL&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25104;&#26412;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#26469;&#23637;&#31034;CLDSSMs&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#35777;&#23454;&#65292;&#34429;&#28982;&#21508;&#31181;&#31454;&#20105;&#30340;CL&#26041;&#27861;&#20855;&#26377;&#19981;&#21516;&#30340;&#20248;&#28857;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;CLDSSMs&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10123v1 Announce Type: new  Abstract: Deep state-space models (DSSMs) have gained popularity in recent years due to their potent modeling capacity for dynamic systems. However, existing DSSM works are limited to single-task modeling, which requires retraining with historical task data upon revisiting a forepassed task. To address this limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of adapting to evolving tasks without catastrophic forgetting. Our proposed CLDSSMs integrate mainstream regularization-based continual learning (CL) methods, ensuring efficient updates with constant computational and memory costs for modeling multiple dynamic systems. We also conduct a comprehensive cost analysis of each CL method applied to the respective CLDSSMs, and demonstrate the efficacy of CLDSSMs through experiments on real-world datasets. The results corroborate that while various competing CL methods exhibit different merits, the proposed CLDSSMs consistentl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08477</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#25554;&#20540;&#19987;&#23478;&#37322;&#25918;&#20803;&#35843;&#25972;&#30340;&#21147;&#37327;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26234;&#24935;&#24314;&#35758;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#26159;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#35832;&#22914;&#20803;&#23398;&#20064;&#20043;&#31867;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#21033;&#30410;&#65292;&#20803;&#35843;&#25972;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38543;&#21518;&#20248;&#21270;&#38454;&#27573;&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#29616;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#20851;&#38190;&#22320;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#30340; Sparse MetA-Tuning&#65288;SMAT&#65289;&#26041;&#27861;&#65292;&#23427;&#32463;&#36807;&#35757;&#32451;&#20197;&#33258;&#21160;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#38548;&#31163;&#39044;&#35757;&#32451;&#21442;&#25968;&#23376;&#38598;&#20197;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;SMAT&#25104;&#21151;&#20811;&#26381;&#20102;OOD&#25935;&#24863;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#25215;&#35834;&#12290;&#25105;&#20204;&#22312;Meta-Dataset&#19982;&#39069;&#22806;&#30340;OO&#25361;&#25112;&#32452;&#21512;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07362</link><description>&lt;p&gt;
&#25361;&#25112;&#36951;&#24536;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#20013;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;
&lt;/p&gt;
&lt;p&gt;
Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#36951;&#24536;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#65292;&#26469;&#22686;&#24378;&#23545;&#24433;&#21709;&#25830;&#38500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38752;&#35889;&#30340;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#31038;&#21306;&#36234;&#26469;&#36234;&#35748;&#35782;&#21040;&#27169;&#22411;&#22312;&#35757;&#32451;&#21518;&#26377;&#36873;&#25321;&#24615;&#22320;&#8220;&#36951;&#24536;&#8221;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#24341;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;(Machine Unlearning, MU)&#38382;&#39064;&#65292;&#26088;&#22312;&#28040;&#38500;&#36873;&#23450;&#25968;&#25454;&#28857;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#27169;&#22411;&#22312;&#36951;&#24536;&#21518;&#30340;&#23454;&#29992;&#24615;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;MU&#26041;&#27861;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#65292;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#38543;&#26426;&#25968;&#25454;&#36951;&#24536;&#19978;&#65292;&#24573;&#35270;&#20102;&#23545;&#20110;&#30495;&#23454;&#34913;&#37327;&#36951;&#24536;&#24615;&#33021;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#37325;&#35201;&#25506;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MU&#35780;&#20272;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20986;&#30830;&#23450;&#37027;&#20123;&#23545;&#24433;&#21709;&#25830;&#38500;&#26500;&#25104;&#26368;&#22823;&#25361;&#25112;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#21363;&#25214;&#20986;&#26368;&#22351;&#24773;&#20917;&#36951;&#24536;&#38598;&#12290;&#21033;&#29992;&#21452;&#23618;&#20248;&#21270;&#21407;&#21017;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#22312;&#19978;&#23618;&#20248;&#21270;&#20013;&#30340;&#36951;&#24536;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07362v1 Announce Type: cross  Abstract: The trustworthy machine learning (ML) community is increasingly recognizing the crucial need for models capable of selectively 'unlearning' data points after training. This leads to the problem of machine unlearning (MU), aiming to eliminate the influence of chosen data points on model performance, while still maintaining the model's utility post-unlearning. Despite various MU methods for data influence erasure, evaluations have largely focused on random data forgetting, ignoring the vital inquiry into which subset should be chosen to truly gauge the authenticity of unlearning performance. To tackle this issue, we introduce a new evaluative angle for MU from an adversarial viewpoint. We propose identifying the data subset that presents the most significant challenge for influence erasure, i.e., pinpointing the worst-case forget set. Utilizing a bi-level optimization principle, we amplify unlearning challenges at the upper optimization 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31354;&#38388;CO2&#27987;&#24230;&#20998;&#24067;&#25552;&#20986;&#20004;&#31181;&#26032;&#29305;&#24449;&#65292;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#26368;&#22810;&#21487;&#25552;&#39640;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.06643</link><description>&lt;p&gt;
CO2&#22312;&#33258;&#28982;&#36890;&#39118;&#23398;&#26657;&#24314;&#31569;&#29289;&#20013;&#29992;&#20110;&#21344;&#29992;&#26816;&#27979;&#30340;&#31354;&#38388;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spatial features of CO2 for occupancy detection in a naturally ventilated school building
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06643
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;CO2&#27987;&#24230;&#20998;&#24067;&#25552;&#20986;&#20004;&#31181;&#26032;&#29305;&#24449;&#65292;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#36827;&#34892;&#37327;&#21270;&#20998;&#26512;&#21518;&#21457;&#29616;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#26368;&#22810;&#21487;&#25552;&#39640;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#21344;&#29992;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#24314;&#31569;&#33021;&#28304;&#25928;&#29575;&#21644;&#23621;&#20303;&#32773;&#33298;&#36866;&#24230;&#12290;&#22522;&#20110;CO2&#20256;&#24863;&#22120;&#30340;&#21344;&#29992;&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#25104;&#26412;&#20302;&#12289;&#24178;&#25200;&#23567;&#65292;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#33258;&#28982;&#36890;&#39118;&#24314;&#31569;&#20013;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#36890;&#39118;&#34892;&#20026;&#21644;&#27979;&#37327;&#31383;&#25143;&#23454;&#38469;&#25442;&#27668;&#37327;&#30340;&#22256;&#38590;&#65292;&#22522;&#20110;CO2&#30340;&#21344;&#29992;&#26816;&#27979;&#20934;&#30830;&#24615;&#36890;&#24120;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;CO2&#27987;&#24230;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#20004;&#31181;&#26032;&#39062;&#29305;&#24449;&#29992;&#20110;&#21344;&#29992;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#37327;&#21270;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;&#22312;&#27809;&#26377;&#20219;&#20309;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#28982;&#36890;&#39118;&#25151;&#38388;&#20013;&#30340;&#21344;&#29992;&#29366;&#24577;&#26816;&#27979;&#20934;&#30830;&#29575;&#21487;&#25552;&#39640;&#26368;&#22810;14.8&#20010;&#30334;&#20998;&#28857;&#65292;&#36798;&#21040;83.2&#65285;&#65288;F1&#20998;&#25968;0.84&#65289;&#12290;&#26377;&#36890;&#39118;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;87.6&#65285;&#65288;F1&#20998;&#25968;0. &#65294;84&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06643v1 Announce Type: new  Abstract: Accurate occupancy information helps to improve building energy efficiency and occupant comfort. Occupancy detection methods based on CO2 sensors have received attention due to their low cost and low intrusiveness. In naturally ventilated buildings, the accuracy of CO2-based occupancy detection is generally low in related studies due to the complex ventilation behavior and the difficulty in measuring the actual air exchange through windows. In this study, we present two novel features for occupancy detection based on the spatial distribution of the CO2 concentration. After a quantitative analysis with Support Vector Machine (SVM) as classifier, it was found that the accuracy of occupancy state detection in naturally ventilated rooms could be improved by up to 14.8 percentage points compared to the baseline, reaching 83.2 % (F1 score 0.84) without any ventilation information. With ventilation information, the accuracy reached 87.6 % (F1 s
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.01046</link><description>&lt;p&gt;
&#19968;&#20010;&#38236;&#23376;&#30340;&#24211;&#65306;&#20302;&#32500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#21453;&#23556;&#29305;&#24449;&#30340;&#20984;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01046
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#65292;&#20026;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#22312;1-D&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#31561;&#20215;&#20110;&#35299;&#20915;&#19968;&#20010;&#24102;&#26377;&#22266;&#23450;&#12289;&#26126;&#30830;&#23450;&#20041;&#30340;&#29305;&#24449;&#23383;&#20856;&#30697;&#38453;&#30340;&#20984;Lasso&#38382;&#39064;&#12290;&#20855;&#20307;&#30340;&#23383;&#20856;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#12290;&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#65292;&#28145;&#31364;&#30340;ReLU&#32593;&#32476;&#26368;&#22810;&#26377;4&#23618;&#65292;&#20197;&#21450;&#20855;&#26377;&#31526;&#21495;&#28608;&#27963;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#30697;&#24418;&#21644;&#26641;&#32593;&#32476;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;ReLU&#32593;&#32476;&#20013;&#65292;&#31532;&#22235;&#23618;&#21019;&#24314;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#20851;&#20110;&#33258;&#36523;&#30340;&#21453;&#23556;&#30340;&#29305;&#24449;&#12290;Lasso&#34920;&#31034;&#27861;&#25581;&#31034;&#20102;&#20840;&#23616;&#26368;&#20248;&#32593;&#32476;&#21644;&#35299;&#31354;&#38388;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.18839</link><description>&lt;p&gt;
&#25193;&#23637;&#27969;&#21305;&#37197;&#65306;&#20855;&#26377;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Flow Matching&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#38750;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#26159;&#29983;&#25104;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#24212;&#29992;&#20043;&#19968;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#33879;&#21517;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#20197;&#22522;&#20110;&#24341;&#23548;&#30340;&#26080;&#20998;&#31867;&#22120;&#26041;&#27861;&#20026;&#39318;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#24341;&#23548;&#30340;&#26041;&#27861;&#30340;&#29702;&#35770;&#19981;&#20165;&#35201;&#27714;&#29992;&#25143;&#24494;&#35843;&#8220;&#24341;&#23548;&#24378;&#24230;&#8221;&#65292;&#32780;&#19988;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#19981;&#19968;&#23450;&#23545;&#24212;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#26412;&#25991;&#22522;&#20110;&#27969;&#21305;&#37197;&#21457;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#29702;&#35770;&#65292;&#27969;&#21305;&#37197;&#26159;&#25193;&#25955;&#26041;&#27861;&#30340;&#24403;&#21069;&#24378;&#22823;&#31454;&#20105;&#32773;&#20043;&#19968;&#12290;&#21463;&#23558;&#27010;&#29575;&#36335;&#24452;&#35299;&#37322;&#20026;&#36335;&#24452;&#31354;&#38388;&#19978;&#30340;&#20998;&#24067;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27969;&#22522;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#29702;&#35770;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#36830;&#32493;&#24615;&#26041;&#31243;&#30340;&#25968;&#23398;&#26694;&#26550;&#32780;&#19981;&#26159;&#27969;&#21305;&#37197;&#20013;&#30340;&#36830;&#32493;&#24615;&#26041;&#31243;&#12290;&#36825;&#19968;&#29702;&#35770;&#33258;&#28982;&#22320;&#25512;&#23548;&#20986;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.16795</link><description>&lt;p&gt;
&#22914;&#26524;&#22312;&#19968;&#20010;&#20247;&#21253;&#25968;&#25454;&#26631;&#27880;&#31649;&#36947;&#20013;&#65292;GPT-4
&lt;/p&gt;
&lt;p&gt;
If in a Crowdsourced Data Annotation Pipeline, a GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102; GPT-4 &#21644; MTurk &#31649;&#36947;&#30340;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#23613;&#31649; MTurk &#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;&#20294; GPT-4 &#30340;&#20934;&#30830;&#29575;&#26356;&#39640;&#65292;&#24182;&#19988;&#32467;&#21512; GPT-4 &#21644;&#20247;&#21253;&#26631;&#31614;&#20351;&#29992;&#32858;&#21512;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;GPT-4&#22312;&#25968;&#25454;&#26631;&#27880;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22312;&#32447;&#20247;&#21253;&#24037;&#20316;&#32773;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#65288;MTurk&#65289;&#30340;&#24037;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22240;&#20559;&#31163;&#26631;&#20934;&#20247;&#21253;&#23454;&#36341;&#24182;&#24378;&#35843;&#20010;&#21035;&#24037;&#20316;&#32773;&#30340;&#34920;&#29616;&#32780;&#21463;&#21040;&#25209;&#35780;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#26631;&#27880;&#36807;&#31243;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;GPT-4&#21644;&#19968;&#20010;&#36947;&#24503;&#19988;&#25191;&#34892;&#33391;&#22909;&#30340;MTurk&#31649;&#36947;&#65292;&#20351;&#29992;415&#21517;&#24037;&#20316;&#32773;&#26631;&#27880;&#20102;&#26469;&#33258;200&#31687;&#23398;&#26415;&#25991;&#31456;&#30340;3,177&#20010;&#21477;&#27573;&#65292;&#20351;&#29992;&#20102;CODA-19&#26041;&#26696;&#12290;&#20004;&#20010;&#24037;&#20316;&#32773;&#30028;&#38754;&#20135;&#29983;&#20102;127,080&#20010;&#26631;&#31614;&#65292;&#28982;&#21518;&#36890;&#36807;&#20843;&#31181;&#26631;&#31614;&#32858;&#21512;&#31639;&#27861;&#25512;&#26029;&#20986;&#26368;&#32456;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#37319;&#29992;&#20102;&#26368;&#20339;&#23454;&#36341;&#65292;MTurk&#31649;&#36947;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#20026;81.5%&#65292;&#32780;GPT-4&#36798;&#21040;&#20102;83.6%&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#23558;GPT-4&#30340;&#26631;&#31614;&#19982;&#36890;&#36807;&#20808;&#36827;&#24037;&#20316;&#32773;&#30028;&#38754;&#25910;&#38598;&#30340;&#20247;&#21253;&#26631;&#31614;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#32858;&#21512;&#26102;&#65292;8&#31181;&#31639;&#27861;&#20013;&#26377;2&#31181;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16795v1 Announce Type: cross  Abstract: Recent studies indicated GPT-4 outperforms online crowd workers in data labeling accuracy, notably workers from Amazon Mechanical Turk (MTurk). However, these studies were criticized for deviating from standard crowdsourcing practices and emphasizing individual workers' performances over the whole data-annotation process. This paper compared GPT-4 and an ethical and well-executed MTurk pipeline, with 415 workers labeling 3,177 sentence segments from 200 scholarly articles using the CODA-19 scheme. Two worker interfaces yielded 127,080 labels, which were then used to infer the final labels through eight label-aggregation algorithms. Our evaluation showed that despite best practices, MTurk pipeline's highest accuracy was 81.5%, whereas GPT-4 achieved 83.6%. Interestingly, when combining GPT-4's labels with crowd labels collected via an advanced worker interface for aggregation, 2 out of the 8 algorithms achieved an even higher accuracy (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#25104;&#26412;&#20998;&#24067;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CABAI&#26041;&#27861;&#20197;&#23454;&#29616;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#19979;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#65292;&#24182;&#35774;&#35745;&#20102;$\mathsf{CTAS}$&#21644;CO&#20004;&#31181;&#31639;&#27861;&#26469;&#36924;&#36817;&#29702;&#35770;&#19979;&#38480;&#24182;&#20248;&#21270;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.16710</link><description>&lt;p&gt;
&#25104;&#26412;&#24847;&#35782;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cost Aware Best Arm Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#25104;&#26412;&#20998;&#24067;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;CABAI&#26041;&#27861;&#20197;&#23454;&#29616;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#19979;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#65292;&#24182;&#35774;&#35745;&#20102;$\mathsf{CTAS}$&#21644;CO&#20004;&#31181;&#31639;&#27861;&#26469;&#36924;&#36817;&#29702;&#35770;&#19979;&#38480;&#24182;&#20248;&#21270;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#21452;&#37325;&#23545;&#35937;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22870;&#21169;&#22806;&#65292;&#27599;&#20010;&#33218;&#36824;&#19982;&#25104;&#26412;&#20998;&#24067;&#30456;&#20851;&#32852;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#26368;&#23567;&#26399;&#26395;&#25104;&#26412;&#35782;&#21035;&#20986;&#26368;&#22823;&#22870;&#21169;&#33218;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25104;&#26412;&#24847;&#35782;&#26368;&#20339;&#33218;&#35782;&#21035;&#8221;&#65288;CABAI&#65289;&#65292;&#23427;&#25429;&#25417;&#20102;&#20135;&#21697;&#24320;&#21457;&#27969;&#31243;&#20013;&#27979;&#35797;&#21644;&#23454;&#26045;&#38454;&#27573;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#24182;&#27169;&#25311;&#20102;&#38454;&#27573;&#20043;&#38388;&#30340;&#30446;&#26631;&#36716;&#21464;&#65292;&#21363;&#27979;&#35797;&#30340;&#25104;&#26412;&#21644;&#23454;&#26045;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#39318;&#20808;&#20026;CABAI&#25512;&#23548;&#20102;&#19968;&#20010;&#29702;&#35770;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\mathsf{CTAS}$&#30340;&#31639;&#27861;&#26469;&#28176;&#36817;&#21305;&#37197;&#23427;&#12290;&#20026;&#20102;&#20943;&#23569;$\mathsf{CTAS}$&#30340;&#35745;&#31639;&#37327;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24179;&#26041;&#26681;&#35268;&#21017;&#30340;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;&#31216;&#20026;CO&#65292;&#22312;&#31616;&#21270;&#30340;&#21452;&#33218;&#27169;&#22411;&#20013;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16710v1 Announce Type: cross  Abstract: In this paper, we study a best arm identification problem with dual objects. In addition to the classic reward, each arm is associated with a cost distribution and the goal is to identify the largest reward arm using the minimum expected cost. We call it \emph{Cost Aware Best Arm Identification} (CABAI), which captures the separation of testing and implementation phases in product development pipelines and models the objective shift between phases, i.e., cost for testing and reward for implementation. We first derive an theoretic lower bound for CABAI and propose an algorithm called $\mathsf{CTAS}$ to match it asymptotically. To reduce the computation of $\mathsf{CTAS}$, we further propose a low-complexity algorithm called CO, based on a square-root rule, which proves optimal in simplified two-armed models and generalizes surprisingly well in numerical experiments. Our results show (i) ignoring the heterogeneous action cost results in 
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15993</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#23398;&#20064;&#24102;&#26377;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#23618;&#30340;S4&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning method for S4 with Diagonal State Space Layers using Balanced Truncation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15993
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#25968;&#25454;&#30340;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#30340;S4&#27169;&#22411;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#36807;&#31243;&#21644;&#20248;&#21270;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#26469;&#36229;&#36234;&#20256;&#32479;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#65288;S4&#65289;&#27169;&#22411;&#24182;&#19988;&#21152;&#20837;&#20102;&#23545;&#35282;&#29366;&#24577;&#31354;&#38388;&#65288;DSS&#65289;&#23618;&#65292;&#36825;&#31181;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36793;&#32536;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#38271;&#24207;&#21015;&#25968;&#25454;&#65292;&#21253;&#25324;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#21644;&#23454;&#26102;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24179;&#34913;&#25130;&#26029;&#25216;&#26415;&#65292;&#22312;&#25511;&#21046;&#29702;&#35770;&#20013;&#24456;&#24120;&#35265;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;DSS&#23618;&#20197;&#38477;&#20302;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;S4&#27169;&#22411;&#30340;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;Skew-HiPPo&#21021;&#22987;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#35757;&#32451;&#30340;&#24102;&#26377;DSS&#23618;&#30340;S4&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#21644;&#25928;&#29575;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#26174;&#31034;&#19968;&#20010;&#31215;&#26497;&#30340;&#30456;&#20851;&#24615;&#65306;&#21407;&#22987;&#27169;&#22411;&#20013;&#30340;&#26356;&#39640;&#20934;&#30830;&#24230;&#19968;&#33268;&#23548;&#33268;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15993v1 Announce Type: new  Abstract: We introduce a novel learning method for Structured State Space Sequence (S4) models incorporating Diagonal State Space (DSS) layers, tailored for processing long-sequence data in edge intelligence applications, including sensor data analysis and real-time analytics. This method utilizes the balanced truncation technique, prevalent in control theory, applied specifically to DSS layers to reduce computational costs during inference. By leveraging parameters from the reduced model, we refine the initialization process of S4 models, outperforming the widely used Skew-HiPPo initialization in terms of performance. Numerical experiments demonstrate that our trained S4 models with DSS layers surpass conventionally trained models in accuracy and efficiency metrics. Furthermore, our observations reveal a positive correlation: higher accuracy in the original model consistently leads to increased accuracy in models trained using our method, suggest
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.14601</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#25945;&#32946;&#20013;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bringing Generative AI to Adaptive Learning in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14601
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#30340;&#20132;&#21449;&#30740;&#31350;&#23558;&#23545;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#26684;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#28608;&#22686;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#36825;&#19968;&#27010;&#24565;&#22312;&#25945;&#32946;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25552;&#39640;&#23398;&#29983;&#23398;&#20064;&#25928;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#30340;&#20132;&#21449;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#19968;&#39046;&#22495;&#30340;&#22909;&#22788;&#12289;&#25361;&#25112;&#21644;&#28508;&#21147;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#32467;&#21512;&#23558;&#20026;&#25945;&#32946;&#20013;&#19979;&#19968;&#38454;&#27573;&#23398;&#20064;&#24418;&#24335;&#30340;&#21457;&#23637;&#20570;&#20986;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14601v1 Announce Type: cross  Abstract: The recent surge in generative AI technologies, such as large language models and diffusion models, have boosted the development of AI applications in various domains, including science, finance, and education. Concurrently, adaptive learning, a concept that has gained substantial interest in the educational sphere, has proven its efficacy in enhancing students' learning efficiency. In this position paper, we aim to shed light on the intersectional studies of these two methods, which combine generative AI with adaptive learning concepts. By presenting discussions about the benefits, challenges, and potentials in this field, we argue that this union will contribute significantly to the development of the next stage learning format in education.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12694</link><description>&lt;p&gt;
&#22797;&#20852;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#21487;&#23398;&#20064;&#20998;&#35299;&#19982;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12694
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#21487;&#23398;&#20064;&#20998;&#35299;&#31574;&#30053;&#21644;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#35201;&#27714;&#31934;&#30830;&#24314;&#27169;&#38169;&#32508;&#22797;&#26434;&#27169;&#24335;&#65292;&#21253;&#25324;&#36328;&#26102;&#38388;&#24207;&#21015;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21160;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#36235;&#21183;&#29305;&#24449;&#24102;&#26469;&#25361;&#25112;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22522;&#26412;&#30340;&#31227;&#21160;&#24179;&#22343;&#26680;&#21487;&#33021;&#38590;&#20197;&#22788;&#29702;&#29616;&#23454;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#32467;&#26500;&#21644;&#22797;&#26434;&#36235;&#21183;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20998;&#35299;&#31574;&#30053;&#65292;&#26356;&#21512;&#29702;&#22320;&#25429;&#25417;&#21160;&#24577;&#36235;&#21183;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#19987;&#38376;&#29992;&#20110;&#21516;&#26102;&#25429;&#25417;&#36328;&#31995;&#21015;&#20381;&#36182;&#20851;&#31995;&#21644;&#20869;&#37096;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#20854;&#20013;&#36890;&#36807;&#36890;&#36947;&#33258;&#27880;&#24847;&#21147;&#21644;&#33258;&#22238;&#24402;&#33258;&#27880;&#24847;&#21147;&#23454;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20843;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340; Leddam...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
&lt;/p&gt;</description></item><item><title>&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;</title><link>https://arxiv.org/abs/2402.11917</link><description>&lt;p&gt;
&#22312;&#31526;&#21495;&#21270;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#30340;&#26426;&#29702;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11917
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#30340;&#26426;&#29702;&#20998;&#26512;&#25581;&#31034;&#20854;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#19968;&#31995;&#21015;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#33021;&#21147;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#23454;&#38469;&#25512;&#29702;&#30340;&#32467;&#26524;&#65292;&#29616;&#26377;&#24037;&#20316;&#38598;&#20013;&#20110;&#24320;&#21457;&#22797;&#26434;&#30340;&#34892;&#20026;&#30740;&#31350;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#24182;&#26410;&#25552;&#20379;&#20851;&#20110;&#39537;&#21160;&#35266;&#23519;&#21040;&#30340;&#33021;&#21147;&#30340;&#20869;&#37096;&#26426;&#21046;&#30340;&#35265;&#35299;&#12290;&#20026;&#20102;&#25913;&#21892;&#25105;&#20204;&#23545;Transformer&#20869;&#37096;&#26426;&#21046;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19968;&#20010;&#22312;&#21512;&#25104;&#25512;&#29702;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;Transformer&#36827;&#34892;&#20840;&#38754;&#30340;&#26426;&#29702;&#20998;&#26512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29992;&#26469;&#35299;&#20915;&#20219;&#21153;&#30340;&#19968;&#32452;&#21487;&#35299;&#37322;&#26426;&#21046;&#65292;&#24182;&#21033;&#29992;&#30456;&#20851;&#21644;&#22240;&#26524;&#35777;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#22312;&#24182;&#34892;&#36816;&#34892;&#30340;&#26377;&#30028;&#28145;&#24230;&#24490;&#29615;&#26426;&#21046;&#65292;&#24182;&#23558;&#20013;&#38388;&#32467;&#26524;&#23384;&#20648;&#22312;&#36873;&#23450;&#30340;&#20196;&#29260;&#20301;&#32622;&#12290;&#25105;&#20204;&#39044;&#26399;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#35782;&#21035;&#30340;&#20027;&#39064;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights
&lt;/p&gt;</description></item><item><title>UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11838</link><description>&lt;p&gt;
UniST&#65306;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11838
&lt;/p&gt;
&lt;p&gt;
UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20132;&#36890;&#31649;&#29702;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#22478;&#24066;&#35268;&#21010;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#22478;&#24066;&#26102;&#31354;&#24314;&#27169;&#33853;&#21518;&#12290;&#29616;&#26377;&#30340;&#22478;&#24066;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#26102;&#31354;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;UniST&#12290;&#20511;&#37492;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;UniST&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21462;&#24471;&#25104;&#21151;&#65306;(i) &#23545;&#19981;&#21516;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#29305;&#24449;&#30340;&#28789;&#27963;&#24615;&#65292;(ii) &#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25513;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#26102;&#38388;&#20851;&#31995;&#65292;(iii) &#26102;&#31354;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11656</link><description>&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#29289;&#29702;&#23618;&#36890;&#20449;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Pre-Trained Language Model with Physical Layer Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11656
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#30340;&#23454;&#29992;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;VQ-VAE&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#65292;&#35774;&#22791;&#30452;&#25509;&#36890;&#36807;&#23884;&#20837;&#24335;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#35821;&#35328;&#27169;&#22411;&#65289;&#20132;&#25442;&#20449;&#24687;&#65292;&#38656;&#35201;&#24378;&#22823;&#12289;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#36890;&#20449;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#26694;&#26550;&#19982;&#29616;&#26377;&#26080;&#32447;&#31995;&#32479;&#38598;&#25104;&#24182;&#26377;&#25928;&#31649;&#29702;&#22122;&#22768;&#21644;&#27604;&#29305;&#35823;&#24046;&#37117;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35774;&#22791;&#38388;&#20154;&#24037;&#26234;&#33021;&#36890;&#20449;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#23618;&#36890;&#20449;&#21151;&#33021;&#65292;&#24182;&#36890;&#36807;&#38142;&#36335;&#32423;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#32467;&#21512;&#20449;&#36947;&#22122;&#22768;&#20197;&#22686;&#24378;&#38887;&#24615;&#65292;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#23454;&#29616;&#39640;&#25928;&#31283;&#20581;&#30340;&#36890;&#20449;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;-&#35299;&#30721;Transformer&#25552;&#21319;&#36890;&#29992;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#36890;&#20449;&#22330;&#26223;&#30340;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10208</link><description>&lt;p&gt;
&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
Recovering the Pre-Fine-Tuning Weights of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10208
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24674;&#22797;&#29983;&#25104;&#27169;&#22411;&#39044;&#24494;&#35843;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#20302;&#31209;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#24674;&#22797;&#20934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#28431;&#27934;&#25915;&#20987;&#22823;&#35268;&#27169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#65292;&#20027;&#27969;&#27169;&#24335;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;i) &#22312;&#22823;&#35268;&#27169;&#20294;&#19981;&#23433;&#20840;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;ii) &#36890;&#36807;&#24494;&#35843;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#36825;&#31181;&#20570;&#27861;&#34987;&#35748;&#20026;&#26159;&#23433;&#20840;&#30340;&#65292;&#22240;&#20026;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#19981;&#23433;&#20840;&#30340;&#39044;&#24494;&#35843;&#27169;&#22411;&#26435;&#37325;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#38169;&#35823;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35889;&#21453;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#20302;&#31209;&#65288;LoRA&#65289;&#24494;&#35843;&#27169;&#22411;&#24674;&#22797;&#39044;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#19982;&#20808;&#21069;&#35797;&#22270;&#24674;&#22797;&#39044;&#24494;&#35843;&#33021;&#21147;&#30340;&#25915;&#20987;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#24674;&#22797;&#31934;&#30830;&#30340;&#39044;&#24494;&#35843;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20010;&#26032;&#30340;&#23545;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#28431;&#27934;&#65292;&#20363;&#22914;&#20010;&#24615;&#21270;&#30340;&#31283;&#23450;&#25193;&#25955;&#21644;&#23545;&#40784;&#30340;Mistral&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10208v1 Announce Type: cross  Abstract: The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07025</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#19979;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of Graph Neural Networks in the Mean-field Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65306;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#35823;&#24046;&#30340;&#29616;&#26377;&#30028;&#38480;&#32570;&#20047;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#26159;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#19978;&#30028;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;$O(1/n)$&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36825;&#20123;&#19978;&#30028;&#20026;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#25105;&#20204;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#21644;&#24178;&#25200;&#21442;&#25968;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#30340;&#21487;&#38752;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05330</link><description>&lt;p&gt;
&#22312;&#26080;&#25311;&#26679;&#20284;&#25512;&#26029;&#30340;&#24178;&#25200;&#21442;&#25968;&#21644;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#30340;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#21644;&#24178;&#25200;&#21442;&#25968;&#19979;&#36827;&#34892;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#20998;&#31867;&#22120;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#22312;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#30340;&#21487;&#38752;&#20998;&#31867;&#21644;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#20043;&#38388;&#65292;&#26631;&#31614;&#21644;&#28508;&#22312;&#24178;&#25200;&#21442;&#25968;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#20197;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#23545;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#20010;&#31185;&#23398;&#25361;&#25112;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20998;&#24067;&#36716;&#31227;&#31216;&#20026;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227; (GLS)&#12290;&#30452;&#25509;&#20351;&#29992;&#35266;&#27979;&#25968;&#25454; $\mathbf{X}$ &#36827;&#34892;&#20998;&#31867;&#20250;&#23548;&#33268;&#39044;&#27979;&#32467;&#26524;&#20559;&#24046;&#21644;&#26631;&#31614; $Y$ &#30340;&#26080;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20998;&#31867;&#38382;&#39064;&#35270;&#20026;&#24102;&#26377;&#24178;&#25200;&#21442;&#25968;&#30340;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#26469;&#20811;&#26381;&#36825;&#20123;&#20559;&#24046;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#25972;&#20010;&#24178;&#25200;&#21442;&#25968;&#31354;&#38388;&#20013;&#20272;&#35745;&#20998;&#31867;&#22120;&#30340;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24615; (ROC)&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#22312; GLS &#19979;&#19981;&#21464;&#30340;&#25130;&#26029;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36171;&#20104;&#39044;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#39046;&#22495;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#36820;&#22238;&#26377;&#25928;&#30340;&#39044;&#27979;&#38598;&#21512;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#35780;&#20272;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23433;&#20840;&#26426;&#21046;&#22266;&#26377;&#26131;&#30862;&#24615;&#65292;&#21435;&#38500;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#20294;&#23545;&#25928;&#29992;&#24433;&#21709;&#19981;&#22823;&#65292;&#38656;&#35201;&#26356;&#24378;&#20581;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#23433;&#20840;&#26426;&#21046;&#26041;&#38754;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#26131;&#30862;&#24615;&#65292;&#36825;&#21487;&#20174;&#23427;&#20204;&#26131;&#21463;&#36234;&#29425;&#21644;&#21363;&#20351;&#26159;&#38750;&#24694;&#24847;&#24494;&#35843;&#20063;&#26131;&#21463;&#24433;&#21709;&#26469;&#35828;&#26126;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20462;&#21098;&#21644;&#20302;&#31209;&#20462;&#25913;&#25506;&#35752;&#20102;&#23433;&#20840;&#23545;&#40784;&#30340;&#26131;&#30862;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#23545;&#20110;&#23433;&#20840;&#38450;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#19988;&#22312;&#31070;&#32463;&#20803;&#21644;&#31209;&#32423;&#21035;&#19978;&#19982;&#25928;&#29992;&#30456;&#20851;&#30340;&#21306;&#22495;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30340;&#23396;&#31435;&#21306;&#22495;&#26159;&#31232;&#30095;&#30340;&#65292;&#32422;&#21344;&#21442;&#25968;&#32423;&#21035;&#30340;$3\%$&#21644;&#25490;&#21517;&#32423;&#21035;&#30340;$2.5\%$&#12290;&#21435;&#38500;&#36825;&#20123;&#21306;&#22495;&#20250;&#25439;&#23475;&#23433;&#20840;&#24615;&#65292;&#32780;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#19981;&#22823;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;&#35813;&#27169;&#22411;&#23433;&#20840;&#26426;&#21046;&#30340;&#22266;&#26377;&#26131;&#30862;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#38480;&#21046;&#23545;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#65292;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#20302;&#25104;&#26412;&#30340;&#24494;&#35843;&#25915;&#20987;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;LLMs&#20013;&#26356;&#24378;&#22823;&#30340;&#23433;&#20840;&#31574;&#30053;&#30340;&#32039;&#36843;&#24615;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\%$ at the parameter level and $2.5\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04924</link><description>&lt;p&gt;
&#20004;&#20010;&#20132;&#26131;&#19981;&#20250;&#22256;&#25200;&#65306;&#36890;&#36807;&#26500;&#36896;&#21512;&#29702;&#30340;&#26799;&#24230;&#21305;&#37197;&#26469;&#21387;&#32553;&#22270;&#34920;
&lt;/p&gt;
&lt;p&gt;
Two Trades is not Baffled: Condense Graph via Crafting Rational Gradient Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;CTRL&#65292;&#36890;&#36807;&#20248;&#21270;&#36215;&#28857;&#21644;&#31934;&#32454;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#21305;&#37197;&#26041;&#21521;&#23548;&#33268;&#30340;&#35757;&#32451;&#36712;&#36857;&#20559;&#24046;&#21644;&#32047;&#31215;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#22270;&#34920;&#19978;&#35757;&#32451;&#24050;&#32463;&#22312;&#22270;&#34920;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#20043;&#19968;&#65292;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23558;&#23436;&#25972;&#30340;&#22270;&#34920;&#21387;&#32553;&#25104;&#26356;&#31616;&#27905;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#21512;&#25104;&#38598;&#12290;&#23613;&#31649;&#20196;&#20154;&#40723;&#33310;&#65292;&#20294;&#36825;&#20123;&#31574;&#30053;&#20027;&#35201;&#24378;&#35843;&#26799;&#24230;&#30340;&#21305;&#37197;&#26041;&#21521;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#36712;&#36857;&#30340;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#36827;&#19968;&#27493;&#30001;&#21387;&#32553;&#21644;&#35780;&#20272;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#24322;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#32047;&#31215;&#35823;&#24046;&#65292;&#23545;&#21387;&#32553;&#22270;&#34920;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory&#65288;\textbf{CTRL}&#65289;&#30340;&#26032;&#22411;&#22270;&#34920;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#38598;&#29305;&#24449;&#20998;&#24067;&#30340;&#20248;&#21270;&#36215;&#28857;&#21644;&#19968;&#20010;&#26356;&#31934;&#32454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \textbf{C}raf\textbf{T}ing \textbf{R}ationa\textbf{L} trajectory (\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for 
&lt;/p&gt;</description></item><item><title>CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04858</link><description>&lt;p&gt;
CodeIt&#65306;&#20855;&#26377;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04858
&lt;/p&gt;
&lt;p&gt;
CodeIt&#26159;&#19968;&#31181;&#20855;&#22791;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#36890;&#24120;&#34987;&#35748;&#20026;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#25512;&#29702;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36890;&#29992;&#26234;&#33021;&#22522;&#20934;&#27979;&#35797;&#20363;&#22914;&#25277;&#35937;&#21644;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#19978;&#34920;&#29616;&#20173;&#28982;&#38750;&#24120;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ARC&#35270;&#20026;&#19968;&#20010;&#20197;&#32534;&#31243;&#31034;&#20363;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Code Iteration&#65288;CodeIt&#65289;&#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;1&#65289;&#31243;&#24207;&#25277;&#26679;&#21644;&#22238;&#39038;&#37325;&#26631;&#35760;&#20197;&#21450;2&#65289;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#32463;&#39564;&#22238;&#25918;&#20043;&#38388;&#36827;&#34892;&#36845;&#20195;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;episode&#30340;&#30446;&#26631;&#65288;&#21363;&#32473;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#31243;&#24207;&#36755;&#20986;&#65289;&#37325;&#26631;&#35760;&#20026;&#37319;&#26679;&#31243;&#24207;&#20135;&#29983;&#30340;&#23454;&#38469;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#31243;&#24207;&#21512;&#25104;&#20013;&#22870;&#21169;&#26497;&#24230;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#24212;&#29992;CodeIt&#20110;ARC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20248;&#20808;&#32423;&#22238;&#39038;&#37325;&#25918;&#12289;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#23454;&#29616;&#25104;&#21151;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#12290;CodeIt&#26159;&#31532;&#19968;&#20010;&#31070;&#32463;&#20803;-&#21512;&#25104;&#26426;&#21046;&#19968;&#20307;&#30340;&#33258;&#25105;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are increasingly solving tasks that are commonly believed to require human-level reasoning ability. However, these models still perform very poorly on benchmarks of general intelligence such as the Abstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a programming-by-examples problem, and introduce a novel and scalable method for language model self-improvement called Code Iteration (CodeIt). Our method iterates between 1) program sampling and hindsight relabeling, and 2) learning from prioritized experience replay. By relabeling the goal of an episode (i.e., the target program output given input) to the realized output produced by the sampled program, our method effectively deals with the extreme sparsity of rewards in program synthesis. Applying CodeIt to the ARC dataset, we demonstrate that prioritized hindsight replay, along with pre-training and data-augmentation, leads to successful inter-task generalization. CodeIt is the first neuro-sy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.03220</link><description>&lt;p&gt;
&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#22312;&#20004;&#23618;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#22909;&#22788;&#65306;&#25171;&#30772;&#20449;&#24687;&#21644;&#36339;&#36291;&#25351;&#25968;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Reusing Batches for Gradient Descent in Two-Layer Networks: Breaking the Curse of Information and Leap Exponents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#21333;&#27425;GD&#30456;&#27604;&#65292;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#23454;&#29616;&#32593;&#32476;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#30340;&#37325;&#21472;&#65292;&#23637;&#31034;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#24191;&#27867;&#20989;&#25968;&#31867;&#12290;&#36825;&#20123;&#32467;&#26524;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#22810;&#25351;&#25968;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#20851;&#27880;&#37325;&#22797;&#22810;&#27425;&#20351;&#29992;&#25209;&#27425;&#30340;&#22810;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#65292;&#24182;&#23637;&#31034;&#23427;&#19982;&#21333;&#27425;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#26174;&#33879;&#25913;&#21464;&#20102;&#23545;&#20110;&#21738;&#20123;&#20989;&#25968;&#26159;&#21487;&#23398;&#20064;&#30340;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#26377;&#38480;&#27493;&#38271;&#30340;&#22810;&#27425;GD&#33021;&#22815;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#30340;&#20449;&#24687;&#25351;&#25968;&#65288;Ben Arous&#31561;&#20154;&#65292;2021&#65289;&#21644;&#36339;&#36291;&#25351;&#25968;&#65288;Abbe&#31561;&#20154;&#65292;2023&#65289;&#25152;&#32473;&#20986;&#30340;&#26799;&#24230;&#27969;&#21644;&#21333;&#27425;GD&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;&#32593;&#32476;&#20165;&#38656;&#20004;&#20010;&#26102;&#38388;&#27493;&#39588;&#23601;&#33021;&#19982;&#30446;&#26631;&#23376;&#31354;&#38388;&#36798;&#25104;&#37325;&#21472;&#65292;&#21363;&#20351;&#20989;&#25968;&#19981;&#28385;&#36275;&#38454;&#26799;&#24615;&#36136;&#65288;Abbe&#31561;&#20154;&#65292;2021&#65289;&#12290;&#25105;&#20204;&#23545;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#26377;&#25928;&#23398;&#20064;&#30340;&#65288;&#24191;&#27867;&#30340;&#65289;&#20989;&#25968;&#31867;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#22522;&#20110;&#21160;&#21147;&#24179;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#21160;&#24577;&#30340;&#38381;&#24335;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the training dynamics of two-layer neural networks when learning multi-index target functions. We focus on multi-pass gradient descent (GD) that reuses the batches multiple times and show that it significantly changes the conclusion about which functions are learnable compared to single-pass gradient descent. In particular, multi-pass GD with finite stepsize is found to overcome the limitations of gradient flow and single-pass GD given by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et al., 2023) of the target function. We show that upon re-using batches, the network achieves in just two time steps an overlap with the target subspace even for functions not satisfying the staircase property (Abbe et al., 2021). We characterize the (broad) class of functions efficiently learned in finite time. The proof of our results is based on the analysis of the Dynamical Mean-Field Theory (DMFT). We further provide a closed-form description of the dynamica
&lt;/p&gt;</description></item><item><title>AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.02956</link><description>&lt;p&gt;
AdaTreeFormer: &#20174;&#19968;&#24352;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#36827;&#34892;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a Single High-Resolution Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02956
&lt;/p&gt;
&lt;p&gt;
AdaTreeFormer&#26159;&#19968;&#31181;&#20174;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#26641;&#26408;&#35745;&#25968;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#24352;&#33322;&#31354;&#25110;&#21355;&#26143;&#22270;&#20687;&#26469;&#20272;&#35745;&#21644;&#35745;&#25968;&#26641;&#26408;&#23494;&#24230;&#26159;&#25668;&#24433;&#27979;&#37327;&#21644;&#36965;&#24863;&#39046;&#22495;&#20013;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26862;&#26519;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19981;&#21516;&#22320;&#24418;&#19978;&#21508;&#31181;&#21508;&#26679;&#30340;&#26641;&#26408;&#31181;&#31867;&#20005;&#37325;&#38459;&#30861;&#20102;&#26641;&#26408;&#35745;&#25968;&#27169;&#22411;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#20174;&#20855;&#26377;&#36275;&#22815;&#26631;&#27880;&#26641;&#26408;&#30340;&#28304;&#39046;&#22495;&#23398;&#20064;&#24182;&#36866;&#24212;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#27880;&#26641;&#26408;&#30340;&#30446;&#26631;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;AdaTreeFormer&#65292;&#21253;&#21547;&#19968;&#20010;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#20998;&#23618;&#29305;&#24449;&#25552;&#21462;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20013;&#25552;&#21462;&#31283;&#20581;&#30340;&#29305;&#24449;&#12290;&#23427;&#36824;&#21253;&#25324;&#19977;&#20010;&#23376;&#32593;&#32476;&#65306;&#20004;&#20010;&#29992;&#20110;&#20998;&#21035;&#20174;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#25552;&#21462;&#33258;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#19968;&#20010;&#29992;&#20110;&#25552;&#21462;&#36328;&#39046;&#22495;&#27880;&#24847;&#21147;&#22270;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#36866;&#24212;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of estimating and counting tree density using only a single aerial or satellite image is a difficult task in the fields of photogrammetry and remote sensing. However, it plays a crucial role in the management of forests. The huge variety of trees in varied topography severely hinders tree counting models to perform well. The purpose of this paper is to propose a framework that is learnt from the source domain with sufficient labeled trees and is adapted to the target domain with only a limited number of labeled trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a hierarchical feature extraction scheme to extract robust features from the source and target domains. It also consists of three subnets: two for extracting self-domain attention maps from source and target domains respectively and one for extracting cross-domain attention maps. For the latter, an attention-to-adapt mechanism is introduced to distill relevant information from different doma
&lt;/p&gt;</description></item><item><title>Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02361</link><description>&lt;p&gt;
Pruner:&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02361
&lt;/p&gt;
&lt;p&gt;
Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65288;DLAs&#65289;&#19978;&#30340;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#23545;&#20110;&#26377;&#25928;&#30340;&#27169;&#22411;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#25628;&#32034;&#30340;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#65288;DLC&#65289;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30528;&#25628;&#32034;&#25928;&#29575;&#20302;&#21644;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#24046;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pruner&#65292;&#36981;&#24490;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21407;&#21017;&#26469;&#20998;&#23618;&#25552;&#21319;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#12290;Pruner&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#12290;&#21069;&#32773;&#20316;&#20026;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#21644;&#20844;&#24335;&#21270;&#30340;&#24615;&#33021;&#20998;&#26512;&#24037;&#20855;&#65292;&#24341;&#23548;&#25628;&#32034;&#31354;&#38388;&#30340;&#20462;&#21098;&#65292;&#32780;&#21518;&#32773;&#26681;&#25454;&#20851;&#38190;&#30340;&#25968;&#25454;&#27969;&#27169;&#24335;&#23454;&#29616;&#20102;&#23545;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#35777;&#26377;&#25928;&#30340;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17780</link><description>&lt;p&gt;
&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#32422;&#26463;MDPs&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Primal-Dual Algorithm for Constrained MDPs with Uniform PAC Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#22343;&#21248;PAC&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#20445;&#35777;&#20102;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#23545;&#20598;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#25506;&#32034;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#28385;&#36275;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22238;&#25253;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#29616;&#26377;&#30340;&#29702;&#35770;&#25991;&#29486;&#20165;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#20445;&#35777;&#65292;&#24182;&#26410;&#33021;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#22343;&#21248;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#24615;&#65288;Uniform-PAC&#65289;&#20445;&#35777;&#30340;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;&#23545;&#20598;&#31639;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12289;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#20197;&#23454;&#29616;&#20219;&#20309;&#30446;&#26631;&#31934;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#32447;CMDP&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;Uniform-PAC&#31639;&#27861;&#12290;&#38500;&#20102;&#29702;&#35770;&#20445;&#35777;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;CMDP&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#25391;&#33633;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a primal-dual reinforcement learning (RL) algorithm for the online constrained Markov decision processes (CMDP) problem, wherein the agent explores an optimal policy that maximizes return while satisfying constraints. Despite its widespread practical use, the existing theoretical literature on primal-dual RL algorithms for this problem only provides sublinear regret guarantees and fails to ensure convergence to optimal policies. In this paper, we introduce a novel policy gradient primal-dual algorithm with uniform probably approximate correctness (Uniform-PAC) guarantees, simultaneously ensuring convergence to optimal policies, sublinear regret, and polynomial sample complexity for any target accuracy. Notably, this represents the first Uniform-PAC algorithm for the online CMDP problem. In addition to the theoretical guarantees, we empirically demonstrate in a simple CMDP that our algorithm converges to optimal policies, while an existing algorithm exhibits oscillatory perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.12112</link><description>&lt;p&gt;
LLM&#31934;&#36873;&#65306;&#22312;&#36229;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CLLM&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#21644;&#25968;&#25454;&#31579;&#36873;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#34920;&#26684;&#22686;&#24378;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#21450;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#31579;&#36873;&#26426;&#21046;&#65292;CLLM&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#34987;&#20302;&#20272;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22686;&#21152;ML&#25152;&#38656;&#30340;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23545;&#20110;&#37322;&#25918;ML&#22312;&#25968;&#25454;&#21294;&#20047;&#30340;&#22320;&#21306;&#21644;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#38480;&#21046;&#20102;&#20256;&#32479;&#30340;&#34920;&#26684;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#22312;&#29983;&#25104;ML&#20219;&#21153;&#25152;&#38656;&#30340;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CLLM&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20687;&#20219;&#20309;&#29983;&#25104;&#27169;&#22411;&#19968;&#26679;&#65292;&#24182;&#38750;LLMs&#29983;&#25104;&#30340;&#25152;&#26377;&#25968;&#25454;&#37117;&#33021;&#25552;&#39640;&#19979;&#28216;&#30340;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#21160;&#24577;&#12289;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24230;&#25351;&#26631;&#30340;&#21407;&#21017;&#24615;&#31579;&#36873;&#26426;&#21046;&#65292;&#20197;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CLLM&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) in low-data settings remains an underappreciated yet crucial problem. Hence, data augmentation methods to increase the sample size of datasets needed for ML are key to unlocking the transformative potential of ML in data-deprived regions and domains. Unfortunately, the limited training set constrains traditional tabular synthetic data generators in their ability to generate a large and diverse augmented dataset needed for ML tasks. To address this challenge, we introduce CLLM, which leverages the prior knowledge of Large Language Models (LLMs) for data augmentation in the low-data regime. However, not all the data generated by LLMs will improve downstream utility, as for any generative model. Consequently, we introduce a principled curation mechanism, leveraging learning dynamics, coupled with confidence and uncertainty metrics, to obtain a high-quality dataset. Empirically, on multiple real-world datasets, we demonstrate the superior performance of CLLM in the lo
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;</title><link>https://arxiv.org/abs/2312.08489</link><description>&lt;p&gt;
&#39044;&#27979;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connectivity Oracles for Predictable Vertex Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08489
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#39044;&#27979;&#31639;&#27861;&#33539;&#24335;&#19979;&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#39044;&#22788;&#29702;&#26102;&#38388;&#21644;&#26597;&#35810;&#26102;&#38388;&#30340;&#22810;&#39033;&#24335;&#20851;&#31995;&#26469;&#22788;&#29702;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25903;&#25345;&#39030;&#28857;&#22833;&#36133;&#30340;&#36830;&#36890;&#24615;&#39044;&#27979;&#22120;&#26159;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#38382;&#39064;&#20043;&#19968;&#12290;&#24050;&#26377;&#30340;&#30740;&#31350;&#22312;&#26597;&#35810;&#26102;&#38388;&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#24456;&#22909;&#30340;&#29702;&#35299;&#65306;&#20197;&#21069;&#30340;&#20316;&#21697;[Duan-Pettie STOC'10; Long-Saranurak FOCS'22]&#23454;&#29616;&#20102;&#19982;&#22833;&#36133;&#39030;&#28857;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#26597;&#35810;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#39044;&#22788;&#29702;&#21644;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#26356;&#26032;&#30340;&#26465;&#20214;&#19979;&#26159;&#26377;&#26465;&#20214;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#22312;&#39044;&#27979;&#31639;&#27861;&#30340;&#33539;&#24335;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#38382;&#65292;&#22914;&#26524;&#21487;&#20197;&#39044;&#27979;&#21040;&#22833;&#36133;&#39030;&#28857;&#38598;&#21512;&#65292;&#26597;&#35810;&#26102;&#38388;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#32467;&#26500;&#65292;&#32473;&#23450;&#19968;&#20010;&#22270;G=(V,E)&#21644;&#19968;&#20010;&#39044;&#27979;&#20250;&#22833;&#36133;&#30340;&#39030;&#28857;&#38598;&#21512;\widehat{D} \subseteq V&#65288;&#20854;&#20013;d=|\widehat{D}|&#65289;&#65292;&#23558;&#20854;&#39044;&#22788;&#29702;&#26102;&#38388;&#20026;$\tilde{O}(d|E|)$&#65292;&#28982;&#21518;&#21487;&#20197;&#25509;&#25910;&#19968;&#20010;&#26356;&#26032;&#65292;&#35813;&#26356;&#26032;&#20197;&#23545;&#31216;&#24046;&#20998;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08489v2 Announce Type: replace-cross  Abstract: The problem of designing connectivity oracles supporting vertex failures is one of the basic data structures problems for undirected graphs. It is already well understood: previous works [Duan--Pettie STOC'10; Long--Saranurak FOCS'22] achieve query time linear in the number of failed vertices, and it is conditionally optimal as long as we require preprocessing time polynomial in the size of the graph and update time polynomial in the number of failed vertices.   We revisit this problem in the paradigm of algorithms with predictions: we ask if the query time can be improved if the set of failed vertices can be predicted beforehand up to a small number of errors. More specifically, we design a data structure that, given a graph $G=(V,E)$ and a set of vertices predicted to fail $\widehat{D} \subseteq V$ of size $d=|\widehat{D}|$, preprocesses it in time $\tilde{O}(d|E|)$ and then can receive an update given as the symmetric differ
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.08364</link><description>&lt;p&gt;
Plum: &#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Plum: Prompt Learning using Metaheuristic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#20803;&#21551;&#21457;&#24335;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#20845;&#31181;&#20856;&#22411;&#30340;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20986;&#29616;&#20197;&#26469;&#65292;&#25552;&#31034;&#23398;&#20064;&#24050;&#25104;&#20026;&#20248;&#21270;&#21644;&#23450;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#29305;&#27530;&#25552;&#31034;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#29978;&#33267;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#20808;&#21069;&#26410;&#30693;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#26377;&#25928;&#25552;&#31034;&#30340;&#36827;&#23637;&#32531;&#24930;&#65292;&#20419;&#20351;&#20154;&#20204;&#28212;&#26395;&#19968;&#31181;&#36890;&#29992;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#20013;&#24456;&#23569;&#26377;&#28385;&#36275;&#8220;&#36890;&#29992;&#8221;&#30340;&#26631;&#20934;&#65292;&#21363;&#21516;&#26102;&#20855;&#22791;&#33258;&#21160;&#12289;&#31163;&#25955;&#12289;&#40657;&#30418;&#12289;&#26080;&#26799;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20803;&#21551;&#21457;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#31163;&#25955;&#38750;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#25903;&#65292;&#25317;&#26377;100&#22810;&#31181;&#36873;&#39033;&#12290;&#22312;&#25105;&#20204;&#30340;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#20845;&#31181;&#20856;&#22411;&#26041;&#27861;&#65306;&#29228;&#23665;&#12289;&#27169;&#25311;&#36864;&#28779;&#12289;&#36951;&#20256;&#31639;&#27861;&#65288;&#24102;/&#19981;&#24102;&#20132;&#21449;&#65289;&#12289;&#31105;&#24524;&#25628;&#32034;&#21644;&#21644;&#35856;&#25628;&#32034;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#30333;&#30418;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.07548</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Interpretable Fine-Tuning for Graph Neural Network Surrogate Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;GNNs&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#21450;&#20854;&#23545;&#24212;&#30340;&#23376;&#22270;&#65292;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#29289;&#29702;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#24314;&#27169;&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20986;&#29616;&#22312;&#26368;&#36817;&#20960;&#24180;&#20869;&#34028;&#21187;&#21457;&#23637;&#65292;GNNs&#21487;&#20197;&#30452;&#25509;&#22312;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#34920;&#31034;&#19978;&#36816;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#20026;GNN&#24341;&#20837;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#38750;&#32467;&#26500;&#32593;&#26684;&#21270;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#12290;&#26368;&#32456;&#32467;&#26524;&#26159;&#19968;&#20010;&#22686;&#24378;&#30340;&#24494;&#35843;&#27169;&#22411;&#65292;&#23427;&#38548;&#31163;&#20102;&#19982;&#39044;&#27979;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#30340;&#29289;&#29702;&#31354;&#38388;&#21306;&#22495;&#65292;&#30456;&#24212;&#20110;&#23376;&#22270;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22522;&#32447;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#20123;&#30001;&#24494;&#35843;&#30340;GNN&#35782;&#21035;&#20986;&#30340;&#32467;&#26500;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#26159;&#33258;&#36866;&#24212;&#29983;&#25104;&#30340;&#65292;&#24182;&#20316;&#20026;&#21487;&#35299;&#37322;&#30340;&#38142;&#25509;&#23384;&#22312;&#20110;&#22522;&#32447;&#27169;&#22411;&#26550;&#26500;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#24050;&#30693;&#38382;&#39064;&#29305;&#23450;&#29289;&#29702;&#20043;&#38388;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31243;&#24207;&#65292;&#24494;&#35843;&#30340;GNNs&#36824;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#29992;&#20110;&#35782;&#21035;&#23545;&#24212;&#30340;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;&#26426;&#21046;&#65292;&#20026;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#32463;&#27982;&#28608;&#21169;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2310.14992</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;
&lt;/p&gt;
&lt;p&gt;
Bayesian Regression Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#22238;&#24402;&#24066;&#22330;&#26426;&#21046;&#65292;&#20026;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#32463;&#27982;&#28608;&#21169;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#30340;&#36130;&#21153;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#23545;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#36136;&#37327;&#24456;&#25935;&#24863;&#12290;&#28982;&#32780;&#65292;&#20844;&#21496;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#36275;&#22815;&#30340;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#36825;&#20123;&#25968;&#25454;&#33258;&#28982;&#20998;&#24067;&#22312;&#21508;&#20010;&#25152;&#26377;&#32773;&#20043;&#38388;&#65292;&#32780;&#36825;&#20123;&#25152;&#26377;&#32773;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#31454;&#20105;&#23545;&#25163;&#65292;&#19981;&#24895;&#24847;&#20849;&#20139;&#20449;&#24687;&#12290;&#25105;&#20204;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#22238;&#24402;&#24066;&#22330;&#65292;&#20197;&#25552;&#20379;&#25968;&#25454;&#20849;&#20139;&#30340;&#32463;&#27982;&#28608;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26426;&#21046;&#37319;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#32771;&#34385;&#26356;&#19968;&#33324;&#30340;&#22238;&#24402;&#20219;&#21153;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;&#24066;&#22330;&#23646;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#25506;&#35752;&#65292;&#24182;&#23637;&#31034;&#20102;&#30446;&#21069;&#25991;&#29486;&#20013;&#31867;&#20284;&#25552;&#35758;&#26292;&#38706;&#24066;&#22330;&#20195;&#29702;&#21830;&#38754;&#20020;&#21487;&#35266;&#30340;&#36130;&#21153;&#39118;&#38505;&#65292;&#32780;&#36825;&#20123;&#39118;&#38505;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#21487;&#20197;&#24471;&#21040;&#32531;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14992v2 Announce Type: replace  Abstract: Machine learning tasks are vulnerable to the quality of data used as input. Yet, it is often challenging for firms to obtain adequate datasets, with them being naturally distributed amongst owners, that in practice, may be competitors in a downstream market and reluctant to share information. Focusing on supervised learning for regression tasks, we develop a regression market to provide a monetary incentive for data sharing. Our proposed mechanism adopts a Bayesian framework, allowing us to consider a more general class of regression tasks. We present a thorough exploration of the market properties, and show that similar proposals in current literature expose the market agents to sizeable financial risks, which can be mitigated in our setup.
&lt;/p&gt;</description></item><item><title>&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.05196</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#26159;&#21542;&#20250;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Writing with Language Models Reduce Content Diversity?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05196
&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#26102;&#20351;&#29992;InstructGPT&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#26174;&#33879;&#38477;&#20302;&#20869;&#23481;&#22810;&#26679;&#24615;&#65292;&#22686;&#21152;&#19981;&#21516;&#20316;&#32773;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20943;&#23569;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#19982;&#27169;&#22411;&#36741;&#21161;&#21512;&#20316;&#20889;&#20316;&#30340;&#28608;&#22686;&#12290;&#24403;&#19981;&#21516;&#29992;&#25143;&#32435;&#20837;&#21516;&#19968;&#27169;&#22411;&#30340;&#24314;&#35758;&#26102;&#65292;&#20250;&#23384;&#22312;&#20869;&#23481;&#22810;&#26679;&#24615;&#20943;&#23569;&#30340;&#39118;&#38505;&#65292;&#21487;&#33021;&#38480;&#21046;&#20844;&#20849;&#35805;&#35821;&#20013;&#30340;&#22810;&#20803;&#35266;&#28857;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#27979;&#37327;&#20102;&#21327;&#21516;&#20889;&#20316;&#23545;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#22312;&#35813;&#23454;&#39564;&#20013;&#65292;&#29992;&#25143;&#20197;&#19977;&#31181;&#35774;&#32622;&#25776;&#20889;&#35758;&#35770;&#24615;&#25991;&#31456;--&#20351;&#29992;&#22522;&#26412;LLM&#65288;GPT3&#65289;&#12289;&#32463;&#36807;&#21453;&#39304;&#35843;&#25972;&#30340;LLM&#65288;InstructGPT&#65289;&#20197;&#21450;&#19981;&#20351;&#29992;&#27169;&#22411;&#24110;&#21161;&#20889;&#20316;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#25351;&#26631;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;InstructGPT&#36827;&#34892;&#20889;&#20316;&#65288;&#32780;&#19981;&#26159;GPT3&#65289;&#20250;&#23548;&#33268;&#22810;&#26679;&#24615;&#26126;&#26174;&#38477;&#20302;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#22686;&#21152;&#20102;&#19981;&#21516;&#20316;&#32773;&#30340;&#20889;&#20316;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20943;&#23569;&#20102;&#25972;&#20307;&#30340;&#35789;&#27719;&#21644;&#20869;&#23481;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#31181;&#24433;&#21709;&#20027;&#35201;&#26469;&#28304;&#20110;InstructGPT&#23545;&#20849;&#21516;&#25776;&#20889;&#30340;&#25991;&#26412;&#36129;&#29486;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05196v2 Announce Type: replace  Abstract: Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-writt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.05248</link><description>&lt;p&gt;
&#21152;&#36895;&#31639;&#27861;&#29992;&#20110;&#32422;&#26463;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max Optimization and Comonotone Inclusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#30340;&#21152;&#36895;&#31639;&#27861;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#65292;&#19968;&#31867;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;-&#38750;&#20985;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#20197;&#21450;&#23427;&#20204;&#23545;&#20849;&#21333;&#35843;&#21253;&#21547;&#30340;&#25512;&#24191;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#26368;&#21021;&#30001;Yoon&#21644;Ryu&#65288;2021&#65289;&#25552;&#20986;&#30340;&#26080;&#32422;&#26463;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#30340;Extra Anchored Gradient&#65288;EAG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#25152;&#26377;&#19968;&#38454;&#26041;&#27861;&#20013;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;$O\left(\frac{1}{T}\right)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#36845;&#20195;&#25910;&#25947;&#21040;&#35299;&#38598;&#20013;&#30340;&#19968;&#20010;&#28857;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#20013;&#65292;&#25105;&#20204;&#23558;&#30001;Lee&#21644;Kim&#65288;2021&#65289;&#24320;&#21457;&#30340;&#24555;&#36895;&#39069;&#22806;&#26799;&#24230;&#65288;FEG&#65289;&#31639;&#27861;&#25193;&#23637;&#21040;&#32422;&#26463;&#20849;&#21333;&#35843;&#26497;&#23567;-&#26497;&#22823;&#20248;&#21270;&#21644;&#20849;&#21333;&#35843;&#21253;&#21547;&#65292;&#24182;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;$O\left(\frac{1}{T}\right)$&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#20010;&#36895;&#29575;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#30740;&#31350;&#36807;&#30340;&#26368;&#24191;&#27867;&#30340;&#20849;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#38598;&#21512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;s&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study constrained comonotone min-max optimization, a structured class of nonconvex-nonconcave min-max optimization problems, and their generalization to comonotone inclusion. In our first contribution, we extend the Extra Anchored Gradient (EAG) algorithm, originally proposed by Yoon and Ryu (2021) for unconstrained min-max optimization, to constrained comonotone min-max optimization and comonotone inclusion, achieving an optimal convergence rate of $O\left(\frac{1}{T}\right)$ among all first-order methods. Additionally, we prove that the algorithm's iterations converge to a point in the solution set. In our second contribution, we extend the Fast Extra Gradient (FEG) algorithm, as developed by Lee and Kim (2021), to constrained comonotone min-max optimization and comonotone inclusion, achieving the same $O\left(\frac{1}{T}\right)$ convergence rate. This rate is applicable to the broadest set of comonotone inclusion problems yet studied in the literature. Our analyses are based on s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.16594</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;: &#23439;&#35266;at-$k$&#24230;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Consistent algorithms for multi-label classification with macro-at-$k$ metrics. (arXiv:2401.16594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23558;&#24230;&#37327;&#32447;&#24615;&#20998;&#35299;&#20026;&#27599;&#20010;&#26631;&#31614;&#20998;&#21035;&#24212;&#29992;&#30340;&#20108;&#20998;&#31867;&#25928;&#29992;&#30340;&#24635;&#21644;&#65292;&#24182;&#23545;&#27599;&#20010;&#23454;&#20363;&#39044;&#27979;&#24688;&#22909;&#26377;$k$&#20010;&#26631;&#31614;&#30340;&#39069;&#22806;&#35201;&#27714;&#12290;&#8220;&#23439;&#35266;at-$k$&#8221;&#24230;&#37327;&#22312;&#20855;&#26377;&#38271;&#23614;&#26631;&#31614;&#30340;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;at-$k$&#32422;&#26463;&#23558;&#21407;&#26412;&#29420;&#31435;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#27604;&#26631;&#20934;&#23439;&#24179;&#22343;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#21644;&#24418;&#24335;&#65292;&#24182;&#22522;&#20110;Frank-Wolfe&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#19968;&#33268;&#19988;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#36824;&#28041;&#21450;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26356;&#19968;&#33324;&#24230;&#37327;&#65292;&#36825;&#20123;&#20989;&#25968;&#26159;&#25353;&#26631;&#31614;&#36827;&#34892;&#30340;&#28151;&#28102;&#30697;&#38453;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16251</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#22686;&#24378;&#30340;&#32852;&#21512;&#23398;&#20064;&#25104;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#26696;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#35760;&#24405;&#30340;&#38544;&#31169;&#39044;&#31639;&#22343;&#30456;&#21516;&#65292;&#25552;&#20379;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#35760;&#24405;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#27599;&#20010;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#26410;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#28151;&#21512;&#25277;&#26679;&#26041;&#26696;&#65292;&#26082;&#21253;&#25324;&#23458;&#25143;&#31471;&#32423;&#21035;&#25277;&#26679;&#65292;&#21448;&#21253;&#25324;&#38750;&#22343;&#21248;&#35760;&#24405;&#32423;&#21035;&#25277;&#26679;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#19968;&#20010;&#20851;&#38190;&#19988;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#20010;&#24615;&#21270;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#29702;&#24819;&#30340;&#27599;&#35760;&#24405;&#25277;&#26679;&#27010;&#29575;q&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13200</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22238;&#25918;&#30340;&#25216;&#26415;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#26159;&#30452;&#25509;&#24212;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#23558;&#20869;&#23384;&#31354;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(nd^L)$&#38477;&#20302;&#21040;$\mathcal{O}(n)$&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#25299;&#25169;&#20449;&#24687;&#36827;&#34892;&#35760;&#24518;&#22238;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$ to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \textit{Topology-aware Embeddings} 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.13054</link><description>&lt;p&gt;
&#26080;&#35745;&#31639;&#22256;&#38590;&#30340;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#36317;&#31163;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Frustrated Random Walks: A Fast Method to Compute Node Distances on Hypergraphs. (arXiv:2401.13054v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#36229;&#22270;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#24182;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#36229;&#22270;&#20013;&#33410;&#28857;&#36317;&#31163;&#35745;&#31639;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#25299;&#23637;&#20102;&#36229;&#22270;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#24403;&#32771;&#34385;&#23454;&#20307;&#38388;&#30340;&#23646;&#24615;&#20849;&#20139;&#26102;&#20250;&#33258;&#28982;&#20135;&#29983;&#12290;&#23613;&#31649;&#21487;&#20197;&#36890;&#36807;&#23558;&#36229;&#36793;&#25193;&#23637;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23376;&#22270;&#26469;&#23558;&#36229;&#22270;&#36716;&#25442;&#20026;&#22270;&#65292;&#20294;&#36870;&#21521;&#25805;&#20316;&#22312;&#35745;&#31639;&#19978;&#38750;&#24120;&#22797;&#26434;&#19988;&#23646;&#20110;NP-complete&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#36229;&#22270;&#21253;&#21547;&#27604;&#22270;&#26356;&#22810;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#25805;&#20316;&#36229;&#22270;&#27604;&#23558;&#20854;&#25193;&#23637;&#20026;&#22270;&#26356;&#20026;&#26041;&#20415;&#12290;&#36229;&#22270;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#26159;&#22914;&#20309;&#31934;&#30830;&#39640;&#25928;&#22320;&#35745;&#31639;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#20272;&#35745;&#33410;&#28857;&#36317;&#31163;&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#33410;&#28857;&#30340;&#26368;&#36817;&#37051;&#23621;&#65292;&#24182;&#20351;&#29992;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#26041;&#27861;&#22312;&#36229;&#22270;&#19978;&#25191;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#26631;&#31614;&#20256;&#25773;&#12290;&#25105;&#20204;&#23558;&#33410;&#28857;&#36317;&#31163;&#20272;&#35745;&#20026;&#38543;&#26426;&#28216;&#36208;&#30340;&#39044;&#26399;&#21040;&#36798;&#26102;&#38388;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#31616;&#21333;&#38543;&#26426;&#28216;&#36208;&#65288;SRW&#65289;&#26080;&#27861;&#20934;&#30830;&#25551;&#36848;&#33410;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;"frustrated"&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hypergraph is a generalization of a graph that arises naturally when attribute-sharing among entities is considered. Although a hypergraph can be converted into a graph by expanding its hyperedges into fully connected subgraphs, going the reverse way is computationally complex and NP-complete. We therefore hypothesize that a hypergraph contains more information than a graph. In addition, it is more convenient to manipulate a hypergraph directly, rather than expand it into a graph. An open problem in hypergraphs is how to accurately and efficiently calculate their node distances. Estimating node distances enables us to find a node's nearest neighbors, and perform label propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this paper, we propose a novel approach based on random walks to achieve label propagation on hypergraphs. We estimate node distances as the expected hitting times of random walks. We note that simple random walks (SRW) cannot accurately describe 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.09870</link><description>&lt;p&gt;
&#35843;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#25277;&#35937;&#21270;&#20197;&#23454;&#29616;&#30446;&#26631;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#29702;&#35770;&#36951;&#25022;&#36793;&#30028;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#34920;&#31034;&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#23398;&#20064;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23481;&#26131;&#30340;&#23376;&#20219;&#21153;&#26469;&#24433;&#21709;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20445;&#30041;&#26102;&#38388;&#25277;&#35937;&#29615;&#22659;&#21160;&#24577;&#30340;&#34920;&#31034;&#26041;&#27861;&#22312;&#35299;&#20915;&#22256;&#38590;&#38382;&#39064;&#21644;&#25552;&#20379;&#20248;&#21270;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#26159;&#25104;&#21151;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29615;&#22659;&#21160;&#24577;&#36234;&#26469;&#36234;&#22797;&#26434;&#65288;&#21363;&#26102;&#38388;&#25277;&#35937;&#36716;&#25442;&#20851;&#31995;&#20381;&#36182;&#26356;&#22810;&#21464;&#37327;&#65289;&#30340;&#20219;&#21153;&#20013;&#26080;&#27861;&#25193;&#23637;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20854;&#20182;&#26041;&#27861;&#21017;&#23581;&#35797;&#20351;&#29992;&#31354;&#38388;&#25277;&#35937;&#26469;&#32531;&#35299;&#21069;&#38754;&#30340;&#38382;&#39064;&#12290;&#23427;&#20204;&#30340;&#38480;&#21046;&#21253;&#25324;&#26080;&#27861;&#36866;&#24212;&#39640;&#32500;&#29615;&#22659;&#21644;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#20381;&#36182;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#23618;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#23618;&#32467;&#26500;&#30340;&#19981;&#21516;&#23618;&#27425;&#24341;&#20837;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#30446;&#26631;&#25277;&#35937;&#21270;&#12290;&#25105;&#20204;&#23545;&#23398;&#20064;&#31574;&#30053;&#30340;&#36951;&#25022;&#36793;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal representation affects the performance of Hierarchical Reinforcement Learning (HRL) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge.  In this paper, we propose a novel three-layer HRL algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide a theoretical study of the regret bounds of the learned policies. We evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.12023</link><description>&lt;p&gt;
LQ-LoRA: &#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12023
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#31639;&#27861;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#37327;&#21270;&#37096;&#20998;&#20445;&#25345;&#22266;&#23450;&#65292;&#21482;&#26377;&#20302;&#31209;&#37096;&#20998;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#37096;&#20998;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#34920;&#36798;&#65292;&#21487;&#20197;&#26681;&#25454;&#24635;&#20307;&#20869;&#23384;&#39044;&#31639;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#65288;&#20363;&#22914;&#27604;&#29305;&#23485;&#24230;&#12289;&#22359;&#22823;&#23567;&#65289;&#32473;&#23450;&#27599;&#20010;&#30697;&#38453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#36817;&#20284;&#26469;&#21152;&#26435;&#30697;&#38453;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#37325;&#26500;&#30446;&#26631;&#12290;&#22312;RoBERTa&#21644;LLaMA-2&#65288;7B&#21644;70B&#65289;&#30340;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;LQ-LoRA&#65289;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;QLoRA&#21644;GPTQ-LoRA&#65292;&#24182;&#23454;&#29616;&#20102;&#28608;&#36827;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19917</link><description>&lt;p&gt;
&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#20559;&#35265;&#26816;&#27979;&#21644;&#32531;&#35299;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#21307;&#30103;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#32508;&#36848;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#12290;&#26041;&#27861;&#65306;&#36981;&#24490;Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)&#20934;&#21017;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#20174;PubMed&#12289;Web of Science&#21644;&#30005;&#27668;&#21644;&#30005;&#23376;&#24037;&#31243;&#24072;&#23398;&#20250;&#20013;&#26816;&#32034;&#20102;2010&#24180;1&#26376;1&#26085;&#33267;2022&#24180;10&#26376;31&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;&#32467;&#26524;&#65306;&#22312;&#26816;&#32034;&#21040;&#30340;252&#31687;&#25991;&#31456;&#20013;&#65292;&#26377;20&#31687;&#31526;&#21512;&#26368;&#32456;&#32508;&#36848;&#30340;&#32435;&#20837;&#26631;&#20934;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#20845;&#31181;&#20559;&#35265;&#20013;&#30340;&#20116;&#31181;&#65306;&#20843;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#36873;&#25321;&#20559;&#35265;&#65307;&#20845;&#39033;&#30740;&#31350;&#38024;&#23545;&#38544;&#24615;&#20559;&#35265;&#65307;&#20116;&#39033;&#30740;&#31350;&#23545;&#28151;&#26434;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#22235;&#39033;&#30740;&#31350;&#23545;&#27979;&#37327;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#20004;&#39033;&#30740;&#31350;&#23545;&#31639;&#27861;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#26041;&#38754;&#65292;&#26377;&#21313;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.12806</link><description>&lt;p&gt;
DCSI -- &#22522;&#20110;&#20998;&#31163;&#21644;&#36830;&#36890;&#24615;&#30340;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#26631;&#31614;&#26159;&#21542;&#23545;&#24212;&#20110;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#23545;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#12290;&#29616;&#26377;&#25991;&#29486;&#30340;&#32508;&#36848;&#26174;&#31034;&#65292;&#26082;&#26377;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#22797;&#26434;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631; (CVIs) &#37117;&#27809;&#26377;&#20805;&#20998;&#34701;&#20837;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#12290;&#19968;&#31181;&#26032;&#24320;&#21457;&#30340;&#24230;&#37327;&#26041;&#27861; (&#23494;&#24230;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;, DCSI) &#26088;&#22312;&#37327;&#21270;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#24182;&#19988;&#20063;&#21487;&#29992;&#20316; CVI&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DCSI &#19982;&#36890;&#36807;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968; (ARI) &#27979;&#37327;&#30340;DBSCAN&#30340;&#24615;&#33021;&#20043;&#38388;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#23545;&#22810;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#24230;&#32858;&#31867;&#19981;&#36866;&#24403;&#30340;&#37325;&#21472;&#31867;&#21035;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23545;&#32463;&#24120;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;&#65292;DCSI &#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23494;&#24230;&#32858;&#31867;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2310.11829</link><description>&lt;p&gt;
&#36208;&#21521;&#22270;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;&#19982;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#65288;GFMs&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#38416;&#36848;&#12290;&#21516;&#26102;&#65292;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#22270;&#23398;&#20064;&#33539;&#24335;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#25104;&#21151;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#32463;&#21382;&#20102;&#30001;&#27973;&#23618;&#26041;&#27861;&#21521;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#36716;&#21464;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#21644;&#21516;&#21270;&#33021;&#21147;&#24341;&#36215;&#20102;&#22270;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#30340;&#20852;&#36259;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#24320;&#21457;&#19979;&#19968;&#20010;&#39044;&#35757;&#32451;&#20110;&#24191;&#27867;&#22270;&#25968;&#25454;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#22270;&#20219;&#21153;&#30340;&#22270;&#23398;&#20064;&#33539;&#24335;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#36825;&#31867;&#24037;&#20316;&#23578;&#26080;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;(GFMs)&#30340;&#27010;&#24565;&#65292;&#24182;&#39318;&#27425;&#23545;&#20854;&#20851;&#38190;&#29305;&#24449;&#21644;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#38416;&#36848;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26681;&#25454;&#20854;&#21487;&#38752;&#24615;&#23558;&#29616;&#26377;GFMs&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11566</link><description>&lt;p&gt;
&#20855;&#26377;&#31070;&#32463;&#24863;&#30693;&#26426;&#21046;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Stochastic Games with Neural Perception Mechanisms. (arXiv:2310.11566v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#34701;&#21512;&#24863;&#30693;&#26426;&#21046;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24207;&#21015;&#20915;&#31574;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#21482;&#26377;&#37096;&#20998;&#35266;&#27979;&#20449;&#24687;&#30340;&#26234;&#33021;&#20307;&#21644;&#19968;&#31181;&#23436;&#20840;&#35266;&#27979;&#30340;&#26234;&#33021;&#20307;&#30340;&#21333;&#26041;&#38754;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;NS-POSGs&#20540;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21338;&#24328;&#26159;&#19968;&#20010;&#20026;&#22810;&#26234;&#33021;&#20307;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#21482;&#26377;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#26234;&#33021;&#20307;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#36830;&#32493;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#31526;&#21495;&#21270;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;NS-POSGs&#65289;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#36830;&#32493;&#31354;&#38388;&#24182;&#21457;&#38543;&#26426;&#21338;&#24328;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#24863;&#30693;&#26426;&#21046;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#21333;&#26041;&#38754;&#30340;&#35774;&#32622;&#65292;&#21253;&#21547;&#20102;&#19968;&#20010;&#20855;&#26377;&#31163;&#25955;&#12289;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#35266;&#27979;&#21644;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#35266;&#27979;&#30340;&#20805;&#20998;&#20102;&#35299;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#36793;NS-HSVI&#30340;&#22522;&#20110;&#28857;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#36817;&#20284;&#35745;&#31639;&#21333;&#26041;&#38754;NS-POSGs&#30340;&#20540;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic games are a well established model for multi-agent sequential decision making under uncertainty. In reality, though, agents have only partial observability of their environment, which makes the problem computationally challenging, even in the single-agent setting of partially observable Markov decision processes. Furthermore, in practice, agents increasingly perceive their environment using data-driven approaches such as neural networks trained on continuous data. To tackle this problem, we propose the model of neuro-symbolic partially-observable stochastic games (NS-POSGs), a variant of continuous-space concurrent stochastic games that explicitly incorporates perception mechanisms. We focus on a one-sided setting, comprising a partially-informed agent with discrete, data-driven observations and a fully-informed agent with continuous observations. We present a new point-based method, called one-sided NS-HSVI, for approximating values of one-sided NS-POSGs and implement it ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09484</link><description>&lt;p&gt;
&#25506;&#32034;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring the Design Space of Diffusion Autoencoders for Face Morphing. (arXiv:2310.09484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09484
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#38754;&#21521;&#20154;&#33080;&#21464;&#24418;&#30340;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#37319;&#26679;&#31639;&#27861;&#12289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#21644;&#37096;&#20998;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#21019;&#24314;&#30340;&#20154;&#33080;&#21464;&#24418;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#21019;&#26032;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;1&#65289;&#37319;&#26679;&#31639;&#27861;&#65292;2&#65289;&#36870;&#21521;DDIM&#27714;&#35299;&#22120;&#65292;&#20197;&#21450;3&#65289;&#36890;&#36807;&#28155;&#21152;&#23569;&#37327;&#22122;&#22768;&#36827;&#34892;&#37096;&#20998;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphs created by Diffusion Autoencoders are a recent innovation and the design space of such an approach has not been well explored. We explore three axes of the design space, i.e., 1) sampling algorithms, 2) the reverse DDIM solver, and 3) partial sampling through small amounts of added noise.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.17382</link><description>&lt;p&gt;
&#26410;&#26469;&#30340;&#21407;&#22240;&#65292;&#29616;&#22312;&#30340;&#34892;&#21160;&#65306;&#19968;&#31181;&#21487;&#35777;&#26126;&#26679;&#26412;&#25928;&#29575;&#30340;&#33258;&#20027;LLM&#26234;&#33021;&#20307;&#30340;&#21407;&#21017;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Reason for Future, Act for Now: A Principled Framework for Autonomous LLM Agents with Provable Sample Efficiency. (arXiv:2309.17382v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17382
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"RAFA"&#30340;&#21407;&#21017;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;LLM&#20013;&#23558;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#65292;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#12290;&#36890;&#36807;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#36827;&#34892;&#25512;&#29702;&#65292;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#36712;&#36857;&#35268;&#21010;&#65292;&#28982;&#21518;&#22312;&#27599;&#19968;&#27493;&#20013;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#24182;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#36712;&#36857;&#12290;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23558;&#25512;&#29702;&#36716;&#21270;&#20026;&#34892;&#21160;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#20309;&#36890;&#36807;&#25512;&#29702;&#30340;&#20869;&#37096;&#26426;&#21046;&#22312;&#19982;&#22806;&#37096;&#29615;&#22659;&#30340;&#26368;&#23569;&#20132;&#20114;&#27425;&#25968;&#20869;&#21487;&#35777;&#26126;&#22320;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21487;&#35777;&#26126;&#36951;&#25022;&#20445;&#35777;&#30340;&#21407;&#21017;&#26694;&#26550;&#26469;&#21327;&#35843;&#25512;&#29702;&#21644;&#34892;&#21160;&#65292;&#31216;&#20043;&#20026;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#65292;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65288;RAFA&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25512;&#29702;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20174;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#23398;&#20064;&#24182;&#21046;&#23450;&#26410;&#26469;&#30340;&#38271;&#26399;&#36712;&#36857;&#35268;&#21010;&#65288;&#8220;&#20026;&#26410;&#26469;&#32780;&#25512;&#29702;&#8221;&#65289;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;LLM&#26234;&#33021;&#20307;&#37319;&#21462;&#35745;&#21010;&#36712;&#36857;&#30340;&#21021;&#22987;&#34892;&#21160;&#65288;&#8220;&#20026;&#29616;&#22312;&#32780;&#34892;&#21160;&#8221;&#65289;&#65292;&#23558;&#25910;&#38598;&#21040;&#30340;&#21453;&#39304;&#23384;&#20648;&#22312;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#37325;&#26032;&#35843;&#29992;&#25512;&#29702;&#36807;&#31243;&#20174;&#26032;&#29366;&#24577;&#37325;&#26032;&#35268;&#21010;&#26410;&#26469;&#30340;&#36712;&#36857;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;LLM&#20013;&#30340;&#25512;&#29702;&#35270;&#20026;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#36125;&#21494;&#26031;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive reasoning abilities, but translating reasoning into actions in the real world remains challenging. In particular, it remains unclear how to complete a given task provably within a minimum number of interactions with the external environment, e.g., through an internal mechanism of reasoning. To this end, we propose a principled framework with provable regret guarantees to orchestrate reasoning and acting, which we call "reason for future, act for now" (\texttt{RAFA}). Specifically, we design a prompt template for reasoning that learns from the memory buffer and plans a future trajectory over a long horizon ("reason for future"). At each step, the LLM agent takes the initial action of the planned trajectory ("act for now"), stores the collected feedback in the memory buffer, and reinvokes the reasoning routine to replan the future trajectory from the new state.  The key idea is to cast reasoning in LLMs as learning and planning in Bayes
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06212</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06212
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#19994;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#23545;&#20110;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38271;&#26399;&#20915;&#31574;&#65292;&#25552;&#21069;&#19968;&#24180;&#36827;&#34892;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#20869;&#21508;&#31181;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#39044;&#27979;&#36825;&#19968;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#26681;&#25454;Palmer&#24178;&#26097;&#20005;&#37325;&#25351;&#25968;&#65288;PDSI&#65289;&#39044;&#27979;&#24863;&#20852;&#36259;&#20122;&#21306;&#30340;&#24178;&#26097;&#24378;&#24230;&#65292;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#30340;&#20869;&#22312;&#22240;&#32032;&#21644;&#35265;&#35299;&#26469;&#25552;&#39640;&#24178;&#26097;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26799;&#24230;&#25552;&#21319;&#21644;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#21069;&#20004;&#31181;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;ROC AUC&#20998;&#25968;&#65292;&#39640;&#36798;0.90
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.06090</link><description>&lt;p&gt;
&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#30340;&#21160;&#24577;&#19982;&#25511;&#21046;&#27169;&#22411;&#30340;&#36890;&#29992;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A General Verification Framework for Dynamical and Control Models via Certificate Synthesis. (arXiv:2309.06090v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06090
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#26469;&#36890;&#36807;&#35777;&#20070;&#21512;&#25104;&#39564;&#35777;&#21160;&#24577;&#21644;&#25511;&#21046;&#27169;&#22411;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#22797;&#26434;&#35268;&#33539;&#12290;&#36825;&#20010;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#20505;&#36873;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#24182;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35770;&#30340;&#19968;&#20010;&#26032;&#20852;&#20998;&#25903;&#19987;&#38376;&#30740;&#31350;&#35777;&#20070;&#23398;&#20064;&#65292;&#28041;&#21450;&#23545;&#33258;&#20027;&#25110;&#25511;&#21046;&#27169;&#22411;&#30340;&#25152;&#38656;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#31995;&#32479;&#34892;&#20026;&#30340;&#35268;&#33539;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20989;&#25968;&#30340;&#35777;&#26126;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#22797;&#26434;&#35201;&#27714;&#30340;&#25511;&#21046;&#22120;&#30340;&#21512;&#25104;&#36890;&#24120;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#19987;&#23478;&#25511;&#21046;&#24037;&#31243;&#24072;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33258;&#21160;&#25216;&#26415;&#33021;&#22815;&#35774;&#35745;&#25511;&#21046;&#22120;&#24182;&#20998;&#26512;&#21508;&#31181;&#22797;&#26434;&#35268;&#33539;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#32534;&#30721;&#31995;&#32479;&#35268;&#33539;&#24182;&#23450;&#20041;&#30456;&#24212;&#30340;&#35777;&#20070;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#27491;&#24335;&#21512;&#25104;&#25511;&#21046;&#22120;&#21644;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25511;&#21046;&#30340;&#23433;&#20840;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#25552;&#20379;&#20505;&#36873;&#30340;&#25511;&#21046;&#21644;&#35777;&#20070;&#20989;&#25968;&#65292;&#21516;&#26102;&#20351;&#29992;SMT&#27714;&#35299;&#22120;&#26469;&#25552;&#20379;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging branch of control theory specialises in certificate learning, concerning the specification of a desired (possibly complex) system behaviour for an autonomous or control model, which is then analytically verified by means of a function-based proof. However, the synthesis of controllers abiding by these complex requirements is in general a non-trivial task and may elude the most expert control engineers. This results in a need for automatic techniques that are able to design controllers and to analyse a wide range of elaborate specifications. In this paper, we provide a general framework to encode system specifications and define corresponding certificates, and we present an automated approach to formally synthesise controllers and certificates. Our approach contributes to the broad field of safe learning for control, exploiting the flexibility of neural networks to provide candidate control and certificate functions, whilst using SMT-solvers to offer a formal guarantee of co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;ReLU&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04742</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Affine Invariant Ensemble Transform Methods to Improve Predictive Uncertainty in ReLU Networks. (arXiv:2309.04742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20223;&#23556;&#19981;&#21464;&#38598;&#25104;&#21464;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#22312;ReLU&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#25910;&#25947;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21512;&#36866;&#30340;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#25193;&#23637;&#36827;&#34892;&#36923;&#36753;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#20174;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#24182;&#35777;&#26126;&#24403;&#31890;&#23376;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#26102;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#25910;&#25947;&#21040;&#22343;&#22330;&#26497;&#38480;&#30340;&#37327;&#21270;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#24182;&#32771;&#23519;&#23427;&#20204;&#20316;&#20026;&#36125;&#21494;&#26031;&#36817;&#20284;&#26041;&#27861;&#22312;ReLU&#32593;&#32476;&#20013;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of performing Bayesian inference for logistic regression using appropriate extensions of the ensemble Kalman filter. Two interacting particle systems are proposed that sample from an approximate posterior and prove quantitative convergence rates of these interacting particle systems to their mean-field limit as the number of particles tends to infinity. Furthermore, we apply these techniques and examine their effectiveness as methods of Bayesian approximation for quantifying predictive uncertainty in ReLU networks.
&lt;/p&gt;</description></item><item><title>&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2308.13320</link><description>&lt;p&gt;
&#24494;&#35843;&#21487;&#33021;&#21066;&#24369;&#22522;&#30784;&#27169;&#22411;&#65307;&#20445;&#30041;&#29305;&#24449;&#21487;&#33021;&#26159;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning can cripple your foundation model; preserving features may be the solution. (arXiv:2308.13320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13320
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#21487;&#33021;&#20250;&#36951;&#24536;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#35777;&#26126;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#20027;&#35201;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#23481;&#37327;&#21644;&#23545;&#20174;&#20114;&#32852;&#32593;&#19978;&#29228;&#21462;&#30340;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#26292;&#38706;&#65292;&#20139;&#26377;&#23384;&#20648;&#20851;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#20135;&#29983;&#20986;&#33394;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#24494;&#35843;&#27169;&#22411;&#22312;&#19982;&#19979;&#28216;&#20219;&#21153;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#35782;&#21035;&#27010;&#24565;&#30340;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26174;&#28982;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#22312;&#39318;&#27425;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#26102;&#65292;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19981;&#21487;&#21462;&#30340;&#29616;&#35937;&#31216;&#20026;&#8220;&#27010;&#24565;&#36951;&#24536;&#8221;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#37117;&#20005;&#37325;&#21463;&#21040;&#36825;&#31181;&#21103;&#20316;&#29992;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#30456;&#24403;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;LDIFS&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained foundation models, owing primarily to their enormous capacity and exposure to vast amount of training data scraped from the internet, enjoy the advantage of storing knowledge about plenty of real-world concepts. Such models are typically fine-tuned on downstream datasets to produce remarkable state-of-the-art performances. While various fine-tuning methods have been devised and are shown to be highly effective, we observe that a fine-tuned model's ability to recognize concepts on tasks $\textit{different}$ from the downstream one is reduced significantly compared to its pre-trained counterpart. This is clearly undesirable as a huge amount of time and money went into learning those very concepts in the first place. We call this undesirable phenomenon "concept forgetting" and via experiments show that most end-to-end fine-tuning approaches suffer heavily from this side effect. To this end, we also propose a rather simple fix to this problem by designing a method called LDIFS 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#30340;&#32593;&#32476;&#24178;&#25200;&#12290;&#36890;&#36807;&#32771;&#34385;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#24314;&#31435;&#36866;&#21512;&#30340;&#26333;&#20809;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#23454;&#39564;&#21644;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.09790</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#32593;&#32476;&#24178;&#25200;&#30340;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Two-Part Machine Learning Approach to Characterizing Network Interference in A/B Testing. (arXiv:2308.09790v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25551;&#36848;A/B&#27979;&#35797;&#20013;&#30340;&#32593;&#32476;&#24178;&#25200;&#12290;&#36890;&#36807;&#32771;&#34385;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#21644;&#24314;&#31435;&#36866;&#21512;&#30340;&#26333;&#20809;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#23454;&#39564;&#21644;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32593;&#32476;&#24178;&#25200;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#25511;&#21046;&#23454;&#39564;&#25110;"A/B&#27979;&#35797;"&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#25551;&#36848;&#24322;&#36136;&#32593;&#32476;&#24178;&#25200;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#28508;&#22312;&#30340;&#22797;&#26434;&#32593;&#32476;&#32467;&#26500;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;"&#26333;&#20809;&#26144;&#23556;"&#30830;&#23450;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#22240;&#26524;&#32593;&#32476;&#27169;&#24335;"&#65292;&#24182;&#37319;&#29992;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#24314;&#31435;&#26368;&#36866;&#21512;&#21453;&#26144;&#28508;&#22312;&#32593;&#32476;&#24178;&#25200;&#27169;&#24335;&#30340;&#26333;&#20809;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20004;&#20010;&#21512;&#25104;&#23454;&#39564;&#21644;&#19968;&#20010;&#28041;&#21450;100-200&#19975;Instagram&#29992;&#25143;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#27979;&#35797;&#20013;&#30340;&#27169;&#25311;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#35774;&#35745;&#30340;&#38598;&#32676;&#38543;&#26426;&#21270;&#21644;&#22522;&#20110;&#20998;&#26512;&#30340;&#37051;&#22495;&#26333;&#20809;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of controlled experiments, or "A/B tests," can often be compromised due to the phenomenon of network interference, wherein the outcome for one unit is influenced by other units. To tackle this challenge, we propose a machine learning-based method to identify and characterize heterogeneous network interference. Our approach accounts for latent complex network structures and automates the task of "exposure mapping'' determination, which addresses the two major limitations in the existing literature. We introduce "causal network motifs'' and employ transparent machine learning models to establish the most suitable exposure mapping that reflects underlying network interference patterns. Our method's efficacy has been validated through simulations on two synthetic experiments and a real-world, large-scale test involving 1-2 million Instagram users, outperforming conventional methods such as design-based cluster randomization and analysis-based neighborhood exposure mapping. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.11465</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25972;&#20307;&#29983;&#23384;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#32954;&#30284;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#30149;&#20154;&#29366;&#24577;&#30340;&#25972;&#20307;&#29983;&#23384;&#65288;OS&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#29983;&#23384;&#27010;&#29575;&#19981;&#21516;&#30340;&#20122;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#21644;&#25913;&#21892;&#25972;&#20307;&#29983;&#23384;&#29575;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#30149;&#20154;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#21033;&#29992;&#26410;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#27515;&#20129;&#65289;&#21644;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#24184;&#23384;&#32773;&#65289;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#20063;&#35201;&#32771;&#34385;&#21040;&#27515;&#20129;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#19981;&#23436;&#25972;&#25968;&#25454;&#22788;&#29702;&#26159;&#21307;&#23398;&#39046;&#22495;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#25554;&#34917;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30149;&#20154;&#21450;&#20854;&#21487;&#29992;&#29305;&#24449;&#20013;&#26377;&#25928;&#23398;&#20064;&#65292;&#39044;&#27979;NSCLC&#30149;&#20154;&#30340;OS&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;</title><link>http://arxiv.org/abs/2307.10529</link><description>&lt;p&gt;
&#24555;&#36895;&#26080;&#30417;&#30563;&#28145;&#24230;&#24322;&#24120;&#20540;&#27169;&#22411;&#36873;&#25321;&#19982;&#36229;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Unsupervised Deep Outlier Model Selection with Hypernetworks. (arXiv:2307.10529v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HYPER&#29992;&#20110;&#35843;&#25972;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;DOD&#27169;&#22411;&#20013;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#36229;&#32593;&#32476;(HN)&#23558;&#36229;&#21442;&#25968;&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;(OD)&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#65292;&#24182;&#26377;&#35768;&#22810;&#25216;&#26415;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;OD(DOD)&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#36827;&#23637;&#32780;&#21463;&#21040;&#20102;&#26368;&#36817;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#38382;&#39064;&#65292;&#21363;&#26080;&#30417;&#30563;DOD&#30340;&#26377;&#25928;&#36229;&#21442;&#25968;(HP)&#35843;&#25972;/&#27169;&#22411;&#36873;&#25321;&#12290;&#34429;&#28982;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#25253;&#21578;&#20102;OD&#27169;&#22411;&#23545;HP&#30340;&#25935;&#24863;&#24615;&#65292;&#20294;&#23545;&#20110;&#23637;&#31034;&#20102;&#38271;&#21015;&#34920;HP&#30340;&#29616;&#20195;DOD&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HYPER&#26469;&#35843;&#25972;DOD&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20004;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;(1)&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#30340;&#39564;&#35777;(&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#30340;&#24322;&#24120;&#20540;)&#65292;&#20197;&#21450;(2) HP/&#27169;&#22411;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034; (&#30001;&#20110;HP&#25968;&#37327;&#30340;&#25351;&#25968;&#22686;&#38271;)&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#26032;&#39062;&#30340;&#36229;&#32593;&#32476;(HN)&#65292;&#20854;&#23558;HP&#26144;&#23556;&#21040;&#20027;&#35201;DOD&#27169;&#22411;&#30340;&#26368;&#20248;&#26435;&#37325;&#19978;&#12290;&#21453;&#36807;&#26469;&#65292;HYPER&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;HN&#65292;&#21487;&#20197;&#21160;&#24577;&#29983;&#25104;&#22810;&#20010;DOD&#27169;&#22411;&#30340;&#26435;&#37325; (&#23545;&#24212;&#20110;...)&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection (OD) finds many applications with a rich literature of numerous techniques. Deep neural network based OD (DOD) has seen a recent surge of attention thanks to the many advances in deep learning. In this paper, we consider a critical-yet-understudied challenge with unsupervised DOD, that is, effective hyperparameter (HP) tuning/model selection. While several prior work report the sensitivity of OD models to HPs, it becomes ever so critical for the modern DOD models that exhibit a long list of HPs. We introduce HYPER for tuning DOD models, tackling two fundamental challenges: (1) validation without supervision (due to lack of labeled anomalies), and (2) efficient search of the HP/model space (due to exponential growth in the number of HPs). A key idea is to design and train a novel hypernetwork (HN) that maps HPs onto optimal weights of the main DOD model. In turn, HYPER capitalizes on a single HN that can dynamically generate weights for many DOD models (corresponding t
&lt;/p&gt;</description></item><item><title>&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.10438</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#36827;&#34892;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search. (arXiv:2307.10438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10438
&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;AutoGNNUQ&#65292;&#36890;&#36807;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#39640;&#24615;&#33021;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#20998;&#35299;&#23558;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#24320;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20013;&#31361;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;GNN&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#26080;&#27861;&#37327;&#21270;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21487;&#20449;&#22320;&#20351;&#29992;&#21644;&#37096;&#32626;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoGNNUQ&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;AutoGNNUQ&#21033;&#29992;&#26550;&#26500;&#25628;&#32034;&#29983;&#25104;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;GNN&#38598;&#21512;&#65292;&#33021;&#22815;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26041;&#24046;&#20998;&#35299;&#26469;&#20998;&#31163;&#25968;&#25454;&#65288;aleatoric&#65289;&#21644;&#27169;&#22411;&#65288;epistemic&#65289;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#20943;&#23569;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#22312;&#25105;&#20204;&#30340;&#35745;&#31639;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoGNNUQ&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35299;&#37322;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a prominent class of data-driven methods for molecular property prediction. However, a key limitation of typical GNN models is their inability to quantify uncertainties in the predictions. This capability is crucial for ensuring the trustworthy use and deployment of models in downstream tasks. To that end, we introduce AutoGNNUQ, an automated uncertainty quantification (UQ) approach for molecular property prediction. AutoGNNUQ leverages architecture search to generate an ensemble of high-performing GNNs, enabling the estimation of predictive uncertainties. Our approach employs variance decomposition to separate data (aleatoric) and model (epistemic) uncertainties, providing valuable insights for reducing them. In our computational experiments, we demonstrate that AutoGNNUQ outperforms existing UQ methods in terms of both prediction accuracy and UQ performance on multiple benchmark datasets. Additionally, we utilize t-SNE visualization to exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2307.07344</link><description>&lt;p&gt;
&#36870;&#36827;&#21270;&#23618;:&#29289;&#29702;&#20449;&#24687;&#21270;&#27491;&#21017;&#21270;&#22120;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#23558;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#36827;&#21270;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36827;&#21270;&#26041;&#31243;&#30340;&#36870;&#36827;&#21270;&#23618;&#65288;IELs&#65289;&#12290;&#36825;&#20123;&#23618;&#21487;&#20197;&#23454;&#29616;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#65292;&#24182;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#36827;&#21270;&#27169;&#22411;&#23545;&#24212;&#30340;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#36870;&#36827;&#21270;&#23618;&#30340;&#26500;&#24314;&#21644;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20026;&#19981;&#21516;&#30340;&#29289;&#29702;&#36827;&#21270;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#23618;&#30340;&#35774;&#35745;&#36807;&#31243;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30452;&#35266;&#21644;&#25968;&#23398;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#31616;&#21333;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#36171;&#20104;&#28909;&#25193;&#25955;&#27169;&#22411;&#24179;&#28369;&#24615;&#23646;&#24615;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach to integrating partial differential equation (PDE)-based evolution models into neural networks through a new type of regularization. Specifically, we propose inverse evolution layers (IELs) based on evolution equations. These layers can achieve specific regularization objectives and endow neural networks' outputs with corresponding properties of the evolution models. Moreover, IELs are straightforward to construct and implement, and can be easily designed for various physical evolutions and neural networks. Additionally, the design process for these layers can provide neural networks with intuitive and mathematical interpretability, thus enhancing the transparency and explainability of the approach. To demonstrate the effectiveness, efficiency, and simplicity of our approach, we present an example of endowing semantic segmentation models with the smoothness property based on the heat diffusion model. To achieve this goal, we design heat-diffusion IE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32479;&#35745;&#23398;&#20064;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#12289;&#24378;&#20984;&#21644;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04679</link><description>&lt;p&gt;
&#19968;&#38454;&#26041;&#27861;&#22312;&#20855;&#26377;&#36890;&#29992;&#39044;&#27979;&#22120;&#30340;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#32479;&#35745;&#23398;&#20064;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#35813;&#26694;&#26550;&#36866;&#29992;&#20110;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#12289;&#24378;&#20984;&#21644;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#22312;&#32479;&#35745;&#23398;&#20064;&#20013;&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#26102;&#65292;&#24403;&#26799;&#24230;&#21482;&#33021;&#36890;&#36807;&#39044;&#27979;&#22120;&#32473;&#20986;&#30340;&#37096;&#20998;&#35266;&#27979;&#26469;&#35775;&#38382;&#26102;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20381;&#36182;&#20110;&#26799;&#24230;&#23545;&#25968;&#25454;&#26679;&#26412;&#30340;&#35268;&#21017;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#23548;&#20986;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#32039;&#23494;&#21305;&#37197;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#65292;&#21253;&#25324;&#30417;&#30563;&#23398;&#20064;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#40065;&#26834;&#23398;&#20064;&#12289;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#20351;&#29992;&#26799;&#24230;&#37327;&#21270;&#30340;&#36890;&#20449;&#25928;&#29575;&#23398;&#20064;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#20809;&#28369;&#19988;&#24378;&#20984;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#21450;&#28385;&#36275;Polyak-Lojasiewicz&#20551;&#35774;&#30340;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#20381;&#36182;&#20110;&#19968;&#20010;&#25193;&#23637;&#20102;&#26465;&#20214;&#26631;&#20934;&#24046;&#27010;&#24565;&#30340;&#26032;&#37327;&#65292;&#23427;&#34913;&#37327;&#20102;&#36890;&#36807;&#35775;&#38382;&#39044;&#27979;&#22120;&#21487;&#20197;&#36817;&#20284;&#26799;&#24230;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a novel framework for the analysis of generalization error of first-order optimization algorithms for statistical learning when the gradient can only be accessed through partial observations given by an oracle. Our analysis relies on the regularity of the gradient w.r.t. the data samples, and allows to derive near matching upper and lower bounds for the generalization error of multiple learning problems, including supervised learning, transfer learning, robust learning, distributed learning and communication efficient learning using gradient quantization. These results hold for smooth and strongly-convex optimization problems, as well as smooth non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In particular, our upper and lower bounds depend on a novel quantity that extends the notion of conditional standard deviation, and is a measure of the extent to which the gradient can be approximated by having access to the oracle. As a consequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04033</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20013;&#21482;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#26412;&#36523;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#26679;&#26412;&#30340;&#27010;&#29575;&#20266;&#26631;&#31614;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#27979;&#35797;&#26102;&#30340;&#25512;&#24191;&#24314;&#27169;&#20026;&#21464;&#20998;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#24314;&#27169;&#20026;&#20998;&#24067;&#65292;&#32771;&#34385;&#27867;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20943;&#36731;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#24102;&#26469;&#30340;&#35823;&#23548;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#65292;&#23558;&#37051;&#36817;&#30446;&#26631;&#26679;&#26412;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#24378;&#40065;&#26834;&#20266;&#26631;&#31614;&#30340;&#36807;&#31243;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#23398;&#20064;&#23558;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30446;&#26631;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#20934;&#30830;&#12289;&#26356;&#24378;&#40065;&#26834;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#30340;&#33021;&#21147;&#20013;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#21516;&#26102;&#36866;&#29992;&#20110;&#20559;&#20381;&#36182;&#12289;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#30340;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00541</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#36827;&#34892;&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Decomposing Global Feature Effects Based on Feature Interactions. (arXiv:2306.00541v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00541
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#21516;&#26102;&#36866;&#29992;&#20110;&#20559;&#20381;&#36182;&#12289;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#30340;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#26041;&#27861;&#65292;&#22914;&#20559;&#20381;&#36182;&#22270;&#65292;&#25552;&#20379;&#20102;&#39044;&#26399;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#30340;&#21487;&#29702;&#35299;&#30340;&#21487;&#35270;&#21270;&#12290;&#20294;&#26159;&#65292;&#24403;&#23384;&#22312;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#26102;&#65292;&#36825;&#31181;&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#26041;&#27861;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#21333;&#20010;&#35266;&#27979;&#30340;&#23616;&#37096;&#29305;&#24449;&#25928;&#24212;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#22522;&#20110;&#36882;&#24402;&#20998;&#21306;&#30340;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#35299;&#37322;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#21306;&#22495;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#20026;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#23427;&#36866;&#29992;&#20110;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#26469;&#21487;&#35270;&#21270;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#65292;&#21363;&#20559;&#20381;&#36182;&#65292;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global feature effect methods, such as partial dependence plots, provide an intelligible visualization of the expected marginal feature effect. However, such global feature effect methods can be misleading, as they do not represent local feature effects of single observations well when feature interactions are present. We formally introduce generalized additive decomposition of global effects (GADGET), which is a new framework based on recursive partitioning to find interpretable regions in the feature space such that the interaction-related heterogeneity of local feature effects is minimized. We provide a mathematical foundation of the framework and show that it is applicable to the most popular methods to visualize marginal feature effects, namely partial dependence, accumulated local effects, and Shapley additive explanations (SHAP) dependence. Furthermore, we introduce a new permutation-based interaction test to detect significant feature interactions that is applicable to any feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19476</link><description>&lt;p&gt;
&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#25506;&#32034;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#26159;&#36890;&#36807;&#40723;&#21169;&#23545;&#35775;&#38382;&#29366;&#24577;&#31354;&#38388;&#30340;&#22343;&#21248;&#35206;&#30422;&#26469;&#26368;&#22823;&#21270;&#24050;&#35775;&#38382;&#29366;&#24577;&#20998;&#24067;&#30340;&#29109;&#65292;&#21363;&#29366;&#24577;&#29109;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26377;&#20219;&#21153;&#22870;&#21169;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#65292;&#20854;&#20013;&#20195;&#29702;&#36235;&#21521;&#20110;&#35775;&#38382;&#39640;&#20215;&#20540;&#29366;&#24577;&#20197;&#21033;&#29992;&#20219;&#21153;&#22870;&#21169;&#12290;&#36825;&#20010;&#20559;&#22909;&#20250;&#23548;&#33268;&#39640;&#20215;&#20540;&#29366;&#24577;&#21644;&#20302;&#20215;&#20540;&#29366;&#24577;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24403;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#22343;&#21248;&#26102;&#65292;&#29366;&#24577;&#29109;&#20250;&#22686;&#21152;&#65292;&#20174;&#32780;&#20559;&#21521;&#20110;&#25506;&#32034;&#20302;&#20215;&#20540;&#21306;&#22495;&#12290;&#24403;&#39640;&#20215;&#20540;&#29366;&#24577;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20998;&#24067;&#29421;&#31364;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#65292;&#23427;&#20998;&#21035;&#20272;&#35745;&#27599;&#20010;&#29366;&#24577;&#20215;&#20540;&#20272;&#35745;&#26465;&#20214;&#19979;&#30340;&#29366;&#24577;&#29109;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#21152;&#26435;&#21644;&#12290;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#37327;&#21270;&#20102;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#21306;&#22495;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#20351;&#20854;&#23545;&#19981;&#24179;&#34913;&#38382;&#39064;&#26356;&#21152;&#20581;&#22766;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#30340;&#31639;&#27861;&#65292;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#25311;&#21512;&#24555;&#36895;&#39044;&#27979;&#25925;&#38556;&#65292;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#12289;&#21361;&#38505;&#12289;&#35745;&#31639;&#26114;&#36149;&#30340;&#31995;&#32479;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.02449</link><description>&lt;p&gt;
&#40657;&#30418;&#31995;&#32479;&#30340;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Bayesian Safety Validation for Black-Box Systems. (arXiv:2305.02449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#30340;&#31639;&#27861;&#65292;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#25311;&#21512;&#24555;&#36895;&#39044;&#27979;&#25925;&#38556;&#65292;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#39640;&#32500;&#12289;&#21361;&#38505;&#12289;&#35745;&#31639;&#26114;&#36149;&#30340;&#31995;&#32479;&#30340;&#39640;&#25928;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20934;&#30830;&#20272;&#35745;&#25925;&#38556;&#27010;&#29575;&#23545;&#35748;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#39640;&#32500;&#36755;&#20837;&#31354;&#38388;&#12289;&#21361;&#38505;&#27979;&#35797;&#22330;&#26223;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#20223;&#30495;&#22120;&#65292;&#20272;&#35745;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#27492;&#30740;&#31350;&#39640;&#25928;&#20272;&#35745;&#25216;&#26415;&#21313;&#20998;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;&#40657;&#30418;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31639;&#27861;&#8212;&#8212;&#36125;&#21494;&#26031;&#23433;&#20840;&#39564;&#35777;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#25311;&#21512;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#26469;&#39640;&#25928;&#39044;&#27979;&#25925;&#38556;&#12290;&#35813;&#31639;&#27861;&#26088;&#22312;&#25628;&#32034;&#25925;&#38556;&#12289;&#35745;&#31639;&#26368;&#21487;&#33021;&#30340;&#25925;&#38556;&#65292;&#24182;&#21033;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#20272;&#35745;&#25805;&#20316;&#22495;&#20869;&#30340;&#25925;&#38556;&#27010;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#37319;&#38598;&#20989;&#25968;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35206;&#30422;&#35774;&#35745;&#31354;&#38388;&#12289;&#20248;&#21270;&#35299;&#26512;&#27966;&#29983;&#30340;&#25925;&#38556;&#36793;&#30028;&#21644;&#37319;&#26679;&#39044;&#27979;&#30340;&#25925;&#38556;&#21306;&#22495;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#12290;&#20027;&#35201;&#28041;&#21450;&#21482;&#36755;&#20986;&#20108;&#36827;&#21046;&#25351;&#26631;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately estimating the probability of failure for safety-critical systems is important for certification. Estimation is often challenging due to high-dimensional input spaces, dangerous test scenarios, and computationally expensive simulators; thus, efficient estimation techniques are important to study. This work reframes the problem of black-box safety validation as a Bayesian optimization problem and introduces an algorithm, Bayesian safety validation, that iteratively fits a probabilistic surrogate model to efficiently predict failures. The algorithm is designed to search for failures, compute the most-likely failure, and estimate the failure probability over an operating domain using importance sampling. We introduce a set of three acquisition functions that focus on reducing uncertainty by covering the design space, optimizing the analytically derived failure boundaries, and sampling the predicted failure regions. Mainly concerned with systems that only output a binary indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#19978;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#22312;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#20197;&#21450;&#26356;&#26032;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#19979;&#65292;&#20135;&#29983;&#20102;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.07203</link><description>&lt;p&gt;
&#20851;&#20110;&#36229;&#22270;&#19978;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs. (arXiv:2304.07203v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#19978;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#22312;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#20197;&#21450;&#26356;&#26032;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#19979;&#65292;&#20135;&#29983;&#20102;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#36229;&#20986;&#31616;&#21333;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#20132;&#20114;&#12290;&#36229;&#22270;&#20316;&#20026;&#25551;&#36848;&#21644;&#20998;&#26512;&#20855;&#26377;&#22810;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;&#22797;&#26434;&#34892;&#20026;&#30340;&#24378;&#22823;&#24314;&#27169;&#24037;&#20855;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65306;&#24213;&#23618;&#36229;&#22270;&#30001;&#19977;&#20803;&#32452;&#20316;&#20026;&#36229;&#36793;&#30028;&#23450;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26500;&#65292;&#32780;&#39030;&#28857;&#36890;&#36807;&#21152;&#26435;&#30340;&#29366;&#24577;&#20381;&#36182;&#30340;&#37051;&#22495;&#23545;&#30340;&#29366;&#24577;&#24179;&#22343;&#26356;&#26032;&#20854;&#29366;&#24577;&#12290;&#30456;&#36739;&#20110;&#24102;&#26377;&#20108;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#19978;&#30340;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#36825;&#20010;&#21160;&#21147;&#23398;&#19981;&#20250;&#25910;&#25947;&#21040;&#21021;&#22987;&#29366;&#24577;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#26159;&#20943;&#23569;&#20102;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#26356;&#26032;&#30340;&#38750;&#32447;&#24615;&#20135;&#29983;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networked systems in fields such as physics, biology, and social sciences often involve interactions that extend beyond simple pairwise ones. Hypergraphs serve as powerful modeling tools for describing and analyzing the intricate behaviors of systems with multi-body interactions. Herein, we investigate a discrete-time nonlinear averaging dynamics with three-body interactions: an underlying hypergraph, comprising triples as hyperedges, delineates the structure of these interactions, while the vertices update their states through a weighted, state-dependent average of neighboring pairs' states. This dynamics captures reinforcing group effects, such as peer pressure, and exhibits higher-order dynamical effects resulting from a complex interplay between initial states, hypergraph topology, and nonlinearity of the update. Differently from linear averaging dynamics on graphs with two-body interactions, this model does not converge to the average of the initial states but rather induc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.13113</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21516;&#26102;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#12290;&#23454;&#39564;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#25351;&#22312;&#32500;&#25345;&#20808;&#21069;&#23398;&#20064;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#26356;&#26032;&#20855;&#26377;&#26032;&#31867;&#21035;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#26469;&#38450;&#27490;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#26159;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25972;&#20010;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#20351;&#29992;&#24658;&#23450;&#30340;&#24378;&#24230;&#65292;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#25152;&#36935;&#21040;&#30340;&#20219;&#21153;&#38590;&#24230;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#35843;&#25972;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#33258;&#21160;&#30830;&#23450;&#27599;&#20010;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#27491;&#21017;&#21270;&#24378;&#24230;&#12290;&#36890;&#36807;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#36866;&#24212;&#24615;&#27491;&#21017;&#21270;&#23545;&#20110;&#23454;&#29616;&#26356;&#21152;&#20934;&#30830;&#21644;&#19981;&#26131;&#36951;&#24536;&#30340;&#35270;&#35273;&#22686;&#37327;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09100</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models. (arXiv:2303.09100v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25552;&#31034;&#24037;&#31243;&#30340;&#38382;&#39064;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26500;&#24314;&#26377;&#25928;&#25552;&#31034;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#25163;&#21160;&#35774;&#35745;&#65292;&#35201;&#20040;&#23558;&#25552;&#31034;&#35843;&#20248;&#20316;&#20026;&#28857;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25551;&#36848;&#31867;&#21035;&#30340;&#22810;&#26679;&#29305;&#24449;&#24182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#27010;&#29575;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#20174;&#28508;&#22312;&#20998;&#24067;&#20013;&#39318;&#20808;&#37319;&#26679;&#38544;&#21521;&#37327;&#65292;&#28982;&#21518;&#37319;&#29992;&#36731;&#37327;&#32423;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#26631;&#31614;&#29305;&#23450;&#30340;&#38543;&#26426;&#25552;&#31034;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#35270;&#35273;&#30693;&#35782;&#19982;&#22270;&#20687;&#30340;&#35821;&#20041;&#35268;&#21017;&#21270;&#65292;&#24182;&#23558;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#25552;&#31034;&#35270;&#20026;&#34917;&#19969;&#21644;&#20196;&#29260;&#38598;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23558;&#25552;&#31034;&#26631;&#35760;&#25512;&#21521;&#24544;&#23454;&#25429;&#25417;&#26631;&#31614;&#29305;&#23450;&#30340;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#20855;&#20449;&#24687;&#37327;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#31034;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34917;&#19969;-&#20196;&#29260;&#23545;&#40784;&#30340;&#36125;&#21494;&#26031;&#25552;&#31034;&#23398;&#20064;&#65288;PTBPL&#65289;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;</title><link>http://arxiv.org/abs/2301.13088</link><description>&lt;p&gt;
Lie &#32676;&#21644;&#23427;&#20204;&#30340;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#38745;&#27490;&#26680;&#21644;&#39640;&#26031;&#36807;&#31243; II&#65306;&#38750;&#32039;&#23545;&#31216;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. (arXiv:2301.13088v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#26500;&#24314;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#19978;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#23454;&#29992;&#25216;&#26415;&#65292;&#33021;&#22815;&#23545;&#23450;&#20041;&#22312;&#36825;&#20123;&#31354;&#38388;&#19978;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#36827;&#34892;&#23454;&#38469;&#37319;&#26679;&#21644;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26102;&#31354;&#27169;&#22411;&#20043;&#19968;&#65292;&#23427;&#21487;&#20197;&#32534;&#30721;&#26377;&#20851;&#24314;&#27169;&#20989;&#25968;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21487;&#29992;&#20110;&#31934;&#30830;&#25110;&#36817;&#20284;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#29289;&#29702;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65292;&#20197;&#21450;&#22320;&#36136;&#32479;&#35745;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#31561;&#39046;&#22495;&#65292;&#23545;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#26159;&#21487;&#20197;&#32771;&#34385;&#30340;&#26368;&#22522;&#26412;&#24418;&#24335;&#20043;&#19968;&#12290;&#39640;&#26031;&#36807;&#31243;&#21327;&#26041;&#24046;&#23545;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#19981;&#21464;&#24615;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#31354;&#38388;&#30340;&#24179;&#31283;&#24615;&#27010;&#24565;&#30340;&#26368;&#33258;&#28982;&#30340;&#25512;&#24191;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#24314;&#31435;&#38745;&#27490;&#39640;&#26031;&#36807;&#31243;&#30340;&#26500;&#36896;&#24615;&#21644;&#23454;&#29992;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#23545;&#31216;&#24615;&#32972;&#26223;&#19979;&#20986;&#29616;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30340;&#38750;&#24120;&#22823;&#30340;&#31867;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20351;&#24471;&#33021;&#22815;&#65288;i&#65289;&#35745;&#31639;&#21327;&#26041;&#24046;&#26680;&#21644;&#65288;ii&#65289;&#20174;&#36825;&#20123;&#31354;&#38388;&#19978;&#23450;&#20041;&#30340;&#20808;&#39564;&#21644;&#21518;&#39564;&#39640;&#26031;&#36807;&#31243;&#20013;&#23454;&#38469;&#22320;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2211.07866</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21512;&#24182;&#19979;&#30340;&#32437;&#21521;&#32593;&#32476;&#26377;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation for Longitudinal Network via Adaptive Merging. (arXiv:2211.07866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#31561;&#26041;&#27861;&#26469;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#32593;&#32476;&#30001;&#22810;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#26102;&#38388;&#36793;&#24207;&#21015;&#32452;&#25104;&#65292;&#20854;&#20013;&#26102;&#38388;&#36793;&#22312;&#23454;&#26102;&#20013;&#34987;&#35266;&#23519;&#21040;&#12290;&#38543;&#30528;&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#20852;&#36215;&#65292;&#23427;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#32437;&#21521;&#32593;&#32476;&#20272;&#35745;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#32593;&#32476;&#21512;&#24182;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#28857;&#36807;&#31243;&#30340;&#20248;&#21183;&#12290;&#23427;&#21512;&#24182;&#30456;&#37051;&#30340;&#31232;&#30095;&#32593;&#32476;&#65292;&#20197;&#25193;&#22823;&#35266;&#27979;&#36793;&#30340;&#25968;&#37327;&#24182;&#20943;&#23569;&#20272;&#35745;&#26041;&#24046;&#65292;&#21516;&#26102;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#26102;&#38388;&#32467;&#26500;&#36827;&#34892;&#33258;&#36866;&#24212;&#32593;&#32476;&#37051;&#22495;&#25511;&#21046;&#24341;&#20837;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20419;&#36827;&#20272;&#35745;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#30340;&#20272;&#35745;&#38169;&#35823;&#19978;&#30028;&#34987;&#24314;&#31435;&#12290;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#26174;&#30528;&#20943;&#23569;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal network consists of a sequence of temporal edges among multiple nodes, where the temporal edges are observed in real time. It has become ubiquitous with the rise of online social platform and e-commerce, but largely under-investigated in literature. In this paper, we propose an efficient estimation framework for longitudinal network, leveraging strengths of adaptive network merging, tensor decomposition and point process. It merges neighboring sparse networks so as to enlarge the number of observed edges and reduce estimation variance, whereas the estimation bias introduced by network merging is controlled by exploiting local temporal structures for adaptive network neighborhood. A projected gradient descent algorithm is proposed to facilitate estimation, where the upper bound of the estimation error in each iteration is established. A thorough analysis is conducted to quantify the asymptotic behavior of the proposed method, which shows that it can significantly reduce the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.07484</link><description>&lt;p&gt;
&#24102;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#65306;&#22522;&#20110;&#22238;&#24402;&#30340;&#27169;&#22359;&#21270;Lagrangian&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Packing and Covering Constraints: A Modular Lagrangian Approach via Regression. (arXiv:2211.07484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#24102;&#26377;&#36164;&#28304;&#32447;&#24615;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#36739;&#20302;&#30340;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#31639;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#31639;&#27861;&#22312;&#24635;&#28040;&#36153;&#30340;&#32447;&#24615;&#32422;&#26463;&#19979;&#20351;&#29992;&#22810;&#20010;&#36164;&#28304;&#12290;&#36825;&#20010;&#38382;&#39064;&#25512;&#24191;&#20102;&#24102;&#32972;&#21253;&#30340;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#38382;&#39064;(CBwK)&#65292;&#20801;&#35768;&#35013;&#36733;&#21644;&#35206;&#30422;&#32422;&#26463;&#65292;&#20197;&#21450;&#27491;&#36127;&#36164;&#28304;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31616;&#21333;&#12289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#33021;&#22815;&#23454;&#29616;&#36864;&#21270;&#30340;&#21518;&#24724;&#12290;&#24403;&#26576;&#20123;&#32422;&#26463;&#34987;&#36829;&#21453;&#26102;&#65292;&#23545;&#20110;CBwK&#65292;&#23427;&#22312;&#32479;&#35745;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;LagrangianBwK(Immorlica&#31561;&#20154;&#65292;FOCS 2019)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;CBwK&#30340;Lagrangian&#25216;&#26415;&#65292;&#20197;&#21450;SquareCB(Foster&#21644;Rakhlin&#65292;ICML 2020)&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#19978;&#19979;&#25991;&#24184;&#23384;&#32773;&#30340;&#22238;&#24402;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26412;&#36136;&#19978;&#30340;&#27169;&#22359;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of contextual bandits in which the algorithm consumes multiple resources subject to linear constraints on total consumption. This problem generalizes contextual bandits with knapsacks (CBwK), allowing for packing and covering constraints, as well as positive and negative resource consumption. We present a new algorithm that is simple, computationally efficient, and admits vanishing regret. It is statistically optimal for CBwK when an algorithm must stop once some constraint is violated. Our algorithm builds on LagrangeBwK (Immorlica et al., FOCS 2019) , a Lagrangian-based technique for CBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based technique for contextual bandits. Our analysis leverages the inherent modularity of both techniques.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2210.15304</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explaining the Explainers in Graph Neural Networks: a Comparative Study. (arXiv:2210.15304v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15304
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21313;&#31181;&#35299;&#37322;&#22120;&#30340;&#34920;&#29616;&#65292;&#25552;&#20379;&#20102;&#19981;&#21516;GNN&#20307;&#31995;&#32467;&#26500;&#26131;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#21518;&#65292;GNN&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24212;&#29992;&#24191;&#27867;&#65292;&#36825;&#20419;&#20351;&#38656;&#35201;&#26041;&#27861;&#26469;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;GNN&#35299;&#37322;&#22120;&#24320;&#22987;&#20986;&#29616;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#65292;&#19968;&#20123;&#26159;&#26032;&#39062;&#30340;&#65292;&#19968;&#20123;&#26159;&#20174;&#20854;&#20182;&#39046;&#22495;&#25913;&#32534;&#32780;&#26469;&#30340;&#12290;&#20026;&#20102;&#25972;&#29702;&#36825;&#31181;&#28023;&#37327;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#21508;&#31181;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#26041;&#38754;&#23545;&#19981;&#21516;&#30340;&#35299;&#37322;&#22120;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#24037;&#20316;&#27809;&#26377;&#23581;&#35797;&#25552;&#20379;&#20851;&#20110;&#19981;&#21516;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#26356;&#25110;&#19981;&#26131;&#35299;&#37322;&#30340;&#27934;&#23519;&#65292;&#20063;&#27809;&#26377;&#35828;&#26126;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#35299;&#37322;&#22120;&#12290;&#22312;&#26412;&#27425;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#31995;&#32479;&#24615;&#23454;&#39564;&#30740;&#31350;&#65292;&#23545;&#20843;&#20010;&#20195;&#34920;&#24615;&#20307;&#31995;&#32467;&#26500;&#19978;&#35757;&#32451;&#30340;&#21313;&#31181;&#35299;&#37322;&#22120;&#22312;&#20845;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22270;&#21644;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22635;&#34917;&#20102;&#36825;&#20123;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following a fast initial breakthrough in graph based learning, Graph Neural Networks (GNNs) have reached a widespread application in many science and engineering fields, prompting the need for methods to understand their decision process.  GNN explainers have started to emerge in recent years, with a multitude of methods both novel or adapted from other domains. To sort out this plethora of alternative approaches, several studies have benchmarked the performance of different explainers in terms of various explainability metrics. However, these earlier works make no attempts at providing insights into why different GNN architectures are more or less explainable, or which explainer should be preferred in a given setting.  In this survey, we fill these gaps by devising a systematic experimental study, which tests ten explainers on eight representative architectures trained on six carefully designed graph and node classification datasets. With our results we provide key insights on the cho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23398;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#20248;&#20110;&#30446;&#21069;&#26368;&#20248;&#26041;&#27861;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.08907</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23398;&#20064;&#31526;&#21495;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Symbolic Model-Agnostic Loss Functions via Meta-Learning. (arXiv:2209.08907v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23545;&#22810;&#20010;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23398;&#21040;&#30340;&#25439;&#22833;&#20989;&#25968;&#20248;&#20110;&#30446;&#21069;&#26368;&#20248;&#26041;&#27861;&#21644;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#30340;&#26032;&#20852;&#20027;&#39064;&#65292;&#26088;&#22312;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#28151;&#21512;&#31070;&#32463;&#31526;&#21495;&#25628;&#32034;&#26041;&#27861;&#23398;&#20064;&#27169;&#22411;&#26080;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#36827;&#21270;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#23398;&#25805;&#20316;&#31354;&#38388;&#20013;&#25628;&#32034;&#31526;&#21495;&#25439;&#22833;&#20989;&#25968;&#30340;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#23398;&#20064;&#21040;&#30340;&#19968;&#32452;&#25439;&#22833;&#20989;&#25968;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#26799;&#24230;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#19978;&#24471;&#21040;&#20102;&#32463;&#39564;&#35777;&#23454;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26032;&#25552;&#20986;&#30340;&#26041;&#27861;&#21457;&#29616;&#30340;&#20803;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#22312;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop upon the emerging topic of loss function learning, which aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for learning model-agnostic loss functions via a hybrid neuro-symbolic search approach. The framework first uses evolution-based methods to search the space of primitive mathematical operations to find a set of symbolic loss functions. Second, the set of learned loss functions are subsequently parameterized and optimized via an end-to-end gradient-based training procedure. The versatility of the proposed framework is empirically validated on a diverse set of supervised learning tasks. Results show that the meta-learned loss functions discovered by the newly proposed method outperform both the cross-entropy loss and state-of-the-art loss function learning methods on a diverse range of neural network architectures and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2208.13065</link><description>&lt;p&gt;
&#25913;&#21892;&#36816;&#33829;&#32463;&#27982;&#23398;&#65306;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#26469;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#30340;&#25805;&#20316;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment. (arXiv:2208.13065v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#22312;&#24320;&#29615;&#39044;&#27979;&#20248;&#21270;&#36807;&#31243;&#20013;&#36827;&#34892;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#65306;&#39318;&#20808;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;(RES)&#30340;&#21487;&#29992;&#24615;&#21644;&#31995;&#32479;&#20648;&#22791;&#38656;&#27714;&#65307;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#35299;&#20915;&#35832;&#22914;&#26426;&#32452;&#32452;&#21512;(UC)&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#30456;&#24212;&#30340;&#32463;&#27982;&#36816;&#34892;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#29615;&#36807;&#31243;&#21487;&#33021;&#20250;&#23454;&#36136;&#24615;&#22320;&#25439;&#23475;&#25805;&#20316;&#32463;&#27982;&#24615;&#65292;&#22240;&#20026;&#23427;&#30340;&#39044;&#27979;&#22120;&#30446;&#20809;&#30701;&#27973;&#22320;&#23547;&#27714;&#25913;&#21892;&#21363;&#26102;&#30340;&#32479;&#35745;&#39044;&#27979;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#26368;&#32456;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#19968;&#31181;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#20197;&#25913;&#21892;&#25805;&#20316;&#32463;&#27982;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#21452;&#23618;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#38024;&#23545;&#26368;&#20339;&#31995;&#32479;&#25805;&#20316;&#35757;&#32451;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#12290;&#19978;&#23618;&#22522;&#20110;&#20854;&#24341;&#36215;&#30340;&#25805;&#20316;&#25104;&#26412;&#26469;&#35757;&#32451; RES &#21644;&#20648;&#22791;&#39044;&#27979;&#22120;&#65307;&#19979;&#23618;&#21017;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340; RES &#21644;&#20648;&#22791;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#25454;&#26368;&#20339;&#25805;&#20316;&#21407;&#21017;&#27714;&#35299; UC&#12290;&#36825;&#20004;&#20010;&#23618;&#32423;&#36890;&#36807;&#21453;&#39304;&#29615;&#36335;&#36827;&#34892;&#20132;&#20114;&#24615;&#20114;&#21160;&#65292;&#30452;&#21040;&#25910;&#25947;&#20026;&#27490;&#12290;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 24-bus&#31995;&#32479;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; UC &#22522;&#20934;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, system operators conduct the economic operation of power systems in an open-loop predict-then-optimize process: the renewable energy source (RES) availability and system reserve requirements are first predicted; given the predictions, system operators solve optimization models such as unit commitment (UC) to determine the economical operation plans accordingly. However, such an open-loop process could essentially compromise the operation economics because its predictors myopically seek to improve the immediate statistical prediction errors instead of the ultimate operation cost. To this end, this paper presents a closed-loop predict-and-optimize framework, offering a prescriptive UC to improve the operation economics. First, a bilevel mixed-integer programming model is leveraged to train cost-oriented predictors tailored for optimal system operations: the upper level trains the RES and reserve predictors based on their induced operation cost; the lower level, with given pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2202.08465</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#30340;&#22238;&#35793;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#65292;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#38388;&#31163;&#25955;&#23646;&#24615;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#31471;&#21040;&#31471;&#24335;&#30340;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#35793;&#26159;&#19968;&#31181;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#39044;&#20808;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#32763;&#35793;&#21333;&#35821;&#21477;&#23376;&#24182;&#29983;&#25104;&#21512;&#25104;&#30340;&#21452;&#35821;&#21477;&#23545;&#20197;&#35757;&#32451;&#21478;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#23558;&#20004;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20998;&#21035;&#29702;&#35299;&#20026;&#25512;&#29702;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#22521;&#35757;&#26694;&#26550;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32763;&#35793;&#21477;&#23376;&#30340;&#31163;&#25955;&#23646;&#24615;&#20351;&#24471;&#26799;&#24230;&#20449;&#24687;&#26080;&#27861;&#22312;&#20004;&#20010;NMT&#27169;&#22411;&#20043;&#38388;&#27969;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#65292;&#20351;&#24471;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#21487;&#24494;&#20998;&#30340;&#21477;&#23376;&#65292;&#20351;&#24471;VAE&#30340;&#35757;&#32451;&#26694;&#26550;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#26041;&#24335;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;NMT&#27169;&#22411;&#65292;&#24182;&#22312;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#27604;&#20197;&#21069;&#22522;&#20934;&#27979;&#35797;&#26356;&#22909;&#30340;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;</title><link>http://arxiv.org/abs/2106.02797</link><description>&lt;p&gt;
&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;(DSC)&#26159;&#22312;&#27809;&#26377;&#30456;&#20114;&#20851;&#32852;&#30340;&#36793;&#38469;&#20449;&#24687;&#21487;&#20379;&#35299;&#30721;&#22120;&#20351;&#29992;&#30340;&#24773;&#20917;&#19979;&#23545;&#36755;&#20837;&#36827;&#34892;&#32534;&#30721;&#30340;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Slepian&#21644;Wolf&#22312;1973&#24180;&#35777;&#26126;&#65292;&#27809;&#26377;&#35775;&#38382;&#36793;&#38469;&#20449;&#24687;&#30340;&#32534;&#30721;&#22120;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#19982;&#36793;&#38469;&#20449;&#24687;&#21487;&#29992;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#21387;&#32553;&#29575;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#20294;&#23454;&#36341;&#20013;&#30340;DSC&#19968;&#30452;&#23616;&#38480;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29305;&#23450;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#30456;&#20851;&#32467;&#26500;&#19981;&#21487;&#30693;&#19988;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#30340;&#26377;&#25439;DSC&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#28304;&#27169;&#22411;&#65292;&#32780;&#26159;&#21033;&#29992;&#26465;&#20214;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VQ-VAE)&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed source coding (DSC) is the task of encoding an input in the absence of correlated side information that is only available to the decoder. Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to the side information can asymptotically achieve the same compression rate as when the side information is available to it. While there is vast prior work on this topic, practical DSC has been limited to synthetic datasets and specific correlation structures. Here we present a framework for lossy DSC that is agnostic to the correlation structure and can scale to high dimensions. Rather than relying on hand-crafted source modeling, our method utilizes a conditional Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed encoder and decoder. We evaluate our method on multiple datasets and show that our method can handle complex correlations and achieves state-of-the-art PSNR.
&lt;/p&gt;</description></item></channel></rss>