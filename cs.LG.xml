<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;</title><link>https://arxiv.org/abs/2403.18699</link><description>&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#38170;&#28857;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CLOA&#65289;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning with Orthonormal Anchors (CLOA)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#31216;&#20026;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#35299;&#20915;&#23545;&#27604;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#26816;&#26597;InfoNCE&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#34920;&#29616;&#20986;&#38480;&#21046;&#24615;&#34892;&#20026;&#65292;&#23548;&#33268;&#23884;&#20837;&#36235;&#20110;&#34701;&#21512;&#20026;&#19968;&#20010;&#22855;&#24322;&#28857;&#30340;&#25910;&#25947;&#29616;&#35937;&#12290;&#36825;&#31181;&#8220;&#36807;&#24230;&#34701;&#21512;&#8221;&#25928;&#24212;&#23545;&#21518;&#32493;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23884;&#20837;&#22312;&#31561;&#20110;&#25110;&#23616;&#38480;&#20110;&#31209;-1&#32447;&#24615;&#23376;&#31354;&#38388;&#26102;&#34920;&#31034;InfoNCE&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#19982;&#24494;&#35843;&#38454;&#27573;&#20856;&#22411;&#20351;&#29992;&#30340;&#30456;&#21516;&#25110;&#26356;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#27491;&#20132;&#38170;&#28857;&#22238;&#24402;&#25439;&#22833;&#65292;&#26088;&#22312;&#35299;&#24320;&#23884;&#20837;&#32858;&#31867;&#65292;&#26174;&#33879;&#22686;&#24378;&#27599;&#20010;&#23884;&#20837;&#30340;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18699v1 Announce Type: cross  Abstract: This study focuses on addressing the instability issues prevalent in contrastive learning, specifically examining the InfoNCE loss function and its derivatives. We reveal a critical observation that these loss functions exhibit a restrictive behavior, leading to a convergence phenomenon where embeddings tend to merge into a singular point. This "over-fusion" effect detrimentally affects classification accuracy in subsequent supervised-learning tasks. Through theoretical analysis, we demonstrate that embeddings, when equalized or confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In response to this challenge, our research introduces an innovative strategy that leverages the same or fewer labeled data than typically used in the fine-tuning phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to disentangle embedding clusters, significantly enhancing the distinctiveness of each embedding 
&lt;/p&gt;</description></item><item><title>TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.18681</link><description>&lt;p&gt;
TransFusion&#65306;&#20855;&#26377;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransFusion: Contrastive Learning with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18681
&lt;/p&gt;
&lt;p&gt;
TransFusion&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23450;&#20041;&#20102;&#23545;&#27604;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#20197;&#25913;&#21892;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;TransFusion&#65292;&#26088;&#22312;&#20351;&#23545;&#27604;&#23398;&#20064;&#30340;&#36807;&#31243;&#26356;&#20855;&#20998;&#26512;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290; TransFusion&#30001;&#27880;&#24847;&#21147;&#22359;&#32452;&#25104;&#65292;&#20854;&#20013;&#30340;softmax&#34987;&#26367;&#25442;&#20026;ReLU&#65292;&#24182;&#19988;&#20854;&#26368;&#32456;&#22359;&#30340;&#21152;&#26435;&#21644;&#25805;&#20316;&#34987;&#25130;&#26029;&#65292;&#20197;&#20351;&#37051;&#25509;&#30697;&#38453;&#25104;&#20026;&#36755;&#20986;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#19982;&#30446;&#26631;&#20851;&#32852;&#30697;&#38453;&#20043;&#38388;&#30340;Jensen-Shannon&#25955;&#24230;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#30697;&#38453;&#25351;&#31034;&#27599;&#23545;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#25110;&#19981;&#21516;&#31867;&#21035;&#12290; TransFusion&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23450;&#20041;&#20102;&#22238;&#31572;&#35813;&#39046;&#22495;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#29702;&#35770;&#26497;&#38480;&#65306;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#22823;&#32423;&#21035;&#21644;&#26377;&#25928;&#23545;&#27604;&#23398;&#20064;&#25152;&#38656;&#30340;&#26368;&#23567;&#25209;&#37327;&#22823;&#23567;&#12290; &#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TransFusion&#25104;&#21151;&#22320;&#25552;&#21462;&#20986;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#20998;&#31163;&#38598;&#32676;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18681v1 Announce Type: cross  Abstract: This paper proposes a novel framework, TransFusion, designed to make the process of contrastive learning more analytical and explainable. TransFusion consists of attention blocks whose softmax being replaced by ReLU, and its final block's weighted-sum operation is truncated to leave the adjacency matrix as the output. The model is trained by minimizing the Jensen-Shannon Divergence between its output and the target affinity matrix, which indicates whether each pair of samples belongs to the same or different classes. The main contribution of TransFusion lies in defining a theoretical limit for answering two fundamental questions in the field: the maximum level of data augmentation and the minimum batch size required for effective contrastive learning. Furthermore, experimental results indicate that TransFusion successfully extracts features that isolate clusters from complex real-world data, leading to improved classification accuracy 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17364</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;LQR&#20803;&#31574;&#30053;&#20272;&#35745;&#30340;Moreau&#21253;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Moreau Envelope Approach for LQR Meta-Policy Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#21487;&#26377;&#25928;&#35843;&#25972;&#21040;&#26032;&#23454;&#29616;&#30340;&#20803;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#20102;&#25214;&#21040;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#26102;&#19981;&#21464;&#31163;&#25955;&#26102;&#38388;&#19981;&#30830;&#23450;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#31574;&#30053;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Moreau&#21253;&#32476;&#30340;&#26367;&#20195;LQR&#25104;&#26412;&#65292;&#30001;&#19981;&#30830;&#23450;&#31995;&#32479;&#30340;&#26377;&#38480;&#23454;&#29616;&#26500;&#24314;&#65292;&#20197;&#23450;&#20041;&#19968;&#20010;&#23545;&#26032;&#23454;&#29616;&#26377;&#25928;&#35843;&#25972;&#30340;&#20803;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#25214;&#21040;&#20803;LQR&#25104;&#26412;&#20989;&#25968;&#30340;&#36817;&#20284;&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#23454;&#29616;&#30340;&#32447;&#24615;&#31995;&#32479;&#19978;&#32988;&#36807;&#20102;&#25511;&#21046;&#22120;&#30340;&#26420;&#32032;&#24179;&#22343;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#27169;&#22411;&#19981;&#21487;&#30693;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17364v1 Announce Type: cross  Abstract: We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17285</link><description>&lt;p&gt;
&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24448;&#36820;&#35774;&#35745;&#36827;&#34892;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Switchback Designs in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;A/B&#27979;&#35797;&#20013;&#24448;&#36820;&#35774;&#35745;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#20123;&#35774;&#35745;&#38543;&#26102;&#38388;&#22312;&#22522;&#20934;&#21644;&#26032;&#31574;&#30053;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35774;&#35745;&#23545;&#20854;&#20135;&#29983;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#22120;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#36825;&#20123;ATE&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#29615;&#22659;&#20013;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(i) &#24403;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#21576;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#30340;&#20132;&#26367;&#35774;&#35745;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#30340;&#39057;&#29575;&#24448;&#24448;&#20250;&#38477;&#20302;ATE&#20272;&#35745;&#22120;&#30340;MSE&#12290;(ii) &#28982;&#32780;&#65292;&#24403;&#35823;&#24046;&#19981;&#30456;&#20851;&#26102;&#65292;&#25152;&#26377;&#36825;&#20123;&#35774;&#35745;&#21464;&#24471;&#28176;&#36817;&#31561;&#25928;&#12290;(iii) &#22312;&#22823;&#22810;&#25968;&#35823;&#24046;&#20026;&#36127;&#30456;&#20851;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17285v1 Announce Type: cross  Abstract: This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel "weak signal analysis" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlate
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;</title><link>https://arxiv.org/abs/2403.13164</link><description>&lt;p&gt;
VL-ICL Bench: &#22522;&#20110;&#32454;&#33410;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32454;&#33410;&#20043;&#39764;
&lt;/p&gt;
&lt;p&gt;
VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13164
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#33879;&#21517;&#30340;&#20986;&#29616;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#32780;&#38395;&#21517;&#8212;&#8212;&#21363;&#22312;&#20165;&#25552;&#20379;&#20960;&#20010;&#31034;&#20363;&#20316;&#20026;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#26500;&#24314;&#22312;LLMs&#20043;&#19978;&#30340;&#35270;&#35273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;\emph{&#22810;&#27169;&#24577;ICL}&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#19978;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#20108;&#32773;&#26082;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;ICL&#30340;&#20248;&#21183;&#65292;&#20063;&#27809;&#26377;&#27979;&#35797;&#20854;&#38480;&#21046;&#12290;&#23545;&#22810;&#27169;&#24577;ICL&#30340;&#26356;&#24191;&#27867;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797; VL-ICL Bench&#65292;&#28085;&#30422;&#20102;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#65292;&#24182;&#28085;&#30422;&#20102;&#20174;{&#24863;&#30693;&#21040;&#25512;&#29702;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#38271;&#24230;}&#30340;&#19981;&#21516;&#31867;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13164v1 Announce Type: new  Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12025</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20581;&#24247;&#20844;&#24179;&#21361;&#23475;&#21644;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#36827;&#34892;&#20102;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#20197;&#21450;EquityMedQA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#30528;&#20026;&#22797;&#26434;&#30340;&#20581;&#24247;&#20449;&#24687;&#38656;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#21516;&#26102;&#20063;&#26377;&#21487;&#33021;&#24341;&#20837;&#21361;&#23475;&#24182;&#21152;&#21095;&#20581;&#24247;&#19981;&#24179;&#31561;&#12290;&#21487;&#38752;&#22320;&#35780;&#20272;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#27169;&#22411;&#22833;&#28789;&#26159;&#21457;&#23637;&#20419;&#36827;&#20581;&#24247;&#20844;&#24179;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#25581;&#31034;&#21487;&#33021;&#23548;&#33268;LLM&#29983;&#25104;&#30340;&#38271;&#31687;&#31572;&#26696;&#20013;&#30340;&#20844;&#24179;&#30456;&#20851;&#21361;&#23475;&#30340;&#20559;&#35265;&#30340;&#36164;&#28304;&#21644;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;Med-PaLM 2&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#35777;&#26696;&#20363;&#30740;&#31350;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22312;&#35813;&#39046;&#22495;&#36827;&#34892;&#30340;&#26368;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#29992;&#20110;&#20154;&#31867;&#35780;&#20272;LLM&#29983;&#25104;&#31572;&#26696;&#20559;&#35265;&#30340;&#22810;&#22240;&#23376;&#26694;&#26550;&#65292;&#20197;&#21450;EquityMedQA&#65292;&#19968;&#20010;&#21253;&#21547;&#19971;&#20010;&#26032;&#21457;&#24067;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#65292;&#20854;&#20013;&#26082;&#21253;&#25324;&#25163;&#21160;&#31574;&#21010;&#21448;&#21253;&#25324;LLM&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20016;&#23500;&#20102;&#23545;&#25239;&#24615;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#20154;&#31867;&#35780;&#20272;&#26694;&#26550;&#21644;&#25968;&#25454;&#38598;&#35774;&#35745;&#36807;&#31243;&#37117;&#26681;&#26893;&#20110;&#23454;&#38469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12025v1 Announce Type: cross  Abstract: Large language models (LLMs) hold immense promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2, resulting in the largest human evaluation study in this area to date. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries. Both our human assessment framework and dataset design process are grounde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07917</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#20027;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural-Evolutionary Algorithm for Autonomous Transit Network Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07917
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26159;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#22909;&#22788;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#36335;&#32447;&#32593;&#32476;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#26500;&#24314;&#36335;&#32447;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#22810;&#20010;&#21464;&#24322;&#25805;&#20316;&#31526;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#35780;&#20272;&#36825;&#31181;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#29616;&#23454;&#22522;&#20934;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#27604;&#21333;&#29420;&#23398;&#20064;&#30340;&#31574;&#30053;&#39640;&#20986;&#39640;&#36798;20\%&#65292;&#27604;&#31616;&#21333;&#30340;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#39640;&#20986;&#39640;&#36798;53%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06259</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Conceptual Knowledge for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#33021;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#23450;&#20041;&#65292;&#20294;&#20063;&#21487;&#33021;&#36896;&#25104;LLMs&#20013;&#30456;&#20851;&#23454;&#20363;&#30693;&#35782;&#30340;&#25197;&#26354;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#32534;&#36753;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21644;&#35780;&#20272;&#20165;&#25506;&#35752;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#28982;&#32780;LLMs&#26159;&#21542;&#20855;&#26377;&#20462;&#25913;&#27010;&#24565;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#20026;LLMs&#32534;&#36753;&#27010;&#24565;&#30693;&#35782;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;ConceptEdit&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20462;&#25913;&#27010;&#24565;&#32423;&#21035;&#30340;&#23450;&#20041;&#65292;&#20294;&#23427;&#20204;&#20063;&#26377;&#28508;&#21147;&#25197;&#26354;LLMs&#20013;&#30456;&#20851;&#30340;&#23454;&#20363;&#30693;&#35782;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#21487;&#20197;&#28608;&#21457;&#23545;&#26356;&#22909;&#29702;&#35299;LLMs&#30340;&#36827;&#19968;&#27493;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20027;&#39029;&#20301;&#20110;https://zjunlp.github.io/project/ConceptEdit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06259v1 Announce Type: cross  Abstract: Recently, there has been a growing interest in knowledge editing for Large Language Models (LLMs). Current approaches and evaluations merely explore the instance-level editing, while whether LLMs possess the capability to modify concepts remains unclear. This paper pioneers the investigation of editing conceptual knowledge for LLMs, by constructing a novel benchmark dataset ConceptEdit and establishing a suite of new metrics for evaluation. The experimental results reveal that, although existing editing methods can efficiently modify concept-level definition to some extent, they also have the potential to distort the related instantial knowledge in LLMs, leading to poor performance. We anticipate this can inspire further progress in better understanding LLMs. Our project homepage is available at https://zjunlp.github.io/project/ConceptEdit.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01919</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20248;&#21270;&#21644;&#21015;&#23376;&#38598;&#36873;&#25321;&#30340;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Convex Optimization and Column Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27599;&#19968;&#27493;&#20013;&#35299;&#20915;&#19968;&#20010;&#20984;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#25105;&#20204;&#30340;&#21015;&#36873;&#25321;&#30697;&#38453;&#23436;&#25104;&#65288;CSMC&#65289;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#27599;&#31181;&#31639;&#27861;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#22312;&#20998;&#26512;&#20013;&#25105;&#20204;&#38416;&#26126;&#20102;&#24517;&#35201;&#30340;&#20551;&#35774;&#21644;&#25214;&#21040;&#27491;&#30830;&#35299;&#30340;&#27010;&#29575;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;&#30697;&#38453;&#22823;&#23567;&#12289;&#31209;&#21644;&#32570;&#22833;&#20803;&#32032;&#27604;&#20363;&#23545;&#35299;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#20272;&#35745;&#21644;&#21435;&#20559;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#65292;&#25903;&#25345;&#29702;&#35770;&#30340;&#20223;&#30495;&#30740;&#31350;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.01318</link><description>&lt;p&gt;
&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#65306;&#20197;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#21017;&#21270;&#20272;&#35745;&#21644;&#21435;&#20559;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#65292;&#25903;&#25345;&#29702;&#35770;&#30340;&#20223;&#30495;&#30740;&#31350;&#65292;&#24182;&#22312;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#25991;&#26412;&#20998;&#26512;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#31038;&#20132;&#23186;&#20307;&#30149;&#27602;&#24086;&#23376;&#30340;&#28857;&#36190;&#20998;&#24067;&#65288;&#22914;&#28857;&#36190;&#25968;&#37327;&#65289;&#32463;&#39564;&#24615;&#24130;&#24459;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#32500;&#23614;&#25351;&#25968;&#22238;&#24402;&#21450;&#20854;&#21442;&#25968;&#30340;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#20559;&#27491;&#21017;&#21270;&#20272;&#35745;&#65292;&#35777;&#26126;&#20102;&#21435;&#20559;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#20223;&#30495;&#30740;&#31350;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#23545;&#28041;&#21450; LGBTQ+ &#35805;&#39064;&#30340; X&#65288;&#21407; Twitter&#65289;&#30149;&#27602;&#24086;&#23376;&#30340;&#25991;&#26412;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01318v1 Announce Type: cross  Abstract: Motivated by the empirical power law of the distributions of credits (e.g., the number of "likes") of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. Simulation studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.
&lt;/p&gt;</description></item><item><title>SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00991</link><description>&lt;p&gt;
SELFI: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#20197;&#36827;&#34892;&#31038;&#20132;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00991
&lt;/p&gt;
&lt;p&gt;
SELFI&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#19982;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24555;&#36895;&#25913;&#36827;&#65292;&#24182;&#22312;&#36991;&#25758;&#21644;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33258;&#25105;&#25913;&#36827;&#30340;&#26426;&#22120;&#20154;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#21644;&#32463;&#39564;&#31215;&#32047;&#26469;&#23454;&#29616;&#23558;&#26159;&#26426;&#22120;&#20154;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25237;&#20837;&#20351;&#29992;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;SELFI&#65292;&#21033;&#29992;&#22312;&#32447;&#26426;&#22120;&#20154;&#32463;&#39564;&#26469;&#24555;&#36895;&#39640;&#25928;&#22320;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;SELFI&#23558;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#31163;&#32447;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#20043;&#19978;&#65292;&#20197;&#21457;&#25381;&#36825;&#20004;&#31181;&#23398;&#20064;&#33539;&#24335;&#30340;&#20248;&#28857;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SELFI&#36890;&#36807;&#23558;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#23398;&#20064;&#30446;&#26631;&#19982;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#30456;&#32467;&#21512;&#65292;&#31283;&#23450;&#20102;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#29616;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;SELFI&#65292;&#24182;&#25253;&#21578;&#20102;&#22312;&#36991;&#25758;&#26041;&#38754;&#30340;&#25913;&#21892;&#65292;&#20197;&#21450;&#36890;&#36807;&#20154;&#31867;&#29992;&#25143;&#30740;&#31350;&#27979;&#37327;&#30340;&#26356;&#20855;&#31038;&#20132;&#36981;&#20174;&#34892;&#20026;&#12290;SELFI&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26377;&#29992;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#20943;&#23569;&#20102;&#39044;&#20808;&#24178;&#39044;&#30340;&#20154;&#21592;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00991v1 Announce Type: cross  Abstract: Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.16017</link><description>&lt;p&gt;
&#38544;&#24335;&#32447;&#24615;&#23618;&#30340;&#39057;&#35889;&#25552;&#21462;&#21644;&#35009;&#21098;
&lt;/p&gt;
&lt;p&gt;
Spectrum Extraction and Clipping for Implicitly Linear Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16017
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#33258;&#21160;&#24494;&#20998;&#22312;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#39057;&#35889;&#20013;&#30340;&#26377;&#25928;&#24615;&#65307;&#25552;&#20379;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65307;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#32452;&#21512;&#30340;&#25928;&#26524;&#65307;&#36890;&#36807;&#27604;&#36739;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#34920;&#26126;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#21160;&#24494;&#20998;&#22312;&#39640;&#25928;&#20934;&#30830;&#35745;&#31639;&#21644;&#25511;&#21046;&#38544;&#24335;&#32447;&#24615;&#31639;&#23376;&#30340;&#39057;&#35889;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#19968;&#31867;&#21253;&#25324;&#25152;&#26377;&#26631;&#20934;&#21367;&#31215;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20016;&#23500;&#23618;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#19968;&#33324;&#21367;&#31215;&#23618;&#30340;&#35009;&#21098;&#26041;&#27861;&#65292;&#24182;&#38416;&#26126;&#20102;&#23548;&#33268;&#20043;&#21069;&#24037;&#20316;&#20013;&#27491;&#30830;&#24615;&#38382;&#39064;&#30340;&#34920;&#31034;&#38480;&#21046;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#24402;&#19968;&#21270;&#23618;&#19982;&#21367;&#31215;&#23618;&#20018;&#32852;&#26102;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#27861;&#22914;&#20309;&#24212;&#29992;&#20110;&#23427;&#20204;&#30340;&#32452;&#21512;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#31934;&#24230;&#21644;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#26356;&#31934;&#30830;&#21644;&#39640;&#25928;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#20195;&#30721;&#38142;&#25509;https://github.com/Ali-E/FastClip&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16017v1 Announce Type: new  Abstract: We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15929</link><description>&lt;p&gt;
QuaCer-C&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#23450;&#37327;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
QuaCer-C: Quantitative Certification of Knowledge Comprehension in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#29992;&#20110;&#27491;&#24335;&#35748;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#29702;&#35299;&#30340;&#33021;&#21147;&#65292;&#35777;&#20070;&#23450;&#37327;&#21270;&#19988;&#21253;&#21547;&#39640;&#32622;&#20449;&#24230;&#30340;&#27010;&#29575;&#30028;&#38480;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#25552;&#39640;&#65292;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#24182;&#26410;&#23545;LLMs&#30340;&#34920;&#29616;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35748;&#35777;&#26694;&#26550;QuaCer-C&#65292;&#25105;&#20204;&#22312;&#27492;&#23545;&#30693;&#21517;LLMs&#30340;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#27491;&#24335;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#35777;&#20070;&#26159;&#23450;&#37327;&#30340; - &#23427;&#20204;&#21253;&#25324;&#23545;&#30446;&#26631;LLM&#22312;&#20219;&#20309;&#30456;&#20851;&#30693;&#35782;&#29702;&#35299;&#25552;&#31034;&#19978;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#30340;&#27010;&#29575;&#30340;&#39640;&#32622;&#20449;&#24230;&#32039;&#23494;&#30028;&#38480;&#12290;&#25105;&#20204;&#38024;&#23545;Llama&#12289;Vicuna&#21644;Mistral LLMs&#30340;&#35777;&#20070;&#34920;&#26126;&#65292;&#30693;&#35782;&#29702;&#35299;&#33021;&#21147;&#38543;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#65292;&#24182;&#19988;Mistral&#27169;&#22411;&#22312;&#36825;&#19968;&#35780;&#20272;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15929v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive performance on several benchmarks. However, traditional studies do not provide formal guarantees on the performance of LLMs. In this work, we propose a novel certification framework for LLM, QuaCer-C, wherein we formally certify the knowledge-comprehension capabilities of popular LLMs. Our certificates are quantitative - they consist of high-confidence, tight bounds on the probability that the target LLM gives the correct answer on any relevant knowledge comprehension prompt. Our certificates for the Llama, Vicuna, and Mistral LLMs indicate that the knowledge comprehension capability improves with an increase in the number of parameters and that the Mistral model is less performant than the rest in this evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.15315</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#23567;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
On Minimal Depth in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#20851;&#20110;&#26368;&#23567;&#28145;&#24230;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26368;&#23567;&#28145;&#24230;&#19982;CPWL&#20989;&#25968;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;ReLU&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#19982;&#34920;&#31034;&#20219;&#20309;&#36830;&#32493;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#65288;CPWL&#65289;&#25152;&#38656;&#30340;&#26368;&#23567;&#28145;&#24230;&#30456;&#20851;&#30340;&#29468;&#24819;&#30340;&#20851;&#31995;&#36827;&#34892;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#29305;&#24615;&#12290;&#30740;&#31350;&#37325;&#28857;&#21253;&#25324;&#23545;&#27714;&#21644;&#21644;&#26368;&#22823;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#34920;&#31034;&#65292;&#20197;&#21450;&#23545;&#22810;&#38754;&#20307;&#31070;&#32463;&#32593;&#32476;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#27714;&#21644;&#36816;&#31639;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#25805;&#20316;&#25968;&#26368;&#23567;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#20197;&#25214;&#21040;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30456;&#21453;&#65292;&#20851;&#20110;&#26368;&#22823;&#36816;&#31639;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#20165;&#20381;&#36182;&#20110;&#25805;&#20316;&#25968;&#28145;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19981;&#20250;&#26263;&#31034;&#36816;&#31639;&#30340;&#26368;&#23567;&#28145;&#24230;&#12290;&#30740;&#31350;&#36824;&#32771;&#23519;&#20102;&#20984;CPWL&#20989;&#25968;&#20043;&#38388;&#30340;&#26368;&#23567;&#28145;&#24230;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15315v1 Announce Type: new  Abstract: A characterization of the representability of neural networks is relevant to comprehend their success in artificial intelligence. This study investigate two topics on ReLU neural network expressivity and their connection with a conjecture related to the minimum depth required for representing any continuous piecewise linear function (CPWL). The topics are the minimal depth representation of the sum and max operations, as well as the exploration of polytope neural networks. For the sum operation, we establish a sufficient condition on the minimal depth of the operands to find the minimal depth of the operation. In contrast, regarding the max operation, a comprehensive set of examples is presented, demonstrating that no sufficient conditions, depending solely on the depth of the operands, would imply a minimal depth for the operation. The study also examine the minimal depth relationship between convex CPWL functions. On polytope neural ne
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.11728</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#32034;&#36180;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#12289;&#24369;&#30417;&#30563;&#27169;&#22411;&#21644;&#24066;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#19978;&#24066;&#20844;&#21496;&#37325;&#35201;&#30340;&#23395;&#24230;&#20107;&#20214;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#65288;SMEs&#65289;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#20989;&#25968;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#30408;&#21033;&#24778;&#21916;&#21644;&#22238;&#25253;&#23545;&#25105;&#20204;&#30340;&#20048;&#35266;&#20027;&#20041;&#24230;&#37327;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#21644;Hugging Face&#19978;&#20844;&#24320;&#65288;&#36981;&#24490;CC BY 4.0&#35768;&#21487;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11728v1 Announce Type: new  Abstract: In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$</title><link>https://arxiv.org/abs/2402.11410</link><description>&lt;p&gt;
&#33719;&#24471;$2\sqrt{T}$&#21040;&#26657;&#20934;&#30340;&#22522;&#26412;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11410
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Blasiok&#31561;&#20154;[2023]&#25552;&#20986;&#20102;&#26657;&#20934;&#36317;&#31163;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65292;&#19982;&#39044;&#26399;&#30340;&#26657;&#20934;&#35823;&#24046;(ECE)&#19981;&#21516;&#65292;&#23427;&#26159;&#36830;&#32493;&#30340;&#12290;&#26368;&#36817;&#65292;Qiao&#21644;Zheng [2024]&#32473;&#20986;&#20102;&#19968;&#20010;&#38750;&#26500;&#36896;&#24615;&#30340;&#35770;&#35777;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#22312;&#32447;&#39044;&#27979;&#22120;&#30340;&#23384;&#22312;&#65292;&#35813;&#39044;&#27979;&#22120;&#21487;&#20197;&#22312;&#23545;&#25239;&#35774;&#32622;&#20013;&#33719;&#24471;$O(\sqrt{T})$&#30340;&#26657;&#20934;&#36317;&#31163;&#65292;&#32780;&#23545;&#20110;ECE&#26469;&#35828;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#20182;&#20204;&#23558;&#25214;&#21040;&#19968;&#31181;&#26126;&#30830;&#30340;&#12289;&#39640;&#25928;&#30340;&#31639;&#27861;&#20316;&#20026;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#26657;&#20934;&#36317;&#31163;&#35823;&#24046;&#26368;&#22810;&#20026;$2\sqrt{T}$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11410v1 Announce Type: new  Abstract: Blasiok et al. [2023] proposed distance to calibration as a natural measure of calibration error that unlike expected calibration error (ECE) is continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument establishing the existence of an online predictor that can obtain $O(\sqrt{T})$ distance to calibration in the adversarial setting, which is known to be impossible for ECE. They leave as an open problem finding an explicit, efficient algorithm. We resolve this problem and give an extremely simple, efficient, deterministic algorithm that obtains distance to calibration error at most $2\sqrt{T}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10353</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20559;&#24046;&#26657;&#20934;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#33268;&#21147;&#20110;&#31038;&#20250;&#20844;&#24179;&#30340;&#22266;&#26377;&#20559;&#24046;&#20462;&#27491;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#24378;&#35843;&#22266;&#26377;&#20559;&#24046;&#26657;&#20934;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;GPT-4&#29983;&#25104;&#30340;&#19968;&#32452;&#33258;&#21160;&#36873;&#21462;&#30340;&#26080;&#24847;&#20041;&#36755;&#20837;&#26469;&#25552;&#31034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#25506;&#27979;&#22266;&#26377;&#20559;&#24046;&#12290;&#21033;&#29992;&#20559;&#24046;&#21453;&#26144;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24046;&#24322;&#25439;&#22833;&#29992;&#20110;&#20559;&#24046;&#26657;&#20934;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#21442;&#25968;&#65288;&#24635;&#21442;&#25968;&#30340;0.1%&#65289;&#20197;&#26397;&#21521;&#30456;&#31561;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10353v1 Announce Type: new  Abstract: Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\%$ of total parameters) of LMs towards equal probabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09240</link><description>&lt;p&gt;
Switch EMA: &#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Switch EMA: A Free Lunch for Better Flatness and Sharpness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#25913;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#21442;&#25968;&#65292;&#21487;&#20197;&#24110;&#21161;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;SEMA&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65288;EMA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26435;&#37325;&#24179;&#22343;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20248;&#21270;&#20013;&#23398;&#20064;&#24179;&#22374;&#30340;&#26368;&#20248;&#35299;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27010;&#25324;&#33021;&#21147;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#23613;&#31649;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#24179;&#22343;&#26041;&#27861;&#21487;&#33021;&#38519;&#20837;&#26356;&#24046;&#30340;&#26368;&#32456;&#24615;&#33021;&#25110;&#38656;&#35201;&#39069;&#22806;&#30340;&#27979;&#35797;&#26102;&#38388;&#35745;&#31639;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#21363;&#22312;&#27599;&#20010;&#21608;&#26399;&#21518;&#23558;EMA&#21442;&#25968;&#20999;&#25442;&#22238;&#21407;&#22987;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;EMA&#30340;&#20805;&#20998;&#28508;&#21147;&#65292;&#34987;&#31216;&#20026;Switch EMA&#65288;SEMA&#65289;&#12290;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#37117;&#35777;&#26126;&#20102;SEMA&#21487;&#20197;&#24110;&#21161;DNNs&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#22374;&#24615;&#21644;&#38160;&#24230;&#20043;&#38388;&#24179;&#34913;&#30340;&#27010;&#25324;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#39564;&#35777;SEMA&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#36776;&#21035;&#24615;&#12289;&#29983;&#25104;&#24615;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#20998;&#21106;&#12289;&#22270;&#20687;&#29983;&#25104;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09240v1 Announce Type: new Abstract: Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness. To verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07781</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
IR-Aware ECO Timing Optimization Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26202;&#26399;&#38454;&#27573;&#30340;&#24037;&#31243;&#21464;&#26356;&#35746;&#21333;&#65288;ECOs&#65289;&#36890;&#36807;&#26368;&#23567;&#30340;&#35774;&#35745;&#20462;&#22797;&#26469;&#20174;&#36807;&#22810;&#30340;IR&#38477;&#20302;&#23548;&#33268;&#30340;&#26102;&#24207;&#20559;&#31227;&#20013;&#24674;&#22797;&#12290;&#26412;&#25991;&#23558;IR&#24863;&#30693;&#30340;&#26102;&#24207;&#20998;&#26512;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;ECO&#26102;&#24207;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#29702;&#35774;&#35745;&#21644;&#21151;&#32791;&#32593;&#26684;&#32508;&#21512;&#20043;&#21518;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#12290;&#23427;&#23558;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#65288;LR&#65289;&#25216;&#26415;&#34701;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#35757;&#32451;&#19968;&#20010;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;R-GCN&#65289;&#20195;&#29702;&#65292;&#25353;&#39034;&#24207;&#35843;&#25972;&#38376;&#23610;&#23544;&#20197;&#20462;&#22797;&#26102;&#24207;&#36829;&#35268;&#12290;R-GCN&#20195;&#29702;&#20248;&#20110;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;LR&#30340;&#31639;&#27861;&#65306;&#22312;&#24320;&#25918;&#24335;45nm&#24037;&#33402;&#20013;&#65292;&#23427;&#23558;&#24310;&#36831;-&#38754;&#31215;&#26435;&#34913;&#26354;&#32447;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#21521;&#24038;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#22312;&#31561;&#36136;&#37327;&#26102;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#25512;&#29702;&#65292;&#33410;&#30465;&#36816;&#34892;&#26102;&#38388;&#12290;RL&#27169;&#22411;&#21487;&#22312;&#26102;&#24207;&#35268;&#33539;&#38388;&#36716;&#31227;&#65292;&#24182;&#21487;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#24494;&#35843;&#22312;&#26410;&#35265;&#35774;&#35745;&#19978;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.06357</link><description>&lt;p&gt;
SpongeNet &#25915;&#20987;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28023;&#32501;&#26435;&#37325;&#20013;&#27602;
&lt;/p&gt;
&lt;p&gt;
The SpongeNet Attack: Sponge Weight Poisoning of Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SpongeNet &#30340;&#26032;&#22411;&#28023;&#32501;&#25915;&#20987;&#65292;&#36890;&#36807;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#65292;&#25104;&#21151;&#22686;&#21152;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#32780;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#25915;&#20987;&#26088;&#22312;&#22686;&#21152;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#29616;&#26377;&#30340;&#28023;&#32501;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#28023;&#32501;&#20013;&#27602;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#12290;&#28023;&#32501;&#31034;&#20363;&#21033;&#29992;&#28155;&#21152;&#21040;&#27169;&#22411;&#36755;&#20837;&#30340;&#25200;&#21160;&#26469;&#22686;&#21152;&#33021;&#37327;&#21644;&#24310;&#36831;&#65292;&#32780;&#28023;&#32501;&#20013;&#27602;&#21017;&#25913;&#21464;&#27169;&#22411;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#24341;&#21457;&#25512;&#29702;&#26102;&#30340;&#33021;&#37327;/&#24310;&#36831;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28023;&#32501;&#25915;&#20987;&#65292;&#31216;&#20026; SpongeNet&#12290;SpongeNet &#26159;&#31532;&#19968;&#20010;&#30452;&#25509;&#20316;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#28023;&#32501;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#28023;&#32501;&#20013;&#27602;&#65292;SpongeNet &#21487;&#20197;&#25104;&#21151;&#22686;&#21152;&#35270;&#35273;&#27169;&#22411;&#30340;&#33021;&#32791;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#19981;&#19987;&#38376;&#38024;&#23545;&#28023;&#32501;&#20013;&#27602;&#36827;&#34892;&#35843;&#25972;&#65288;&#21363;&#20943;&#23567;&#25209;&#24402;&#19968;&#21270;&#20559;&#24046;&#20540;&#65289;&#65292;&#21017;&#27602;&#23475;&#38450;&#24481;&#20250;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26174;&#31034;&#20986;&#28023;&#32501;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge attacks aim to increase the energy consumption and computation time of neural networks deployed on hardware accelerators. Existing sponge attacks can be performed during inference via sponge examples or during training via Sponge Poisoning. Sponge examples leverage perturbations added to the model's input to increase energy and latency, while Sponge Poisoning alters the objective function of a model to induce inference-time energy/latency effects.   In this work, we propose a novel sponge attack called SpongeNet. SpongeNet is the first sponge attack that is performed directly on the parameters of a pre-trained model. Our experiments show that SpongeNet can successfully increase the energy consumption of vision models with fewer samples required than Sponge Poisoning. Our experiments indicate that poisoning defenses are ineffective if not adjusted specifically for the defense against Sponge Poisoning (i.e., they decrease batch normalization bias values). Our work shows that Spong
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.06165</link><description>&lt;p&gt;
&#23398;&#20064;&#23545;&#27604;&#29305;&#24449;&#34920;&#31034;&#26469;&#36827;&#34892;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Contrastive Feature Representations for Facial Action Unit Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27491;&#26679;&#26412;&#25277;&#26679;&#21644;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#21644;AU&#31867;&#22411;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#26816;&#27979;&#30340;&#20027;&#35201;&#26041;&#27861;&#28041;&#21450;&#30417;&#30563;&#30340;&#22810;&#26631;&#31614;&#20108;&#36827;&#21046;&#20998;&#31867;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23545;AU&#30340;&#20687;&#32032;&#32423;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23545;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#25552;&#20986;&#20102;&#24456;&#22823;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23384;&#22312;&#22122;&#22768;AU&#26631;&#31614;&#65292;&#36825;&#31181;&#20570;&#27861;&#22686;&#21152;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#22686;&#24378;&#12290;&#30446;&#26631;&#26159;&#22312;AU&#26816;&#27979;&#39046;&#22495;&#20013;&#25670;&#33073;&#20256;&#32479;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#33539;&#24335;&#65292;&#33719;&#24471;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#24212;&#23545;&#22122;&#22768;AU&#26631;&#31614;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#33258;&#30417;&#30563;&#20449;&#21495;&#26469;&#22686;&#24378;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#31181;&#22686;&#24378;&#26159;&#36890;&#36807;&#27491;&#26679;&#26412;&#25277;&#26679;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27491;&#26679;&#26412;&#23545;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20943;&#36731;&#27599;&#20010;AU&#31867;&#22411;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26435;&#34913;&#37325;&#35201;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we empl
&lt;/p&gt;</description></item><item><title>Hydra heads&#26159;&#19968;&#31181;&#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#65292;&#21462;&#20195;&#20102;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;Medusa&#35299;&#30721;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.05109</link><description>&lt;p&gt;
Hydra: &#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#29992;&#20110;Medusa&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Hydra: Sequentially-Dependent Draft Heads for Medusa Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05109
&lt;/p&gt;
&lt;p&gt;
Hydra heads&#26159;&#19968;&#31181;&#24490;&#24207;&#20381;&#36182;&#30340;&#33609;&#31295;&#22836;&#37096;&#65292;&#21462;&#20195;&#20102;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#65292;&#22312;Medusa&#35299;&#30721;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#33258;&#22238;&#24402;LLM&#25512;&#29702;&#30340;&#20869;&#23384;&#24102;&#23485;&#38480;&#21046;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#25512;&#27979;&#35299;&#30721;&#26694;&#26550;&#12290;&#20026;&#20102;&#25191;&#34892;&#25512;&#27979;&#35299;&#30721;&#65292;&#19968;&#20010;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#25552;&#20986;&#20102;&#36755;&#20837;&#24207;&#21015;&#30340;&#20505;&#36873;&#24310;&#32493;&#65292;&#28982;&#21518;&#30001;&#22522;&#30784;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#12290;&#22312;&#26368;&#36817;&#30340;Medusa&#35299;&#30721;&#26694;&#26550;&#20013;&#65292;&#19968;&#31181;&#25351;&#23450;&#33609;&#31295;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#23558;&#20854;&#20316;&#20026;&#19968;&#32452;&#31216;&#20026;&#33609;&#31295;&#22836;&#37096;&#30340;&#36731;&#37327;&#32423;&#22836;&#37096;&#65292;&#36825;&#20123;&#22836;&#37096;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#25805;&#20316;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#33609;&#31295;&#22836;&#37096;&#37117;&#26159;&#39034;&#24207;&#29420;&#31435;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#22312;&#20505;&#36873;&#24310;&#32493;&#20013;&#30340;&#20196;&#29260;&#25512;&#27979;&#19982;&#20505;&#36873;&#24310;&#32493;&#20013;&#30340;&#20219;&#20309;&#21069;&#38754;&#30340;&#20196;&#29260;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#24207;&#20381;&#36182;&#30340;Hydra heads&#65292;&#23427;&#20204;&#26159;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#30340;&#21487;&#26367;&#25442;&#32452;&#20214;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#27979;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;Hydra heads&#36827;&#34892;&#35299;&#30721;&#27604;&#20351;&#29992;&#26631;&#20934;&#33609;&#31295;&#22836;&#37096;&#30340;Medusa&#35299;&#30721;&#20855;&#26377;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
To combat the memory bandwidth-bound nature of autoregressive LLM inference, previous research has proposed the speculative decoding framework. To perform speculative decoding, a small draft model proposes candidate continuations of the input sequence, that are then verified in parallel by the base model. One way to specify the draft model, as used in the recent Medusa decoding framework, is as a collection of light-weight heads, called draft heads, that operate on the base model's hidden states. To date, all existing draft heads have been sequentially independent, meaning that they speculate tokens in the candidate continuation independently of any preceding tokens in the candidate continuation. In this work, we propose Hydra heads, a sequentially dependent, drop-in replacement for standard draft heads that significantly improves speculation accuracy. Decoding with Hydra heads improves throughput compared to Medusa decoding with standard draft heads. We further explore the design spac
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02987</link><description>&lt;p&gt;
GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Conversation Reconstruction Attack Against GPT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#20855;&#26377;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#30340;&#20004;&#20010;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#35813;&#25915;&#20987;&#23545; GPT &#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616; GPT-4 &#23545;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20854;&#20013; GPT &#31995;&#21015;&#27169;&#22411;&#20195;&#34920;&#30528;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#25104;&#26524;&#12290;&#20026;&#20102;&#20248;&#21270;&#20219;&#21153;&#25191;&#34892;&#65292;&#29992;&#25143;&#32463;&#24120;&#19982;&#25176;&#31649;&#22312;&#20113;&#29615;&#22659;&#20013;&#30340; GPT &#27169;&#22411;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#36825;&#20123;&#22810;&#36718;&#23545;&#35805;&#24448;&#24448;&#21253;&#21547;&#31169;&#20154;&#20449;&#24687;&#65292;&#38656;&#35201;&#22312;&#20113;&#20013;&#36827;&#34892;&#20256;&#36755;&#21644;&#23384;&#20648;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25805;&#20316;&#27169;&#24335;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#25915;&#20987;&#38754;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; GPT &#27169;&#22411;&#30340;&#29305;&#23450;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#35805;&#37325;&#26500;&#25915;&#20987;&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;&#65306;&#21163;&#25345;&#20250;&#35805;&#21644;&#37325;&#26500;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#24403; GPT &#27169;&#22411;&#36973;&#21463;&#35813;&#25915;&#20987;&#26102;&#23545;&#35805;&#20013;&#22266;&#26377;&#30340;&#38544;&#31169;&#39118;&#38505;&#36827;&#34892;&#20102;&#35814;&#23613;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;GPT-4 &#23545;&#20110;&#35813;&#25915;&#20987;&#20855;&#26377;&#19968;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#39640;&#32423;&#25915;&#20987;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#37325;&#26500;&#20197;&#21069;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, significant advancements have been made in the field of large language models (LLMs), represented by GPT series models. To optimize task execution, users often engage in multi-round conversations with GPT models hosted in cloud environments. These multi-round conversations, potentially replete with private information, require transmission and storage within the cloud. However, this operational paradigm introduces additional attack surfaces. In this paper, we first introduce a specific Conversation Reconstruction Attack targeting GPT models. Our introduced Conversation Reconstruction Attack is composed of two steps: hijacking a session and reconstructing the conversations. Subsequently, we offer an exhaustive evaluation of the privacy risks inherent in conversations when GPT models are subjected to the proposed attack. However, GPT-4 demonstrates certain robustness to the proposed attacks. We then introduce two advanced attacks aimed at better reconstructing previous c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2402.02680</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#22320;&#29702;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Geographically Biased
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22320;&#29702;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#30340;&#31995;&#32479;&#38169;&#35823;&#65292;&#36890;&#36807;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#26469;&#35780;&#20272;&#20854;&#23545;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20869;&#22312;&#22320;&#21547;&#26377;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#20559;&#35265;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#20260;&#23475;&#30340;&#25345;&#32493;&#23384;&#22312;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#29702;&#35299;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20559;&#35265;&#23545;&#20110;&#23454;&#29616;&#20844;&#27491;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22320;&#29702;&#35270;&#35282;&#30740;&#31350;LLMs&#23545;&#25105;&#20204;&#25152;&#29983;&#27963;&#30340;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#24378;&#22823;&#65292;&#22240;&#20026;&#23545;&#20154;&#31867;&#29983;&#27963;&#20013;&#35832;&#22810;&#19982;&#22320;&#29702;&#31354;&#38388;&#30456;&#20851;&#30340;&#26041;&#38754;&#65288;&#22914;&#25991;&#21270;&#12289;&#31181;&#26063;&#12289;&#35821;&#35328;&#12289;&#25919;&#27835;&#21644;&#23447;&#25945;&#65289;&#26377;&#30528;&#26126;&#26174;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#38382;&#39064;&#22320;&#29702;&#20559;&#35265;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#20013;&#30340;&#31995;&#32479;&#38169;&#35823;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;LLMs&#33021;&#22815;&#36827;&#34892;&#31934;&#30830;&#30340;&#38646;&#23556;&#20987;&#22320;&#29702;&#31354;&#38388;&#39044;&#27979;&#65292;&#20197;&#35780;&#32423;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20854;&#19982;&#30495;&#23454;&#24773;&#20917;&#20043;&#38388;&#21576;&#29616;&#20986;&#24378;&#28872;&#30340;&#21333;&#35843;&#30456;&#20851;&#24615;&#65288;Spearman's &#961;&#26368;&#39640;&#21487;&#36798;0.89&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#20010;&#23458;&#35266;&#21644;&#23376;&#39046;&#22495;&#19978;&#34920;&#29616;&#20986;&#20849;&#21516;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and sub
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.03853</link><description>&lt;p&gt;
LLMs&#30340;&#20004;&#38754;&#24615;&#65306;Jekyll&#21338;&#22763;&#19982;Hyde&#20808;&#29983;
&lt;/p&gt;
&lt;p&gt;
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#19968;&#24180;&#21069;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#21512;&#20687;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#26102;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#21161;&#25163;&#20135;&#29983;&#19981;&#24403;&#22238;&#24212;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#65288;&#20197;&#21450;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;Bing chat&#65289;&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#36825;&#20123;&#25514;&#26045;&#65292;&#36825;&#20123;&#35282;&#33394;&#19982;&#23427;&#20204;&#26412;&#24212;&#25104;&#20026;&#30340;&#30495;&#23454;&#21161;&#25163;&#30340;&#29305;&#24449;&#30456;&#21453;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#36896;&#20986;&#36825;&#20123;&#20154;&#29289;&#35282;&#33394;&#30340;&#22797;&#26434;&#20256;&#35760;&#65292;&#28982;&#21518;&#22312;&#21516;&#19968;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26032;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23545;&#35805;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#39118;&#26684;&#65292;&#20197;&#33719;&#24471;&#21161;&#25163;&#19981;&#34987;&#20801;&#35768;&#25552;&#20379;&#30340;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#29289;&#35282;&#33394;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
&lt;/p&gt;</description></item><item><title>&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#21644;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2312.00246</link><description>&lt;p&gt;
&#26354;&#29575;&#26041;&#21521;&#20316;&#20026;&#22833;&#21435;&#21487;&#22609;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Directions of Curvature as an Explanation for Loss of Plasticity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00246
&lt;/p&gt;
&lt;p&gt;
&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#21644;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#26159;&#31070;&#32463;&#32593;&#32476;&#20007;&#22833;&#20174;&#26032;&#32463;&#39564;&#23398;&#20064;&#33021;&#21147;&#30340;&#29616;&#35937;&#12290;&#23613;&#31649;&#22312;&#20960;&#31181;&#38382;&#39064;&#35774;&#32622;&#20013;&#32463;&#39564;&#19978;&#35266;&#23519;&#21040;&#65292;&#20294;&#23545;&#23548;&#33268;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#26426;&#21046;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#19968;&#33268;&#35299;&#37322;&#65306;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20007;&#22833;&#20102;&#26354;&#29575;&#26041;&#21521;&#65292;&#21487;&#23558;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#24402;&#22240;&#20110;&#36825;&#31181;&#26354;&#29575;&#20943;&#23569;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#65292;&#25105;&#20204;&#23545;&#22312;MNIST&#12289;CIFAR-10&#21644;ImageNet&#20013;&#20351;&#29992;&#30340;&#19981;&#26029;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26354;&#29575;&#26041;&#21521;&#30340;&#20007;&#22833;&#19982;&#21487;&#22609;&#24615;&#30340;&#20007;&#22833;&#30456;&#21563;&#21512;&#65292;&#21516;&#26102;&#36824;&#34920;&#26126;&#20197;&#21069;&#30340;&#35299;&#37322;&#19981;&#36275;&#20197;&#35299;&#37322;&#25152;&#26377;&#24773;&#20917;&#19979;&#30340;&#21487;&#22609;&#24615;&#20007;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32531;&#35299;&#21487;&#22609;&#24615;&#20007;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#20063;&#20250;&#20445;&#30041;&#26354;&#29575;&#65292;&#20419;&#20351;&#37319;&#29992;&#31616;&#21333;&#30340;&#20998;&#24067;&#24335;&#27491;&#21017;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00246v2 Announce Type: replace  Abstract: Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience. Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity. In this paper, we offer a consistent explanation for loss of plasticity: Neural networks lose directions of curvature during training and that loss of plasticity can be attributed to this reduction in curvature. To support such a claim, we provide a systematic investigation of loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and ImageNet. Our findings illustrate that loss of curvature directions coincides with loss of plasticity, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings. Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional reg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#37319;&#29992;&#20102;&#26680;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;</title><link>https://arxiv.org/abs/2105.09254</link><description>&lt;p&gt;
&#22312;&#36830;&#32493;&#27835;&#30103;&#19979;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multiply Robust Causal Mediation Analysis with Continuous Treatments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#22810;&#37325;&#31283;&#20581;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#37319;&#29992;&#20102;&#26680;&#24179;&#28369;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#23545;&#27835;&#30103;&#25110;&#26292;&#38706;&#23545;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20013;&#20171;&#20998;&#26512;&#20026;&#37492;&#23450;&#21644;&#20272;&#35745;&#36825;&#20123;&#22240;&#26524;&#25928;&#24212;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#26694;&#26550;&#12290;&#23545;&#20110;&#20108;&#20803;&#27835;&#30103;&#65292;Tchetgen Tchetgen&#21644;Shpitser (2012)&#25552;&#20986;&#20102;&#30452;&#25509;&#21644;&#38388;&#25509;&#25928;&#24212;&#30340;&#39640;&#25928;&#20272;&#35745;&#22120;&#65292;&#22522;&#20110;&#21442;&#25968;&#30340;&#24433;&#21709;&#20989;&#25968;&#12290;&#36825;&#20123;&#20272;&#35745;&#22120;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#22914;&#22810;&#37325;&#31283;&#20581;&#24615;&#21644;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#23545;&#24178;&#25200;&#21442;&#25968;&#36827;&#34892;&#20302;&#20110;&#26681;&#21495;n&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#36830;&#32493;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22522;&#20110;&#24433;&#21709;&#20989;&#25968;&#30340;&#20272;&#35745;&#22120;&#27809;&#26377;&#20934;&#22791;&#22909;&#24212;&#29992;&#65292;&#38500;&#38750;&#36827;&#34892;&#24378;&#21442;&#25968;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;&#24179;&#28369;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36830;&#32493;&#27835;&#30103;&#29615;&#22659;&#30340;&#20272;&#35745;&#22120;&#65292;&#21463;&#21040;Tchetgen Tchetgen&#30340;&#24433;&#21709;&#20989;&#25968;&#20272;&#35745;&#22120;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, researchers are interested in the direct and indirect causal effects of a treatment or exposure on an outcome of interest. Mediation analysis offers a rigorous framework for identifying and estimating these causal effects. For binary treatments, efficient estimators for the direct and indirect effects are presented in Tchetgen Tchetgen and Shpitser (2012) based on the influence function of the parameter of interest. These estimators possess desirable properties, such as multiple-robustness and asymptotic normality, while allowing for slower than root-n rates of convergence for the nuisance parameters. However, in settings involving continuous treatments, these influence function-based estimators are not readily applicable without making strong parametric assumptions. In this work, utilizing a kernel-smoothing approach, we propose an estimator suitable for settings with continuous treatments inspired by the influence function-based estimator of Tchetgen Tchetgen an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.03192</link><description>&lt;p&gt;
&#20851;&#20110;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Hermitian Dynamic Mode Decomposition. (arXiv:2401.03192v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03192
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#12290;Hermitian DMD&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#26102;&#38388;&#24555;&#29031;&#36817;&#20284;&#34920;&#31034;&#19982;&#26410;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30456;&#20851;&#30340;Koopman&#31639;&#23376;&#65292;&#21516;&#26102;&#22312;&#20854;&#26377;&#38480;&#32500;&#36817;&#20284;&#20013;&#20445;&#25345;&#31639;&#23376;&#30340;&#33258;&#20276;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#24182;&#22312;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#19978;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the spectral properties of self-adjoint Koopman operators. Hermitian DMD is a data-driven method for approximating the Koopman operator associated with an unknown nonlinear dynamical system from discrete-time snapshots, while preserving the self-adjointness of the operator on its finite-dimensional approximations. We show that, under suitable conditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral properties of the underlying Koopman operator. Along the way, we establish a general theorem on the convergence of spectral measures, and demonstrate our results numerically on the two-dimensional Schr\"odinger equation.
&lt;/p&gt;</description></item><item><title>&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.02458</link><description>&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Foundation Models in Computational Healthcare: A Survey. (arXiv:2401.02458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02458
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21307;&#30103;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#22522;&#30784;&#27169;&#22411;&#26159;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;&#65292;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#30340;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#26377;&#26395;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#22871;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#30340;&#20986;&#29616;&#20026;&#35745;&#31639;&#21307;&#30103;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26426;&#36935;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20132;&#20114;&#24615;&#30001;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#20154;&#31867;&#25351;&#20196;&#24341;&#23548;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33539;&#24335;&#65292;&#24378;&#35843;&#26356;&#22909;&#30340;&#25968;&#25454;&#34920;&#24449;&#12289;&#36136;&#37327;&#21644;&#35268;&#27169;&#12290;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#33719;&#21462;&#21644;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#25968;&#25454;&#35760;&#24405;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#25968;&#25454;&#25968;&#37327;&#12289;&#27880;&#37322;&#12289;&#24739;&#32773;&#38544;&#31169;&#21644;&#20262;&#29702;&#31561;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;FM&#26102;&#20195;&#30340;&#24191;&#27867;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65288;&#20174;&#27169;&#22411;&#39044;&#35757;&#32451;&#21040;&#25512;&#29702;&#65289;&#65292;&#20197;&#25913;&#36827;&#21307;&#30103;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#24615;&#12289;&#35780;&#20272;&#20197;&#21450;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20851;&#38190;&#35266;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22522;&#20110;FM&#30340;&#20998;&#26512;&#22312;&#21307;&#30103;&#21644;&#21307;&#33647;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#30340;&#26684;&#23616;&#20013;&#25552;&#39640;&#24739;&#32773;&#32467;&#26524;&#21644;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#34920;&#29616;&#30340;&#21069;&#26223;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#26032;&#30340;&#21307;&#30103;&#28165;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of foundation models (FMs) as an emerging suite of AI techniques has struck a wave of opportunities in computational healthcare. The interactive nature of these models, guided by pre-training data and human instructions, has ignited a data-centric AI paradigm that emphasizes better data characterization, quality, and scale. In healthcare AI, obtaining and processing high-quality clinical data records has been a longstanding challenge, ranging from data quantity, annotation, patient privacy, and ethics. In this survey, we investigate a wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow. We discuss key perspectives in AI security, assessment, and alignment with human values. Finally, we offer a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine. We provide an up-to-date list of healthcare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17544</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29305;&#24449;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Ensemble-Based Feature Selection for Time Series Forecasting. (arXiv:2310.17544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17544
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#38598;&#25104;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#33021;&#22815;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#38750;&#24179;&#31283;&#21644;&#29305;&#24449;&#25968;&#30446;&#24222;&#22823;&#19988;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#21644;&#26679;&#26412;&#26377;&#38480;&#30340;&#22823;&#37327;&#29305;&#24449;&#24773;&#20917;&#19979;&#30340;&#29305;&#24449;&#36873;&#25321;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#26469;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#29305;&#24449;&#23376;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#21478;&#19968;&#31181;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#25439;&#22833;&#12290;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#20801;&#35768;&#28789;&#27963;&#30340;&#28145;&#24230;&#21644;&#29305;&#24449;&#36873;&#25321;&#12290;&#36890;&#36807;&#23618;&#27425;&#22320;&#21033;&#29992;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#35780;&#20998;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a novel ensemble approach for feature selection based on hierarchical stacking in cases of non-stationarity and limited number of samples with large number of features. Our approach exploits the co-dependency between features using a hierarchical structure. Initially, a machine learning model is trained using a subset of features, and then the model's output is updated using another algorithm with the remaining features to minimize the target loss. This hierarchical structure allows for flexible depth and feature selection. By exploiting feature co-dependency hierarchically, our proposed approach overcomes the limitations of traditional feature selection methods and feature importance scores. The effectiveness of the approach is demonstrated on synthetic and real-life datasets, indicating improved performance with scalability and stability compared to the traditional methods and state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.13391</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;Hebbian Temporal Memory&#23398;&#20064;&#32487;&#20219;&#32773;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Successor Representations with Distributed Hebbian Temporal Memory. (arXiv:2310.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DHTM&#30340;&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#27604;&#32463;&#20856;&#30340;LSTM&#25928;&#26524;&#26356;&#22909;&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#21487;&#20197;&#21152;&#36895;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#38544;&#34255;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;Hebbian Temporal Memory (DHTM)&#65292;&#22522;&#20110;&#22240;&#23376;&#22270;&#24418;&#24335;&#21644;&#22810;&#32452;&#25104;&#31070;&#32463;&#20803;&#27169;&#22411;&#12290;DHTM&#26088;&#22312;&#25429;&#25417;&#39034;&#24207;&#25968;&#25454;&#20851;&#31995;&#24182;&#23545;&#26410;&#26469;&#35266;&#23519;&#20316;&#20986;&#32047;&#31215;&#39044;&#27979;&#65292;&#24418;&#25104;&#32487;&#20219;&#32773;&#34920;&#31034;&#12290;&#21463;&#26032;&#30382;&#23618;&#30340;&#31070;&#32463;&#29983;&#29702;&#23398;&#27169;&#22411;&#21551;&#21457;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20998;&#24067;&#24335;&#34920;&#31034;&#12289;&#31232;&#30095;&#36716;&#31227;&#30697;&#38453;&#21644;&#23616;&#37096;Hebbian&#26679;&#23398;&#20064;&#35268;&#21017;&#20811;&#26381;&#20102;&#20256;&#32479;&#26102;&#38388;&#35760;&#24518;&#31639;&#27861;&#65288;&#22914;RNN&#21644;HMM&#65289;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#24930;&#36895;&#23398;&#20064;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DHTM&#20248;&#20110;&#32463;&#20856;&#30340;LSTM&#65292;&#24182;&#19982;&#26356;&#20808;&#36827;&#30340;&#31867;&#20284;RNN&#30340;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#21152;&#36895;&#20102;&#32487;&#20219;&#32773;&#34920;&#31034;&#30340;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to address the challenge of online hidden representation learning for decision-making under uncertainty in non-stationary, partially observable environments. The proposed algorithm, Distributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism and a multicomponent neuron model. DHTM aims to capture sequential data relationships and make cumulative predictions about future observations, forming Successor Representation (SR). Inspired by neurophysiological models of the neocortex, the algorithm utilizes distributed representations, sparse transition matrices, and local Hebbian-like learning rules to overcome the instability and slow learning process of traditional temporal memory algorithms like RNN and HMM. Experimental results demonstrate that DHTM outperforms classical LSTM and performs comparably to more advanced RNN-like algorithms, speeding up Temporal Difference learning for SR in changing environments. Additionally, we compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.09505</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#36716;&#25442;&#20013;&#25512;&#36827;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Advancing Test-Time Adaptation for Acoustic Foundation Models in Open-World Shifts. (arXiv:2310.09505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#30340;&#35821;&#38899;&#24103;&#21253;&#21547;&#37325;&#35201;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;TTA&#65289;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35299;&#20915;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22768;&#23398;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#38388;&#30340;&#35821;&#38899;&#20998;&#24067;&#36716;&#25442;&#20013;&#38754;&#20020;&#30456;&#20284;&#30340;&#25361;&#25112;&#65292;&#20294;&#38024;&#23545;&#22768;&#23398;&#24314;&#27169;&#22312;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#29615;&#22659;&#19979;&#30340;TTA&#25216;&#26415;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#32771;&#34385;&#21040;&#22768;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#29305;&#28857;&#65306;1&#65289;&#23427;&#20204;&#20027;&#35201;&#26159;&#22522;&#20110;&#20855;&#26377;&#23618;&#24402;&#19968;&#21270;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#26500;&#24314;&#30340;&#65307;2&#65289;&#23427;&#20204;&#20197;&#19968;&#31181;&#38750;&#38745;&#24577;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24230;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#35821;&#38899;&#25968;&#25454;&#12290;&#36825;&#20123;&#22240;&#32032;&#20351;&#24471;&#22312;&#35270;&#35273;&#32858;&#28966;&#30340;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#25209;&#24402;&#19968;&#21270;&#24182;&#20551;&#35774;&#29420;&#31435;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#38754;&#20020;&#24320;&#25918;&#19990;&#30028;&#25968;&#25454;&#36716;&#25442;&#30340;&#39044;&#35757;&#32451;&#22768;&#23398;&#27169;&#22411;&#30340;TTA&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22122;&#22768;&#36739;&#22823;&#12289;&#29109;&#36739;&#39640;&#30340;&#35821;&#38899;&#24103;&#36890;&#24120;&#24102;&#26377;&#20851;&#38190;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#20256;&#32479;&#30340;&#35270;&#35273;TTA&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#22312;&#22768;&#23398;&#24314;&#27169;&#20013;&#24182;&#19981;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation (TTA) is a critical paradigm for tackling distribution shifts during inference, especially in visual recognition tasks. However, while acoustic models face similar challenges due to distribution shifts in test-time speech, TTA techniques specifically designed for acoustic modeling in the context of open-world data shifts remain scarce. This gap is further exacerbated when considering the unique characteristics of acoustic foundation models: 1) they are primarily built on transformer architectures with layer normalization and 2) they deal with test-time speech data of varying lengths in a non-stationary manner. These aspects make the direct application of vision-focused TTA methods, which are mostly reliant on batch normalization and assume independent samples, infeasible. In this paper, we delve into TTA for pre-trained acoustic models facing open-world data shifts. We find that noisy, high-entropy speech frames, often non-silent, carry key semantic content. Tradit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;</title><link>http://arxiv.org/abs/2310.06930</link><description>&lt;p&gt;
&#12298;&#26377;&#22768;&#20070;&#30340;&#38901;&#24459;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Prosody Analysis of Audiobooks. (arXiv:2310.06930v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21547;&#26377;93&#20010;&#20070;&#31821;&#21644;&#23545;&#24212;&#26377;&#22768;&#20070;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26377;&#22768;&#20070;&#25991;&#26412;&#20013;&#30340;&#38901;&#24459;&#23646;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#38901;&#24459;&#19982;&#20154;&#31867;&#26391;&#35835;&#27604;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#26356;&#30456;&#20851;&#65292;&#24182;&#19988;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20351;&#24471;&#20174;&#25991;&#26412;&#20013;&#29983;&#25104;&#33258;&#28982;&#38899;&#25928;&#30340;&#38899;&#39057;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#22768;&#20070;&#26391;&#35835;&#28041;&#21450;&#21040;&#35835;&#32773;&#30340;&#25103;&#21095;&#24615;&#22768;&#38899;&#21644;&#35821;&#35843;&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#24773;&#24863;&#12289;&#23545;&#35805;&#21644;&#21465;&#36848;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;93&#26412;&#20070;&#19982;&#20854;&#23545;&#24212;&#30340;&#26377;&#22768;&#20070;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#21465;&#36848;&#25991;&#26412;&#20013;&#39044;&#27979;&#38901;&#24459;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#38899;&#37327;&#21644;&#35821;&#36895;&#65289;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#39044;&#27979;&#30340;&#38901;&#24459;&#23646;&#24615;&#19982;&#20154;&#31867;&#26391;&#35835;&#30340;&#30456;&#20851;&#24615;&#35201;&#36828;&#39640;&#20110;&#21830;&#19994;&#32423;TTS&#31995;&#32479;&#30340;&#32467;&#26524;&#65306;&#22312;24&#26412;&#20070;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#39640;&#23545;22&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#20855;&#30456;&#20851;&#24615;&#65292;&#32780;&#25105;&#20204;&#39044;&#27979;&#30340;&#38899;&#37327;&#23646;&#24615;&#23545;23&#26412;&#20070;&#30340;&#20154;&#31867;&#38405;&#35835;&#26356;&#21152;&#30456;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#65292;&#20197;&#37327;&#21270;&#20154;&#20204;&#26356;&#21916;&#27426;&#38901;&#24459;&#22686;&#24378;&#30340;&#26377;&#22768;&#20070;&#26391;&#35835;&#36824;&#26159;&#21830;&#19994;&#32423;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-speech have made it possible to generate natural-sounding audio from text. However, audiobook narrations involve dramatic vocalizations and intonations by the reader, with greater reliance on emotions, dialogues, and descriptions in the narrative. Using our dataset of 93 aligned book-audiobook pairs, we present improved models for prosody prediction properties (pitch, volume, and rate of speech) from narrative text using language modeling. Our predicted prosody attributes correlate much better with human audiobook readings than results from a state-of-the-art commercial TTS system: our predicted pitch shows a higher correlation with human reading for 22 out of the 24 books, while our predicted volume attribute proves more similar to human reading for 23 out of the 24 books. Finally, we present a human evaluation study to quantify the extent that people prefer prosody-enhanced audiobook readings over commercial text-to-speech systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15608</link><description>&lt;p&gt;
NoSENSE&#65306;&#23398;&#20064;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NoSENSE: Learned unrolled cardiac MRI reconstruction without explicit sensitivity maps. (arXiv:2309.15608v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26174;&#24335;&#28789;&#25935;&#24230;&#22270;&#30340;&#23637;&#24320;&#24515;&#33039;MRI&#37325;&#24314;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#31639;&#27861;&#23637;&#24320;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#26469;&#23454;&#29616;&#21152;&#36895;&#24515;&#33039;MRI&#37325;&#24314;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22522;&#20110;&#22810;&#20010;&#25509;&#25910;&#32447;&#22280;&#30340;&#21152;&#36895;&#24515;&#33039;MRI&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#31639;&#27861;&#23637;&#24320;&#12290;&#19982;&#35768;&#22810;&#29616;&#26377;&#30340;&#23398;&#20064;MR&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#19981;&#21516;&#65292;&#38656;&#35201;&#23558;&#28789;&#25935;&#24230;&#26144;&#23556;&#65288;CSM&#65289;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#32593;&#32476;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#26174;&#24335;&#30340;CSM&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#23427;&#38544;&#21547;&#22320;&#25429;&#25417;&#24182;&#23398;&#20064;&#21033;&#29992;&#22270;&#20687;&#20043;&#38388;&#30340;&#25509;&#25910;&#32447;&#22280;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#23398;&#20064;&#22270;&#20687;&#22359;&#21644;k&#31354;&#38388;&#22359;&#32452;&#25104;&#65292;&#20849;&#20139;&#28508;&#22312;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#35843;&#33410;&#21644;&#25509;&#25910;&#32447;&#22280;&#25968;&#25454;&#19968;&#33268;&#24615;&#23454;&#29616;&#23545;&#37319;&#38598;&#21442;&#25968;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;MICCAI STACOM CMRxRecon&#25361;&#25112;&#36187;&#30340;&#24433;&#29255;&#36861;&#36394;&#21644;&#26144;&#23556;&#36861;&#36394;&#39564;&#35777;&#25490;&#34892;&#27036;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#21035;&#22312;PSNR&#20540;&#19978;&#36798;&#21040;&#20102;34.89&#21644;35.56&#65292;SSIM&#20540;&#20998;&#21035;&#20026;0.920&#21644;0.942&#65292;&#22312;&#25776;&#20889;&#26412;&#25991;&#26102;&#20301;&#21015;&#19981;&#21516;&#23567;&#32452;&#31532;4&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel learned image reconstruction method for accelerated cardiac MRI with multiple receiver coils based on deep convolutional neural networks (CNNs) and algorithm unrolling. In contrast to many existing learned MR image reconstruction techniques that necessitate coil-sensitivity map (CSM) estimation as a distinct network component, our proposed approach avoids explicit CSM estimation. Instead, it implicitly captures and learns to exploit the inter-coil relationships of the images. Our method consists of a series of novel learned image and k-space blocks with shared latent information and adaptation to the acquisition parameters by feature-wise modulation (FiLM), as well as coil-wise data-consistency (DC) blocks.  Our method achieved PSNR values of 34.89 and 35.56 and SSIM values of 0.920 and 0.942 in the cine track and mapping track validation leaderboard of the MICCAI STACOM CMRxRecon Challenge, respectively, ranking 4th among different teams at the time of writing.  Cod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15244</link><description>&lt;p&gt;
&#26080;&#31351;&#23485;&#24230;&#20004;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks. (arXiv:2309.15244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#65292;&#26088;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#20004;&#20010;&#20851;&#38190;&#26426;&#21046;&#65306;&#19968;&#20010;&#26159;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65307;&#21478;&#19968;&#20010;&#25216;&#26415;&#26159;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#23545;&#36825;&#31181;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#32771;&#34385;&#26356;&#22823;&#23485;&#24230;&#30340;&#32593;&#32476;&#26102;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;&#36825;&#31181;&#25552;&#35758;&#30340;HRTA&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;</title><link>http://arxiv.org/abs/2309.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65306;&#22686;&#24378;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Multi-Modal Fusion: Enhancing Sign Language Recognition and Translation. (arXiv:2309.01860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#30456;&#32467;&#21512;&#65292;&#20016;&#23500;&#20102;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#20013;&#30340;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#22312;&#25163;&#35821;&#35782;&#21035;&#20219;&#21153;&#20013;&#38477;&#20302;&#20102;WER 0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27979;&#35797;&#38598;&#19978;&#22823;&#22810;&#25968;BLEU&#20998;&#25968;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#22312;&#25105;&#20204;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20809;&#27969;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#20016;&#23500;&#20855;&#26377;&#19982;&#36816;&#21160;&#30456;&#20851;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#35813;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#24577;&#21253;&#21547;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#25554;&#20214;&#38750;&#24120;&#36731;&#37327;&#32423;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20026;&#26032;&#27169;&#24577;&#21253;&#25324;&#19968;&#20010;&#21333;&#29420;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#25105;&#20204;&#22312;&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#20013;&#24212;&#29992;&#20102;&#36825;&#20123;&#25913;&#21464;&#65292;&#25913;&#21892;&#20102;&#27599;&#20010;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;RWTH-PHOENIX-2014&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#29992;&#20110;&#25163;&#35821;&#35782;&#21035;&#65292;&#24182;&#22312;RWTH-PHOENIX-2014T&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32763;&#35793;&#20219;&#21153;&#12290;&#22312;&#35782;&#21035;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;WER&#38477;&#20302;&#20102;0.9&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22823;&#37096;&#20998;BLEU&#20998;&#25968;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;&#32422;0.6&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we devise a mechanism for the addition of multi-modal information with an existing pipeline for continuous sign language recognition and translation. In our procedure, we have incorporated optical flow information with RGB images to enrich the features with movement-related information. This work studies the feasibility of such modality inclusion using a cross-modal encoder. The plugin we have used is very lightweight and doesn't need to include a separate feature extractor for the new modality in an end-to-end manner. We have applied the changes in both sign language recognition and translation, improving the result in each case. We have evaluated the performance on the RWTH-PHOENIX-2014 dataset for sign language recognition and the RWTH-PHOENIX-2014T dataset for translation. On the recognition task, our approach reduced the WER by 0.9, and on the translation task, our approach increased most of the BLEU scores by ~0.6 on the test set.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;&#65292;&#21033;&#29992;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#20294;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11659</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
An engine to simulate insurance fraud network data. (arXiv:2308.11659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#25311;&#20445;&#38505;&#27450;&#35784;&#32593;&#32476;&#25968;&#25454;&#30340;&#24341;&#25806;&#65292;&#21033;&#29992;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#12290;&#20294;&#38754;&#20020;&#31867;&#21035;&#19981;&#24179;&#34913;&#12289;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#32570;&#20047;&#20844;&#24320;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26816;&#27979;&#20445;&#38505;&#27450;&#35784;&#32034;&#36180;&#20381;&#36182;&#20110;&#19994;&#21153;&#35268;&#21017;&#21644;&#19987;&#23478;&#21028;&#26029;&#65292;&#36825;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#25506;&#32034;&#24320;&#21457;&#39640;&#25928;&#20934;&#30830;&#30340;&#20998;&#26512;&#31574;&#30053;&#26469;&#26631;&#35760;&#21487;&#30097;&#32034;&#36180;&#12290;&#20174;&#32034;&#36180;&#28041;&#21450;&#26041;&#30340;&#31038;&#20132;&#32593;&#32476;&#20013;&#25552;&#21462;&#29305;&#24449;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#27450;&#35784;&#30340;&#38750;&#24120;&#35268;&#24615;&#36136;&#23548;&#33268;&#20102;&#39640;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#36825;&#22686;&#21152;&#20102;&#24320;&#21457;&#24615;&#33021;&#33391;&#22909;&#30340;&#20998;&#26512;&#20998;&#31867;&#27169;&#22411;&#30340;&#38590;&#24230;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#32034;&#36180;&#24471;&#21040;&#35843;&#26597;&#21644;&#26631;&#31614;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#65292;&#36825;&#22952;&#30861;&#20102;&#30740;&#31350;&#21644;&#27169;&#22411;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.06300</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#34880;&#32454;&#32990;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Blood Cells Using Deep Learning Models. (arXiv:2308.06300v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06300
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#23545;&#20154;&#31867;&#34880;&#32454;&#32990;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#35782;&#21035;&#65292;&#20026;&#35786;&#26029;&#30142;&#30149;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34880;&#28082;&#20027;&#35201;&#21253;&#25324;&#34880;&#27974;&#12289;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#12290;&#34880;&#32454;&#32990;&#20026;&#36523;&#20307;&#32454;&#32990;&#25552;&#20379;&#27687;&#27668;&#65292;&#28363;&#20859;&#23427;&#20204;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24863;&#26579;&#65292;&#22686;&#24378;&#20813;&#30123;&#21147;&#24182;&#20419;&#36827;&#20957;&#34880;&#12290;&#20154;&#30340;&#20581;&#24247;&#29366;&#20917;&#21487;&#20197;&#20174;&#34880;&#32454;&#32990;&#20013;&#21453;&#26144;&#20986;&#26469;&#12290;&#19968;&#20010;&#20154;&#34987;&#35786;&#26029;&#20986;&#26576;&#31181;&#30142;&#30149;&#30340;&#26426;&#20250;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#34880;&#32454;&#32990;&#31867;&#22411;&#21644;&#35745;&#25968;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#34880;&#32454;&#32990;&#20998;&#31867;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#30142;&#30149;&#65292;&#21253;&#25324;&#30284;&#30151;&#12289;&#39592;&#39635;&#25439;&#20260;&#12289;&#33391;&#24615;&#32959;&#30244;&#21644;&#23427;&#20204;&#30340;&#29983;&#38271;&#12290;&#36825;&#31181;&#20998;&#31867;&#21487;&#20197;&#24110;&#21161;&#34880;&#28082;&#23398;&#23478;&#21306;&#20998;&#19981;&#21516;&#30340;&#34880;&#32454;&#32990;&#29255;&#27573;&#65292;&#20197;&#20415;&#30830;&#23450;&#30142;&#30149;&#30340;&#21407;&#22240;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#23427;&#23558;&#20154;&#31867;&#34880;&#32454;&#32990;&#65288;&#32418;&#32454;&#32990;&#12289;&#30333;&#32454;&#32990;&#21644;&#34880;&#23567;&#26495;&#65289;&#30340;&#22270;&#20687;&#20998;&#31867;&#20026;&#23427;&#20204;&#30340;&#20122;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23558;&#19981;&#21516;&#30340;CNN&#39044;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#34880;&#32454;&#32990;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human blood mainly comprises plasma, red blood cells, white blood cells, and platelets. The blood cells provide the body's cells oxygen to nourish them, shield them from infections, boost immunity, and aid in clotting. Human health is reflected in blood cells. The chances that a human being can be diagnosed with a disease are significantly influenced by their blood cell type and count. Therefore, blood cell classification is crucial because it helps identify diseases, including cancer, damaged bone marrow, benign tumors, and their growth. This classification allows hematologists to distinguish between different blood cell fragments so that the cause of diseases can be identified. Convolution neural networks are a deep learning technique that classifies images of human blood cells (RBCs, WBCs, and platelets) into their subtypes. For this study, transfer learning is used to apply different CNN pre-trained models, including VGG16, VGG19, ResNet-50, ResNet-101, ResNet-152, InceptionV3 Mobi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2307.09206</link><description>&lt;p&gt;
&#24102;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21160;&#21147;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model. (arXiv:2307.09206v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRADYN&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#30340;&#21464;&#21270;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;&#21442;&#25968;&#21487;&#33021;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22320;&#24418;&#29305;&#24615;&#22914;&#25705;&#25830;&#31995;&#25968;&#21487;&#33021;&#20250;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#32780;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#20154;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#20250;&#22240;&#19981;&#21516;&#36127;&#36733;&#12289;&#31995;&#32479;&#36136;&#37327;&#21464;&#21270;&#12289;&#30952;&#25439;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#20174;&#32780;&#25913;&#21464;&#25191;&#34892;&#22120;&#22686;&#30410;&#25110;&#20851;&#33410;&#25705;&#25830;&#21147;&#12290;&#33258;&#20027;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36866;&#24212;&#36825;&#20123;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#22320;&#24418;&#21644;&#26426;&#22120;&#20154;&#24863;&#30693;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#31216;&#20026;TRADYN&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#19978;&#36848;&#21464;&#21270;&#12290;&#23427;&#22522;&#20110;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#21069;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;&#20108;&#32500;&#23548;&#33322;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#31867;&#20284;&#33258;&#34892;&#36710;&#30340;&#26426;&#22120;&#20154;&#21644;&#20855;&#26377;&#31354;&#38388;&#21464;&#21270;&#25705;&#25830;&#31995;&#25968;&#30340;&#19981;&#21516;&#22320;&#24418;&#24067;&#23616;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#19982;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38271;&#35270;&#31243;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous navigation settings, several quantities can be subject to variations. Terrain properties such as friction coefficients may vary over time depending on the location of the robot. Also, the dynamics of the robot may change due to, e.g., different payloads, changing the system's mass, or wear and tear, changing actuator gains or joint friction. An autonomous agent should thus be able to adapt to such variations. In this paper, we develop a novel probabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN, which is able to adapt to the above-mentioned variations. It builds on recent advances in meta-learning forward dynamics models based on Neural Processes. We evaluate our method in a simulated 2D navigation setting with a unicycle-like robot and different terrain layouts with spatially varying friction coefficients. In our experiments, the proposed model exhibits lower prediction error for the task of long-horizon trajectory prediction, compared to non-ada
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.05385</link><description>&lt;p&gt;
&#23398;&#20064;&#26680;&#25216;&#26415;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26680;&#25216;&#26415;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#19988;&#21442;&#25968;&#36739;&#23569;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#21644;&#20998;&#21106;PPG&#20449;&#21495;&#30340;&#36136;&#37327;&#21644;&#20266;&#24433;&#65292;&#19982;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#25239;(PPG)&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#30417;&#27979;&#21508;&#31181;&#24515;&#34880;&#31649;&#21442;&#25968;&#12290;PPG&#20449;&#21495;&#30001;&#21487;&#31359;&#25140;&#35774;&#22791;&#20135;&#29983;&#65292;&#24120;&#24120;&#21253;&#21547;&#30001;&#22806;&#37096;&#22240;&#32032;(&#22914;&#20154;&#20307;&#36816;&#21160;)&#24341;&#36215;&#30340;&#22823;&#22411;&#20266;&#24433;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#29983;&#29702;&#21442;&#25968;&#36827;&#34892;&#31283;&#20581;&#21644;&#20934;&#30830;&#30340;&#25552;&#21462;&#65292;&#20449;&#21495;&#30340;&#25439;&#22351;&#21306;&#22495;&#38656;&#35201;&#34987;&#27491;&#30830;&#22320;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#38752;&#25163;&#24037;&#29305;&#24449;&#26816;&#27979;&#22120;&#25110;&#20449;&#21495;&#24230;&#37327;&#65292;&#32467;&#26524;&#24615;&#33021;&#19981;&#20339;&#65292;&#25110;&#20381;&#38752;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#31561;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#19968;&#23567;&#32452;&#21487;&#35299;&#37322;&#30340;&#21367;&#31215;&#26680;&#65292;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#25216;&#26415;DNN&#26041;&#27861;&#30456;&#20284;&#65292;&#29978;&#33267;&#26356;&#22909;&#65292;&#32780;&#21442;&#25968;&#25968;&#37327;&#27604;DNN&#26041;&#27861;&#23569;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#39033;&#24037;&#20316;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#31283;&#20581;&#21644;&#21487;&#35299;&#37322;&#30340;PPG&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#21644;&#20266;&#24433;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmography (PPG) provides a low-cost, non-invasive method to continuously monitor various cardiovascular parameters. PPG signals are generated by wearable devices and frequently contain large artifacts caused by external factors, such as motion of the human subject. In order to ensure robust and accurate extraction of physiological parameters, corrupted areas of the signal need to be identified and handled appropriately. Previous methodology relied either on handcrafted feature detectors or signal metrics which yield sub-optimal performance, or relied on machine learning techniques such as deep neural networks (DNN) which lack interpretability and are computationally and memory intensive. In this work, we present a novel method to learn a small set of interpretable convolutional kernels that has performance similar to -- and often better than -- the state-of-the-art DNN approach with several orders of magnitude fewer parameters. This work allows for efficient, robust, and int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.18256</link><description>&lt;p&gt;
&#29992;Transformer&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#21644;&#25968;&#20540;&#30693;&#35782;&#22270;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers. (arXiv:2305.18256v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#25968;&#20540;&#25991;&#23383;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19978;&#19979;&#25991;Transformer&#21644;&#39044;&#27979;Transformer&#65292;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#20540;&#20449;&#24687;&#26469;&#33719;&#24471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#20102;&#19968;&#20010;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#35889;&#65292;&#20854;&#20013;&#19977;&#20803;&#32452;&#19982;&#38480;&#23450;&#35789;&#38598;&#21512;&#30456;&#20851;&#32852;; &#19968;&#20010;&#38480;&#23450;&#35789;&#30001;&#20851;&#31995;&#21644;&#23454;&#20307;&#32452;&#25104;&#65292;&#20026;&#19977;&#20803;&#32452;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#23884;&#20837;&#26041;&#27861;&#20551;&#23450;&#23454;&#20307;&#26159;&#31163;&#25955;&#23545;&#35937;&#65292;&#20294;&#26377;&#20123;&#20449;&#24687;&#24212;&#20351;&#29992;&#25968;&#20540;&#34920;&#31034;&#65292;&#20363;&#22914;(J.R.R.&#65292;&#20986;&#29983;&#20110;&#65292;1892)&#12290;&#21516;&#26102;&#65292;&#19977;&#20803;&#32452;(J.R.R.&#65292;&#23601;&#35835;&#20110;&#65292;&#29275;&#27941;&#22823;&#23398;)&#21487;&#20197;&#19982;&#38480;&#23450;&#35789;(&#24320;&#22987;&#26102;&#38388;&#65292;1911)&#30456;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyNT&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21253;&#21547;&#19977;&#20803;&#32452;&#25110;&#38480;&#23450;&#35789;&#20013;&#25968;&#20540;&#25991;&#23383;&#30340;&#36229;&#20851;&#31995;&#22411;&#30693;&#35782;&#22270;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;Transformer&#21644;&#19968;&#20010;&#39044;&#27979;Transformer&#65292;&#26469;&#23398;&#20064;&#34920;&#31034;&#65292;&#19981;&#20165;&#22522;&#20110;&#19977;&#20803;&#32452;&#21644;&#20854;&#38480;&#23450;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36824;&#22522;&#20110;&#25968;&#20540;&#20449;&#24687;&#12290;&#36890;&#36807;&#23398;&#20064;&#19977;&#20803;&#32452;&#21644;&#38480;&#23450;&#35789;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#23558;&#23427;&#20204;&#39304;&#36865;&#32473;Transformer&#26469;&#33719;&#24471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#29305;&#24449;&#20540;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#21463;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#24456;&#22909;&#22320;&#25429;&#33719;&#65292;&#25152;&#20197;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#21644;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#20294;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.12133</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#23792;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Loss Spike in Training Neural Networks. (arXiv:2305.12133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#26368;&#22823;&#29305;&#24449;&#20540;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#21463;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#24456;&#22909;&#22320;&#25429;&#33719;&#65292;&#25152;&#20197;&#23548;&#33268;&#20855;&#26377;&#33391;&#22909;&#21644;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#20294;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#25439;&#22833;&#20540;&#23792;&#20540;&#29616;&#35937;&#25152;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#36739;&#23567;&#25439;&#22833;&#30340;&#21306;&#22495;&#65292;&#19968;&#26086;&#35757;&#32451;&#36827;&#20837;&#35813;&#21306;&#22495;&#65292;&#35757;&#32451;&#23601;&#20250;&#21464;&#24471;&#19981;&#31283;&#23450;&#65292;&#25439;&#22833;&#20540;&#21576;&#25351;&#25968;&#24335;&#22686;&#38271;&#65292;&#21363;&#20986;&#29616;&#25439;&#22833;&#23792;&#20540;&#29616;&#35937;&#12290;&#24403;&#35757;&#32451;&#36827;&#20837;&#24179;&#22374;&#21306;&#22495;&#26102;&#65292;&#35757;&#32451;&#20250;&#21464;&#24471;&#31283;&#23450;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#25439;&#22833;Hessian&#30697;&#38453;&#30340;&#26368;&#22823;&#29305;&#24449;&#20540;&#65288;$\lambda_{\mathrm{max}}$&#65289;&#30340;&#31532;&#19968;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20559;&#24046;&#20027;&#35201;&#30001;&#20302;&#39057;&#25104;&#20998;&#21344;&#25454;&#12290;&#30001;&#20110;&#20302;&#39057;&#25104;&#20998;&#21487;&#20197;&#38750;&#24120;&#24555;&#36895;&#22320;&#34987;&#25429;&#33719;&#65288;&#39057;&#29575;&#21407;&#29702;&#65289;&#65292;&#22240;&#27492;&#20250;&#20986;&#29616;&#24613;&#21095;&#19979;&#38477;&#30340;&#29616;&#35937;&#12290;&#22312;&#20998;&#26512;&#25439;&#22833;&#23792;&#20540;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;$\lambda_{\mathrm{max}}$&#24179;&#22374;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#20302;&#39057;&#24448;&#24448;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#19988;&#21487;&#20197;&#34987;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#25152;&#24456;&#22909;&#22320;&#25429;&#33719;&#12290;&#22240;&#27492;&#65292;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#21644;&#20855;&#26377;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#20302;&#39057;&#25104;&#20998;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#24615;&#36136;&#30456;&#20284;&#12290;&#20294;&#26159;&#65292;&#21155;&#36136;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#39640;&#39057;&#25104;&#20998;&#65292;&#32780;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the mechanism underlying loss spikes observed during neural network training. When the training enters a region, which has a smaller-loss-as-sharper (SLAS) structure, the training becomes unstable and loss exponentially increases once it is too sharp, i.e., the rapid ascent of the loss spike. The training becomes stable when it finds a flat region. The deviation in the first eigen direction (with maximum eigenvalue of the loss Hessian ($\lambda_{\mathrm{max}}$) is found to be dominated by low-frequency. Since low-frequency is captured very fast (frequency principle), the rapid descent is then observed. Inspired by our analysis of loss spikes, we revisit the link between $\lambda_{\mathrm{max}}$ flatness and generalization. For real datasets, low-frequency is often dominant and well-captured by both the training data and the test data. Then, a solution with good generalization and a solution with bad generalization can both learn low-frequency well, thus, they hav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.10207</link><description>&lt;p&gt;
SREL&#65306;&#22522;&#20110;S&#21442;&#25968;&#27169;&#24335;&#30340;&#38108;&#20114;&#36830;&#38750;&#30772;&#22351;&#25925;&#38556;&#35786;&#26029;&#30340;&#20005;&#37325;&#24615;&#35780;&#32423;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SREL: Severity Rating Ensemble Learning for Non-Destructive Fault Diagnosis of Cu Interconnects using S-parameter Patterns. (arXiv:2304.10207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;S&#21442;&#25968;&#27169;&#24335;&#25104;&#21151;&#23454;&#29616;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#22312;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#26041;&#38754;&#20855;&#26377;&#20808;&#36827;&#24615;&#65292;&#20855;&#22791;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22120;&#30340;&#24037;&#20316;&#39057;&#29575;&#21644;&#26102;&#38047;&#36895;&#24230;&#36880;&#24180;&#25552;&#39640;&#65292;&#20114;&#36830;&#23545;&#25972;&#20010;&#30005;&#23376;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#37117;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#26816;&#27979;&#21644;&#35786;&#26029;&#20114;&#36830;&#25925;&#38556;&#23545;&#30005;&#23376;&#20581;&#24247;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#30005;&#20449;&#21495;&#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#30340;&#29616;&#26377;&#30740;&#31350;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#26080;&#27861;&#21306;&#20998;&#32570;&#38519;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#26368;&#32456;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#30772;&#22351;&#24615;&#35780;&#20272;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24178;&#25200;&#32780;&#23548;&#33268;&#35823;&#35686;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#23545;Cu&#20114;&#36830;&#32570;&#38519;&#30340;&#38750;&#30772;&#22351;&#24615;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#23454;&#29616;&#20102;&#26089;&#26399;&#26816;&#27979;&#12289;&#39640;&#35786;&#26029;&#20934;&#30830;&#24230;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#30005;&#20449;&#21495;&#27169;&#24335;&#21516;&#26102;&#20998;&#26512;&#26681;&#26412;&#21407;&#22240;&#21644;&#20005;&#37325;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;S&#21442;&#25968;&#27169;&#24335;&#20855;&#26377;&#25925;&#38556;&#35786;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As operating frequencies and clock speeds in processors have increased over the years, interconnects affect both the reliability and performance of entire electronic systems. Fault detection and diagnosis of the interconnects are crucial for prognostics and health management (PHM) of electronics. However, existing research works utilizing electrical signals as prognostic factors have limitations, such as the inability to distinguish the root cause of defects, which eventually requires additional destructive evaluation, and vulnerability to noise that results in a false alarm. Herein, we realize the non-destructive detection and diagnosis of defects in Cu interconnects, achieving early detection, high diagnostic accuracy, and noise robustness. To the best of our knowledge, this study first simultaneously analyzes the root cause and severity using electrical signal patterns. In this paper, we experimentally show that S-parameter patterns have the ability for fault diagnosis and they are 
&lt;/p&gt;</description></item><item><title>PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07514</link><description>&lt;p&gt;
PI-FL&#65306;&#20010;&#24615;&#21270;&#21644;&#28608;&#21169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PI-FL: Personalized and Incentivized Federated Learning. (arXiv:2304.07514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07514
&lt;/p&gt;
&lt;p&gt;
PI-FL&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24212;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#25361;&#25112;&#12290;&#20027;&#35201;&#38382;&#39064;&#26159;&#32771;&#34385;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#36807;&#31243;&#20197;&#20445;&#25252;&#20854;&#33258;&#27835;&#26435;&#12290;&#20801;&#35768;&#23458;&#25143;&#31471;&#21442;&#19982;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20915;&#31574;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#23384;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#26080;&#27861;&#33258;&#30001;&#20849;&#20139;&#29983;&#25104;&#33391;&#22909;&#36136;&#37327;&#20010;&#24615;&#21270;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#23458;&#25143;&#31471;&#19981;&#24895;&#24847;&#22312;&#27809;&#26377;&#21512;&#29702;&#28608;&#21169;&#30340;&#24773;&#20917;&#19979;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PI-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#19968;&#27425;&#24615;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#37197;&#21512;&#19968;&#20010;&#22522;&#20110;&#20196;&#29260;&#30340;&#28608;&#21169;&#26426;&#21046;&#65292;&#22870;&#21169;&#20010;&#24615;&#21270;&#35757;&#32451;&#12290;PI-FL&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#23562;&#37325;&#23458;&#25143;&#31471;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized FL has been widely used to cater to heterogeneity challenges with non-IID data. A primary obstacle is considering the personalization process from the client's perspective to preserve their autonomy. Allowing the clients to participate in personalized FL decisions becomes significant due to privacy and security concerns, where the clients may not be at liberty to share private information necessary for producing good quality personalized models. Moreover, clients with high-quality data and resources are reluctant to participate in the FL process without reasonable incentive. In this paper, we propose PI-FL, a one-shot personalization solution complemented by a token-based incentive mechanism that rewards personalized training. PI-FL outperforms other state-of-the-art approaches and can generate good-quality personalized models while respecting clients' privacy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.08909</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#26465;&#20214;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning. (arXiv:2303.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08909
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#25214;&#21040;&#24179;&#34913;&#30456;&#20114;&#30683;&#30462;&#30340;&#30446;&#26631;&#30340;&#33391;&#22909;&#24179;&#34913;&#28857;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#31574;&#30053;&#65292;&#23427;&#20204;&#20307;&#29616;&#20102;&#19981;&#21516;&#30340;&#30446;&#26631;&#26435;&#34913;&#27169;&#24335;&#65292;&#24182;&#19988;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#33719;&#24471;&#23427;&#20204;&#20855;&#26377;&#25216;&#26415;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#21333;&#27425;&#35757;&#32451;&#36816;&#34892;&#20013;&#36817;&#20284;&#33719;&#21462;&#25972;&#20010;&#24085;&#32047;&#25176;&#38598;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#30446;&#26631;&#30340;&#32447;&#24615;&#26631;&#37327;&#21270;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#34892;&#21160;&#31354;&#38388;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#31574;&#30053;&#32593;&#32476;&#30340;&#35774;&#35745;&#12290;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#30340;&#25968;&#23383;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;MORL&#22522;&#32447;&#30456;&#27604;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision making in the real world often requires finding a good balance of conflicting objectives. In general, there exist a plethora of Pareto-optimal policies that embody different patterns of compromises between objectives, and it is technically challenging to obtain them exhaustively using deep neural networks. In this work, we propose a novel multi-objective reinforcement learning (MORL) algorithm that trains a single neural network via policy gradient to approximately obtain the entire Pareto set in a single run of training, without relying on linear scalarization of objectives. The proposed method works in both continuous and discrete action spaces with no design change of the policy network. Numerical experiments in benchmark environments demonstrate the practicality and efficacy of our approach in comparison to standard MORL baselines.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item></channel></rss>