<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.17164</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36136;&#37327;&#22810;&#26679;&#24615;&#29992;&#20110;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Objective Quality-Diversity for Crystal Structure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#26088;&#22312;&#21457;&#29616;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#65292;&#21487;&#20197;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#22312;&#20174;&#30005;&#27744;&#21040;&#22826;&#38451;&#33021;&#30005;&#27744;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#38024;&#23545;&#20854;&#21407;&#23376;&#37197;&#32622;&#39044;&#27979;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;&#35782;&#21035;&#33021;&#37327;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#22788;&#30340;&#26368;&#31283;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#24573;&#30053;&#20102;&#37027;&#20123;&#21487;&#33021;&#20301;&#20110;&#30456;&#37051;&#23616;&#37096;&#26497;&#23567;&#20540;&#22788;&#12289;&#20855;&#26377;&#19981;&#21516;&#26448;&#26009;&#29305;&#24615;&#65288;&#22914;&#30005;&#23548;&#29575;&#25110;&#25239;&#21464;&#24418;&#24615;&#65289;&#30340;&#20854;&#20182;&#26377;&#36259;&#26448;&#26009;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#20026;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#25214;&#21040;&#20855;&#26377;&#22810;&#26679;&#29305;&#24449;&#30340;&#39640;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#26230;&#20307;&#32467;&#26500;&#31283;&#23450;&#24615;&#20197;&#21450;&#20854;&#20182;&#30446;&#26631;&#65288;&#22914;&#30913;&#24615;&#25110;&#28909;&#30005;&#25928;&#29575;&#65289;&#20063;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;</title><link>https://arxiv.org/abs/2403.13041</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#31169;&#23494;&#39044;&#22788;&#29702;&#30340;&#21487;&#35777;&#26126;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Provable Privacy with Non-Private Pre-Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13041
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#33021;&#22815;&#35780;&#20272;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;DP&#21644;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#24314;&#31435;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20998;&#26512;&#24046;&#20998;&#31169;&#23494;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#25968;&#25454;&#30456;&#20851;&#30340;&#39044;&#22788;&#29702;&#30340;&#28508;&#22312;&#38544;&#31169;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#30001;&#38750;&#31169;&#23494;&#25968;&#25454;&#30456;&#20851;&#39044;&#22788;&#29702;&#31639;&#27861;&#24341;&#36215;&#30340;&#39069;&#22806;&#38544;&#31169;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#26032;&#30340;&#25216;&#26415;&#27010;&#24565;&#24314;&#31435;&#20102;&#25972;&#20307;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#38480;&#65306;&#19968;&#31181;&#31216;&#20026;&#24179;&#28369;DP&#30340;DP&#21464;&#20307;&#20197;&#21450;&#39044;&#22788;&#29702;&#31639;&#27861;&#30340;&#26377;&#30028;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11925</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#32780;&#26080;&#38656;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#65306;&#22522;&#20110;&#22810;&#32423;Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11925
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#28151;&#21512;&#26102;&#38388;&#30340;&#39044;&#27979;&#30340;oracle&#30693;&#35782;&#35201;&#27714;&#65292;&#21363;&#24230;&#37327;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#22266;&#23450;&#31574;&#30053;&#19979;&#36798;&#21040;&#20854;&#31283;&#24577;&#20998;&#24067;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#23545;&#20110;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#29699;&#25910;&#25947;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#32423;Actor-Critic&#65288;MAC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#26102;&#38388;&#30693;&#35782;&#30340;&#20381;&#36182;&#24615;&#30340;&#26377;&#25928;&#20943;&#36731;&#65292;&#36825;&#26159;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;$\mathcal{O}$&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.10842</link><description>&lt;p&gt;
&#20351;&#29992;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#30340;&#21452;Transformer&#22312;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Twin Transformer using Gated Dynamic Learnable Attention mechanism for Fault Detection and Diagnosis in the Tennessee Eastman Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;Transformer&#27169;&#22411;&#65292;&#32467;&#21512;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#30340;&#25925;&#38556;&#26816;&#27979;&#19982;&#35786;&#26029;&#65292;&#25552;&#39640;&#24615;&#33021;&#36890;&#36807;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#21644;&#25552;&#21462;&#22810;&#26679;&#21270;&#20449;&#24687;&#65292;&#20197;&#21450;&#21160;&#24577;&#23398;&#20064;&#36866;&#24212;&#24615;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#26816;&#27979;&#21644;&#35786;&#26029;&#65288;FDD&#65289;&#23545;&#20110;&#30830;&#20445;&#24037;&#19994;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#21644;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FDD&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#30000;&#32435;&#35199;&#20234;&#26031;&#26364;&#36807;&#31243;&#65288;TEP&#65289;&#65292;&#36825;&#26159;&#21270;&#24037;&#36807;&#31243;&#25511;&#21046;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;Transformer&#20998;&#25903;&#65292;&#33021;&#22815;&#29420;&#31435;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#21462;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#38376;&#25511;&#21160;&#24577;&#21487;&#23398;&#20064;&#27880;&#24847;&#65288;GDLAttention&#65289;&#65292;&#23427;&#38598;&#25104;&#20102;&#38376;&#25511;&#26426;&#21046;&#21644;&#21160;&#24577;&#23398;&#20064;&#33021;&#21147;&#12290;&#38376;&#25511;&#26426;&#21046;&#35843;&#33410;&#27880;&#24847;&#26435;&#37325;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20851;&#27880;&#36755;&#20837;&#30340;&#26368;&#30456;&#20851;&#37096;&#20998;&#12290;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#27880;&#24847;&#31574;&#30053;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#24615;&#33021;&#12290;&#27880;&#24847;&#26426;&#21046;&#20351;&#29992;&#21452;&#32447;&#24615;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#26469;&#25429;&#25417;&#26597;&#35810;&#21644;&#36755;&#20837;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10842v1 Announce Type: cross  Abstract: Fault detection and diagnosis (FDD) is a crucial task for ensuring the safety and efficiency of industrial processes. We propose a novel FDD methodology for the Tennessee Eastman Process (TEP), a widely used benchmark for chemical process control. The model employs two separate Transformer branches, enabling independent processing of input data and potential extraction of diverse information. A novel attention mechanism, Gated Dynamic Learnable Attention (GDLAttention), is introduced which integrates a gating mechanism and dynamic learning capabilities. The gating mechanism modulates the attention weights, allowing the model to focus on the most relevant parts of the input. The dynamic learning approach adapts the attention strategy during training, potentially leading to improved performance. The attention mechanism uses a bilinear similarity function, providing greater flexibility in capturing complex relationships between query and 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.05645</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Geometric Neural Network based on Phase Space for BCI decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05645
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#31354;&#38388;&#30340;&#20960;&#20309;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;BCI&#35299;&#30721;&#65292;&#25552;&#20379;&#20102;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#20013;&#21487;&#38752;&#31639;&#27861;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#33298;&#36866;&#24230;&#24182;&#20419;&#36827;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Learning(DL)&#31639;&#27861;&#19982;&#33041;&#20449;&#21495;&#20998;&#26512;&#30340;&#25972;&#21512;&#20173;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#65292;&#30456;&#27604;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;(BCI)&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;BCI&#36890;&#36807;&#35299;&#30721;&#22823;&#33041;&#27963;&#21160;&#25511;&#21046;&#22806;&#37096;&#35774;&#22791;&#32780;&#26080;&#38656;&#32908;&#32905;&#25511;&#21046;&#12290;&#33041;&#30005;&#22270;(EEG)&#26159;&#35774;&#35745;BCI&#31995;&#32479;&#30340;&#24191;&#27867;&#36873;&#25321;&#65292;&#22240;&#20854;&#26080;&#21019;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#21644;&#20986;&#33394;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#32570;&#23569;&#35757;&#32451;&#25968;&#25454;&#12289;&#20449;&#22122;&#27604;&#20302;&#12289;&#20197;&#21450;&#22312;&#20010;&#20307;&#38388;&#21644;&#20869;&#37096;&#30340;&#22823;&#37327;&#21464;&#21270;&#12290; &#26368;&#21518;&#65292;&#20351;&#29992;&#22810;&#20010;&#30005;&#26497;&#35774;&#32622;BCI&#31995;&#32479;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#65292;&#38459;&#30861;&#21487;&#38752;DL&#26550;&#26500;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#20043;&#22806;&#30340;BCI&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290; &#20026;&#20102;&#25552;&#39640;&#37319;&#32435;&#29575;&#65292;&#25105;&#20204;&#38656;&#35201;&#25913;&#21892;&#29992;&#25143;&#33298;&#36866;&#24230;&#65292;&#20363;&#22914;&#20351;&#29992;&#23569;&#37327;&#30005;&#26497;&#25805;&#20316;&#30340;&#21487;&#38752;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05645v1 Announce Type: cross  Abstract: The integration of Deep Learning (DL) algorithms on brain signal analysis is still in its nascent stages compared to their success in fields like Computer Vision, especially in Brain-Computer Interface (BCI), where the brain activity is decoded to control external devices without requiring muscle control. Electroencephalography (EEG) is a widely adopted choice for designing BCI systems due to its non-invasive and cost-effective nature and excellent temporal resolution. Still, it comes at the expense of limited training data, poor signal-to-noise, and a large variability across and within-subject recordings. Finally, setting up a BCI system with many electrodes takes a long time, hindering the widespread adoption of reliable DL architectures in BCIs outside research laboratories. To improve adoption, we need to improve user comfort using, for instance, reliable algorithms that operate with few electrodes. \textbf{Approach:} Our research
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.03333</link><description>&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
Solution Simplex Clustering for Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#24605;&#24819;&#65292;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#21333;&#19968;&#21306;&#22495;&#65292;&#20174;&#32780;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#39640;&#24230;&#24322;&#26500;&#30340;&#23458;&#25143;&#20998;&#24067;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#22256;&#38590;&#37096;&#20998;&#28304;&#20110;&#20004;&#20010;&#30475;&#20284;&#30683;&#30462;&#30340;&#30446;&#26631;&#65306;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#20449;&#24687;&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#23398;&#20064;&#24212;&#36866;&#24212;&#27599;&#20010;&#26412;&#22320;&#20998;&#24067;&#30340;&#26412;&#22320;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solution Simplex Clustered Federated Learning&#65288;SosicFL&#65289;&#26469;&#28040;&#38500;&#36825;&#31181;&#30683;&#30462;&#12290;&#22522;&#20110;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#26368;&#26032;&#24605;&#24819;&#65292;SosicFL&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#19968;&#20010;&#21333;&#32431;&#24418;&#20013;&#30340;&#23376;&#21306;&#22495;&#65292;&#24182;&#25191;&#34892;FL&#26469;&#23398;&#20064;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#12290;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#35299;&#20915;&#26041;&#26696;&#21333;&#32431;&#24418;&#30340;&#33258;&#30001;&#24230;&#33539;&#22260;&#20869;&#20855;&#26377;&#20854;&#29305;&#24449;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#36890;&#29992;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SosicFL&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#24182;&#21152;&#36895;&#20102;&#20840;&#23616;&#21644;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03333v1 Announce Type: new  Abstract: We tackle a major challenge in federated learning (FL) -- achieving good performance under highly heterogeneous client distributions. The difficulty partially arises from two seemingly contradictory goals: learning a common model by aggregating the information from clients, and learning local personalized models that should be adapted to each local distribution. In this work, we propose Solution Simplex Clustered Federated Learning (SosicFL) for dissolving such contradiction. Based on the recent ideas of learning solution simplices, SosicFL assigns a subregion in a simplex to each client, and performs FL to learn a common solution simplex. This allows the client models to possess their characteristics within the degrees of freedom in the solution simplex, and at the same time achieves the goal of learning a global common model. Our experiments show that SosicFL improves the performance and accelerates the training process for global and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#31216;&#21464;&#25442;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Generative Model of Symmetry Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#25429;&#25417;&#25968;&#25454;&#30340;&#23545;&#31216;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#23613;&#31649;&#28041;&#21450;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#22312;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36825;&#20123;&#23545;&#31216;&#24615;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21028;&#21035;&#35774;&#32622;&#19978;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26126;&#30830;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#20197;&#21487;&#35299;&#37322;&#26041;&#24335;&#23398;&#20064;&#25968;&#25454;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#23398;&#20064;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20223;&#23556;&#21644;&#39068;&#33394;&#21464;&#25442;&#19979;&#25429;&#25417;&#23545;&#31216;&#24615;&#30340;&#33021;&#21147;&#12290;&#23558;&#25105;&#20204;&#30340;&#23545;&#31216;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#36793;&#38469;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#21644;&#23545;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01946v1 Announce Type: new  Abstract: Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we construct a generative model that explicitly aims to capture symmetries in the data, resulting in a model that learns which symmetries are present in an interpretable way. We provide a simple algorithm for efficiently learning our generative model and demonstrate its ability to capture symmetries under affine and color transformations. Combining our symmetry model with existing generative models results in higher marginal test-log-likelihoods and robustness to data sparsification.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00794</link><description>&lt;p&gt;
&#35748;&#30495;&#23545;&#24453;&#24189;&#40664;&#65306;&#21033;&#29992;&#19981;&#39118;&#36259;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#24189;&#40664;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00794
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#24189;&#40664;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#21462;&#28040;&#24189;&#40664;&#20803;&#32032;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24189;&#40664;&#26159;&#20154;&#31867;&#35748;&#30693;&#21644;&#20114;&#21160;&#30340;&#22522;&#26412;&#35201;&#32032;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#36827;&#23637;&#65292;&#24189;&#40664;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#22240;&#20026;&#24189;&#40664;&#25991;&#26412;&#19982;&#31867;&#20284;&#38750;&#24189;&#40664;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21542;&#36890;&#36807;&#32534;&#36753;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;&#24189;&#40664;&#26816;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#20154;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#24403;&#21069;LLMs&#22312;&#8220;&#21462;&#28040;&#39118;&#36259;&#8221;&#31505;&#35805;&#26041;&#38754;&#26174;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#20154;&#31867;&#21028;&#26029;&#21644;&#24189;&#40664;&#26816;&#27979;&#30340;&#19979;&#28216;&#20219;&#21153;&#34913;&#37327;&#32780;&#24471;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#19968;&#20010;&#28151;&#21512;&#32534;&#30721;&#30340;&#33521;&#35821;-&#21360;&#22320;&#35821;&#24189;&#40664;&#25968;&#25454;&#38598;&#65292;&#22312;&#37027;&#37324;&#25105;&#20204;&#21457;&#29616;GPT-4&#30340;&#21512;&#25104;&#25968;&#25454;&#34987;&#21452;&#35821;&#27880;&#37322;&#21592;&#39640;&#24230;&#35780;&#20215;&#65292;&#24182;&#20026;&#24189;&#40664;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00188</link><description>&lt;p&gt;
&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Decentralized Learning on Player Utilities in Stackelberg Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00188
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20998;&#25955;&#23398;&#20064;&#23545;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#29609;&#23478;&#25928;&#29992;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#26469;&#26356;&#22909;&#25429;&#25417;&#31995;&#32479;&#29305;&#24449;&#65292;&#24182;&#24320;&#21457;&#20102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26102;&#65292;&#36890;&#24120;&#20250;&#38543;&#26102;&#38388;&#21453;&#22797;&#19982;&#21478;&#19968;&#20010;&#23398;&#20064;&#20195;&#29702;&#65288;&#22914;&#29992;&#25143;&#65289;&#20132;&#20114;&#12290;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#21452;&#20195;&#29702;&#31995;&#32479;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#21333;&#29420;&#23398;&#20064;&#65292;&#32780;&#20004;&#20010;&#20195;&#29702;&#30340;&#22870;&#21169;&#24182;&#19981;&#23436;&#20840;&#19968;&#33268;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31867;&#24773;&#20917;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20195;&#29702;&#31995;&#32479;&#30340;&#23398;&#20064;&#21160;&#24577;&#20197;&#21450;&#23545;&#27599;&#20010;&#20195;&#29702;&#30446;&#26631;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#31995;&#32479;&#24314;&#27169;&#20026;&#20855;&#26377;&#20998;&#25955;&#23398;&#20064;&#30340;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#65292;&#24182;&#23637;&#31034;&#26631;&#20934;&#36951;&#25022;&#22522;&#20934;&#65288;&#22914;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#22343;&#34913;&#22238;&#25253;&#65289;&#23548;&#33268;&#33267;&#23569;&#26377;&#19968;&#21517;&#29609;&#23478;&#20986;&#29616;&#26368;&#22351;&#24773;&#20917;&#30340;&#32447;&#24615;&#36951;&#25022;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#36825;&#20123;&#31995;&#32479;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#23545;&#20195;&#29702;&#30340;&#23398;&#20064;&#35823;&#24046;&#23481;&#24525;&#30340;&#25918;&#26494;&#36951;&#25022;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#23398;&#20064;&#31639;&#27861;&#26410;&#33021;&#25552;&#20379;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#24320;&#21457;&#20102;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#21452;&#26041;&#29609;&#23478;&#32780;&#35328;&#19982;&#29702;&#24819;$O(T^{2/3})$&#36951;&#25022;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00188v1 Announce Type: new  Abstract: When deployed in the world, a learning agent such as a recommender system or a chatbot often repeatedly interacts with another learning agent (such as a user) over time. In many such two-agent systems, each agent learns separately and the rewards of the two agents are not perfectly aligned. To better understand such cases, we examine the learning dynamics of the two-agent system and the implications for each agent's objective. We model these systems as Stackelberg games with decentralized learning and show that standard regret benchmarks (such as Stackelberg equilibrium payoffs) result in worst-case linear regret for at least one player. To better capture these systems, we construct a relaxed regret benchmark that is tolerant to small learning errors by agents. We show that standard learning algorithms fail to provide sublinear regret, and we develop algorithms to achieve near-optimal $O(T^{2/3})$ regret for both players with respect to 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.17840</link><description>&lt;p&gt;
&#36981;&#24490;&#25105;&#30340;&#25351;&#31034;&#24182;&#35828;&#20986;&#30495;&#30456;&#65306;&#26469;&#33258;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17840
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#65292;&#25351;&#20986;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#25915;&#20987;&#23545;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#36896;&#25104;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#23450;&#21046;&#36866;&#24212;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;Retrieval-In-Context RAG&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23545;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#30340;LMs&#26500;&#24314;&#30340;RAG&#31995;&#32479;&#36827;&#34892;&#25552;&#31034;&#27880;&#20837;&#26102;&#65292;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;LMs&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#36731;&#26494;&#22320;&#20174;&#25968;&#25454;&#23384;&#20648;&#20013;&#30452;&#25509;&#25552;&#21462;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#31181;&#28431;&#27934;&#23384;&#22312;&#20110;&#35206;&#30422;Llama2&#12289;Mistral/Mixtral&#12289;Vicuna&#12289;SOLAR&#12289;WizardLM&#12289;Qwen1.5&#21644;Platypus2&#31561;&#22810;&#31181;&#29616;&#20195;LMs&#30340;&#24191;&#27867;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#21033;&#29992;&#33021;&#21147;&#21152;&#21095;&#12290;&#23558;&#30740;&#31350;&#25193;&#23637;&#21040;&#29983;&#20135;RAG&#27169;&#22411;GPTs&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#21487;&#20197;&#22312;&#23545;25&#20010;&#38543;&#26426;&#36873;&#25321;&#30340;&#23450;&#21046;GPTs&#26045;&#21152;&#26368;&#22810;2&#20010;&#26597;&#35810;&#26102;&#20197;100%&#25104;&#21151;&#29575;&#23548;&#33268;&#25968;&#25454;&#23384;&#20648;&#27844;&#28431;&#65292;&#24182;&#19988;&#25105;&#20204;&#33021;&#22815;&#20197;77,000&#23383;&#30340;&#20070;&#31821;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;41%&#65292;&#20197;&#21450;&#22312;&#21547;&#26377;1,569,00&#35789;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#25552;&#21462;&#29575;&#20026;3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17840v1 Announce Type: cross  Abstract: Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. We study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. The vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. Extending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,00
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#31639;&#27861;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#22320;&#35299;&#20915;&#26657;&#20934;&#24615;&#27979;&#35797;&#38382;&#39064;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#20493;&#25968;&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.13187</link><description>&lt;p&gt;
&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#27979;&#35797;&#26657;&#20934;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing Calibration in Subquadratic Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#31639;&#27861;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#32447;&#24615;&#35268;&#21010;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#22320;&#35299;&#20915;&#26657;&#20934;&#24615;&#27979;&#35797;&#38382;&#39064;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#20493;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#25991;&#29486;&#20013;&#65292;&#26657;&#20934;&#24615;&#24050;&#32463;&#25104;&#20026;&#20108;&#20803;&#39044;&#27979;&#27169;&#22411;&#36755;&#20986;&#30340;&#19968;&#20010;&#20540;&#24471;&#26399;&#26395;&#21644;&#24191;&#27867;&#30740;&#31350;&#30340;&#32479;&#35745;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#27979;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#31639;&#27861;&#26041;&#38754;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#35770;&#25991; [BGHN23] &#30340;&#21551;&#21457;&#19979;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#26469;&#34913;&#37327;&#21040;&#26657;&#20934;&#24615;&#30340;&#36317;&#31163;&#65292;&#25105;&#20204;&#36890;&#36807;&#23646;&#24615;&#27979;&#35797;&#30340;&#35270;&#35282;&#24341;&#20837;&#20102;&#26657;&#20934;&#24615;&#30740;&#31350;&#30340;&#31639;&#27861;&#26041;&#38754;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#26657;&#20934;&#24615;&#27979;&#35797;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20174;&#20998;&#24067; $\mathcal{D}$&#65288;&#39044;&#27979;&#65292;&#20108;&#20803;&#32467;&#26524;&#65289;&#20013;&#32473;&#20986; $n$ &#27425;&#25277;&#26679;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998; $\mathcal{D}$ &#23436;&#20840;&#26657;&#20934;&#21644; $\mathcal{D}$ &#36317;&#31163;&#26657;&#20934;&#24615;&#20026; $\varepsilon$ &#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13187v1 Announce Type: new  Abstract: In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;</title><link>https://arxiv.org/abs/2402.10748</link><description>&lt;p&gt;
&#20215;&#20540;16&#20010;&#23383;&#30340;&#22122;&#22768;&#33410;&#25293;: &#19968;&#31181;&#29992;&#20110;&#24494;&#25511;&#21046;&#22120;&#20302;&#21151;&#29575;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#24494;&#22411;Transformer
&lt;/p&gt;
&lt;p&gt;
A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10748
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#30417;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#21487;&#31359;&#25140;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#24191;&#27867;&#24212;&#29992;&#19988;&#26377;&#20215;&#20540;&#30340;&#36164;&#20135;&#12290;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#20998;&#26512;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#20197;&#21450;&#26816;&#27979;&#24515;&#33039;&#29366;&#20917;&#65288;&#22914;&#24515;&#24459;&#22833;&#24120;&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26159;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#22312;&#21487;&#31359;&#25140;&#39046;&#22495;&#30340;&#39640;&#25928;&#23454;&#29616;&#21364;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#35774;&#35745;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#20860;&#39038;&#36275;&#22815;&#31934;&#24230;&#21644;&#36866;&#24403;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;ECG&#20449;&#21495;&#30340;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#20165;&#38656;&#35201;6k&#20010;&#21442;&#25968;&#65292;&#22312;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#35782;&#21035;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;&#26102;&#36798;&#21040;&#20102;98.97%&#30340;&#20934;&#30830;&#29575;&#65292;&#32771;&#34385;&#21040;&#23545;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#35774;&#22791;&#36827;&#34892;&#39640;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;8&#20301;&#25972;&#25968;&#25512;&#29702;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10748v1 Announce Type: cross  Abstract: Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model. Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an 
&lt;/p&gt;</description></item><item><title>&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.10517</link><description>&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#65306;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLM&#30340;&#20302;&#25104;&#26412;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10517
&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#36825;&#20123;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#20307;&#31215;&#32780;&#23548;&#33268;&#37096;&#32626;&#25104;&#26412;&#39640;&#26114;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#37096;&#32626;&#30340;&#25104;&#26412;&#22312;&#23454;&#38469;&#24847;&#20041;&#19978;&#24456;&#37325;&#35201;&#65292;&#20294;&#21364;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#31934;&#24230;LLM&#8221;&#65292;&#23558;&#20219;&#24847;&#31934;&#24230;DNN&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;LLMs&#12290;&#35299;&#20915;&#20102;&#20219;&#24847;&#31934;&#24230;LLM&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;LLMs&#20219;&#24847;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#36719;&#20214;&#24341;&#25806;&#26469;&#23454;&#29616;&#20854;&#26377;&#25928;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20197;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;&#8230;&#65292;n&#20301;&#65289;&#37327;&#21270;&#30340;LLMs&#21472;&#21152;&#21040;&#20869;&#23384;&#36275;&#21360;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#30340;&#39640;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10088</link><description>&lt;p&gt;
&#20998;&#23618;&#28151;&#21512;&#24314;&#27169;&#29992;&#20110;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical hybrid modeling for flexible tool use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#20013;&#65292;&#31163;&#25955;&#27169;&#22411;&#21487;&#20197;&#19982;&#36830;&#32493;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#31616;&#21333;&#30340;&#20195;&#29702;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19990;&#30028;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#28151;&#21512; - &#36830;&#32493;&#21644;&#31163;&#25955; - &#21333;&#20803;&#32452;&#25104;&#65292;&#22797;&#21046;&#20195;&#29702;&#30340;&#37197;&#32622;&#65292;&#30001;&#39640;&#32423;&#31163;&#25955;&#27169;&#22411;&#25511;&#21046;&#65292;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;&#21644;&#21516;&#27493;&#34892;&#20026;&#12290;&#27599;&#20010;&#23618;&#27425;&#20869;&#37096;&#30340;&#36827;&#19968;&#27493;&#20998;&#35299;&#21487;&#20197;&#20197;&#20998;&#23618;&#26041;&#24335;&#34920;&#31034;&#19982;self&#30456;&#20851;&#30340;&#20854;&#20182;&#20195;&#29702;&#21644;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65306;&#22312;&#25342;&#21462;&#19968;&#20010;&#31227;&#21160;&#24037;&#20855;&#21518;&#21040;&#36798;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#25512;&#29702;&#20026;&#25511;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10088v1 Announce Type: cross  Abstract: In a recent computational framework called active inference, discrete models can be linked to their continuous counterparts to perform decision-making in changing environments. From another perspective, simple agents can be combined to better capture the causal relationships of the world. How can we use these two features together to achieve efficient goal-directed behavior? We present an architecture composed of several hybrid -- continuous and discrete -- units replicating the agent's configuration, controlled by a high-level discrete model that achieves dynamic planning and synchronized behavior. Additional factorizations within each level allow to represent hierarchically other agents and objects in relation to the self. We evaluate this hierarchical hybrid model on a non-trivial task: reaching a moving object after having picked a moving tool. This study extends past work on control as inference and proposes an alternative directi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#65292;&#21457;&#29616;&#26435;&#37325;&#20250;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#21644;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#12290;</title><link>https://arxiv.org/abs/2402.09226</link><description>&lt;p&gt;
&#22312;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#21021;&#20540;&#21644;&#38797;&#28857;&#38468;&#36817;&#30340;&#26041;&#21521;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#65292;&#21457;&#29616;&#26435;&#37325;&#20250;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#21644;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#27425;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#22312;&#23567;&#21021;&#20540;&#38468;&#36817;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#25152;&#26377;&#26435;&#37325;&#37117;&#21021;&#22987;&#21270;&#22312;&#21407;&#28857;&#38468;&#36817;&#12290;&#38024;&#23545;&#24179;&#26041;&#35823;&#24046;&#21644;&#36923;&#36753;&#25439;&#22833;&#65292;&#35770;&#25991;&#35777;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#20540;&#65292;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#22312;&#21407;&#28857;&#38468;&#36817;&#33457;&#36153;&#36275;&#22815;&#30340;&#26102;&#38388;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#21487;&#20197;&#36817;&#20284;&#22320;&#22312;&#26041;&#21521;&#19978;&#25910;&#25947;&#21040;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#28857;&#65292;&#35813;&#20989;&#25968;&#37327;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09226v1 Announce Type: new Abstract: This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65288;PHATGOOSE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05859</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#19987;&#23478;&#36335;&#30001;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning to Route Among Specialized Experts for Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65288;PHATGOOSE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#65292;&#20174;&#32780;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#8220;&#19987;&#23478;&#8221;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#36890;&#36807;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#25110;&#39046;&#22495;&#12290;&#25105;&#20204;&#22914;&#20309;&#37325;&#29992;&#22823;&#37327;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#21602;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36335;&#30001;&#20110;&#36890;&#36807;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#29983;&#25104;&#30340;&#19987;&#23478;&#27169;&#22359;&#20043;&#38388;&#12290;&#19982;&#36807;&#21435;&#23398;&#20064;&#22312;&#19987;&#19994;&#27169;&#22411;&#20043;&#38388;&#36335;&#30001;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32493;&#33258;&#36866;&#24212;&#36880;&#26631;&#35760;&#38376;&#25511;&#26426;&#21046;&#25506;&#35752;&#20102;&#22914;&#26524;&#36890;&#36807;&#23545;&#27599;&#20010;&#20196;&#29260;&#21644;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#23618;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#19981;&#21516;&#30340;&#19987;&#23478;&#65292;&#38646;&#26679;&#26412;&#27867;&#21270;&#26159;&#21542;&#20250;&#24471;&#21040;&#25913;&#21892;&#30340;&#21487;&#33021;&#24615;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21518;&#32493;&#30340;&#65292;&#19981;&#38656;&#35201;&#21516;&#26102;&#35775;&#38382;&#29992;&#20110;&#21019;&#24314;&#19987;&#19994;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#27599;&#20010;&#19987;&#23478;&#27169;&#22411;&#21518;&#21482;&#38656;&#35201;&#36866;&#37327;&#30340;&#39069;&#22806;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a widespread proliferation of "expert" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range 
&lt;/p&gt;</description></item><item><title>DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.05421</link><description>&lt;p&gt;
DiffTOP: &#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05421
&lt;/p&gt;
&lt;p&gt;
DiffTOP&#20351;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#26469;&#29983;&#25104;&#21160;&#20316;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#24182;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiffTOP&#65292;&#23427;&#21033;&#29992;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#65292;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#29983;&#25104;&#21160;&#20316;&#12290;&#36712;&#36857;&#20248;&#21270;&#26159;&#19968;&#31181;&#22312;&#25511;&#21046;&#39046;&#22495;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#30001;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#21487;&#24494;&#20998;&#36712;&#36857;&#20248;&#21270;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#25439;&#22833;&#23545;&#20110;&#36712;&#36857;&#20248;&#21270;&#30340;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#22240;&#27492;&#65292;&#36712;&#36857;&#20248;&#21270;&#30340;&#25104;&#26412;&#21644;&#21160;&#21147;&#23398;&#20989;&#25968;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#12290;DiffTOP&#35299;&#20915;&#20102;&#20043;&#21069;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#8220;&#30446;&#26631;&#19981;&#21305;&#37197;&#8221;&#38382;&#39064;&#65292;&#22240;&#20026;DiffTOP&#20013;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36890;&#36807;&#36712;&#36857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#25439;&#22833;&#30452;&#25509;&#26368;&#22823;&#21270;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;DiffTOP&#22312;&#26631;&#20934;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#22871;&#20214;&#20013;&#36827;&#34892;&#20102;&#27169;&#20223;&#23398;&#20064;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.05147</link><description>&lt;p&gt;
ApiQ&#65306;2&#20301;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ApiQ: Finetuning of 2-Bit Quantized Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#20869;&#23384;&#39640;&#25928;&#30340;&#27169;&#22411;&#24494;&#35843;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#20869;&#23384;&#38480;&#21046;&#21644;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#21487;&#27604;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#32422;&#26463;&#12290;&#23613;&#31649;&#26377;&#20102;&#36827;&#23637;&#65292;&#22914;QLoRA&#36825;&#26679;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#37327;&#21270;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#20027;&#35201;&#26469;&#33258;&#20110;&#37327;&#21270;&#36807;&#31243;&#23545;&#20445;&#30041;&#30693;&#35782;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24494;&#35843;&#20013;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;LLM&#30340;&#26435;&#37325;&#26469;&#24674;&#22797;&#37327;&#21270;&#25439;&#22833;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21407;&#22987;LLM&#30340;&#28608;&#27963;&#31934;&#24230;&#30340;&#32500;&#25345;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03325</link><description>&lt;p&gt;
&#36830;&#25509;&#24310;&#36831;&#65306;&#21033;&#29992;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#24494;&#35843;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#37326;&#29983;&#21160;&#29289;&#30456;&#26426;&#38519;&#38449;&#30340;&#26631;&#35760;&#22270;&#20687;&#65289;&#36890;&#24120;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65288;&#20363;&#22914;&#26032;&#30340;&#30456;&#26426;&#38519;&#38449;&#20301;&#32622;&#30340;&#22270;&#20687;&#65289;&#26102;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#23384;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#22495;&#36866;&#24212;&#35774;&#32622;&#20013;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#25110;&#23545;&#27604;&#23398;&#20064;&#65289;&#26159;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#23558;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30456;&#36830;&#25509;&#30340;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#36974;&#34109;&#25110;&#21098;&#35009;&#65289;&#26469;&#25552;&#39640;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#21363;&#20351;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20004;&#20010;&#39046;&#22495;&#30456;&#24046;&#24456;&#36828;&#12290;&#26412;&#25991;&#36890;&#36807;&#30495;&#23454;&#20219;&#21153;&#23637;&#31034;&#20102;&#22312;&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#24182;&#19981;&#33021;&#25345;&#32493;&#25913;&#21892;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#30456;&#27604;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#20174;&#22836;&#35757;&#32451;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#26469;&#24212;&#23545;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#25509;&#24310;&#36831;&#65288;Connect Later&#65289;&#65306;&#22312;&#20351;&#29992;&#36890;&#29992;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;&#29992;&#22522;&#20110;&#23545;&#30446;&#26631;&#39046;&#22495;&#20102;&#35299;&#30340;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03268</link><description>&lt;p&gt;
&#20174;&#25512;&#29702;&#36335;&#24452;&#32858;&#21512;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding the Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;&#32858;&#21512;&#38388;&#25509;&#25512;&#29702;&#36335;&#24452;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20135;&#29983;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26126;&#30830;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#29702;&#35299;&#39044;&#35757;&#32451;&#19982;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30446;&#26631;&#30340;&#20851;&#31995;&#22914;&#20309;&#20419;&#20351;&#25512;&#29702;&#33021;&#21147;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#35270;&#20026;&#22312;&#39044;&#35757;&#32451;&#26102;&#36890;&#36807;&#32858;&#21512;&#38388;&#25509;&#30340;&#25512;&#29702;&#36335;&#24452;&#26469;&#24471;&#20986;&#26032;&#32467;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35270;&#35282;&#22312;&#36923;&#36753;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#31561;&#20851;&#38190;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#36335;&#24452;&#24418;&#24335;&#21270;&#20026;&#22312;&#30693;&#35782;/&#25512;&#29702;&#22270;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#12290;&#23545;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#24067;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20851;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#27010;&#29575;&#30340;&#21152;&#26435;&#21644;&#26159;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#21512;&#29702;&#26041;&#24335;&#12290;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#21644;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#35757;&#32451;&#23545;&#38543;&#26426;&#28216;&#36208;&#36335;&#24452;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#22686;&#21152;&#26080;&#26631;&#31614;&#30340;&#38543;&#26426;&#28216;&#36208;&#25512;&#29702;&#36335;&#24452;&#21487;&#20197;&#25552;&#39640;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and math reasoning with math word problems (MWPs). More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and MWP datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01796</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring transfer learning for pathological speech feature prediction: Impact of layer selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#20110;&#30149;&#29702;&#35821;&#38899;&#29305;&#24449;&#39044;&#27979;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#21457;&#29616;&#36873;&#25321;&#36866;&#24403;&#30340;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#24182;&#19988;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23545;&#20020;&#24202;&#35821;&#38899;&#36827;&#34892;&#33258;&#21160;&#23458;&#35266;&#35780;&#20272;&#65292;&#24182;&#20419;&#36827;&#35821;&#38899;&#38556;&#30861;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#39044;&#27979;&#30149;&#29702;&#35821;&#38899;&#23384;&#22312;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#23618;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#26368;&#20339;&#23618;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65288;&#24179;&#22343;&#24179;&#34913;&#20934;&#30830;&#29575;&#22686;&#21152;12.4%&#65289;&#65292;&#23613;&#31649;&#26368;&#20339;&#23618;&#22240;&#39044;&#27979;&#29305;&#24449;&#32780;&#24322;&#65292;&#24182;&#19988;&#24182;&#19981;&#24635;&#26159;&#23545;&#26410;&#35265;&#25968;&#25454;&#27867;&#21270;&#33391;&#22909;&#12290;&#23398;&#24471;&#30340;&#21152;&#26435;&#21644;&#22312;&#20998;&#24067;&#20869;&#19982;&#24179;&#22343;&#26368;&#20339;&#23618;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is interest in leveraging AI to conduct automatic, objective assessments of clinical speech, in turn facilitating diagnosis and treatment of speech disorders. We explore transfer learning, focusing on the impact of layer selection, for the downstream task of predicting the presence of pathological speech. We find that selecting an optimal layer offers large performance improvements (12.4% average increase in balanced accuracy), though the best layer varies by predicted feature and does not always generalize well to unseen data. A learned weighted sum offers comparable performance to the average best layer in-distribution and has better generalization for out-of-distribution data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;</title><link>https://arxiv.org/abs/2402.00795</link><description>&lt;p&gt;
LLMs&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#25511;&#21046;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LLMs&#23545;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#21457;&#29616;LLaMA 2&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#36234;&#38271;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#36234;&#39640;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;-shot&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#28982;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#21463;&#29289;&#29702;&#21407;&#29702;&#25511;&#21046;&#30340;&#21160;&#21147;&#31995;&#32479;&#34892;&#20026;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#35201;&#22312;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;LLaMA 2&#22312;&#27809;&#26377;&#24494;&#35843;&#25110;&#25552;&#31034;&#24037;&#31243;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#29289;&#29702;&#35268;&#21017;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#36755;&#20837;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20013;&#30340;&#31070;&#32463;&#27604;&#20363;&#23450;&#24459;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20301;&#25968;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#36827;&#34892;&#24555;&#36895;&#37319;&#26679;&#12290;&#36825;&#31181;&#31639;&#27861;&#26159;&#23545;Mirror Langevin&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#28155;&#21152;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#26469;&#28040;&#38500;&#28176;&#36817;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2312.08823</link><description>&lt;p&gt;
&#20351;&#29992;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#24555;&#36895;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#36827;&#34892;&#24555;&#36895;&#37319;&#26679;&#12290;&#36825;&#31181;&#31639;&#27861;&#26159;&#23545;Mirror Langevin&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#28155;&#21152;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#26469;&#28040;&#38500;&#28176;&#36817;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20854;&#25903;&#25345;&#26159;&#32039;&#20984;&#38598;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#36817;&#20284;&#37319;&#26679;&#12290;&#35813;&#31639;&#27861;&#22312;Mirror Langevin&#31639;&#27861;&#65288;Zhang et al., 2020&#65289;&#30340;&#21333;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#65292;Mirror Langevin&#31639;&#27861;&#26159;Mirror Langevin&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#31163;&#25955;&#21270;&#12290;&#30001;&#20110;&#21253;&#21547;&#20102;&#36825;&#20010;&#36807;&#28388;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30446;&#26631;&#26159;&#26080;&#20559;&#30340;&#65292;&#32780;&#24050;&#30693;&#30340;Mirror Langevin&#31639;&#27861;&#31561;Mirror Langevin&#21160;&#21147;&#23398;&#30340;&#31163;&#25955;&#21270;&#20855;&#26377;&#28176;&#36817;&#20559;&#24046;&#12290;&#23545;&#20110;&#35813;&#31639;&#27861;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#28151;&#21512;&#21040;&#19968;&#20010;&#30456;&#23545;&#24179;&#28369;&#12289;&#20984;&#24615;&#22909;&#19988;&#19982;&#33258;&#20849;&#36717;&#38236;&#20687;&#20989;&#25968;&#30456;&#20851;&#30340;&#32422;&#26463;&#20998;&#24067;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#19978;&#30028;&#12290;&#30001;&#20110;&#21253;&#21547;Metropolis-Hastings&#36807;&#28388;&#22120;&#23548;&#33268;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#26159;&#21487;&#36870;&#30340;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23545;&#35823;&#24046;&#30340;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the erro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;</title><link>https://arxiv.org/abs/2312.01678</link><description>&lt;p&gt;
Jellyfish&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jellyfish: A Large Language Model for Data Preprocessing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#31649;&#36947;&#20013;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#21033;&#20110;&#31616;&#21333;&#22788;&#29702;&#30340;&#24178;&#20928;&#26684;&#24335;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65288;DP&#65289;&#20013;LLMs&#30340;&#21033;&#29992;&#12290;&#19982;&#20351;&#29992;LLMs&#20026;DP&#35774;&#35745;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20852;&#36259;&#30456;&#27604;&#65292;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20513;&#35758;&#36890;&#24120;&#20381;&#36182;&#20110;GPT API&#65292;&#24341;&#21457;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#25968;&#25454;&#27844;&#38671;&#25285;&#24551;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#65288;7-13B&#27169;&#22411;&#65289;&#20316;&#20026;&#36890;&#29992;DP&#38382;&#35299;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;DP&#20219;&#21153;&#30340;&#22235;&#32452;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;DP&#23450;&#21046;&#30340;&#24207;&#21015;&#21270;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;DP&#25163;&#21160;&#21046;&#23450;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#21333;&#19968;&#21644;&#20215;&#26684;&#20302;&#24265;&#30340;GPU&#19978;&#36816;&#34892;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#23454;&#29616;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20026;DP&#25351;&#23548;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.16054</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;
&lt;/p&gt;
&lt;p&gt;
Metric Space Magnitude for Evaluating the Diversity of Latent Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16054
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#21487;&#31283;&#23450;&#35745;&#31639;&#65292;&#33021;&#22815;&#36827;&#34892;&#22810;&#23610;&#24230;&#27604;&#36739;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#30340;&#22823;&#23567;&#26159;&#19968;&#31181;&#36817;&#26399;&#24314;&#31435;&#30340;&#19981;&#21464;&#24615;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#25552;&#20379;&#31354;&#38388;&#30340;&#8220;&#26377;&#25928;&#22823;&#23567;&#8221;&#30340;&#34913;&#37327;&#65292;&#24182;&#25429;&#25417;&#21040;&#35768;&#22810;&#20960;&#20309;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;&#22823;&#23567;&#30340;&#28508;&#22312;&#34920;&#31034;&#20869;&#22312;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#24418;&#24335;&#21270;&#20102;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#20989;&#25968;&#20043;&#38388;&#30340;&#26032;&#39062;&#19981;&#30456;&#20284;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#25968;&#25454;&#25200;&#21160;&#19979;&#20445;&#35777;&#31283;&#23450;&#65292;&#21487;&#20197;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#19988;&#33021;&#22815;&#23545;&#28508;&#22312;&#34920;&#31034;&#36827;&#34892;&#20005;&#26684;&#30340;&#22810;&#23610;&#24230;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24230;&#37327;&#22312;&#23454;&#39564;&#22871;&#20214;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#21331;&#36234;&#24615;&#33021;&#65292;&#21253;&#25324;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#35780;&#20272;&#12289;&#27169;&#24335;&#23849;&#28291;&#26816;&#27979;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The magnitude of a metric space is a recently-established invariant, providing a measure of the 'effective size' of a space across multiple scales while also capturing numerous geometrical properties. We develop a family of magnitude-based measures of the intrinsic diversity of latent representations, formalising a novel notion of dissimilarity between magnitude functions of finite metric spaces. Our measures are provably stable under perturbations of the data, can be efficiently calculated, and enable a rigorous multi-scale comparison of latent representations. We show the utility and superior performance of our measures in an experimental suite that comprises different domains and tasks, including the evaluation of diversity, the detection of mode collapse, and the evaluation of generative models for text, image, and graph data.
&lt;/p&gt;</description></item><item><title>&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2311.11772</link><description>&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#23601;&#26159;&#20320;&#22312;&#24369;&#30417;&#30563;&#30149;&#29702;&#23398;&#20999;&#29255;&#20998;&#31867;&#20013;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
A Good Feature Extractor Is All You Need for Weakly Supervised Pathology Slide Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24369;&#30417;&#30563;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#20110;&#24120;&#35268;&#35748;&#30693;&#30340;&#35266;&#24565;&#65292;&#30740;&#31350;&#21457;&#29616;&#30465;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#33021;&#33410;&#30465;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#35748;&#20026;&#26579;&#33394;&#26631;&#20934;&#21270;&#26159;&#35745;&#31639;&#30149;&#29702;&#23398;&#27969;&#31243;&#20013;&#20851;&#38190;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#22312;&#24369;&#30417;&#30563;&#30340;&#25972;&#20010;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#29615;&#22659;&#20013;&#23545;&#36825;&#19968;&#20449;&#24565;&#25552;&#20986;&#36136;&#30097;&#65292;&#36825;&#19968;&#20449;&#24565;&#26159;&#30001;&#35757;&#32451;&#22312;&#22810;&#26679;&#21270;&#30149;&#29702;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24378;&#22823;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#20986;&#29616;&#25152;&#28608;&#21169;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#36804;&#20170;&#20026;&#27490;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#30149;&#29702;&#23398;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#20102;&#26368;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#28041;&#21450;&#20061;&#20010;&#20219;&#21153;&#12289;&#20116;&#20010;&#25968;&#25454;&#38598;&#12289;&#19977;&#20010;&#19979;&#28216;&#26550;&#26500;&#21644;&#21508;&#31181;&#39044;&#22788;&#29702;&#35774;&#32622;&#20013;&#30340;8000&#22810;&#20010;&#35757;&#32451;&#36816;&#34892;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24573;&#30053;&#26579;&#33394;&#26631;&#20934;&#21270;&#21644;&#22270;&#20687;&#22686;&#24378;&#24182;&#19981;&#20250;&#25439;&#23475;&#19979;&#28216;&#20999;&#29255;&#32423;&#21035;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20250;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#19978;&#24102;&#26469;&#22823;&#37327;&#33410;&#30465;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20419;&#36827;&#20102;&#30456;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#22909;&#30340;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#25552;&#21462;&#22120;&#65292;&#24182;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11772v4 Announce Type: replace-cross  Abstract: Stain normalisation is thought to be a crucial preprocessing step in computational pathology pipelines. We question this belief in the context of weakly supervised whole slide image classification, motivated by the emergence of powerful feature extractors trained using self-supervised learning on diverse pathology datasets. To this end, we performed the most comprehensive evaluation of publicly available pathology feature extractors to date, involving more than 8,000 training runs across nine tasks, five datasets, three downstream architectures, and various preprocessing setups. Notably, we find that omitting stain normalisation and image augmentations does not compromise downstream slide-level classification performance, while incurring substantial savings in memory and compute. Using a new evaluation metric that facilitates relative downstream performance comparison, we identify the best publicly available extractors, and sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.02761</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#22312;&#26410;&#30693;&#25104;&#26412;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
One-Shot Strategic Classification Under Unknown Costs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#24773;&#26223;&#65292;&#38024;&#23545;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#24182;&#23558;&#20219;&#21153;&#23450;&#20041;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#20998;&#31867;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#23545;&#31574;&#30053;&#36755;&#20837;&#25805;&#32437;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20915;&#31574;&#35268;&#21017;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#36825;&#20123;&#21709;&#24212;&#26159;&#24050;&#30693;&#30340;&#65307;&#32780;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22788;&#29702;&#26410;&#30693;&#21709;&#24212;&#65292;&#20294;&#23427;&#20204;&#19987;&#38376;&#30740;&#31350;&#37325;&#22797;&#27169;&#22411;&#37096;&#32626;&#30340;&#22312;&#32447;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#25919;&#31574;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#28608;&#21169;&#29992;&#20363;&#20013;&#65292;&#22810;&#27425;&#37096;&#32626;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#29978;&#33267;&#19968;&#20010;&#31967;&#31957;&#30340;&#36718;&#27425;&#37117;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22312;&#26410;&#30693;&#21709;&#24212;&#19979;&#30340;&#19968;&#27425;&#24615;&#31574;&#30053;&#20998;&#31867;&#30340;&#27491;&#24335;&#30740;&#31350;&#65292;&#36825;&#38656;&#35201;&#22312;&#19968;&#27425;&#24615;&#36873;&#25321;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#30528;&#37325;&#20851;&#27880;&#29992;&#25143;&#25104;&#26412;&#20989;&#25968;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#23545;&#20110;&#19968;&#31867;&#24191;&#27867;&#30340;&#25104;&#26412;&#65292;&#21363;&#20351;&#23545;&#30495;&#23454;&#25104;&#26412;&#30340;&#23567;&#35823;&#24046;&#20063;&#21487;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#33267;&#26497;&#20302;&#27700;&#24179;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#26694;&#23450;&#20026;&#26497;&#23567;-&#26497;&#22823;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02761v2 Announce Type: replace  Abstract: The goal of strategic classification is to learn decision rules which are robust to strategic input manipulation. Earlier works assume that these responses are known; while some recent works handle unknown responses, they exclusively study online settings with repeated model deployments. But there are many domains$\unicode{x2014}$particularly in public policy, a common motivating use case$\unicode{x2014}$where multiple deployments are infeasible, or where even one bad round is unacceptable. To address this gap, we initiate the formal study of one-shot strategic classification under unknown responses, which requires committing to a single classifier once. Focusing on uncertainty in the users' cost function, we begin by proving that for a broad class of costs, even a small mis-estimation of the true cost can entail trivial accuracy in the worst case. In light of this, we frame the task as a minimax problem, with the goal of identifying
&lt;/p&gt;</description></item><item><title>&#23558;&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#23450;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#22312;&#35813;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05723</link><description>&lt;p&gt;
&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36229;&#39046;&#22495;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05723
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22312;&#32447;&#21040;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#23450;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#22312;&#35813;&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#39044;&#35757;&#32451;&#37197;&#21512;&#38745;&#24577;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#22312;&#32447;&#24494;&#35843;&#65288;&#31163;&#32447;&#21040;&#22312;&#32447;&#65292;&#21363;OtO&#65289;&#26159;&#19968;&#20010;&#19982;&#29616;&#23454;&#19990;&#30028;RL&#37096;&#32626;&#36807;&#31243;&#24456;&#21305;&#37197;&#30340;&#33539;&#24335;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#22312;&#32447;&#20132;&#20114;&#39044;&#31639;&#20869;&#25214;&#21040;&#24615;&#33021;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;&#20197;&#21069;&#22312;OtO&#35774;&#32622;&#20013;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#32416;&#27491;&#30001;&#31163;&#32447;RL&#31639;&#27861;&#30340;&#31574;&#30053;&#32422;&#26463;&#26426;&#21046;&#24341;&#20837;&#30340;&#20559;&#24046;&#19978;&#12290;&#36825;&#20123;&#32422;&#26463;&#20351;&#24471;&#23398;&#20064;&#30340;&#31574;&#30053;&#25509;&#36817;&#25910;&#38598;&#25968;&#25454;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#34892;&#20026;&#31574;&#30053;&#36828;&#38750;&#26368;&#20248;&#65292;&#21017;&#36825;&#21487;&#33021;&#20250;&#19981;&#24517;&#35201;&#22320;&#38480;&#21046;&#31574;&#30053;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25918;&#24323;&#32422;&#26463;&#65292;&#25226;OtO RL&#20316;&#20026;&#19968;&#20010;&#25506;&#32034;&#38382;&#39064;&#26469;&#26694;&#23450;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22312;&#32447;&#25968;&#25454;&#37319;&#38598;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22522;&#20110;&#20869;&#22312;&#22870;&#21169;&#21644;UCB&#30340;&#20027;&#35201;&#22312;&#32447;RL&#25506;&#32034;&#26041;&#27861;&#22312;OtO&#35774;&#32622;&#20013;&#30340;&#25928;&#26524;&#65292;&#26174;&#31034;&#20869;&#22312;&#22870;&#21169;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#30340;&#20462;&#25913;&#22686;&#21152;&#20102;&#35757;&#32451;&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05723v2 Announce Type: replace  Abstract: Offline pretraining with a static dataset followed by online fine-tuning (offline-to-online, or OtO) is a paradigm well matched to a real-world RL deployment process. In this scenario, we aim to find the best-performing policy within a limited budget of online interactions. Previous work in the OtO setting has focused on correcting for bias introduced by the policy-constraint mechanisms of offline RL algorithms. Such constraints keep the learned policy close to the behavior policy that collected the dataset, but we show this can unnecessarily limit policy performance if the behavior policy is far from optimal. Instead, we forgo constraints and frame OtO RL as an exploration problem that aims to maximize the benefit of online data-collection. We first study the major online RL exploration methods based on intrinsic rewards and UCB in the OtO setting, showing that intrinsic rewards add training instability through reward-function modif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2309.01243</link><description>&lt;p&gt;
&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#21450;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Normal Distributions Indistinguishability Spectrum and its Application to Privacy-Preserving Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702; (NDIS Theorem)&#65292;&#26088;&#22312;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#25913;&#36827;&#38543;&#26426;&#21270;&#26426;&#22120;&#23398;&#20064;&#26597;&#35810;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;(DP)&#65292;&#36890;&#24120;&#38656;&#35201;&#38543;&#26426;&#21270;&#22522;&#30784;&#26597;&#35810;&#30340;&#36755;&#20986;&#12290;&#22312;&#22823;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#38543;&#26426;&#21270;&#33609;&#22270;/&#32858;&#21512;&#31639;&#27861;&#26469;&#20351;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#21464;&#24471;&#21487;&#34892;&#12290;&#30452;&#35266;&#22320;&#65292;&#36825;&#26679;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#24212;&#35813;&#25552;&#20379;&#19968;&#20123;&#22266;&#26377;&#30340;&#38544;&#31169;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;DP&#26426;&#21046;&#24182;&#27809;&#26377;&#21033;&#29992;&#36825;&#31181;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#65292;&#23548;&#33268;&#28508;&#22312;&#30340;&#22810;&#20313;&#22122;&#38899;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#21160;&#26426;&#38382;&#39064;&#26159;&#65306;(&#22914;&#20309;)&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#26412;&#36523;&#30340;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#38543;&#26426;&#21270;ML&#26597;&#35810;&#30340;DP&#26426;&#21046;&#30340;&#25928;&#29992;&#65311;&#20026;&#20102;&#32473;&#20986;&#31215;&#26497;&#30340;&#31572;&#26696;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27491;&#24577;&#20998;&#24067;&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;&#23450;&#29702;(&#31616;&#31216;&#20026;NDIS&#23450;&#29702;)&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#23454;&#38469;&#24433;&#21709;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;NDIS&#26159;&#19968;&#20010;&#29992;&#20110;$(\epsilon,\delta)$-&#19981;&#21487;&#21306;&#20998;&#24615;&#35889;(&#31616;&#31216;&#20026;$
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01243v2 Announce Type: replace-cross  Abstract: To achieve differential privacy (DP) one typically randomizes the output of the underlying query. In big data analytics, one often uses randomized sketching/aggregation algorithms to make processing high-dimensional data tractable. Intuitively, such machine learning (ML) algorithms should provide some inherent privacy, yet most if not all existing DP mechanisms do not leverage this inherent randomness, resulting in potentially redundant noising.   The motivating question of our work is:   (How) can we improve the utility of DP mechanisms for randomized ML queries, by leveraging the randomness of the query itself?   Towards a (positive) answer, we prove the Normal Distributions Indistinguishability Spectrum Theorem (in short, NDIS Theorem), a theoretical result with far-reaching practical implications. In a nutshell, NDIS is a closed-form analytic computation for the $(\epsilon,\delta)$-indistinguishability-spectrum (in short, $
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.16594</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;: &#23439;&#35266;at-$k$&#24230;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Consistent algorithms for multi-label classification with macro-at-$k$ metrics. (arXiv:2401.16594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23558;&#24230;&#37327;&#32447;&#24615;&#20998;&#35299;&#20026;&#27599;&#20010;&#26631;&#31614;&#20998;&#21035;&#24212;&#29992;&#30340;&#20108;&#20998;&#31867;&#25928;&#29992;&#30340;&#24635;&#21644;&#65292;&#24182;&#23545;&#27599;&#20010;&#23454;&#20363;&#39044;&#27979;&#24688;&#22909;&#26377;$k$&#20010;&#26631;&#31614;&#30340;&#39069;&#22806;&#35201;&#27714;&#12290;&#8220;&#23439;&#35266;at-$k$&#8221;&#24230;&#37327;&#22312;&#20855;&#26377;&#38271;&#23614;&#26631;&#31614;&#30340;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;at-$k$&#32422;&#26463;&#23558;&#21407;&#26412;&#29420;&#31435;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#27604;&#26631;&#20934;&#23439;&#24179;&#22343;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#21644;&#24418;&#24335;&#65292;&#24182;&#22522;&#20110;Frank-Wolfe&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#19968;&#33268;&#19988;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#36824;&#28041;&#21450;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26356;&#19968;&#33324;&#24230;&#37327;&#65292;&#36825;&#20123;&#20989;&#25968;&#26159;&#25353;&#26631;&#31614;&#36827;&#34892;&#30340;&#28151;&#28102;&#30697;&#38453;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.11665</link><description>&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#36817;&#20284; Thompson &#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#21152;&#36895;&#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#36890;&#36807;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#35774;&#35745;&#25913;&#21892;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo &#30340;&#36817;&#20284; Thompson &#37319;&#26679;&#26041;&#27861;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#33539;&#22260;&#65292;&#20174;&#39640;&#26031;&#21518;&#39564;&#37319;&#26679;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24179;&#28369;&#21518;&#39564;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#35201;&#27714;&#39640;&#20934;&#30830;&#24615;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284; Thompson &#37319;&#26679;&#31574;&#30053;&#65292;&#21033;&#29992;&#27424;&#38459;&#23612; Langevin Monte Carlo&#65292;&#21518;&#32773;&#26159;&#27169;&#25311;&#39640;&#32500;&#21518;&#39564;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;&#22522;&#20110;&#26631;&#20934;&#30340;&#24179;&#28369;&#24615;&#21644;&#23545;&#25968;&#20985;&#24615;&#26465;&#20214;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#29305;&#23450;&#21183;&#20989;&#25968;&#30340;&#21152;&#36895;&#21518;&#39564;&#38598;&#20013;&#21644;&#37319;&#26679;&#12290;&#35813;&#35774;&#35745;&#25913;&#36827;&#20102;&#23454;&#29616;&#23545;&#25968;&#36951;&#25022;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20174;$\mathcal{\tilde O}(d)$&#25913;&#36827;&#21040;$\mathcal{\tilde O}(\sqrt{d})$&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#22312;&#39640;&#32500;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#32463;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
&lt;/p&gt;</description></item><item><title>NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11787</link><description>&lt;p&gt;
NeuroCUT&#65306;&#19968;&#31181;&#29992;&#20110;&#40065;&#26834;&#22270;&#20998;&#21306;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroCUT: A Neural Approach for Robust Graph Partitioning. (arXiv:2310.11787v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11787
&lt;/p&gt;
&lt;p&gt;
NeuroCUT&#26159;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#30340;&#22270;&#20998;&#21306;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#65292;&#21363;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#21306;&#26088;&#22312;&#23558;&#22270;&#20998;&#21106;&#20026;k&#20010;&#19981;&#30456;&#20132;&#30340;&#23376;&#38598;&#65292;&#21516;&#26102;&#20248;&#21270;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#30001;&#20110;&#20854;&#32452;&#21512;&#24615;&#36136;&#65292;&#22823;&#37096;&#20998;&#19982;&#22270;&#20998;&#21306;&#30456;&#20851;&#30340;&#38382;&#39064;&#37117;&#21576;&#29616;&#20986;NP&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#36817;&#20284;&#31639;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#26102;&#24102;&#26377;&#36817;&#20284;&#20445;&#35777;&#65292;&#26377;&#26102;&#21017;&#27809;&#26377;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#26041;&#27861;&#38024;&#23545;&#29305;&#23450;&#30340;&#20998;&#21306;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#24050;&#30693;&#30340;&#25991;&#29486;&#20013;&#30340;&#20998;&#21306;&#30446;&#26631;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#31070;&#32463;&#26041;&#27861;&#24212;&#36816;&#32780;&#29983;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;NeuroCut&#25193;&#23637;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#12290;NeuroCut&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#24341;&#20837;&#20102;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#22270;&#25299;&#25169;&#21644;&#20998;&#21306;&#35745;&#25968;&#20855;&#26377;&#24402;&#32435;&#24615;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#26597;&#35810;&#26102;&#25552;&#20379;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Graph partitioning aims to divide a graph into $k$ disjoint subsets while optimizing a specific partitioning objective. The majority of formulations related to graph partitioning exhibit NP-hardness due to their combinatorial nature. As a result, conventional approximation algorithms rely on heuristic methods, sometimes with approximation guarantees and sometimes without. Unfortunately, traditional approaches are tailored for specific partitioning objectives and do not generalize well across other known partitioning objectives from the literature. To overcome this limitation, and learn heuristics from the data directly, neural approaches have emerged, demonstrating promising outcomes. In this study, we extend this line of work through a novel framework, NeuroCut. NeuroCut introduces two key innovations over prevailing methodologies. First, it is inductive to both graph topology and the partition count, which is provided at query time. Second, by leveraging a reinforcement learning base
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13206</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#35770;&#21644;&#37319;&#26679;&#29702;&#35770;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transferability of Graph Neural Networks using Graphon and Sampling Theories. (arXiv:2307.13206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#35889;&#21644;&#37319;&#26679;&#29702;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#65292;&#24182;&#19988;&#22312;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#23454;&#29616;&#20102;&#22312;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22312;&#21508;&#20010;&#39046;&#22495;&#22788;&#29702;&#22522;&#20110;&#22270;&#30340;&#20449;&#24687;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;GNN&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#21487;&#36801;&#31227;&#24615;&#65292;&#21363;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20132;&#25442;&#26469;&#33258;&#19981;&#21516;&#22270;&#30340;&#20449;&#24687;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#19968;&#31181;&#25429;&#25417;GNN&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#22270;&#35889;&#65292;&#23427;&#26159;&#23545;&#22823;&#22411;&#31264;&#23494;&#22270;&#30340;&#26497;&#38480;&#30340;&#23545;&#31216;&#21487;&#27979;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26174;&#24335;&#30340;&#20004;&#23618;&#22270;&#35889;&#31070;&#32463;&#32593;&#32476;&#65288;WNN&#65289;&#26550;&#26500;&#65292;&#23545;&#22270;&#35889;&#24212;&#29992;&#20110;GNN&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20197;&#25351;&#23450;&#35823;&#24046;&#23481;&#38480;&#22312;&#26368;&#23569;&#30340;&#32593;&#32476;&#26435;&#37325;&#25968;&#19979;&#36924;&#36817;&#24102;&#38480;&#20449;&#21495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#22312;&#19968;&#20010;&#25910;&#25947;&#21040;&#22270;&#35889;&#30340;&#24207;&#21015;&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#20004;&#23618;GNN&#22312;&#25152;&#26377;&#36275;&#22815;&#22823;&#30340;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#30830;&#23450;&#24615;&#21152;&#26435;&#22270;&#21644;&#31616;&#21333;&#38543;&#26426;&#22270;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.17100</link><description>&lt;p&gt;
RL4CO: &#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17100
&lt;/p&gt;
&lt;p&gt;
RL4CO&#26159;&#19968;&#20010;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#24191;&#27867;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#30528;&#37325;&#20110;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#26032;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#24046;&#65292;&#24378;&#35843;&#20102;&#23545;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#24615;&#33021;&#30340;&#24179;&#34913;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;RL4CO&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;RL4CO&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#36719;&#20214;&#24211;&#21644;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#27169;&#22359;&#21270;&#21644;&#37197;&#32622;&#31649;&#29702;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36731;&#26494;&#20462;&#25913;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12289;&#29615;&#22659;&#21644;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#19987;&#27880;&#20110;&#29305;&#23450;&#20219;&#21153;&#65288;&#22914;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#65289;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24378;&#35843;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#21508;&#31181;&#20248;&#21270;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;&#27169;&#22411;&#22312;&#26679;&#26412;&#25928;&#29575;&#12289;&#38646;-shot&#27867;&#21270;&#21644;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#20351;&#29992;&#36825;&#20123;&#26032;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#26102;&#33853;&#21518;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#34920;&#26126;&#26377;&#24517;&#35201;&#26356;&#21152;&#24179;&#34913;&#22320;&#35780;&#20272;&#31070;&#32463;CO&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;RL4CO&#33021;&#22815;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2306.07479</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Incentivizing High-Quality Content in Online Recommender Systems. (arXiv:2306.07479v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#28608;&#21169;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#31639;&#27861;&#38382;&#39064;&#65292;&#32463;&#20856;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#36890;&#36807;&#24809;&#32602;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#21019;&#24314;&#32773;&#65292;&#25104;&#21151;&#22320;&#28608;&#21169;&#20102;&#29983;&#20135;&#32773;&#21019;&#36896;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20687;TikTok&#21644;YouTube&#36825;&#26679;&#30340;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#24179;&#21488;&#30340;&#20915;&#31574;&#31639;&#27861;&#22609;&#36896;&#20102;&#20869;&#23481;&#29983;&#20135;&#32773;&#30340;&#28608;&#21169;&#65292;&#21253;&#25324;&#29983;&#20135;&#32773;&#22312;&#20869;&#23481;&#36136;&#37327;&#19978;&#25237;&#20837;&#22810;&#23569;&#21162;&#21147;&#12290;&#35768;&#22810;&#24179;&#21488;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#65292;&#36825;&#20250;&#20135;&#29983;&#36328;&#26102;&#38388;&#30340;&#28608;&#21169;&#65292;&#22240;&#20026;&#20170;&#22825;&#29983;&#20135;&#30340;&#20869;&#23481;&#20250;&#24433;&#21709;&#26410;&#26469;&#20869;&#23481;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#23398;&#20064;&#20135;&#29983;&#30340;&#28608;&#21169;&#65292;&#20998;&#26512;&#20102;&#22312;&#32435;&#20160;&#22343;&#34913;&#19979;&#29983;&#20135;&#30340;&#20869;&#23481;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20687;Hedge&#21644;EXP3&#36825;&#26679;&#30340;&#32463;&#20856;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20250;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#20302;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#29305;&#21035;&#22320;&#65292;&#20869;&#23481;&#36136;&#37327;&#22312;&#23398;&#20064;&#29575;&#26041;&#38754;&#26377;&#19978;&#38480;&#65292;&#24182;&#19988;&#38543;&#30528;&#20856;&#22411;&#23398;&#20064;&#29575;&#36827;&#23637;&#32780;&#36235;&#36817;&#20110;&#38646;&#12290;&#22312;&#36825;&#19968;&#36127;&#38754;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#22522;&#20110;&#24809;&#32602;&#21019;&#24314;&#20302;&#36136;&#37327;&#20869;&#23481;&#30340;&#29983;&#20135;&#32773;&#8212;&#8212;&#27491;&#30830;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#26032;&#39062;&#30340;&#31574;&#30053;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#24212;&#29992;&#23545;&#25239;&#24615;&#25216;&#26415;&#30340;&#25361;&#25112;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#25104;&#21151;&#22320;&#28608;&#21169;&#29983;&#20135;&#32773;&#21019;&#24314;&#39640;&#36136;&#37327;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
For content recommender systems such as TikTok and YouTube, the platform's decision algorithm shapes the incentives of content producers, including how much effort the content producers invest in the quality of their content. Many platforms employ online learning, which creates intertemporal incentives, since content produced today affects recommendations of future content. In this paper, we study the incentives arising from online learning, analyzing the quality of content produced at a Nash equilibrium. We show that classical online learning algorithms, such as Hedge and EXP3, unfortunately incentivize producers to create low-quality content. In particular, the quality of content is upper bounded in terms of the learning rate and approaches zero for typical learning rate schedules. Motivated by this negative result, we design a different learning algorithm -- based on punishing producers who create low-quality content -- that correctly incentivizes producers to create high-quality co
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01639</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Reduction of finite sampling noise in quantum neural networks. (arXiv:2306.01639v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01639
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26041;&#27861;&#65292;&#20943;&#23567;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#22312;QNN&#30340;&#26500;&#36896;&#22949;&#21892;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#38656;&#39069;&#22806;&#30005;&#36335;&#35745;&#31639;&#65292;&#27979;&#35797;&#21457;&#29616;&#21487;&#20197;&#26174;&#33879;&#22320;&#38477;&#20302;&#22122;&#22768;&#27700;&#24179;&#21450;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;(QNNs)&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36755;&#20986;, &#36890;&#36807;&#35745;&#31639;&#26399;&#26395;&#20540;&#24102;&#26469;&#20102;&#22522;&#26412;&#30340;&#26377;&#38480;&#37319;&#26679;&#22122;&#22768;&#65292;&#21363;&#20351;&#22312;&#26080;&#35823;&#24046;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#20063;&#20250;&#20986;&#29616;&#27492;&#29616;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#20171;&#32461;&#26041;&#24046;&#35268;&#33539;&#21270;&#25216;&#26415;&#26469;&#20943;&#23569;&#36825;&#31181;&#22122;&#22768;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20943;&#23567;&#37327;&#23376;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26399;&#26395;&#20540;&#30340;&#26041;&#24046;&#12290;&#22914;&#26524;QNN&#24050;&#32463;&#22949;&#21892;&#26500;&#36896;&#65292;&#21017;&#27492;&#25216;&#26415;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#30005;&#36335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#38477;&#20302;&#26041;&#24046;&#21487;&#20197;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;&#65292;&#38477;&#20302;&#36755;&#20986;&#22122;&#22768;&#65292;&#20943;&#23569;&#26799;&#24230;&#30005;&#36335;&#35780;&#20272;&#20013;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#25105;&#20204;&#23545;&#22810;&#39033;&#24335;&#20989;&#25968;&#22238;&#24402;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25105;&#20204;&#30340;&#31034;&#20363;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35268;&#33539;&#21270;&#26041;&#27861;&#24179;&#22343;&#21487;&#20197;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;&#20102;&#22122;&#22768;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks (QNNs) use parameterized quantum circuits with data-dependent inputs and generate outputs through the evaluation of expectation values. Calculating these expectation values necessitates repeated circuit evaluations, thus introducing fundamental finite-sampling noise even on error-free quantum computers. We reduce this noise by introducing the variance regularization, a technique for reducing the variance of the expectation value during the quantum model training. This technique requires no additional circuit evaluations if the QNN is properly constructed. Our empirical findings demonstrate the reduced variance speeds up the training and lowers the output noise as well as decreases the number of measurements in the gradient circuit evaluation. This regularization method is benchmarked on the regression of multiple functions. We show that in our examples, it lowers the variance by an order of magnitude on average and leads to a significantly reduced noise level of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.00560</link><description>&lt;p&gt;
Hinge-Wasserstein: &#36890;&#36807;&#20998;&#31867;&#36991;&#20813;&#22238;&#24402;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;
&lt;/p&gt;
&lt;p&gt;
Hinge-Wasserstein: Mitigating Overconfidence in Regression by Classification. (arXiv:2306.00560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;hinge-Wasserstein&#65292;&#29992;&#20110;&#32531;&#35299;&#22238;&#24402;&#20219;&#21153;&#20013;&#30001;&#20110;&#36807;&#24230;&#33258;&#20449;&#23548;&#33268;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#26377;&#25928;&#25552;&#39640;&#20102;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24615;&#33021;&#26041;&#38754;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#25552;&#39640;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#12290;&#22312;&#27169;&#31946;&#29978;&#33267;&#19981;&#21487;&#39044;&#27979;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#65292;&#37319;&#29992;&#22238;&#24402;-&#20998;&#31867;&#26041;&#27861;&#26377;&#28508;&#21147;&#32531;&#35299;&#36825;&#20123;&#27495;&#20041;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#39044;&#27979;&#25152;&#38656;&#36755;&#20986;&#30340;&#31163;&#25955;&#27010;&#29575;&#23494;&#24230;&#12290;&#28982;&#32780;&#65292;&#23494;&#24230;&#20272;&#35745;&#20173;&#28982;&#20542;&#21521;&#20110;&#36807;&#24230;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#24120;&#35265;&#30340;NLL&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#26102;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#36807;&#24230;&#33258;&#20449;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;hinge-Wasserstein&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#27492;&#25439;&#22833;&#26174;&#30528;&#25552;&#39640;&#20102;&#20004;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#36136;&#37327;&#65306; aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26032;&#25439;&#22833;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#20998;&#21035;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#28436;&#31034;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks are prone to being overconfident despite their drastically improved performance. In ambiguous or even unpredictable real-world scenarios, this overconfidence can pose a major risk to the safety of applications. For regression tasks, the regression-by-classification approach has the potential to alleviate these ambiguities by instead predicting a discrete probability density over the desired output. However, a density estimator still tends to be overconfident when trained with the common NLL loss. To mitigate the overconfidence problem, we propose a loss function, hinge-Wasserstein, based on the Wasserstein Distance. This loss significantly improves the quality of both aleatoric and epistemic uncertainty, compared to previous work. We demonstrate the capabilities of the new loss on a synthetic dataset, where both types of uncertainty are controlled separately. Moreover, as a demonstration for real-world scenarios, we evaluate our approach on the benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.19640</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimates for Pairwise Learning with Deep ReLU Networks. (arXiv:2305.19640v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#23545;&#23398;&#20064;&#25351;&#30340;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#32771;&#34385;&#19968;&#23545;&#26679;&#26412;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#24182;&#20272;&#35745;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#12290;&#23545;&#20110;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#26465;&#20214;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#20854;&#35823;&#24046;&#20272;&#35745;&#30340;&#38454;&#25968;&#20026;O&#65288;&#65288;Vlog&#65288;n&#65289;/ n&#65289;1 /&#65288;2-&#946;&#65289;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#65292;&#22312;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#28385;&#36275;&#26576;&#20123;&#20809;&#28369;&#24615;&#27491;&#21017;&#24615;&#26102;&#65292;&#26368;&#20248;&#30028;&#38480;&#36798;&#21040;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#65292;&#24046;&#36317;&#20165;&#20026;&#23545;&#25968;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise learning refers to learning tasks where a loss takes a pair of samples into consideration. In this paper, we study pairwise learning with deep ReLU networks and estimate the excess generalization error. For a general loss satisfying some mild conditions, a sharp bound for the estimation error of order $O((V\log(n) /n)^{1/(2-\beta)})$ is established. In particular, with the pairwise least squares loss, we derive a nearly optimal bound of the excess generalization error which achieves the minimax lower bound up to a logrithmic term when the true predictor satisfies some smoothness regularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#32852;&#31995;&#30340;&#21033;&#29992;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.00723</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;: PDE&#21644;&#26377;&#38480;&#24046;&#20998;&#30340;&#28145;&#20837;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Predictions Based on Pixel Data: Insights from PDEs and Finite Differences. (arXiv:2305.00723v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#32852;&#31995;&#30340;&#21033;&#29992;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#39640;&#32500;&#31354;&#38388;&#20013;&#35768;&#22810;&#36924;&#36817;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#36825;&#24471;&#21040;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#38656;&#35201;&#23545;&#23427;&#20204;&#21487;&#20197;&#36924;&#36817;&#30340;&#20869;&#23481;&#20197;&#21450;&#20197;&#20309;&#31181;&#20195;&#20215;&#21644;&#31934;&#24230;&#36924;&#36817;&#26377;&#19968;&#20010;&#22362;&#23454;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#22312;&#28041;&#21450;&#22270;&#20687;&#30340;&#36924;&#36817;&#20219;&#21153;&#20013;&#26377;&#23454;&#38469;&#29992;&#36884;&#30340;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#26159;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#32593;&#32476;&#20013;&#28041;&#21450;&#30340;&#32447;&#24615;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#20998;&#26512;&#27604;&#36890;&#29992;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26356;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#30340;&#26159;&#24207;&#21015;&#36924;&#36817;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#23519;&#20540;&#30001;&#30697;&#38453;&#25110;&#39640;&#38454;&#24352;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#30340;&#32852;&#31995;&#26469;&#26500;&#36896;&#36825;&#20123;&#32467;&#26524;&#12290;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are the state-of-the-art for many approximation tasks in high-dimensional spaces, as supported by an abundance of experimental evidence. However, we still need a solid theoretical understanding of what they can approximate and, more importantly, at what cost and accuracy. One network architecture of practical use, especially for approximation tasks involving images, is convolutional (residual) networks. However, due to the locality of the linear operators involved in these networks, their analysis is more complicated than for generic fully connected neural networks. This paper focuses on sequence approximation tasks, where a matrix or a higher-order tensor represents each observation. We show that when approximating sequences arising from space-time discretisations of PDEs we may use relatively small networks. We constructively derive these results by exploiting connections between discrete convolution and finite difference operators. Throughout, we design our network a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#36845;&#20195;&#31639;&#27861;&#22312;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23637;&#24320;&#36845;&#20195;&#31639;&#27861;&#24182;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#20272;&#35745;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#27169;&#22411;&#21644;&#20449;&#22122;&#27604;&#33539;&#22260;&#20869;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03678</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#36845;&#20195;&#31639;&#27861;&#22312;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Deep Learning and Iterative Algorithms for Joint Channel Estimation and Signal Detection. (arXiv:2303.03678v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#36845;&#20195;&#31639;&#27861;&#22312;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#20013;&#30340;&#25928;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23637;&#24320;&#36845;&#20195;&#31639;&#27861;&#24182;&#20351;&#29992;&#36229;&#32593;&#32476;&#26469;&#20272;&#35745;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#36866;&#24212;&#20102;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#27169;&#22411;&#21644;&#20449;&#22122;&#27604;&#33539;&#22260;&#20869;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#23427;&#22266;&#26377;&#22320;&#28041;&#21450;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#22312;&#20302;&#20449;&#22122;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;&#31639;&#27861;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20294;&#26159;&#20154;&#20204;&#23545;&#35745;&#31639;&#36153;&#29992;&#21644;&#22312;&#20302;&#20449;&#22122;&#27604;&#29615;&#22659;&#20013;&#32570;&#20047;&#39564;&#35777;&#30340;&#25285;&#24551;&#20381;&#28982;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#19988;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#20869;&#25552;&#20379;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#38750;&#24120;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#22522;&#20934;&#65292;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#27169;&#22411;&#12289;&#22810;&#26222;&#21202;&#21644;&#20449;&#22122;&#27604;&#35774;&#32622;&#19979;&#39564;&#35777;&#20256;&#32479;&#31639;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#39592;&#26550;&#32593;&#32476;&#30001;&#23637;&#24320;&#36845;&#20195;&#31639;&#27861;&#24418;&#25104;&#65292;&#24182;&#36890;&#36807;&#36229;&#32593;&#32476;&#20272;&#35745;&#36229;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#36731;&#37327;&#32423;&#30340;DenseNet&#35843;&#25972;&#20026;&#36866;&#24212;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#20449;&#21495;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint channel estimation and signal detection (JCESD) in wireless communication systems is a crucial and challenging task, especially since it inherently poses a nonlinear inverse problem. This challenge is further highlighted in low signal-to-noise ratio (SNR) scenarios, where traditional algorithms often perform poorly. Deep learning (DL) methods have been investigated, but concerns regarding computational expense and lack of validation in low-SNR settings remain. Hence, the development of a robust and low-complexity model that can deliver excellent performance across a wide range of SNRs is highly desirable. In this paper, we aim to establish a benchmark where traditional algorithms and DL methods are validated on different channel models, Doppler, and SNR settings. In particular, we propose a new DL model where the backbone network is formed by unrolling the iterative algorithm, and the hyperparameters are estimated by hypernetworks. Additionally, we adapt a lightweight DenseNet to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#21518;&#32456;&#27490;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.04527</link><description>&lt;p&gt;
&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A policy gradient approach for Finite Horizon Constrained Markov Decision Processes. (arXiv:2210.04527v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#26102;&#22495;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#21518;&#32456;&#27490;&#65292;&#36890;&#36807;&#20989;&#25968;&#36924;&#36817;&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#35774;&#32622;&#36890;&#24120;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#23548;&#33268;&#20135;&#29983;&#26368;&#20248;&#30340;&#22266;&#23450;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#26377;&#38480;&#26102;&#22495;&#25511;&#21046;&#38382;&#39064;&#26356;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#36890;&#24120;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#26368;&#36817;&#65292;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#35774;&#32622;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20854;&#20013;&#20195;&#29702;&#21516;&#26102;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#28385;&#36275;&#26576;&#20123;&#32473;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35774;&#32622;&#20165;&#22312;&#26080;&#38480;&#26102;&#22495;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#22266;&#23450;&#31574;&#30053;&#26159;&#26368;&#20248;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#26102;&#22495;&#35774;&#32622;&#19979;&#36827;&#34892;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#20854;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#26102;&#38388;&#21518;&#32456;&#27490;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#65292;&#36825;&#22312;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#25110;&#36830;&#32493;&#30340;&#24773;&#20917;&#19979;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#21462;&#20915;&#20110;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The infinite horizon setting is widely adopted for problems of reinforcement learning (RL). These invariably result in stationary policies that are optimal. In many situations, finite horizon control problems are of interest and for such problems, the optimal policies are time-varying in general. Another setting that has become popular in recent times is of Constrained Reinforcement Learning, where the agent maximizes its rewards while it also aims to satisfy some given constraint criteria. However, this setting has only been studied in the context of infinite horizon MDPs where stationary policies are optimal. We present an algorithm for constrained RL in the Finite Horizon Setting where the horizon terminates after a fixed (finite) time. We use function approximation in our algorithm which is essential when the state and action spaces are large or continuous and use the policy gradient method to find the optimal policy. The optimal policy that we obtain depends on the stage and so is
&lt;/p&gt;</description></item></channel></rss>