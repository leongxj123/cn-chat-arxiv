<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14270</link><description>&lt;p&gt;
&#22330;&#26223;&#22270;ViT&#65306;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#65292;&#36890;&#36807;Transformer-based&#22270;&#20687;&#32534;&#30721;&#22120;&#38544;&#24335;&#24314;&#27169;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#25968;&#25454;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#29616;&#26377;&#30446;&#26631;&#26816;&#27979;&#26550;&#26500;&#20013;&#28155;&#21152;&#21333;&#29420;&#30340;&#20851;&#31995;&#27169;&#22359;&#25110;&#35299;&#30721;&#22120;&#26469;&#22788;&#29702;&#27492;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31163;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#65292;&#38459;&#30861;&#20102;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#26080;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#30340;&#35270;&#35273;&#20851;&#31995;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#22522;&#20110;Transformer&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#38544;&#21547;&#22320;&#24314;&#27169;&#23427;&#20204;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#25552;&#21462;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36873;&#25321;&#21487;&#33021;&#24418;&#25104;&#20851;&#31995;&#30340;&#23545;&#35937;&#23545;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21333;&#38454;&#27573;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#23545;&#35937;&#21644;&#20851;&#31995;&#26816;&#27979;&#25968;&#25454;&#19978;&#35757;&#32451;&#27492;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Visual Genome&#21644;&#22823;&#35789;&#27719;GQA&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20851;&#31995;&#26816;&#27979;&#24615;&#33021;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14270v1 Announce Type: cross  Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-tim
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14200</link><description>&lt;p&gt;
&#25163;&#26415;&#21592;&#21435;&#20559;&#35265;&#65306;&#31070;&#22855;&#30340;&#26435;&#37325;&#21450;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Debiasing surgeon: fantastic weights and how to find them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14200
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#29616;&#35937;&#26159;&#31639;&#27861;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21435;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26356;&#25110;&#22810;&#25110;&#23569;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#22823;&#35268;&#27169;&#22320;&#20351;&#29992;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#30495;&#30340;&#26377;&#24517;&#35201;&#21527;&#65311;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#20123;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#30340;&#8220;&#26080;&#20559;&#23376;&#32593;&#32476;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#32780;&#19981;&#20381;&#36182;&#20110;&#31639;&#27861;&#20559;&#35265;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23376;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#29305;&#23450;&#30340;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#35265;&#65292;&#34920;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#21487;&#33021;&#36890;&#36807;&#26550;&#26500;&#19978;&#30340;&#23545;&#31574;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14200v1 Announce Type: cross  Abstract: Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.12034</link><description>&lt;p&gt;
VFusion3D: &#20174;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12034
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29983;&#25104;&#22823;&#35268;&#27169;3D&#25968;&#25454;&#38598;&#30340;VFusion3D&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#21487;&#25193;&#23637;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#12290;&#26500;&#24314;&#22522;&#30784;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;3D&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#12290;&#19982;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#35270;&#39057;&#19981;&#21516;&#65292;3D&#25968;&#25454;&#19981;&#23481;&#26131;&#33719;&#21462;&#19988;&#38590;&#20197;&#33719;&#24471;&#65292;&#36825;&#23548;&#33268;&#19982;&#20854;&#20182;&#31867;&#22411;&#25968;&#25454;&#30340;&#25968;&#37327;&#30456;&#27604;&#23384;&#22312;&#26174;&#30528;&#30340;&#35268;&#27169;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#36890;&#36807;&#22823;&#37327;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#35757;&#32451;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;3D&#25968;&#25454;&#30340;&#30693;&#35782;&#28304;&#12290;&#36890;&#36807;&#24494;&#35843;&#35299;&#38145;&#20854;&#22810;&#35270;&#35282;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#21069;&#39304;3D&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12034v1 Announce Type: cross  Abstract: This paper presents a novel paradigm for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data. To address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07955</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#35299;&#37322;&#65306;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26469;&#22686;&#24378;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#21457;&#20102;&#26377;&#36873;&#25321;&#24615;&#30340;&#29702;&#24615;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSR&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26041;&#27861;&#26469;&#26816;&#27979;&#20960;&#20010;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24341;&#20837;&#35782;&#21035;&#20986;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07955v1 Announce Type: cross  Abstract: The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15776</link><description>&lt;p&gt;
&#21463;&#38480;&#21046;MDP&#20013;&#30340;&#30495;&#27491;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Truly No-Regret Learning in Constrained MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#32943;&#23450;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23558;&#19968;&#31181;&#24120;&#35265;&#30340;&#23433;&#20840;&#32422;&#26463;&#27169;&#22411;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24314;&#27169;&#23433;&#20840;&#32422;&#26463;&#30340;&#24120;&#35265;&#26041;&#24335;&#12290;&#30446;&#21069;&#29992;&#20110;&#39640;&#25928;&#35299;&#20915;CMDPs&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#31639;&#27861;&#65292;&#25152;&#26377;&#24403;&#21069;&#24050;&#30693;&#30340;&#21518;&#24724;&#30028;&#37117;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#8212;&#8212;&#21487;&#20197;&#36890;&#36807;&#22312;&#19968;&#20010;&#22238;&#21512;&#20013;&#30340;&#32422;&#26463;&#36829;&#21453;&#26469;&#29992;&#20005;&#26684;&#30340;&#32422;&#26463;&#28385;&#36275;&#22312;&#21478;&#19968;&#20010;&#22238;&#21512;&#20013;&#12290;&#36825;&#20351;&#24471;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#19981;&#23433;&#20840;&#65292;&#22240;&#20026;&#23427;&#20165;&#20445;&#35777;&#26368;&#32456;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#27491;&#22914;Efroni&#31561;&#20154;&#65288;2020&#24180;&#65289;&#25351;&#20986;&#30340;&#65292;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#19981;&#20801;&#35768;&#38169;&#35823;&#25269;&#28040;&#30340;&#24773;&#20917;&#19979;&#21487;&#35777;&#26126;&#22320;&#23454;&#29616;&#27425;&#32447;&#24615;&#21518;&#24724;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31532;&#19968;&#20010;&#32943;&#23450;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20851;&#20110;&#27491;&#21017;&#21270;&#21407;&#22987;-&#23545;&#20598;&#26041;&#26696;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#24615;&#36890;&#29992;&#21270;&#21040;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#30340;CMDPs&#19978;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21407;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15776v1 Announce Type: new  Abstract: Constrained Markov decision processes (CMDPs) are a common way to model safety constraints in reinforcement learning. State-of-the-art methods for efficiently solving CMDPs are based on primal-dual algorithms. For these algorithms, all currently known regret bounds allow for error cancellations -- one can compensate for a constraint violation in one round with a strict constraint satisfaction in another. This makes the online learning process unsafe since it only guarantees safety for the final (mixture) policy but not during learning. As Efroni et al. (2020) pointed out, it is an open question whether primal-dual algorithms can provably achieve sublinear regret if we do not allow error cancellations. In this paper, we give the first affirmative answer. We first generalize a result on last-iterate convergence of regularized primal-dual schemes to CMDPs with multiple constraints. Building upon this insight, we propose a model-based primal
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;</title><link>https://arxiv.org/abs/2402.12231</link><description>&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#25913;&#21892;&#27010;&#29575;&#31215;&#20998;&#22120;&#23545;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12231
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25551;&#36848;&#31185;&#23398;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20294;&#30830;&#23450;&#35299;&#37322;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#30340;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#22238;&#28779;&#36825;&#19968;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#38024;&#23545;ODEs&#20013;&#30340;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#26799;&#24230;&#20248;&#21270;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#20943;&#23569;&#27010;&#29575;&#31215;&#20998;&#22120;&#30340;&#19968;&#20010;&#22122;&#22768;&#21442;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#20110;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#21442;&#25968;&#25968;&#37327;&#30340;Hodgkin-Huxley&#27169;&#22411;&#33719;&#24471;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10232</link><description>&lt;p&gt;
Johnson-Lindenstrauss&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple, unified analysis of Johnson-Lindenstrauss with applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#36825;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#38477;&#32500;&#39046;&#22495;&#20013;&#30340;&#22522;&#30707;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#29702;&#35299;&#65292;&#36824;&#23558;&#21508;&#31181;&#26500;&#36896;&#32479;&#19968;&#21040;JL&#26694;&#26550;&#19979;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#12290;&#36825;&#31181;&#31616;&#21270;&#21644;&#32479;&#19968;&#22312;&#20445;&#25345;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23545;&#20174;&#27969;&#31639;&#27861;&#21040;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#31616;&#21270;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#29699;&#24418;&#26500;&#36896;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#35777;&#26126;&#12290;&#25105;&#20204;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#24120;&#25968;&#65292;&#36825;&#26631;&#24535;&#30528;&#25991;&#29486;&#20013;&#36136;&#30340;&#39134;&#36291;&#12290;&#36890;&#36807;&#36816;&#29992;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09542</link><description>&lt;p&gt;
&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65306;&#19968;&#31181;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#36817;&#31471;&#28857;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Layerwise Proximal Replay: A Proximal Point Method for Online Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38024;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#32463;&#39564;&#22238;&#25918;&#36896;&#25104;&#30340;&#20248;&#21270;&#19981;&#31283;&#23450;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20960;&#20309;&#30340;&#20462;&#25913;&#26469;&#24179;&#34913;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22238;&#25918;&#24335;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36880;&#27493;&#20174;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#37117;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#26469;&#21516;&#26102;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36807;&#24230;&#25311;&#21512;&#20808;&#21069;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#23616;&#38480;&#24615;&#65306;&#20351;&#29992;&#32463;&#39564;&#22238;&#25918;&#35757;&#32451;&#30340;&#32593;&#32476;&#24448;&#24448;&#20855;&#26377;&#19981;&#31283;&#23450;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;&#24433;&#21709;&#20854;&#25972;&#20307;&#20934;&#30830;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#22238;&#25918;&#32531;&#20914;&#21306;&#23384;&#20648;&#20102;&#25152;&#26377;&#20808;&#21069;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#38382;&#39064;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#26080;&#20851;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20248;&#21270;&#20960;&#20309;&#30340;&#31616;&#21333;&#20462;&#25913;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36880;&#23618;&#36817;&#31471;&#22238;&#25918;&#65288;LPR&#65289;&#65292;&#22312;&#21482;&#20801;&#35768;&#36880;&#28176;&#25913;&#21464;&#36807;&#21435;&#25968;&#25454;&#30340;&#38544;&#34255;&#28608;&#27963;&#30340;&#21516;&#26102;&#65292;&#24179;&#34913;&#20102;&#20174;&#26032;&#25968;&#25454;&#21644;&#22238;&#25918;&#25968;&#25454;&#20013;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LPR&#22312;&#22522;&#20110;&#22238;&#25918;&#30340;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09542v1 Announce Type: new  Abstract: In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning me
&lt;/p&gt;</description></item><item><title>SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.09025</link><description>&lt;p&gt;
SLEB: &#36890;&#36807;&#20887;&#20313;&#39564;&#35777;&#21644;&#28040;&#38500;Transformer&#22359;&#20248;&#21270;LLM&#30340;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09025
&lt;/p&gt;
&lt;p&gt;
SLEB&#26159;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#32473;&#23454;&#38469;&#37096;&#32626;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#31934;&#31616;&#65292;&#19968;&#31181;&#26088;&#22312;&#20943;&#23567;LLM&#22823;&#23567;&#21644;&#22797;&#26434;&#24230;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#20887;&#20313;&#32452;&#20214;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#31934;&#31616;&#26377;&#24076;&#26395;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#31471;&#21040;&#31471;LLM&#25512;&#29702;&#21152;&#36895;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLEB&#65292;&#19968;&#31181;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#30340;Transformer&#22359;&#26469;&#20248;&#21270;LLM&#27969;&#31243;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;Transformer&#22359;&#20316;&#20026;&#31934;&#31616;&#30340;&#22522;&#26412;&#21333;&#20301;&#65292;&#22240;&#20026;LLM&#22312;&#30456;&#37051;&#22359;&#30340;&#36755;&#20986;&#20043;&#38388;&#20855;&#26377;&#22359;&#32423;&#21035;&#30340;&#20887;&#20313;&#21644;&#39640;&#30456;&#20284;&#24615;&#12290;&#36825;&#20010;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22686;&#24378;LLM&#30340;&#22788;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SLEB&#25104;&#21151;&#21152;&#36895;&#20102;LLM&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09025v1 Announce Type: new Abstract: Large language models (LLMs) have proven to be highly effective across various natural language processing tasks. However, their large number of parameters poses significant challenges for practical deployment. Pruning, a technique aimed at reducing the size and complexity of LLMs, offers a potential solution by removing redundant components from the network. Despite the promise of pruning, existing methods often struggle to achieve substantial end-to-end LLM inference speedup. In this paper, we introduce SLEB, a novel approach designed to streamline LLMs by eliminating redundant transformer blocks. We choose the transformer block as the fundamental unit for pruning, because LLMs exhibit block-level redundancy with high similarity between the outputs of neighboring blocks. This choice allows us to effectively enhance the processing speed of LLMs. Our experimental results demonstrate that SLEB successfully accelerates LLM inference without
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.07613</link><description>&lt;p&gt;
&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global optimality under amenable symmetry constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#28385;&#36275;&#21487;&#25509;&#21463;&#21464;&#25442;&#32676;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#21363;&#21516;&#26102;&#28385;&#36275;&#20197;&#19979;&#20004;&#20010;&#26465;&#20214;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#32473;&#23450;&#30340;&#20984;&#24615;&#27867;&#20989;&#25110;&#39118;&#38505;&#65292;&#65288;2&#65289;&#28385;&#36275;&#21487;&#23481;&#24525;&#23545;&#31216;&#32422;&#26463;&#12290;&#36825;&#31181;&#23545;&#31216;&#24615;&#36136;&#30340;&#20363;&#23376;&#21253;&#25324;&#19981;&#21464;&#24615;&#12289;&#21487;&#21464;&#24615;&#25110;&#20934;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;Stein&#21644;Le Cam&#30340;&#32769;&#24605;&#24819;&#65292;&#20197;&#21450;&#22312;&#21487;&#25509;&#21463;&#32676;&#30340;&#36941;&#21382;&#23450;&#29702;&#20013;&#20986;&#29616;&#30340;&#36817;&#20284;&#32676;&#24179;&#22343;&#20540;&#12290;&#22312;&#20984;&#20998;&#26512;&#20013;&#65292;&#19968;&#31867;&#31216;&#20026;&#36712;&#36947;&#20984;&#20307;&#30340;&#20984;&#38598;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#20013;&#30830;&#23450;&#20102;&#36825;&#31867;&#36712;&#36947;&#20984;&#20307;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#31216;&#20026;coycle&#30340;&#31616;&#21333;&#35013;&#32622;&#22914;&#20309;&#23558;&#19981;&#21516;&#24418;&#24335;&#30340;&#23545;&#31216;&#24615;&#36716;&#21270;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22312;&#23545;&#31216;&#32422;&#26463;&#19979;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#30340;Monge-Kantorovich&#23450;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.04375</link><description>&lt;p&gt;
&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Bounding the Excess Risk for Linear Models Trained on Marginal-Preserving, Differentially-Private, Synthetic Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20445;&#25345;&#36793;&#32536;&#19968;&#33268;&#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#32447;&#24615;&#27169;&#22411;&#30340;&#36807;&#37327;&#39118;&#38505;&#30340;&#26032;&#30028;&#38480;&#65292;&#20026;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;&#27169;&#22411;&#21487;&#33021;&#25581;&#31034;&#35757;&#32451;&#25968;&#25454;&#20013;&#20010;&#20307;&#30340;&#31169;&#23494;&#20449;&#24687;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#38450;&#27490;&#25935;&#24863;&#25968;&#25454;&#30340;&#27844;&#38706;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#30340;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#32780;&#19981;&#26159;&#30495;&#23454;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#21512;&#25104;&#25968;&#25454;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#28857;&#26159;&#33021;&#22815;&#20445;&#25345;&#21407;&#22987;&#20998;&#24067;&#30340;&#20302;&#38454;&#36793;&#32536;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#38024;&#23545;&#22312;&#36825;&#31181;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#38024;&#23545;&#36830;&#32493;&#21644;Lipschitz&#25439;&#22833;&#20989;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#36807;&#37327;&#32463;&#39564;&#39118;&#38505;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#32467;&#26524;&#20043;&#22806;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of machine learning (ML) has raised concerns that an ML model may reveal private information about an individual who has contributed to the training dataset. To prevent leakage of sensitive data, we consider using differentially-private (DP), synthetic training data instead of real training data to train an ML model. A key desirable property of synthetic data is its ability to preserve the low-order marginals of the original distribution. Our main contribution comprises novel upper and lower bounds on the excess empirical risk of linear models trained on such synthetic data, for continuous and Lipschitz loss functions. We perform extensive experimentation alongside our theoretical results.
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04298</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-View Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04298
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;(SR)&#25628;&#32034;&#34920;&#31034;&#35299;&#37322;&#21464;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#30446;&#21069;&#30340;SR&#26041;&#27861;&#20551;&#35774;&#20174;&#21333;&#20010;&#23454;&#39564;&#20013;&#25552;&#21462;&#30340;&#21333;&#20010;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#19981;&#21516;&#35774;&#32622;&#30340;&#22810;&#20010;&#23454;&#39564;&#32467;&#26524;&#38598;&#12290;&#20256;&#32479;&#30340;SR&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#28508;&#22312;&#30340;&#34920;&#36798;&#24335;&#65292;&#22240;&#20026;&#27599;&#20010;&#23454;&#39564;&#30340;&#21442;&#25968;&#21487;&#33021;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#23454;&#39564;&#29615;&#22659;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#21442;&#25968;&#21270;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35780;&#20272;&#30340;&#34920;&#36798;&#24335;&#36866;&#24212;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#36820;&#22238;&#33021;&#22815;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#21442;&#25968;&#20989;&#25968;&#26063;f(x; \theta)&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#24050;&#30693;&#34920;&#36798;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26469;&#23637;&#31034;MvSR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.02769</link><description>&lt;p&gt;
&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#27491;&#21017;&#21270;: &#26131;&#20110;&#27169;&#20223;&#30340;&#21487;&#25512;&#24191;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning from Teaching Regularization: Generalizable Correlations Should be Easy to Imitate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02769
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;LoT&#65289;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#21319;&#20027;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LoT&#33021;&#26377;&#25928;&#35782;&#21035;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21363;&#20174;&#25945;&#23398;&#20013;&#23398;&#20064;&#65288;Learning from Teaching&#65292;&#31616;&#31216;LoT&#65289;&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#12290;&#21463;&#21040;&#20154;&#31867;&#25429;&#25417;&#31616;&#26126;&#25277;&#35937;&#27169;&#24335;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#25512;&#24191;&#30340;&#20851;&#31995;&#26356;&#23481;&#26131;&#25945;&#25480;&#12290;LoT&#36890;&#36807;&#36741;&#21161;&#23398;&#29983;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#20010;&#27010;&#24565;&#65292;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#35757;&#32451;&#20027;&#27169;&#22411;&#21644;&#25913;&#36827;&#20027;&#27169;&#22411;&#65292;&#20197;&#25429;&#25417;&#26356;&#22810;&#20855;&#26377;&#27867;&#21270;&#21644;&#21487;&#25945;&#25480;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24341;&#20837;LoT&#30456;&#27604;&#20165;&#22312;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#12290;&#36825;&#34920;&#26126;&#20102;LoT&#22312;&#35782;&#21035;&#21487;&#25512;&#24191;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization remains a central challenge in machine learning. In this work, we propose Learning from Teaching (LoT), a novel regularization technique for deep neural networks to enhance generalization. Inspired by the human ability to capture concise and abstract patterns, we hypothesize that generalizable correlations are expected to be easier to teach. LoT operationalizes this concept to improve the generalization of the main model with auxiliary student learners. The student learners are trained by the main model and improve the main model to capture more generalizable and teachable correlations by providing feedback. Our experimental results across several domains, including Computer Vision, Natural Language Processing, and Reinforcement Learning, demonstrate that the introduction of LoT brings significant benefits compared to merely training models on the original training data. It suggests the effectiveness of LoT in identifying generalizable information without falling into th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.02009</link><description>&lt;p&gt;
&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#40065;&#26834;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Task Learning with Excess Risks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#26041;&#27861;&#65292;&#26681;&#25454;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20811;&#26381;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#26102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#36890;&#36807;&#20248;&#21270;&#25152;&#26377;&#20219;&#21153;&#25439;&#22833;&#30340;&#20984;&#32452;&#21512;&#26469;&#32771;&#34385;&#20026;&#22810;&#20010;&#20219;&#21153;&#23398;&#20064;&#19968;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26041;&#26696;&#65292;&#26681;&#25454;&#21508;&#33258;&#30340;&#25439;&#22833;&#21160;&#24577;&#35843;&#25972;&#20219;&#21153;&#26435;&#37325;&#65292;&#20197;&#20248;&#20808;&#32771;&#34385;&#22256;&#38590;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#20250;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36807;&#22810;&#30340;&#26435;&#37325;&#24448;&#24448;&#34987;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#23545;&#36739;&#22823;&#36125;&#21494;&#26031;&#26368;&#20248;&#35823;&#24046;&#30340;&#22122;&#22768;&#20219;&#21153;&#65292;&#20174;&#32780;&#25513;&#30422;&#20854;&#20182;&#20219;&#21153;&#24182;&#23548;&#33268;&#25972;&#20307;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#36807;&#22810;&#39118;&#38505;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;ExcessMTL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36807;&#22810;&#39118;&#38505;&#30340;&#20219;&#21153;&#24179;&#34913;&#26041;&#27861;&#65292;&#36890;&#36807;&#20219;&#21153;&#21040;&#25910;&#25947;&#30340;&#36317;&#31163;&#26469;&#26356;&#26032;&#20219;&#21153;&#26435;&#37325;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;ExcessMTL&#23558;&#26356;&#39640;&#30340;&#26435;&#37325;&#20998;&#37197;&#32473;&#36739;&#24046;&#35757;&#32451;&#30340;&#36317;&#31163;&#25910;&#25947;&#36739;&#36828;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20272;&#35745;&#36807;&#22810;&#39118;&#38505;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11436</link><description>&lt;p&gt;
&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#22823;&#33041;&#30382;&#23618;V2&#21306;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Layerwise complexity-matched learning yields an improved model of cortical area V2
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11436
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22797;&#26434;&#24230;&#21305;&#37197;&#23398;&#20064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#26368;&#22823;&#21270;&#20102;&#29305;&#24449;&#30456;&#20284;&#24615;&#21516;&#26102;&#22312;&#19981;&#21516;&#20301;&#32622;&#30340;&#34917;&#19969;&#19978;&#35299;&#38500;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35782;&#21035;&#22797;&#26434;&#35270;&#35273;&#27169;&#24335;&#30340;&#33021;&#21147;&#26159;&#36890;&#36807;&#39034;&#27425;&#21306;&#22495;&#22312;&#33145;&#20391;&#35270;&#35273;&#30382;&#23618;&#20013;&#25191;&#34892;&#30340;&#21464;&#25442;&#25152;&#24418;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#23545;&#23618;&#27425;&#32467;&#26500;&#30340;&#21518;&#26399;&#31070;&#32463;&#21453;&#24212;&#30340;&#26368;&#20339;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#25163;&#24037;&#35774;&#35745;&#27169;&#22411;&#30456;&#27604;&#65292;&#25110;&#32773;&#19982;&#20248;&#21270;&#32534;&#30721;&#25928;&#29575;&#25110;&#39044;&#27979;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#23545;&#21069;&#26399;&#38454;&#27573;&#25552;&#20379;&#20102;&#36739;&#24046;&#30340;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#29983;&#29289;&#19978;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#33258;&#19979;&#32780;&#19978;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#29420;&#31435;&#22320;&#20316;&#29992;&#20110;&#36830;&#32493;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#26368;&#22823;&#21270;&#20102;&#23545;&#23616;&#37096;&#21464;&#24418;&#33258;&#28982;&#22270;&#20687;&#34917;&#19969;&#23545;&#20043;&#38388;&#30340;&#29305;&#24449;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#37319;&#26679;&#33258;&#20854;&#20182;&#20301;&#32622;&#30340;&#34917;&#19969;&#26102;&#20351;&#29305;&#24449;&#21435;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11436v2 Announce Type: replace-cross  Abstract: Human ability to recognize complex visual patterns arises through transformations performed by successive areas in the ventral visual cortex. Deep neural networks trained end-to-end for object recognition approach human capabilities, and offer the best descriptions to date of neural responses in the late stages of the hierarchy. But these networks provide a poor account of the early stages, compared to traditional hand-engineered models, or models optimized for coding efficiency or prediction. Moreover, the gradient backpropagation used in end-to-end learning is generally considered to be biologically implausible. Here, we overcome both of these limitations by developing a bottom-up self-supervised training methodology that operates independently on successive layers. Specifically, we maximize feature similarity between pairs of locally-deformed natural image patches, while decorrelating features across patches sampled from oth
&lt;/p&gt;</description></item><item><title>SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04985</link><description>&lt;p&gt;
SparQ&#27880;&#24847;&#21147;&#65306;&#39640;&#25928;&#24102;&#23485;&#30340;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SparQ Attention: Bandwidth-Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04985
&lt;/p&gt;
&lt;p&gt;
SparQ Attention&#36890;&#36807;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;LLMs&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21019;&#20102;&#35768;&#22810;&#26032;&#21487;&#33021;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#26222;&#36941;&#20351;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SparQ&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#36890;&#36807;&#36873;&#25321;&#24615;&#33719;&#21462;&#32531;&#23384;&#21382;&#21490;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#22359;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;LLMs&#30340;&#25512;&#29702;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04985v2 Announce Type: replace  Abstract: Generative large language models (LLMs) have opened up numerous novel possibilities, but due to their significant computational requirements their ubiquitous use remains challenging. Some of the most useful applications require processing large numbers of samples at a time and using long contexts, both significantly increasing the memory communication load of the models. We introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by reducing the memory bandwidth requirements within the attention blocks through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show how SparQ Attention can decrease the attention memory bandwidth requirements up to eight times without any loss in accuracy by evaluating Llama 2 and Pythia models on a wide ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.04855</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;&#65292;&#22312;&#20998;&#26512;&#22024;&#26434;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22825;&#25991;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#65292;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#36127;&#20540;&#65292;&#21363;&#20351;&#30495;&#23454;&#30340;&#29289;&#29702;&#20449;&#21495;&#20005;&#26684;&#20026;&#27491;&#12290;&#20197;&#24448;&#30340;NMF&#24037;&#20316;&#26410;&#20197;&#32479;&#35745;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#36127;&#25968;&#25454;&#65292;&#36825;&#22312;&#20302;&#20449;&#22122;&#27604;&#25968;&#25454;&#20013;&#20986;&#29616;&#35768;&#22810;&#36127;&#20540;&#26102;&#20250;&#21464;&#24471;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#22024;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20219;&#20309;&#24341;&#20837;&#30340;&#36127;&#20540;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20351;&#29992;&#36127;&#25968;&#25454;&#31354;&#38388;&#32780;&#19981;&#36827;&#34892;&#25130;&#21462;&#65292;&#24182;&#19988;&#22312;&#28040;&#38500;&#36127;&#25968;&#25454;&#26102;&#19981;&#20250;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#12290;&#25105;&#20204;&#22312;&#31616;&#21333;&#21644;&#26356;&#29616;&#23454;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#20855;&#26377;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04855v2 Announce Type: replace-cross  Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotoni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.11447</link><description>&lt;p&gt;
&#39044;&#27979;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#20013;&#24739;&#32773;&#20381;&#20174;&#24615;&#30340;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;&#36807;&#25935;&#24615;&#40763;&#28814;&#20122;&#21345;&#28608;&#32032;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;&#30340;&#31649;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30382;&#19979;&#20813;&#30123;&#27835;&#30103;(SCIT)&#26159;&#36807;&#25935;&#24615;&#40763;&#28814;&#30340;&#38271;&#25928;&#22240;&#26524;&#27835;&#30103;&#12290;&#22914;&#20309;&#25552;&#39640;&#24739;&#32773;&#23545;&#21464;&#24212;&#21407;&#20813;&#30123;&#27835;&#30103;(AIT)&#30340;&#20381;&#20174;&#24615;&#20197;&#26368;&#22823;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#22312;AIT&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20934;&#30830;&#39044;&#27979;&#24739;&#32773;&#30340;&#38750;&#20381;&#20174;&#39118;&#38505;&#21644;&#30456;&#20851;&#30340;&#31995;&#32479;&#30151;&#29366;&#35780;&#20998;&#65292;&#20026;&#38271;&#26399;AIT&#30340;&#31649;&#29702;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#20998;&#26512;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#24207;&#21015;&#28508;&#22312;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#27169;&#22411;(SLAC)&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;(LSTM)&#65292;&#24182;&#22522;&#20110;&#35780;&#20998;&#21644;&#20381;&#20174;&#24615;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;&#22312;&#25490;&#38500;&#31532;&#19968;&#26102;&#38388;&#27493;&#30340;&#20559;&#20506;&#26679;&#26412;&#21518;&#65292;SLAC&#27169;&#22411;&#30340;&#39044;&#27979;&#20381;&#20174;&#20934;&#30830;&#29575;&#20026;60%-72%&#65292;&#32780;LSTM&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;66%-84%&#65292;&#26681;&#25454;&#26102;&#38388;&#27493;&#38271;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;SLAC&#27169;&#22411;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#33539;&#22260;&#22312;0.93&#21040;2.22&#20043;&#38388;&#65292;&#32780;LSTM&#27169;&#22411;&#30340;RMSE&#33539;&#22260;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal treatment of allergic rhinitis. How to enhance the adherence of patients to maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in the management of AIT. This study aims to leverage novel machine learning models to precisely predict the risk of non-adherence of patients and related systematic symptom scores, to provide a novel approach in the management of long-term AIT.  Methods: The research develops and analyzes two models, Sequential Latent Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on scoring and adherence prediction capabilities.  Results: Excluding the biased samples at the first time step, the predictive adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and $2.22$, while for L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.04082</link><description>&lt;p&gt;
&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#25913;&#36827;&#20102;&#22270;&#26696;&#25903;&#26550;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved motif-scaffolding with SE(3) flow matching. (arXiv:2401.04082v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SE(3)&#27969;&#21305;&#37197;&#30340;&#22270;&#26696;&#25903;&#26550;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#26696;&#25674;&#38144;&#21644;&#22270;&#26696;&#24341;&#23548;&#20004;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;&#30340;&#25903;&#26550;&#65292;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#30456;&#24403;&#29978;&#33267;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35774;&#35745;&#36890;&#24120;&#20174;&#19968;&#20010;&#22270;&#26696;&#30340;&#26399;&#26395;&#21151;&#33021;&#24320;&#22987;&#65292;&#22270;&#26696;&#25903;&#26550;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#21151;&#33021;&#24615;&#34507;&#30333;&#36136;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#35774;&#35745;&#21508;&#31181;&#22270;&#26696;&#30340;&#25903;&#26550;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#25903;&#26550;&#24448;&#24448;&#32570;&#20047;&#32467;&#26500;&#22810;&#26679;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#28287;&#23454;&#39564;&#39564;&#35777;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;FrameFlow&#65292;&#19968;&#31181;&#29992;&#20110;&#34507;&#30333;&#36136;&#20027;&#38142;&#29983;&#25104;&#30340;SE(3)&#27969;&#21305;&#37197;&#27169;&#22411;&#25193;&#23637;&#21040;&#20351;&#29992;&#20004;&#31181;&#20114;&#34917;&#30340;&#26041;&#27861;&#36827;&#34892;&#22270;&#26696;&#25903;&#26550;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#25674;&#38144;&#65292;&#21363;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;FrameFlow&#35757;&#32451;&#20026;&#20197;&#22270;&#26696;&#20026;&#36755;&#20837;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#22270;&#26696;&#24341;&#23548;&#65292;&#23427;&#20351;&#29992;FrameFlow&#30340;&#26465;&#20214;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#25903;&#26550;&#26500;&#24314;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#29575;&#19982;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20135;&#29983;&#32467;&#26500;&#19978;&#22810;&#26679;&#24615;&#26356;&#39640;2.5&#20493;&#30340;&#25903;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein design often begins with knowledge of a desired function from a motif which motif-scaffolding aims to construct a functional protein around. Recently, generative models have achieved breakthrough success in designing scaffolds for a diverse range of motifs. However, the generated scaffolds tend to lack structural diversity, which can hinder success in wet-lab validation. In this work, we extend FrameFlow, an SE(3) flow matching model for protein backbone generation, to perform motif-scaffolding with two complementary approaches. The first is motif amortization, in which FrameFlow is trained with the motif as input using a data augmentation strategy. The second is motif guidance, which performs scaffolding using an estimate of the conditional score from FrameFlow, and requires no additional training. Both approaches achieve an equivalent or higher success rate than previous state-of-the-art methods, with 2.5 times more structurally diverse scaffolds. Code: https://github.com/ mi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.04131</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#23450;&#20301;&#36328;&#20219;&#21153;&#24207;&#21015;&#32487;&#32493;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Locating Cross-Task Sequence Continuation Circuits in Transformers. (arXiv:2311.04131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04131
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;Transformer&#27169;&#22411;&#20013;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#65292;&#30740;&#31350;&#21457;&#29616;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#34892;&#20026;&#39044;&#27979;&#33021;&#21147;&#12289;&#38169;&#35823;&#35782;&#21035;&#33021;&#21147;&#21644;&#32534;&#36753;&#36807;&#31243;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;Transformer&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#22797;&#26434;&#30340;&#26550;&#26500;&#20351;&#20854;&#38590;&#20197;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#23558;Transformer&#27169;&#22411;&#36824;&#21407;&#20026;&#21487;&#35835;&#30340;&#30005;&#36335;&#34920;&#31034;&#65292;&#29992;&#20110;&#23454;&#29616;&#31639;&#27861;&#21151;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27604;&#36739;&#31867;&#20284;&#30340;&#24207;&#21015;&#32487;&#32493;&#20219;&#21153;&#30340;&#30005;&#36335;&#26469;&#25193;&#23637;&#36825;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;&#21253;&#25324;&#25968;&#23383;&#12289;&#25968;&#23383;&#35789;&#21644;&#26376;&#20221;&#30340;&#36882;&#22686;&#24207;&#21015;&#12290;&#36890;&#36807;&#24212;&#29992;&#30005;&#36335;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36127;&#36131;&#26816;&#27979;&#24207;&#21015;&#25104;&#21592;&#21644;&#39044;&#27979;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#25104;&#21592;&#30340;&#20851;&#38190;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35821;&#20041;&#30456;&#20851;&#24207;&#21015;&#20381;&#36182;&#20110;&#20855;&#26377;&#31867;&#20284;&#20316;&#29992;&#30340;&#20849;&#20139;&#30005;&#36335;&#23376;&#22270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#35760;&#24405;&#20849;&#20139;&#30340;&#35745;&#31639;&#32467;&#26500;&#33021;&#22815;&#26356;&#22909;&#22320;&#39044;&#27979;&#27169;&#22411;&#34892;&#20026;&#65292;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#26356;&#23433;&#20840;&#30340;&#32534;&#36753;&#36807;&#31243;&#12290;&#36825;&#31181;&#23545;Transformer&#30340;&#26426;&#26800;&#29702;&#35299;&#26159;&#26500;&#24314;&#26356;&#20581;&#22766;&#12289;&#35843;&#35797;&#21644;&#32534;&#36753;&#26356;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2311.01473</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#29305;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#20854;&#29305;&#24449;&#30340;&#20840;&#38754;&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#30740;&#31350;&#65292;&#20197;&#22635;&#34917;&#23545;PAEs&#29420;&#29305;&#29305;&#24449;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23545;&#23545;&#25239;&#26679;&#26412;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#33030;&#24369;&#24615;&#12290;&#38500;&#20102;&#22312;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#25915;&#20987;&#22806;&#65292;&#23545;&#25239;&#26679;&#26412;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#29289;&#29702;&#23545;&#25239;&#26679;&#26412;&#65288;PAEs&#65289;&#30340;&#30740;&#31350;&#32570;&#20047;&#23545;&#20854;&#29420;&#29305;&#29305;&#24449;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#23548;&#33268;&#20854;&#37325;&#35201;&#24615;&#21644;&#29702;&#35299;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#35757;&#32451;&#12289;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#36807;&#31243;&#20013;&#20840;&#38754;&#32771;&#23519;PAEs&#30340;&#29305;&#28857;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#20998;&#26512;&#29289;&#29702;&#23545;&#25239;&#25915;&#20987;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#30830;&#23450;&#21046;&#36896;&#21644;&#37325;&#37319;&#26679;&#26159;PAEs&#20013;&#29420;&#29305;&#23646;&#24615;&#21644;&#29305;&#27530;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#65292;&#25105;&#20204;&#22522;&#20110;&#20854;&#29305;&#23450;&#29305;&#24449;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;PAEs&#20998;&#26512;&#21644;&#20998;&#31867;&#26694;&#26550;&#65292;&#28085;&#30422;&#20102;100&#22810;&#20010;&#29289;&#29702;&#23545;&#25239;&#19990;&#30028;&#30740;&#31350;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated high vulnerability to adversarial examples. Besides the attacks in the digital world, the practical implications of adversarial examples in the physical world present significant challenges and safety concerns. However, current research on physical adversarial examples (PAEs) lacks a comprehensive understanding of their unique characteristics, leading to limited significance and understanding. In this paper, we address this gap by thoroughly examining the characteristics of PAEs within a practical workflow encompassing training, manufacturing, and re-sampling processes. By analyzing the links between physical adversarial attacks, we identify manufacturing and re-sampling as the primary sources of distinct attributes and particularities in PAEs. Leveraging this knowledge, we develop a comprehensive analysis and classification framework for PAEs based on their specific characteristics, covering over 100 studies on physical-world adversarial e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16975</link><description>&lt;p&gt;
&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21450;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Neural Network Approaches for Conditional Optimal Transport with Applications in Bayesian Inference. (arXiv:2310.16975v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#65292;&#23454;&#29616;&#20102;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#30340;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20256;&#36755;&#26144;&#23556;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20998;&#21035;&#36924;&#36817;&#38745;&#24577;&#21644;&#21160;&#24577;&#26465;&#20214;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;&#35299;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#21487;&#20197;&#23545;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#65292;&#36825;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#34920;&#31034;&#20026;&#21487;&#22788;&#29702;&#30340;&#21442;&#32771;&#20998;&#24067;&#30340;&#36716;&#25442;&#65292;&#22240;&#27492;&#23646;&#20110;&#27979;&#24230;&#20256;&#36755;&#30340;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;COT&#26144;&#23556;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#36873;&#25321;&#65292;&#20855;&#26377;&#21807;&#19968;&#24615;&#21644;&#21333;&#35843;&#24615;&#31561;&#21487;&#21462;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#30340;COT&#38382;&#39064;&#22312;&#20013;&#31561;&#32500;&#24230;&#19979;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#30340;&#25968;&#20540;&#31639;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;COT&#26144;&#23556;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;COT&#38382;&#39064;&#30340;&#38745;&#24577;&#21644;&#21160;&#24577;&#34920;&#36798;&#24418;&#24335;&#30340;&#32467;&#26500;&#12290;PCP-Map&#23558;&#26465;&#20214;&#20256;&#36755;&#26144;&#23556;&#24314;&#27169;&#20026;&#37096;&#20998;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;PICNN&#65289;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems, respectively. Both approaches enable sampling and density estimation of conditional probability distributions, which are core tasks in Bayesian inference. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. COT maps are a canonical choice within this framework, with desirable properties such as uniqueness and monotonicity. However, the associated COT problems are computationally challenging, even in moderate dimensions. To improve the scalability, our numerical algorithms leverage neural networks to parameterize COT maps. Our methods exploit the structure of the static and dynamic formulations of the COT problem. PCP-Map models conditional transport maps as the gradient of a partially input convex neural network (PICNN) and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.08419</link><description>&lt;p&gt;
&#22312;&#20108;&#21313;&#20010;&#26597;&#35810;&#20013;&#30772;&#35299;&#40657;&#30418;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Black Box Large Language Models in Twenty Queries. (arXiv:2310.08419v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08419
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PAIR&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#33021;&#40657;&#30418;&#35775;&#38382;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#26159;&#29616;&#26377;&#31639;&#27861;&#30340;&#25968;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#30772;&#35299;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#36843;&#20351;LLMs&#36229;&#36234;&#20854;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;&#28431;&#27934;&#23545;&#20110;&#29702;&#35299;&#22266;&#26377;&#24369;&#28857;&#24182;&#38450;&#27490;&#26410;&#26469;&#30340;&#19981;&#24403;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Automatic Iterative Refinement&#65288;PAIR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20165;&#36890;&#36807;&#23545;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#31639;&#27861;&#29983;&#25104;&#35821;&#20041;&#30772;&#35299;&#12290;PAIR&#21463;&#21040;&#31038;&#20250;&#24037;&#31243;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#20351;&#29992;&#25915;&#20987;&#32773;LLM&#33258;&#21160;&#29983;&#25104;&#38024;&#23545;&#30446;&#26631;LLM&#30340;&#30772;&#35299;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#25915;&#20987;&#32773;LLM&#36890;&#36807;&#36845;&#20195;&#26597;&#35810;&#30446;&#26631;LLM&#26469;&#26356;&#26032;&#21644;&#25913;&#36827;&#20505;&#36873;&#30772;&#35299;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;PAIR&#36890;&#24120;&#21482;&#38656;&#35201;&#23569;&#20110;&#20108;&#21313;&#20010;&#26597;&#35810;&#26469;&#29983;&#25104;&#30772;&#35299;&#65292;&#36825;&#27604;&#29616;&#26377;&#31639;&#27861;&#39640;&#25928;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;PAIR&#36824;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#30772;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.07506</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#36827;&#34892;&#39640;&#25928;&#25968;&#25454;&#38598;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation. (arXiv:2310.07506v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#29305;&#24449;&#20849;&#20139;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65288;HMN&#65289;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21387;&#32553;&#25968;&#25454;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;HMN&#33021;&#22815;&#25429;&#25417;&#21040;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#65292;&#25968;&#25454;&#21387;&#32553;&#65288;DC&#65289;&#26088;&#22312;&#21512;&#25104;&#19968;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#39640;&#24615;&#33021;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#21442;&#25968;&#21270;&#22686;&#24378;DC&#65292;&#23558;&#25968;&#25454;&#21387;&#32553;&#20026;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23481;&#22120;&#32780;&#19981;&#26159;&#20687;&#32032;&#31354;&#38388;&#12290;&#25968;&#25454;&#21442;&#25968;&#21270;&#30340;&#30452;&#35273;&#26159;&#32534;&#30721;&#22270;&#20687;&#30340;&#20849;&#20139;&#29305;&#24449;&#65292;&#20197;&#36991;&#20813;&#39069;&#22806;&#30340;&#23384;&#20648;&#25104;&#26412;&#12290;&#26412;&#25991;&#35748;&#35782;&#21040;&#30001;&#20110;&#20998;&#31867;&#31995;&#32479;&#30340;&#20869;&#22312;&#20998;&#23618;&#32467;&#26500;&#65292;&#22270;&#20687;&#20197;&#20998;&#23618;&#26041;&#24335;&#20849;&#20139;&#20849;&#21516;&#30340;&#29305;&#24449;&#65292;&#36825;&#26159;&#24403;&#21069;&#25968;&#25454;&#21442;&#25968;&#21270;&#26041;&#27861;&#25152;&#24573;&#35270;&#30340;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20351;DC&#19982;&#36825;&#31181;&#20998;&#23618;&#24615;&#36136;&#23545;&#40784;&#65292;&#24182;&#22312;&#25968;&#25454;&#23481;&#22120;&#20869;&#37096;&#40723;&#21169;&#26356;&#39640;&#25928;&#30340;&#20449;&#24687;&#20849;&#20139;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#21442;&#25968;&#21270;&#26550;&#26500;&#65292;&#20998;&#23618;&#35760;&#24518;&#32593;&#32476;&#65288;HMN&#65289;&#12290;HMN&#23558;&#21387;&#32553;&#25968;&#25454;&#23384;&#20648;&#22312;&#19977;&#23618;&#32467;&#26500;&#20013;&#65292;&#34920;&#31034;&#25968;&#25454;&#38598;&#32423;&#21035;&#12289;&#31867;&#21035;&#32423;&#21035;&#21644;&#26679;&#26412;&#32423;&#21035;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a real-world dataset, data condensation (DC) aims to synthesize a significantly smaller dataset that captures the knowledge of this dataset for model training with high performance. Recent works propose to enhance DC with data parameterization, which condenses data into parameterized data containers rather than pixel space. The intuition behind data parameterization is to encode shared features of images to avoid additional storage costs. In this paper, we recognize that images share common features in a hierarchical way due to the inherent hierarchical structure of the classification system, which is overlooked by current data parameterization methods. To better align DC with this hierarchical nature and encourage more efficient information sharing inside data containers, we propose a novel data parameterization architecture, Hierarchical Memory Network (HMN). HMN stores condensed data in a three-tier structure, representing the dataset-level, class-level, and instance-level fea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05990</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#22312;&#33258;&#21160;&#21306;&#22495;&#24615;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#65306;&#20266;&#26631;&#31614;&#27861;&#29992;&#20110;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation through Pseudolabels in Automatic Region Based Coronary Artery Segmentation for Disease Diagnosis. (arXiv:2310.05990v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05990
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;&#20896;&#29366;&#21160;&#33033;&#20998;&#21106;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#26159;&#21487;&#39044;&#38450;&#30340;&#20027;&#35201;&#27515;&#20129;&#21644;&#27531;&#30142;&#21407;&#22240;&#20043;&#19968;&#12290;&#36825;&#20123;&#30142;&#30149;&#30340;&#35786;&#26029;&#36890;&#24120;&#22256;&#38590;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#20013;&#30340;&#21160;&#33033;&#20998;&#21106;&#24050;&#32463;&#28436;&#21464;&#25104;&#20026;&#19968;&#31181;&#36741;&#21161;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#37327;&#26377;&#38480;&#19988;&#26500;&#24314;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#20998;&#21106;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#20266;&#26631;&#31614;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#22522;&#20934;Yolo&#27169;&#22411;&#24615;&#33021;&#30340;&#24605;&#24819;&#12290;&#35813;&#26041;&#27861;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#23558;&#22522;&#32447;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;9&#65285;&#65292;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#25552;&#39640;&#20102;3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronary Artery Diseases(CADs) though preventable are one of the leading causes of death and disability. Diagnosis of these diseases is often difficult and resource intensive. Segmentation of arteries in angiographic images has evolved as a tool for assistance, helping clinicians in making accurate diagnosis. However, due to the limited amount of data and the difficulty in curating a dataset, the task of segmentation has proven challenging. In this study, we introduce the idea of using pseudolabels as a data augmentation technique to improve the performance of the baseline Yolo model. This method increases the F1 score of the baseline by 9% in the validation dataset and by 3% in the test dataset.
&lt;/p&gt;</description></item><item><title>DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;</title><link>http://arxiv.org/abs/2310.02027</link><description>&lt;p&gt;
DeepHGCN&#65306;&#26397;&#30528;&#26356;&#28145;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02027
&lt;/p&gt;
&lt;p&gt;
DeepHGCN&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#23618;&#26550;&#26500;&#30340;&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#21644;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#30340;&#26497;&#22823;&#25913;&#36827;&#21644;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26174;&#33879;&#20943;&#36731;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#26354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;HGCN&#65289;&#22312;&#25552;&#21462;&#20998;&#23618;&#22270;&#20449;&#24687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#21452;&#26354;&#25805;&#20316;&#21644;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;HGCN&#21463;&#38480;&#20110;&#27973;&#23618;&#26550;&#26500;&#12290;&#23613;&#31649;&#22312;GCNs&#20013;&#24050;&#32463;&#24212;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#20294;&#26159;&#24320;&#21457;&#21452;&#26354;&#27835;&#30103;&#26041;&#27861;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25805;&#20316;&#24517;&#39035;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#20197;&#36866;&#24212;&#21452;&#26354;&#24615;&#36136;&#12290;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DeepHGCN&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#22823;&#22823;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#25928;&#26524;&#30340;&#28145;&#23618;&#22810;&#23618;HGCN&#26550;&#26500;&#12290;DeepHGCN&#20855;&#26377;&#20004;&#20010;&#28145;&#23618;HGCN&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#26354;&#29305;&#24449;&#36716;&#25442;&#23618;&#65292;&#33021;&#22815;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#32447;&#24615;&#26144;&#23556;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26377;&#25928;&#30340;&#21452;&#26354;&#27531;&#24046;&#36830;&#25509;&#21644;&#26435;&#37325;&#21644;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#20419;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00161</link><description>&lt;p&gt;
&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection-Oriented Image-Text Pretraining for Open-Vocabulary Detection. (arXiv:2310.00161v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#23545;&#27604;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#22312;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;40.4&#30340;&#25513;&#30721;AP$_r$&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38754;&#21521;&#26816;&#27979;&#30340;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#29992;&#26816;&#27979;&#22120;&#26550;&#26500;&#26367;&#20195;&#24120;&#29992;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#26816;&#27979;&#22120;&#22836;&#37096;&#33021;&#22815;&#20174;&#22122;&#22768;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23398;&#20064;&#65292;&#26356;&#22909;&#22320;&#28385;&#36275;&#26816;&#27979;&#30340;&#21306;&#22495;&#32423;&#35782;&#21035;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#23545;&#27604;&#25439;&#22833;&#32780;&#19981;&#20351;&#29992;&#20266;&#26631;&#31614;&#65292;&#26159;&#23545;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26032;&#20986;&#29616;&#30340;&#29289;&#20307;-&#35821;&#20041;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#24179;&#31227;&#31383;&#21475;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20027;&#24178;&#32593;&#32476;&#30340;&#34920;&#31034;&#26356;&#21152;&#40065;&#26834;&#12289;&#24179;&#31227;&#19981;&#21464;&#65292;&#24182;&#19988;&#19981;&#21463;&#31383;&#21475;&#27169;&#24335;&#30340;&#20559;&#24046;&#24433;&#21709;&#12290;&#22312;&#27969;&#34892;&#30340;LVIS&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24120;&#35265;&#30340;ViT-L&#20027;&#24178;&#32593;&#32476;&#21462;&#24471;&#20102;40.4&#30340;&#25513;&#30721;AP$_r$&#26032;&#30340;&#26368;&#20248;&#32467;&#26524;&#65292;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new open-vocabulary detection approach based on detection-oriented image-text pretraining to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we replace the commonly used classification architecture with the detector architecture, which better serves the region-level recognition needs of detection by enabling the detector heads to learn from noisy image-text pairs. Using only standard contrastive loss and no pseudo-labeling, our approach is a simple yet effective extension of the contrastive learning method to learn emergent object-semantic cues. In addition, we propose a shifted-window learning approach upon window attention to make the backbone representation more robust, translation-invariant, and less biased by the window pattern. On the popular LVIS open-vocabulary detection benchmark, our approach sets a new state of the art of 40.4 mask AP$_r$ using the common ViT-L backbone, significantly outperforming t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16748</link><description>&lt;p&gt;
&#29992;XRM&#21457;&#29616;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#29615;&#22659;&#30340;&#31639;&#27861; XRM&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#24182;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#30340;&#38169;&#35823;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#29615;&#22659;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#38656;&#35201;&#29615;&#22659;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27880;&#37322;&#30340;&#33719;&#21462;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#21463;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#26399;&#26395;&#21644;&#24863;&#30693;&#20559;&#24046;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#24212;&#29992;&#39046;&#22495;&#20840;&#38754;&#27867;&#21270;&#30340;&#40065;&#26834;&#24615;AI&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#24320;&#21457;&#19968;&#31181;&#31639;&#27861;&#26469;&#33258;&#21160;&#21457;&#29616;&#24341;&#21457;&#24191;&#27867;&#27867;&#21270;&#30340;&#29615;&#22659;&#12290;&#30446;&#21069;&#30340;&#25552;&#26696;&#26681;&#25454;&#35757;&#32451;&#35823;&#24046;&#23558;&#31034;&#20363;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#28155;&#21152;&#20102;&#36229;&#21442;&#25968;&#21644;&#26089;&#20572;&#31574;&#30053;&#65292;&#32780;&#36825;&#20123;&#21442;&#25968;&#26159;&#26080;&#27861;&#22312;&#27809;&#26377;&#20154;&#31867;&#27880;&#37322;&#29615;&#22659;&#30340;&#39564;&#35777;&#38598;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35843;&#25972;&#30340;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#27491;&#26159;&#35201;&#21457;&#29616;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Cross-Risk-Minimization (XRM) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;XRM &#35757;&#32451;&#20004;&#20010;&#23402;&#29983;&#32593;&#32476;&#65292;&#27599;&#20010;&#32593;&#32476;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#20010;&#38543;&#26426;&#19968;&#21322;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#27169;&#20223;&#20854;&#20804;&#24351;&#32593;&#32476;&#25152;&#20570;&#30340;&#33258;&#20449;&#30340;&#38169;&#35823;&#20998;&#31867;&#12290;XRM &#25552;&#20379;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2307.15017</link><description>&lt;p&gt;
&#31169;&#26377;&#32852;&#37030;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#37319;&#26679;&#21311;&#21517;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Samplable Anonymous Aggregation for Private Federated Data Analysis. (arXiv:2307.15017v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#35299;&#20915;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#31169;&#26377;&#32479;&#35745;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#35774;&#35745;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27599;&#20010;&#35774;&#22791;&#25345;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#35774;&#35745;&#21487;&#25193;&#23637;&#30340;&#31169;&#26377;&#32479;&#35745;&#21327;&#35758;&#21644;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#35821;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#24378;&#20449;&#20219;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38544;&#31169;&#36134;&#21153;&#65292;&#25509;&#36817;&#20110;&#38598;&#20013;&#35774;&#32622;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#35813;&#21407;&#35821;&#30340;&#31995;&#32479;&#26550;&#26500;&#65292;&#24182;&#23545;&#35813;&#31995;&#32479;&#36827;&#34892;&#20102;&#23433;&#20840;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.13990</link><description>&lt;p&gt;
&#20132;&#21449;&#39564;&#35777;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#65306;&#19968;&#31181;&#32479;&#35745;&#26041;&#27861;&#26469;&#20272;&#35745;&#26631;&#31614;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation. (arXiv:2306.13990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;(Repeated Cross-Validation)&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22122;&#22768;&#30452;&#26041;&#22270;&#24182;&#25552;&#20986;&#19977;&#31181;&#22522;&#20110;&#35813;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#26631;&#31614;&#22122;&#22768;&#24182;&#28165;&#29702;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#22312;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#37492;&#23450;&#21644;&#28040;&#38500;&#26631;&#31614;&#22122;&#22768;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#22823;&#24133;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26631;&#31614;&#22122;&#22768;&#26816;&#27979;&#26041;&#27861;&#37117;&#26159;&#20026;&#20998;&#31867;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#32780;&#22522;&#20110;&#32467;&#26524;&#39044;&#27979;&#20998;&#26512;&#30340;&#25968;&#25454;&#28165;&#29702;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#21463;&#21040;&#20132;&#21449;&#39564;&#35777;&#20013;&#19981;&#21516;&#25240;&#30340;&#24615;&#33021;&#27874;&#21160;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26631;&#31614;&#22122;&#22768;&#20272;&#35745;&#30340;&#37325;&#22797;&#20132;&#21449;&#39564;&#35777;&#65288;ReCoV&#65289;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;ReCoV&#36890;&#36807;&#35760;&#24405;&#27599;&#20010;&#26368;&#24046;&#34920;&#29616;&#25240;&#20013;&#30340;&#26679;&#26412;ID&#26469;&#26500;&#24314;&#19968;&#20010;&#22122;&#22768;&#30452;&#26041;&#22270;&#65292;&#20197;&#27492;&#26469;&#25490;&#21517;&#26679;&#26412;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#22122;&#22768;&#30452;&#26041;&#22270;&#26469;&#37492;&#21035;&#22024;&#26434;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ReCoV&#22312;&#20998;&#31867;&#20219;&#21153;&#22522;&#20934;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26631;&#31614;&#28165;&#29702;&#31639;&#27861;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;
&lt;/p&gt;
&lt;p&gt;
Label noise is prevalent in machine learning datasets. It is crucial to identify and remove label noise because models trained on noisy data can have substantially reduced accuracy and generalizability. Most existing label noise detection approaches are designed for classification tasks, and data cleaning for outcome prediction analysis is relatively unexplored. Inspired by the fluctuations in performance across different folds in cross-validation, we propose Repeated Cross-Validations for label noise estimation (ReCoV) to address this gap. ReCoV constructs a noise histogram that ranks the noise level of samples based on a large number of cross-validations by recording sample IDs in each worst-performing fold. We further propose three approaches for identifying noisy samples based on noise histograms to address increasingly complex noise distributions. We show that ReCoV outperforms state-of-the-art algorithms for label cleaning in a classification task benchmark. More importantly, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#31639;&#27861;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#24207;&#21015;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;SSA&#25351;&#26631;&#26469;&#35780;&#20272;&#35813;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11986</link><description>&lt;p&gt;
&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#35299;&#20915;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing the Rank Degeneration in Sequential Recommendation via Singular Spectrum Smoothing. (arXiv:2306.11986v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22855;&#24322;&#35889;&#24179;&#28369;&#31639;&#27861;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#24207;&#21015;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;SSA&#25351;&#26631;&#26469;&#35780;&#20272;&#35813;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;&#21160;&#24577;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#24182;&#29983;&#25104;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#12290;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#36890;&#24120;&#26159;&#36890;&#36807;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#20043;&#38388;&#30340;&#20146;&#21644;&#24230;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#37117;&#20250;&#36973;&#21463;&#25490;&#21517;&#38477;&#32423;&#38382;&#39064;&#12290;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#20005;&#37325;&#25439;&#23475;&#20102;&#39034;&#24207;&#25512;&#33616;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#29702;&#35770;&#36830;&#25509;&#24207;&#21015;&#34920;&#31034;&#38477;&#32423;&#38382;&#39064;&#19982;&#39033;&#30446;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#24555;&#36895;&#22855;&#24322;&#20540;&#34928;&#20943;&#29616;&#35937;&#19982;&#36716;&#25442;&#22120;&#24207;&#21015;&#36755;&#20986;&#21644;&#39033;&#30446;&#23884;&#20837;&#20013;&#30340;&#25490;&#21517;&#25240;&#21472;&#38382;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22855;&#24322;&#20540;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;SSA&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#21516;&#26102;&#32531;&#35299;&#39034;&#24207;&#25512;&#33616;&#20013;&#30340;&#24207;&#21015;&#21644;&#39033;&#30446;&#34920;&#31034;&#25490;&#21517;&#36864;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommendation (SR) investigates the dynamic user preferences modeling and generates the next-item prediction. The next item preference is typically generated by the affinity between the sequence and item representations. However, both sequence and item representations suffer from the rank degeneration issue due to the data sparsity problem. The rank degeneration issue significantly impairs the representations for SR. This motivates us to measure how severe is the rank degeneration issue and alleviate the sequence and item representation rank degeneration issues simultaneously for SR.  In this work, we theoretically connect the sequence representation degeneration issue with the item rank degeneration, particularly for short sequences and cold items. We also identify the connection between the fast singular value decay phenomenon and the rank collapse issue in transformer sequence output and item embeddings. We propose the area under the singular value curve metric to evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.15759</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Latent Diffusion Models. (arXiv:2305.15759v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#36716;&#21464;&#20026;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#26356;&#39640;&#25928;&#24555;&#36895;&#30340;DMs&#35757;&#32451;&#65292;&#24182;&#19988;&#36890;&#36807;&#21482;&#24494;&#35843;&#27880;&#24847;&#21147;&#27169;&#22359;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;(DMs)&#34987;&#24191;&#27867;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30452;&#25509;&#22312;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;DMs&#30340;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#36825;&#23548;&#33268;&#30001;&#20110;&#24046;&#20998;&#38544;&#31169;&#30340;&#21487;&#32452;&#21512;&#24615;&#23646;&#24615;&#65292;&#22823;&#37327;&#22122;&#38899;&#27880;&#20837;&#21040;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDMs)&#12290;LDMs&#20351;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#23558;&#39640;&#32500;&#20687;&#32032;&#31354;&#38388;&#20943;&#23569;&#21040;&#26356;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20351;&#35757;&#32451;DMs&#26356;&#21152;&#39640;&#25928;&#21644;&#24555;&#36895;&#12290;&#19982;[Ghalebikesabi&#31561;&#20154;&#65292;2023]&#39044;&#20808;&#29992;&#20844;&#20849;&#25968;&#25454;&#39044;&#35757;&#32451;DMs&#65292;&#28982;&#21518;&#20877;&#29992;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;&#25105;&#20204;&#20165;&#24494;&#35843;LDMs&#20013;&#19981;&#21516;&#23618;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#33719;&#24471;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#65292;&#30456;&#23545;&#20110;&#25972;&#20010;DM&#24494;&#35843;&#65292;&#21487;&#20943;&#23569;&#22823;&#32422;96%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) are widely used for generating high-quality image datasets. However, since they operate directly in the high-dimensional pixel space, optimization of DMs is computationally expensive, requiring long training times. This contributes to large amounts of noise being injected into the differentially private learning process, due to the composability property of differential privacy. To address this challenge, we propose training Latent Diffusion Models (LDMs) with differential privacy. LDMs use powerful pre-trained autoencoders to reduce the high-dimensional pixel space to a much lower-dimensional latent space, making training DMs more efficient and fast. Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data then fine-tunes them with private data, we fine-tune only the attention modules of LDMs at varying layers with privacy-sensitive data, reducing the number of trainable parameters by approximately 96% compared to fine-tuning the entire DM. We te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14543</link><description>&lt;p&gt;
DF2M&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#29992;&#20110;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28145;&#24230;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DF2M: An Explainable Deep Bayesian Nonparametric Model for High-Dimensional Functional Time Series. (arXiv:2305.14543v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DF2M&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#25429;&#25417;&#26102;&#38388;&#21160;&#24577;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Deep Functional Factor Model(DF2M)&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#20989;&#25968;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#27169;&#22411;&#12290;DF2M&#21033;&#29992;&#21360;&#24230;&#33258;&#21161;&#39184;&#36807;&#31243;&#21644;&#28145;&#24230;&#26680;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39640;&#26031;&#36807;&#31243;&#26469;&#25429;&#25417;&#38750;&#39532;&#23572;&#31185;&#22827;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#21160;&#24577;&#12290;&#19982;&#35768;&#22810;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;DF2M&#36890;&#36807;&#26500;&#24314;&#22240;&#23376;&#27169;&#22411;&#24182;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34701;&#20837;&#26680;&#20989;&#25968;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#21464;&#20998;&#25512;&#29702;&#31639;&#27861;&#26469;&#25512;&#26029;DF2M&#12290;&#22235;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;DF2M&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21331;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Deep Functional Factor Model (DF2M), a Bayesian nonparametric model for analyzing high-dimensional functional time series. The DF2M makes use of the Indian Buffet Process and the multi-task Gaussian Process with a deep kernel function to capture non-Markovian and nonlinear temporal dynamics. Unlike many black-box deep learning models, the DF2M provides an explainable way to use neural networks by constructing a factor model and incorporating deep neural networks within the kernel function. Additionally, we develop a computationally efficient variational inference algorithm for inferring the DF2M. Empirical results from four real-world datasets demonstrate that the DF2M offers better explainability and superior predictive accuracy compared to conventional deep learning models for high-dimensional functional time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;</title><link>http://arxiv.org/abs/2303.15845</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21487;&#35777;&#26126;&#20855;&#26377;&#20581;&#22766;&#24615;:&#38134;&#28246;&#21453;&#38382;&#39064;&#30340;&#36880;&#28857;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems. (arXiv:2303.15845v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#23545;&#21333;&#20010;&#35266;&#27979;&#32467;&#26524;&#26377;&#20581;&#22766;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#25104;&#20026;&#37319;&#26679;&#38134;&#28246;&#21453;&#38382;&#39064;&#21518;&#39564;&#27010;&#29575;&#30340;&#24378;&#22823;&#24037;&#20855;. &#32463;&#20856;&#30340;&#36125;&#21494;&#26031;&#25991;&#29486;&#24050;&#32463;&#30693;&#36947;&#21518;&#39564;&#27979;&#24230;&#23545;&#20808;&#21069;&#27979;&#24230;&#21644;&#36127;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;(&#21253;&#25324;&#35266;&#23519;&#30340;&#25200;&#21160;)&#38750;&#24120; robust. &#20294;&#26159;, &#23601;&#25105;&#20204;&#25152;&#30693;, &#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#36824;&#27809;&#34987;&#30740;&#31350;&#36807;. &#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#36866;&#24403;&#23398;&#20064;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20010;&#35266;&#27979;&#20540;&#26041;&#38754;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#32467;&#26524;.
&lt;/p&gt;
&lt;p&gt;
Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2211.14839</link><description>&lt;p&gt;
Waveflow&#65306;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24179;&#28369;&#24402;&#19968;&#21270;&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20197;&#36153;&#31859;&#27874;&#20989;&#25968;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions. (arXiv:2211.14839v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#12289;&#23558;&#36793;&#30028;&#26465;&#20214;&#24212;&#29992;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#25216;&#26415;&#12289;&#24341;&#20837;&#20102;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340; I-Spline &#21452;&#23556;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#21019;&#24314;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#36153;&#31859;&#27874;&#20989;&#25968; Ansatz&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#30340;&#21019;&#26032;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#24402;&#19968;&#21270;&#27969;&#25299;&#25169;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#24402;&#19968;&#21270;&#27969;&#19978;&#24378;&#21046;&#26045;&#21152;&#29305;&#23450;&#31867;&#21035;&#36793;&#30028;&#26465;&#20214;&#30340;&#25216;&#26415;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; I-Spline &#21452;&#23556;&#65292;&#23427;&#20687;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#26679;&#21033;&#29992;&#20102;&#26679;&#26465;&#26354;&#32447;&#65292;&#20294;&#19982;&#36825;&#20123;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21487;&#20197;&#34987;&#21046;&#20316;&#25104;&#20219;&#24847;&#27425;&#21487;&#24494;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#21019;&#24314;&#20102; Waveflow&#65292;&#19968;&#31181;&#22522;&#20110;&#24402;&#19968;&#21270;&#27969;&#30340;&#19968;&#32500;&#22810;&#31890;&#23376;&#36153;&#31859;&#27874;&#20989;&#25968;&#30340; Ansatz&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#33945;&#29305;&#21345;&#32599;&#39640;&#25928;&#22320;&#35757;&#32451;&#65292;&#26080;&#38656; MCMC &#25110;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#12290;&#20026;&#20102;&#24378;&#21046;&#36153;&#31859;&#27874;&#20989;&#25968;&#25152;&#38656;&#30340;&#21453;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#20165;&#22312;&#25490;&#21015;&#32676;&#30340;&#22522;&#26412;&#22495;&#19978;&#35757;&#32451;&#24402;&#19968;&#21270;&#27969;&#65292;&#36825;&#26377;&#25928;&#22320;&#23558;&#20854;&#20943;&#23569;&#20026;&#19968;&#20010;&#36793;&#30028;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce four main novelties: First, we present a new way of handling the topology problem of normalizing flows. Second, we describe a technique to enforce certain classes of boundary conditions onto normalizing flows. Third, we introduce the I-Spline bijection, which, similar to previous work, leverages splines but, in contrast to those works, can be made arbitrarily often differentiable. And finally, we use these techniques to create Waveflow, an Ansatz for the one-space-dimensional multi-particle fermionic wave functions in real space based on normalizing flows, that can be efficiently trained with Variational Quantum Monte Carlo without the need for MCMC nor estimation of a normalization constant. To enforce the necessary anti-symmetry of fermionic wave functions, we train the normalizing flow only on the fundamental domain of the permutation group, which effectively reduces it to a boundary value problem.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2211.09619</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09619
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#19982;&#21487;&#24494;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#21644;&#20984;&#26494;&#24347;&#25216;&#26415;&#24471;&#21040;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#26368;&#20339;&#21644;&#40065;&#26834;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#19982;&#20854;&#20182;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#26080;&#27861;&#39044;&#27979;&#25200;&#21160;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
&lt;/p&gt;</description></item></channel></rss>