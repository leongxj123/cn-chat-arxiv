<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;</title><link>https://rss.arxiv.org/abs/2312.02783</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Graphs: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24212;&#29992;&#24773;&#20917;&#65292;&#24182;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT4&#21644;LLaMA&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#25991;&#26412;&#32534;&#30721;/&#35299;&#30721;&#33021;&#21147;&#21644;&#26032;&#21457;&#29616;&#30340;&#32039;&#24613;&#33021;&#21147;&#65288;&#20363;&#22914;&#25512;&#29702;&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#34429;&#28982;LLMs&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#32431;&#25991;&#26412;&#65292;&#20294;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#19982;&#22270;&#24418;&#24418;&#24335;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#30456;&#20851;&#32852;&#65288;&#20363;&#22914;&#23398;&#26415;&#32593;&#32476;&#21644;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#65289;&#65292;&#25110;&#32773;&#22270;&#24418;&#25968;&#25454;&#19982;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#37197;&#23545;&#65288;&#20363;&#22914;&#24102;&#26377;&#25551;&#36848;&#30340;&#20998;&#23376;&#65289;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#20854;&#22522;&#20110;&#32431;&#25991;&#26412;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#27492;&#31867;&#33021;&#21147;&#26159;&#21542;&#21487;&#20197;&#25512;&#24191;&#21040;&#22270;&#24418;&#19978;&#65288;&#21363;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29702;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#22270;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#22330;&#26223;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#24635;&#32467;&#20102;&#37319;&#29992;LLMs&#22312;&#22270;&#24418;&#19978;&#30340;&#28508;&#22312;&#22330;&#26223;&#65292;&#20998;&#20026;&#32431;&#22270;&#24418;&#12289;&#25991;&#26412;&#23646;&#24615;&#22270;&#24418;&#21644;&#25991;&#26412;&#37197;&#23545;&#22270;&#24418;&#19977;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-pa
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;</title><link>https://arxiv.org/abs/2404.00027</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#65306;&#25506;&#35752;&#25152;&#26377;&#26435;&#24863;&#21644;&#25512;&#29702;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00027
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20889;&#20316;&#21161;&#25163;&#24341;&#21457;&#30340;&#20889;&#20316;&#25152;&#26377;&#26435;&#24863;&#21644;&#20316;&#32773;&#36523;&#20221;&#35748;&#30693;&#20043;&#38388;&#30340;&#24515;&#29702;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20889;&#20316;&#20013;&#30340;&#25152;&#26377;&#26435;&#24863;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#24605;&#24819;&#12289;&#26102;&#38388;&#21644;&#36129;&#29486;&#30340;&#25237;&#20837;&#65292;&#23548;&#33268;&#23545;&#20135;&#20986;&#29289;&#30340;&#20381;&#24651;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20889;&#20316;&#21161;&#25163;&#24341;&#20837;&#20102;&#19968;&#31181;&#24515;&#29702;&#22256;&#22659;&#65292;&#22240;&#20026;&#19968;&#20123;&#20869;&#23481;&#24182;&#38750;&#30452;&#25509;&#25105;&#20204;&#30340;&#21019;&#20316;&#12290;&#25105;&#20204;&#24448;&#24448;&#26356;&#20542;&#21521;&#20110;&#22312;&#21019;&#36896;&#24615;&#20219;&#21153;&#20013;&#26356;&#22810;&#22320;&#24402;&#21151;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23613;&#31649;&#23427;&#20204;&#23545;&#25152;&#26377;&#20219;&#21153;&#37117;&#26159;&#24179;&#31561;&#30340;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#25105;&#20204;&#21487;&#33021;&#19981;&#20250;&#23436;&#20840;&#22768;&#31216;&#23545;&#30001;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#25317;&#26377;&#25152;&#26377;&#26435;&#65292;&#20294;&#21364;&#33258;&#30001;&#22320;&#22768;&#31216;&#20316;&#32773;&#36523;&#20221;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31616;&#30701;&#35843;&#26597;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20102;&#35299;&#28508;&#22312;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#20154;&#26426;&#20132;&#20114;&#22312;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#24182;&#25913;&#36827;&#20889;&#20316;&#36741;&#21161;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00027v1 Announce Type: cross  Abstract: Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00026</link><description>&lt;p&gt;
&#22696;&#27700;&#19982;&#20010;&#24615;&#65306;&#22312;LLMs&#26102;&#20195;&#22609;&#36896;&#20010;&#24615;&#21270;&#21465;&#20107;
&lt;/p&gt;
&lt;p&gt;
Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00026
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#20204;&#26085;&#30410;&#20381;&#36182;&#30340;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#23545;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#21487;&#33021;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#26088;&#22312;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#25552;&#21319;&#20889;&#20316;&#21161;&#25163;&#30340;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21644;&#20010;&#24615;&#21270;&#26500;&#25104;&#20102;&#20351;&#27599;&#20010;&#20316;&#23478;&#29420;&#29305;&#24182;&#24433;&#21709;&#20854;&#25991;&#23383;&#20197;&#26377;&#25928;&#21560;&#24341;&#35835;&#32773;&#21516;&#26102;&#20256;&#36798;&#30495;&#23454;&#24615;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26085;&#30410;&#20381;&#36182;&#22522;&#20110;LLM&#30340;&#20889;&#20316;&#21161;&#25163;&#21487;&#33021;&#20250;&#21361;&#21450;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#20010;&#24615;&#12290;&#25105;&#20204;&#32463;&#24120;&#24573;&#35270;&#36825;&#19968;&#36235;&#21183;&#23545;&#25105;&#20204;&#30340;&#21019;&#36896;&#21147;&#21644;&#29420;&#29305;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#36896;&#25104;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36827;&#34892;&#31616;&#35201;&#35843;&#26597;&#25506;&#32034;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#27010;&#24565;&#65292;&#20197;&#21450;&#23581;&#35797;&#29702;&#35299;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#32467;&#21512;&#20197;&#24448;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26469;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#25913;&#36827;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#21644;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#20010;&#24615;&#21270;&#20889;&#20316;&#21161;&#25163;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00026v1 Announce Type: cross  Abstract: Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.
&lt;/p&gt;</description></item><item><title>DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19863</link><description>&lt;p&gt;
DeNetDM: &#36890;&#36807;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#26469;&#28040;&#38500;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DeNetDM: Debiasing by Network Depth Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19863
&lt;/p&gt;
&lt;p&gt;
DeNetDM &#26159;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#28145;&#24230;&#35843;&#21046;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#19987;&#23478;&#20056;&#31215;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#22312;&#21019;&#24314;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#21518;&#65292;&#23558;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#21435;&#20559;&#35265;&#27169;&#22411;&#65292;&#30456;&#27604;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21462;&#24471;&#26356;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#31070;&#32463;&#32593;&#32476;&#22312;&#20559;&#35265;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#26080;&#24847;&#38388;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#30340;&#26041;&#27861;&#36890;&#24120;&#21253;&#25324;&#21033;&#29992;&#20559;&#35265;&#27880;&#37322;&#12289;&#26681;&#25454;&#20266;&#20559;&#35265;&#26631;&#31614;&#36827;&#34892;&#21152;&#26435;&#37325;&#12289;&#25110;&#36890;&#36807;&#22686;&#24378;&#25216;&#26415;&#22686;&#21152;&#20559;&#35265;&#20914;&#31361;&#25968;&#25454;&#28857;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeNetDM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35266;&#23519;&#32467;&#26524;&#30340;&#26032;&#22411;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#20248;&#20808;&#23398;&#20064;&#26680;&#24515;&#23646;&#24615;&#65292;&#32780;&#26356;&#28145;&#23618;&#27425;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#33719;&#21462;&#19981;&#21516;&#20449;&#24687;&#26102;&#24378;&#35843;&#20559;&#35265;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#19987;&#23478;&#20056;&#31215;&#20013;&#25512;&#23548;&#20986;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#21019;&#24314;&#20102;&#28145;&#27973;&#26550;&#26500;&#30340;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#20998;&#25903;&#65292;&#28982;&#21518;&#29992;&#30693;&#35782;&#25552;&#28860;&#20135;&#29983;&#30446;&#26631;&#30340;&#21435;&#20559;&#35265;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19863v1 Announce Type: new  Abstract: When neural networks are trained on biased datasets, they tend to inadvertently learn spurious correlations, leading to challenges in achieving strong generalization and robustness. Current approaches to address such biases typically involve utilizing bias annotations, reweighting based on pseudo-bias labels, or enhancing diversity within bias-conflicting data points through augmentation techniques. We introduce DeNetDM, a novel debiasing method based on the observation that shallow neural networks prioritize learning core attributes, while deeper ones emphasize biases when tasked with acquiring distinct information. Using a training paradigm derived from Product of Experts, we create both biased and debiased branches with deep and shallow architectures and then distill knowledge to produce the target debiased model. Extensive experiments and analyses demonstrate that our approach outperforms current debiasing techniques, achieving a not
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17916</link><description>&lt;p&gt;
CMP&#65306;&#20855;&#26377;&#22810;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CMP: Cooperative Motion Prediction with Multi-Agent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMP&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#21512;&#20316;&#24863;&#30693;&#21644;&#36816;&#21160;&#39044;&#27979;&#27169;&#22359;&#20849;&#20139;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#21457;&#23637;&#21644;&#36710;&#32852;&#32593;&#65288;V2X&#65289;&#36890;&#20449;&#30340;&#25104;&#29087;&#65292;&#21512;&#20316;&#36830;&#25509;&#30340;&#33258;&#21160;&#21270;&#36710;&#36742;&#65288;CAVs&#65289;&#30340;&#21151;&#33021;&#21464;&#24471;&#21487;&#33021;&#12290;&#26412;&#25991;&#22522;&#20110;&#21512;&#20316;&#24863;&#30693;&#65292;&#25506;&#35752;&#20102;&#21512;&#20316;&#36816;&#21160;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CMP&#20197;LiDAR&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#22686;&#24378;&#36319;&#36394;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#19982;&#36807;&#21435;&#19987;&#27880;&#20110;&#21512;&#20316;&#24863;&#30693;&#25110;&#36816;&#21160;&#39044;&#27979;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#20010;&#35299;&#20915;CAVs&#22312;&#24863;&#30693;&#21644;&#39044;&#27979;&#27169;&#22359;&#20013;&#20849;&#20139;&#20449;&#24687;&#30340;&#32479;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20013;&#36824;&#34701;&#20837;&#20102;&#33021;&#22815;&#23481;&#24525;&#29616;&#23454;V2X&#24102;&#23485;&#38480;&#21046;&#21644;&#20256;&#36755;&#24310;&#36831;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#22788;&#29702;&#24222;&#22823;&#30340;&#24863;&#30693;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#39044;&#27979;&#32858;&#21512;&#27169;&#22359;&#65292;&#32479;&#19968;&#20102;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13784</link><description>&lt;p&gt;
&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;: &#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21487;&#37325;&#29616;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#29992;&#24615;&#30340;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13784
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#25490;&#21517;&#20998;&#31867;&#31995;&#32479;&#65292;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#23436;&#25972;&#24615;&#12289;&#24320;&#25918;&#24615;&#20197;&#21450;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#21407;&#21017;&#65292;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#35782;&#21035;&#27169;&#22411;&#30340;&#36879;&#26126;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#20854;&#21830;&#19994;&#21270;&#24341;&#21457;&#20102;&#20851;&#20110;&#36879;&#26126;&#24230;&#12289;&#21487;&#37325;&#29616;&#24615;&#12289;&#20559;&#35265;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#35768;&#22810;"&#24320;&#28304;"&#30340;GAI&#27169;&#22411;&#32570;&#20047;&#23436;&#25972;&#29702;&#35299;&#21644;&#20877;&#29616;&#25152;&#24517;&#38656;&#30340;&#32452;&#20214;&#65292;&#19968;&#20123;&#37319;&#29992;&#38480;&#21046;&#24615;&#35768;&#21487;&#35777;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;"&#24320;&#28304;&#27927;&#30333;"&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#25918;&#26694;&#26550;&#65288;MOF&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26681;&#25454;&#23436;&#25972;&#24615;&#21644;&#24320;&#25918;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25490;&#21517;&#20998;&#31867;&#30340;&#31995;&#32479;&#65292;&#36981;&#24490;&#24320;&#25918;&#31185;&#23398;&#12289;&#24320;&#28304;&#12289;&#24320;&#25918;&#25968;&#25454;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#21407;&#21017;&#12290;MOF&#35201;&#27714;&#27169;&#22411;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#29305;&#23450;&#32452;&#20214;&#34987;&#21253;&#21547;&#24182;&#26681;&#25454;&#36866;&#24403;&#30340;&#24320;&#25918;&#35768;&#21487;&#35777;&#21457;&#24067;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#38450;&#27490;&#23459;&#31216;&#33258;&#24049;&#26159;&#24320;&#25918;&#30340;&#27169;&#22411;&#34987;&#35823;&#35299;&#65292;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#20197;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#21457;&#24067;&#25152;&#26377;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#24110;&#21161;&#20844;&#21496;&#12289;&#23398;&#26415;&#30028;&#21644;&#29233;&#22909;&#32773;&#35782;&#21035;&#21487;&#20197;&#23433;&#20840;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13784v1 Announce Type: new  Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adop
&lt;/p&gt;</description></item><item><title>PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13681</link><description>&lt;p&gt;
PARAMANU-AYN&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#12289;&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13681
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PARAMANU-AYN&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#22522;&#20110;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#25991;&#20214;&#12289;&#21360;&#24230;&#23466;&#27861;&#21644;&#21360;&#24230;&#21009;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26159;&#20174;&#22836;&#24320;&#22987;&#22312;&#19978;&#19979;&#25991;&#22823;&#23567;&#20026;8192&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#22312;&#22256;&#24785;&#24230;&#25351;&#26631;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27861;&#24459;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#21253;&#25324;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65288;&#22914;&#27861;&#24459;&#25512;&#29702;&#12289;&#21028;&#20915;&#35299;&#37322;&#12289;&#27861;&#24459;&#26465;&#27454;&#29983;&#25104;&#12289;&#27861;&#24459;&#33609;&#25311;&#12289;&#27861;&#24459;&#21512;&#21516;&#33609;&#25311;&#12289;&#26696;&#20214;&#25688;&#35201;&#12289;&#23466;&#27861;&#38382;&#39064;&#22238;&#31572;&#31561;&#65289;&#30340;10,763&#26465;&#25351;&#20196;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;GPT-3.5-Turbo&#23545;&#38754;&#21521;&#25351;&#20196;&#30340;&#27169;&#22411;&#30340;&#25552;&#31034;&#21709;&#24212;&#36827;&#34892;&#20102;&#22312;10&#20998;&#21046;&#24230;&#19978;&#30340;&#28165;&#26224;&#24230;&#12289;&#30456;&#20851;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27861;&#24459;&#25512;&#29702;&#25351;&#26631;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;CPU&#19978;&#36816;&#34892;&#65292;&#24182;&#23454;&#29616;&#27599;&#31186;42.46&#20010;&#20196;&#29260;&#30340;CPU&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;</title><link>https://arxiv.org/abs/2403.07179</link><description>&lt;p&gt;
3M-Diffusion&#65306;&#29992;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#20998;&#23376;&#22270;&#30340;&#28508;&#22312;&#22810;&#27169;&#24577;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
3M-Diffusion: Latent Multi-Modal Diffusion for Text-Guided Generation of Molecular Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;3M-Diffusion&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#12289;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#26448;&#26009;&#35774;&#35745;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#20998;&#23376;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#20998;&#23376;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#19982;&#25991;&#26412;&#25551;&#36848;&#31934;&#30830;&#21305;&#37197;&#30340;&#20998;&#23376;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#22810;&#26679;&#21270;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#26159;&#26032;&#39062;&#30340;&#20998;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22270;&#29983;&#25104;&#26041;&#27861;3M-Diffusion&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07179v1 Announce Type: cross  Abstract: Generating molecules with desired properties is a critical task with broad applications in drug discovery and materials design. Inspired by recent advances in large language models, there is a growing interest in using natural language descriptions of molecules to generate molecules with the desired properties. Most existing methods focus on generating molecules that precisely match the text description. However, practical applications call for methods that generate diverse, and ideally novel, molecules with the desired properties. We propose 3M-Diffusion, a novel multi-modal molecular graph generation method, to address this challenge. 3M-Diffusion first encodes molecular graphs into a graph latent space aligned with text descriptions. It then reconstructs the molecular structure and atomic attributes based on the given text descriptions using the molecule decoder. It then learns a probabilistic mapping from the text space to the late
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.06903</link><description>&lt;p&gt;
&#20855;&#26377;&#36866;&#24230;&#36755;&#20837;&#32500;&#24230;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting in leaky ReLU networks with moderate input dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06903
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#25506;&#35752;&#20102;&#19968;&#20010;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#32654;&#22320;&#25311;&#21512;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20108;&#23618;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38024;&#23545;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#21516;&#20449;&#21495;&#21644;&#19968;&#20010;&#38543;&#26426;&#22122;&#22768;&#25104;&#20998;&#30340;&#24635;&#21644;&#65292;&#36825;&#20004;&#32773;&#30456;&#20114;&#27491;&#20132;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#65292;&#23548;&#33268;&#20102;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#65288;&#26377;&#23475;&#65289;&#36807;&#25311;&#21512;&#65306;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;SNR&#24456;&#39640;&#65292;&#21017;&#21457;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#30456;&#21453;&#65292;&#22914;&#26524;SNR&#24456;&#20302;&#65292;&#21017;&#21457;&#29983;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23558;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#24402;&#22240;&#20110;&#19968;&#20010;&#36817;&#20284;&#36793;&#30028;&#26368;&#22823;&#21270;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38128;&#38142;&#25439;&#22833;&#19979;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#28385;&#36275;&#36825;&#19968;&#24615;&#36136;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;nea
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.05754</link><description>&lt;p&gt;
&#27169;&#24335;&#35782;&#21035;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;ResNet&#21644;DenseNet&#21450;&#20854;&#23436;&#25972;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Hybrid Quantum-inspired Resnet and Densenet for Pattern Recognition with Completeness Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05754
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24403;&#20170;&#25968;&#23383;&#25216;&#26415;&#30340;&#25509;&#36817;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#32321;&#33635;&#30340;&#22522;&#30784;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19981;&#26029;&#21457;&#23637;&#30340;&#31038;&#20250;&#38656;&#27714;&#27491;&#22312;&#24378;&#35843;&#26367;&#20195;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#21518;&#25705;&#23572;&#26102;&#20195;&#30340;&#26469;&#20020;&#25512;&#21160;&#20102;&#20855;&#26377;&#21331;&#36234;&#28508;&#21147;&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30446;&#21069;&#26032;&#26087;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#27604;&#36739;&#20013;&#23384;&#22312;&#21547;&#31946;&#25351;&#26631;&#65292;&#22240;&#27492;&#19968;&#22871;&#26126;&#30830;&#30340;&#35780;&#20272;&#31995;&#32479;&#19982;&#35814;&#32454;&#30340;&#25351;&#26631;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#20840;&#38754;&#22320;&#25913;&#36827;&#21644;&#35780;&#20272;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#39044;&#27979;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26681;&#26893;&#20110;&#27531;&#24046;&#36830;&#25509;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#28151;&#21512;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05754v1 Announce Type: new  Abstract: With the contemporary digital technology approaching, deep neural networks are emerging as the foundational algorithm of the artificial intelligence boom. Whereas, the evolving social demands have been emphasizing the necessity of novel methodologies to substitute traditional neural networks. Concurrently, the advent of the post-Moore era has spurred the development of quantum-inspired neural networks with outstanding potentials at certain circumstances. Nonetheless, a definitive evaluating system with detailed metrics is tremendously vital and indispensable owing to the vague indicators in comparison between the novel and traditional deep learning models at present. Hence, to improve and evaluate the performances of the novel neural networks more comprehensively in complex and unpredictable environments, we propose two hybrid quantum-inspired neural networks which are rooted in residual and dense connections respectively for pattern rec
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04405</link><description>&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04405
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Functional Isolation Forest (FIF)&#26159;&#19968;&#31181;&#38024;&#23545;&#21151;&#33021;&#25968;&#25454;&#35774;&#35745;&#30340;&#26368;&#26032;&#19968;&#27969;&#24322;&#24120;&#26816;&#27979;(AD)&#31639;&#27861;&#12290;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#26641;&#20998;&#21306;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26354;&#32447;&#35266;&#27979;&#25237;&#24433;&#21040;&#36890;&#36807;&#32447;&#24615;&#20869;&#31215;&#32472;&#21046;&#30340;&#35789;&#20856;&#19978;&#26469;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;Signature Isolation Forest&#8221;&#65292;&#19968;&#31181;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#31614;&#21517;&#21464;&#25442;&#30340;&#26032;&#39062;AD&#31639;&#27861;&#31867;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#26469;&#28040;&#38500;FIF&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#29305;&#21035;&#38024;&#23545;FIF&#20869;&#31215;&#30340;&#32447;&#24615;&#24615;&#21644;&#35789;&#20856;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;ICU&#20013;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#21512;&#20102;LSTM&#32593;&#32476;&#21644;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#65292;&#24182;&#22312;&#23454;&#38469;&#20020;&#24202;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17917</link><description>&lt;p&gt;
ICU&#29983;&#29702;&#20449;&#21495;&#24120;&#35265;&#28508;&#22312;&#34920;&#31034;&#30340;&#21327;&#21516;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning of common latent representations in routinely collected multivariate ICU physiological signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;ICU&#20013;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#24212;&#29992;&#32467;&#21512;&#20102;LSTM&#32593;&#32476;&#21644;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#65292;&#24182;&#22312;&#23454;&#38469;&#20020;&#24202;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20013;&#65292;&#20016;&#23500;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20026;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#22686;&#24378;&#24739;&#32773;&#34920;&#22411;&#30340;&#26426;&#20250;&#12290;&#19982;&#20808;&#21069;&#19987;&#27880;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24120;&#35268;&#25910;&#38598;&#30340;&#29983;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#34920;&#22411;&#20998;&#31867;&#30340;ML&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#23558;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#19982;&#21327;&#21516;&#36807;&#28388;&#27010;&#24565;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35782;&#21035;&#24739;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#29983;&#29702;&#29366;&#24577;&#12290;&#22312;&#29992;&#20110;&#33041;&#25439;&#20260;&#24739;&#32773;&#39045;&#20869;&#39640;&#21387;&#65288;IH&#65289;&#26816;&#27979;&#30340;&#23454;&#38469;ICU&#20020;&#24202;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;0.889&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#21644;0.725&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#65288;AP&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23398;&#20064;&#29983;&#29702;&#20449;&#21495;&#30340;&#26356;&#32467;&#26500;&#21270;&#28508;&#22312;&#34920;&#31034;&#26041;&#38754;&#20248;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21033;&#29992;&#24120;&#35268;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#24739;&#32773;&#34920;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17917v1 Announce Type: new  Abstract: In Intensive Care Units (ICU), the abundance of multivariate time series presents an opportunity for machine learning (ML) to enhance patient phenotyping. In contrast to previous research focused on electronic health records (EHR), here we propose an ML approach for phenotyping using routinely collected physiological time series data. Our new algorithm integrates Long Short-Term Memory (LSTM) networks with collaborative filtering concepts to identify common physiological states across patients. Tested on real-world ICU clinical data for intracranial hypertension (IH) detection in patients with brain injury, our method achieved an area under the curve (AUC) of 0.889 and average precision (AP) of 0.725. Moreover, our algorithm outperforms autoencoders in learning more structured latent representations of the physiological signals. These findings highlight the promise of our methodology for patient phenotyping, leveraging routinely collecte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.14703</link><description>&lt;p&gt;
&#22312;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20013;&#25506;&#35752;&#26410;&#26469;&#21644;&#21382;&#21490;&#30340;&#35781;&#21650;&#22312;&#31163;&#32447;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#65292;&#20197;&#35299;&#20915;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#26041;&#27861;&#20013;&#30340;&#38271;&#24230;&#25351;&#25968;&#22686;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#22797;&#26434;&#35266;&#27979;&#30340;&#31163;&#32447;&#35780;&#20272;(OPE)&#65292;&#26088;&#22312;&#24320;&#21457;&#33021;&#22815;&#36991;&#20813;&#23545;&#26102;&#38388;&#36328;&#24230;&#25351;&#25968;&#20381;&#36182;&#30340;&#20272;&#35745;&#22120;&#12290;&#26368;&#36817;&#65292;Uehara&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20063;&#21462;&#20915;&#20110;&#26410;&#26469;&#20381;&#36182;&#20215;&#20540;&#20989;&#25968;&#30340;&#26377;&#30028;&#24615;&#20197;&#21450;&#20854;&#20182;&#30456;&#20851;&#25968;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25968;&#37327;&#21487;&#33021;&#20250;&#38543;&#30528;&#38271;&#24230;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20174;&#32780;&#25273;&#21435;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38024;&#23545;POMDP&#32467;&#26500;&#30340;&#26032;&#39062;&#35206;&#30422;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14703v1 Announce Type: cross  Abstract: We study off-policy evaluation (OPE) in partially observable environments with complex observations, with the goal of developing estimators whose guarantee avoids exponential dependence on the horizon. While such estimators exist for MDPs and POMDPs can be converted to history-based MDPs, their estimation errors depend on the state-density ratio for MDPs which becomes history ratios after conversion, an exponential object. Recently, Uehara et al. (2022) proposed future-dependent value functions as a promising framework to address this issue, where the guarantee for memoryless policies depends on the density ratio over the latent state space. However, it also depends on the boundedness of the future-dependent value function and other related quantities, which we show could be exponential-in-length and thus erasing the advantage of the method. In this paper, we discover novel coverage assumptions tailored to the structure of POMDPs, such
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;</title><link>https://arxiv.org/abs/2402.14407</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#25193;&#25955;&#36827;&#34892;&#22823;&#35268;&#27169;&#26080;&#21160;&#20316;&#35270;&#39057;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14407
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#25193;&#25955;&#32467;&#21512;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21644;&#23569;&#37327;&#26426;&#22120;&#20154;&#35270;&#39057;&#24494;&#35843;&#65292;&#23454;&#29616;&#20174;&#20154;&#31867;&#35270;&#39057;&#21040;&#26426;&#22120;&#20154;&#31574;&#30053;&#23398;&#20064;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#23436;&#25104;&#22810;&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#23454;&#20307;&#20195;&#29702;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#33258;&#32570;&#20047;&#26377;&#26631;&#35760;&#21160;&#20316;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23384;&#22312;&#22823;&#37327;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#21644;&#19982;&#29289;&#29702;&#19990;&#30028;&#20114;&#21160;&#30340;&#20154;&#31867;&#35270;&#39057;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21033;&#29992;&#32479;&#19968;&#30340;&#31163;&#25955;&#25193;&#25955;&#23558;&#20154;&#31867;&#35270;&#39057;&#19978;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#19982;&#23569;&#37327;&#26377;&#26631;&#35760;&#26426;&#22120;&#20154;&#35270;&#39057;&#19978;&#30340;&#31574;&#30053;&#24494;&#35843;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#35270;&#39057;&#21387;&#32553;&#25104;&#32479;&#19968;&#30340;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#33945;&#29256;&#26367;&#25442;&#25193;&#25955;&#31574;&#30053;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#26469;&#39044;&#27979;&#28508;&#31354;&#38388;&#20013;&#30340;&#26410;&#26469;&#35270;&#39057;&#26631;&#35760;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204; h
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14407v1 Announce Type: new  Abstract: Learning a generalist embodied agent capable of completing multiple tasks poses challenges, primarily stemming from the scarcity of action-labeled robotic datasets. In contrast, a vast amount of human videos exist, capturing intricate tasks and interactions with the physical world. Promising prospects arise for utilizing actionless human videos for pre-training and transferring the knowledge to facilitate robot policy learning through limited robot demonstrations. In this paper, we introduce a novel framework that leverages a unified discrete diffusion to combine generative pre-training on human videos and policy fine-tuning on a small number of action-labeled robot videos. We start by compressing both human and robot videos into unified video tokens. In the pre-training stage, we employ a discrete diffusion model with a mask-and-replace diffusion strategy to predict future video tokens in the latent space. In the fine-tuning stage, we h
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12817</link><description>&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65306;&#30456;&#20114;&#20316;&#29992;&#21644;&#31995;&#32479;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12817
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#23545;&#38543;&#26426;&#24615;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24573;&#30053;&#30456;&#20114;&#20316;&#29992;&#21487;&#33021;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#23398;&#20064;&#21487;&#20197;&#22312;&#26631;&#31614;&#19981;&#36275;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20063;&#23545;&#25152;&#35859;&#30340;&#38543;&#26426;&#22240;&#32032;&#65288;&#20363;&#22914;&#25968;&#25454;&#30340;&#21464;&#21270;&#39034;&#24207;&#65289;&#24341;&#20837;&#30340;&#26080;&#27861;&#25511;&#21046;&#30340;&#38543;&#26426;&#24615;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#27979;&#37327;&#21333;&#20010;&#38543;&#26426;&#22240;&#32032;&#30340;&#30495;&#23454;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#36731;&#20102;&#20854;&#20182;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#24182;&#35266;&#23519;&#20102;&#24615;&#33021;&#22312;&#22810;&#27425;&#36816;&#34892;&#20013;&#30340;&#21464;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;7&#20010;&#20195;&#34920;&#24615;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24494;&#35843;&#26041;&#27861;&#20197;&#21450;3&#20010;&#20219;&#21153;&#30340;&#20803;&#23398;&#20064;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;&#29616;&#26377;&#20316;&#21697;&#20013;&#24573;&#30053;&#38543;&#26426;&#22240;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#19981;&#19968;&#33268;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#22240;&#20026;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#38543;&#26426;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#21542;&#23450;&#20102;&#19968;&#20123;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12817v1 Announce Type: cross  Abstract: While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consis
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#22823;&#35268;&#27169;&#26723;&#26696;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.12369</link><description>&lt;p&gt;
&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;TESS&#20840;&#30011;&#24133;&#22270;&#20687;&#20809;&#21464;&#26354;&#32447;&#20013;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#22823;&#35268;&#27169;&#26723;&#26696;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12369v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25277;&#35937;&#65306;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#21208;&#27979;&#21355;&#26143;&#65288;TESS&#65289;&#20219;&#21153;&#22312;&#20854;&#20026;&#26399;&#20004;&#24180;&#30340;&#20027;&#35201;&#20219;&#21153;&#26399;&#38388;&#27979;&#37327;&#20102;&#32422;85%&#30340;&#22825;&#31354;&#20013;&#30340;&#24658;&#26143;&#20809;&#65292;&#23548;&#33268;&#25968;&#30334;&#19975;TESS 30&#20998;&#38047;&#38388;&#38548;&#20809;&#21464;&#26354;&#32447;&#29992;&#20110;&#25628;&#32034;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#12290;&#20026;&#20102;&#25628;&#32034;&#36825;&#19968;&#24040;&#22823;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#12289;&#29983;&#20135;&#24615;&#39044;&#27979;&#24615;&#65292;&#21448;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#25152;&#38656;&#20154;&#31867;&#25628;&#32034;&#24037;&#20316;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35782;&#21035;&#30701;&#21608;&#26399;&#21464;&#37327;&#12290;&#20026;&#20102;&#23545;&#32473;&#23450;&#30340;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#39044;&#27979;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#19981;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;&#26041;&#27861;&#30830;&#23450;&#30340;&#20808;&#21069;&#30446;&#26631;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#22312;&#21333;&#20010;GPU&#19978;&#23545;TESS 30&#20998;&#38047;&#38388;&#38548;&#20809;&#21464;&#26354;&#32447;&#36827;&#34892;&#25512;&#26029;&#32422;5ms&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#30340;&#26723;&#26696;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;14156&#20010;&#30001;&#25105;&#20204;&#30340;&#32593;&#32476;&#35782;&#21035;&#20986;&#30340;&#30701;&#21608;&#26399;&#21464;&#37327;&#30340;&#38598;&#21512;&#12290;&#25105;&#20204;&#35782;&#21035;&#30340;&#22823;&#22810;&#25968;&#21464;&#37327;&#23646;&#20110;&#20004;&#20010;&#31361;&#20986;&#30340;&#20154;&#32676;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12369v1 Announce Type: cross  Abstract: The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent popu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#20108;&#38454;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#19968;&#31867;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#30028;&#38480;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;</title><link>https://arxiv.org/abs/2402.08929</link><description>&lt;p&gt;
&#20108;&#38454;&#26041;&#27861;&#29992;&#20110;&#36172;&#24466;&#20248;&#21270;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Second Order Methods for Bandit Optimization and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#20108;&#38454;&#36172;&#24466;&#20984;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#19968;&#31867;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#30028;&#38480;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#33021;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bandit&#20984;&#20248;&#21270;(BCO)&#26159;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#22312;&#32447;&#20915;&#31574;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#23613;&#31649;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#30340;&#32039;&#26463;&#21518;&#26399;&#30028;&#38480;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#19978;&#20855;&#26377;&#38590;&#20197;&#24525;&#21463;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#31639;&#27861;&#21551;&#21457;&#30340;&#31616;&#21333;&#23454;&#29992;&#30340;BCO&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#20110;&#19968;&#31867;&#25105;&#20204;&#31216;&#20043;&#20026;$\kappa$-&#20984;&#30340;&#20984;&#20989;&#25968;&#23454;&#29616;&#20102;&#26368;&#20248;(&#20174;&#23618;&#38754;&#19978;&#35762;)&#30340;&#21518;&#26399;&#30028;&#38480;&#12290;&#36825;&#20010;&#31867;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#23454;&#38469;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#20108;&#27425;&#21644;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#12290;&#38500;&#20102;&#26368;&#20248;&#30340;&#21518;&#26399;&#25439;&#22833;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#26159;&#19968;&#20123;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#24212;&#29992;&#20013;&#24050;&#30693;&#30340;&#26368;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#36172;&#24466;&#36923;&#36753;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08929v1 Announce Type: new Abstract: Bandit convex optimization (BCO) is a general framework for online decision making under uncertainty. While tight regret bounds for general convex losses have been established, existing algorithms achieving these bounds have prohibitive computational costs for high dimensional data.   In this paper, we propose a simple and practical BCO algorithm inspired by the online Newton step algorithm. We show that our algorithm achieves optimal (in terms of horizon) regret bounds for a large class of convex functions that we call $\kappa$-convex. This class contains a wide range of practically relevant loss functions including linear, quadratic, and generalized linear models. In addition to optimal regret, this method is the most efficient known algorithm for several well-studied applications including bandit logistic regression.   Furthermore, we investigate the adaptation of our second-order bandit algorithm to online convex optimization with mem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08593</link><description>&lt;p&gt;
&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#65306;&#23454;&#26102;&#20174;&#20132;&#26131;&#22270;&#20013;&#25552;&#21462;&#22522;&#20110;&#23376;&#22270;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Graph Feature Preprocessor: Real-time Extraction of Subgraph-based Features from Transaction Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#21487;&#20197;&#20174;&#23454;&#26102;&#20132;&#26131;&#22270;&#20013;&#26816;&#27979;&#20856;&#22411;&#30340;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;"&#30340;&#36719;&#20214;&#24211;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#22270;&#20013;&#30340;&#20856;&#22411;&#27927;&#38065;&#21644;&#27450;&#35784;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#34987;&#29992;&#20110;&#29983;&#25104;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#65292;&#29992;&#20110;&#19979;&#28216;&#30340;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#65292;&#22914;&#27927;&#38065;&#26816;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#20016;&#23500;&#30340;&#20132;&#26131;&#29305;&#24449;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#26799;&#24230;&#25552;&#21319;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#24211;&#21033;&#29992;&#22810;&#26680;&#24182;&#34892;&#24615;&#65292;&#32500;&#25252;&#19968;&#20010;&#21160;&#24577;&#30340;&#20869;&#23384;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25366;&#25496;&#20256;&#20837;&#20132;&#26131;&#27969;&#20013;&#30340;&#23376;&#22270;&#27169;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#20197;&#27969;&#30340;&#26041;&#24335;&#25805;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#21512;&#25104;&#21453;&#27927;&#38065;&#65288;AML&#65289;&#21644;&#30495;&#23454;&#30340;&#20197;&#22826;&#22346;&#38035;&#40060;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#38750;&#27861;&#20132;&#26131;&#30340;&#27604;&#20363;&#38750;&#24120;&#23567;&#65292;&#20351;&#24471;&#23398;&#20064;&#36807;&#31243;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#32467;&#21512;&#20102;&#25105;&#20204;&#30340;&#22270;&#29305;&#24449;&#39044;&#22788;&#29702;&#22120;&#21644;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we present "Graph Feature Preprocessor", a software library for detecting typical money laundering and fraud patterns in financial transaction graphs in real time. These patterns are used to produce a rich set of transaction features for downstream machine learning training and inference tasks such as money laundering detection. We show that our enriched transaction features dramatically improve the prediction accuracy of gradient-boosting-based machine learning models. Our library exploits multicore parallelism, maintains a dynamic in-memory graph, and efficiently mines subgraph patterns in the incoming transaction stream, which enables it to be operated in a streaming manner. We evaluate our library using highly-imbalanced synthetic anti-money laundering (AML) and real-life Ethereum phishing datasets. In these datasets, the proportion of illicit transactions is very small, which makes the learning process challenging. Our solution, which combines our Graph Feature Prep
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05689</link><description>&lt;p&gt;
Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#36275;&#20197;&#20445;&#35777;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#30446;&#26631;&#30340;&#28176;&#36827;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05689
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31574;&#30053;&#31867;&#21035;&#20855;&#26377;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#25955;&#26102;&#38388;&#19979;&#30340;&#26080;&#38480;&#26399;&#24179;&#22343;&#22238;&#25253;&#30340;&#22909;&#36716;&#32966;&#20882;&#38505;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#31867;&#21035;&#65292;&#26088;&#22312;&#23558;&#36880;&#28176;&#25193;&#22823;&#30340;&#33218;&#23376;&#38598;&#21521;&#26368;&#20339;&#20998;&#24067;&#26041;&#21521;&#25512;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;N&#33218;&#38382;&#39064;&#20013;&#26159;&#28176;&#36827;&#26368;&#20248;&#30340;&#65292;&#22914;&#26524;&#21333;&#33218;&#26494;&#24347;&#38382;&#39064;&#26159;Unichain&#21644;&#38750;&#21608;&#26399;&#24615;&#30340;&#65292;&#37027;&#20040;&#23601;&#20250;&#26377;&#19968;&#20010;$O(1/\sqrt{N})$&#30340;&#26368;&#20248;&#38388;&#38553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#36825;&#20123;&#30740;&#31350;&#20391;&#37325;&#20110;&#25351;&#25968;&#25110;&#20248;&#20808;&#32423;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#20381;&#36182;&#20110;&#32479;&#19968;&#20840;&#23616;&#21560;&#24341;&#23376;&#23646;&#24615;&#65288;UGAP&#65289;&#26469;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35201;&#27714;&#36981;&#24490;&#21516;&#27493;&#20551;&#35774;&#65288;SA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).
&lt;/p&gt;</description></item><item><title>CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04620</link><description>&lt;p&gt;
CataractBot&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract Patients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04620
&lt;/p&gt;
&lt;p&gt;
CataractBot&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#30333;&#20869;&#38556;&#24739;&#32773;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#26597;&#35810;&#30693;&#35782;&#24211;&#25552;&#20379;&#21363;&#26102;&#30340;&#31572;&#26696;&#21644;&#19987;&#23478;&#39564;&#35777;&#30340;&#22238;&#22797;&#12290;&#22312;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#34892;&#19994;&#30340;&#21457;&#23637;&#65292;&#24739;&#32773;&#36234;&#26469;&#36234;&#36861;&#27714;&#26356;&#21487;&#38752;&#30340;&#20581;&#24247;&#20449;&#24687;&#65292;&#21253;&#25324;&#20182;&#20204;&#30340;&#20581;&#24247;&#29366;&#20917;&#12289;&#27835;&#30103;&#36873;&#25321;&#21644;&#28508;&#22312;&#39118;&#38505;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20449;&#24687;&#26469;&#28304;&#65292;&#20294;&#25968;&#23383;&#26102;&#20195;&#21364;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#36807;&#22810;&#19988;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#24739;&#32773;&#20027;&#35201;&#20449;&#20219;&#21307;&#29983;&#21644;&#21307;&#38498;&#24037;&#20316;&#20154;&#21592;&#65292;&#31361;&#26174;&#20102;&#19987;&#23478;&#35748;&#21487;&#30340;&#20581;&#24247;&#20449;&#24687;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#19987;&#23478;&#38754;&#20020;&#30340;&#21387;&#21147;&#23548;&#33268;&#20102;&#27807;&#36890;&#26102;&#38388;&#30340;&#20943;&#23569;&#65292;&#24433;&#21709;&#20102;&#20449;&#24687;&#30340;&#20849;&#20139;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CataractBot&#65292;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#19987;&#23478;&#36741;&#21161;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#19982;&#21360;&#24230;&#19968;&#23478;&#19977;&#32423;&#30524;&#31185;&#21307;&#38498;&#21512;&#20316;&#24320;&#21457;&#30340;CataractBot&#36890;&#36807;&#26597;&#35810;&#31574;&#21010;&#30340;&#30693;&#35782;&#24211;&#65292;&#21363;&#26102;&#22238;&#31572;&#30333;&#20869;&#38556;&#25163;&#26415;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#24322;&#27493;&#25552;&#20379;&#19987;&#23478;&#39564;&#35777;&#30340;&#31572;&#22797;&#12290;CataractBot&#20855;&#22791;&#22810;&#27169;&#24335;&#25903;&#25345;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#22312;&#19982;49&#21517;&#21442;&#19982;&#32773;&#30340;&#23454;&#22320;&#37096;&#32626;&#30740;&#31350;&#20013;&#65292;CataractBot&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#25152;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
The healthcare landscape is evolving, with patients seeking more reliable information about their health conditions, treatment options, and potential risks. Despite the abundance of information sources, the digital age overwhelms individuals with excess, often inaccurate information. Patients primarily trust doctors and hospital staff, highlighting the need for expert-endorsed health information. However, the pressure on experts has led to reduced communication time, impacting information sharing. To address this gap, we propose CataractBot, an experts-in-the-loop chatbot powered by large language models (LLMs). Developed in collaboration with a tertiary eye hospital in India, CataractBot answers cataract surgery related questions instantly by querying a curated knowledge base, and provides expert-verified responses asynchronously. CataractBot features multimodal support and multilingual capabilities. In an in-the-wild deployment study with 49 participants, CataractBot proved valuable,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04051</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ainsworth&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;&#26435;&#37325;&#21305;&#37197;&#65288;WM&#65289;&#26469;&#26368;&#23567;&#21270;&#25490;&#21015;&#25628;&#32034;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;$L_2$&#36317;&#31163;&#26377;&#25928;&#22320;&#35782;&#21035;&#28385;&#36275;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65288;LMC&#65289;&#30340;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31181;&#23376;&#30340;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#32447;&#24615;&#36335;&#24452;&#19978;&#30340;&#25439;&#22833;&#20445;&#25345;&#20960;&#20046;&#24658;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;WM&#25552;&#20379;&#20102;LMC&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#24182;&#19981;&#26174;&#30528;&#20943;&#23569;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;$L_2$&#36317;&#31163;&#65292;&#32780;LMC&#30340;&#20986;&#29616;&#24182;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;WM&#26412;&#36523;&#30340;&#36317;&#31163;&#20943;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#65292;&#34920;&#26126;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#27599;&#23618;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#20027;&#35201;&#25913;&#21464;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#26041;&#21521;&#65292;&#32780;&#19981;&#26159;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03757</link><description>&lt;p&gt;
&#26412;&#33021;&#20559;&#35265;&#65306;&#34394;&#20551;&#22270;&#20687;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#20351;LLMs&#20855;&#22791;&#20102;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4V&#36825;&#26679;&#24378;&#22823;&#30340;MLLMs&#22312;&#38754;&#23545;&#26576;&#20123;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#26102;&#20173;&#28982;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#22833;&#36133;&#20102;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#20856;&#22411;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#20196;MLLMs&#22256;&#24785;&#65292;&#23427;&#20204;&#30001;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#31572;&#26696;&#19981;&#19968;&#33268;&#30340;&#22270;&#20687;&#32452;&#25104;&#65292;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CorrelationQA&#65292;&#36825;&#26159;&#39318;&#20010;&#35780;&#20272;&#32473;&#23450;&#34394;&#20551;&#22270;&#20687;&#30340;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;13&#20010;&#31867;&#21035;&#30340;7,308&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;CorrelationQA&#65292;&#25105;&#20204;&#23545;9&#20010;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#23427;&#20204;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#31934;&#36873;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#32467;&#26524;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03055</link><description>&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#23398;&#20064;&#20197;PAC-Bayes&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03055
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#32531;&#35299;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#12290;PAC&#36890;&#36807;&#23558;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#26080;&#32541;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28436;&#21592;&#35757;&#32451;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;PAC&#31639;&#27861;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;Probably Approximately Correct-Bayesian&#65288;PAC-Bayes&#65289;&#20998;&#26512;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#25512;&#26029;&#35780;&#35770;&#23478;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#23545;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#30340;&#34701;&#20837;&#20351;PAC&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#35843;&#25972;&#20854;&#25506;&#32034;&#31574;&#30053;&#65292;&#25351;&#23548;&#28436;&#21592;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#22266;&#23450;&#25110;&#39044;&#23450;&#30340;&#25506;&#32034;&#26041;&#26696;&#30456;&#27604;&#65292;PAC&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#24341;&#23548;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26159;&#21521;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#20855;&#33258;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#31574;&#30053;&#36808;&#20986;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#24212;&#23545;&#22122;&#22768;&#26679;&#26412;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.02081</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#25193;&#25955;&#65306;&#20174;&#24102;&#22122;&#22768;&#26679;&#26412;&#23398;&#20064;&#28508;&#22312;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Diffusion: Learning the Underlying Distribution from Noisy Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#26377;&#25928;&#22320;&#24212;&#23545;&#22122;&#22768;&#26679;&#26412;&#30340;&#23384;&#22312;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#29616;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#23384;&#22312;&#22122;&#22768;&#26679;&#26412;&#26102;&#24456;&#33030;&#24369;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#37327;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#29615;&#22659;&#19981;&#20687;&#22270;&#20687;&#21512;&#25104;&#37027;&#26679;&#24178;&#20928;&#12290;&#21463;&#21040;&#25105;&#20204;&#23545;&#22122;&#22768;&#26679;&#26412;&#19982;&#24178;&#20928;&#26679;&#26412;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20998;&#24067;&#24046;&#24322;&#30340;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;SDE&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#39118;&#38505;&#65288;&#21363;&#25968;&#25454;&#8220;&#33039;&#20081;&#24230;&#8221;&#65289;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#29992;&#20110;&#35843;&#25972;&#22122;&#22768;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20943;&#23569;&#35823;&#23548;&#24182;&#20174;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#39118;&#38505;&#25935;&#24863;&#30340;SDE&#30340;&#26368;&#20248;&#34920;&#36798;&#24335;&#21462;&#20915;&#20110;&#29305;&#23450;&#30340;&#22122;&#22768;&#20998;&#24067;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#26368;&#23567;&#21270;&#39640;&#26031;&#21644;&#19968;&#33324;&#38750;&#39640;&#26031;&#25200;&#21160;&#35823;&#23548;&#30340;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#22914;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#65289;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#36712;&#36857;&#25340;&#25509;&#31639;&#27861;&#21644;&#22870;&#21169;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#39640;&#22238;&#25253;&#36712;&#36857;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#22810;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.00807</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#25340;&#25509;&#23558;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#25552;&#28860;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#36712;&#36857;&#25340;&#25509;&#31639;&#27861;&#21644;&#22870;&#21169;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#39640;&#22238;&#25253;&#36712;&#36857;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#22810;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26368;&#36817;&#24050;&#32463;&#25104;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22823;&#27169;&#22411;&#35268;&#27169;&#22312;&#35745;&#31639;&#19978;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#22238;&#25253;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25340;&#25509;&#31639;&#27861;&#23558;&#23427;&#20204;&#19982;&#21407;&#22987;&#36712;&#36857;&#28151;&#21512;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#29983;&#25104;&#22120;&#12290;&#23558;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#24212;&#29992;&#20110;&#34892;&#20026;&#20811;&#38534;&#65292;&#23398;&#24471;&#30340;&#35268;&#27169;&#36739;&#23567;&#30340;&#27973;&#23618;&#31574;&#30053;&#22312;&#20960;&#20010;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#25110;&#25509;&#36817;&#28145;&#24230;&#29983;&#25104;&#35268;&#21010;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2311.04855</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#36127;&#20540;&#30340;&#22024;&#26434;&#25968;&#25454;&#65292;&#22312;&#19981;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#30340;&#24773;&#20917;&#19979;&#27491;&#30830;&#24674;&#22797;&#38750;&#36127;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#26159;&#19968;&#31181;&#38477;&#32500;&#25216;&#26415;&#65292;&#22312;&#20998;&#26512;&#22024;&#26434;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22825;&#25991;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#65292;&#30001;&#20110;&#22122;&#22768;&#65292;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#36127;&#20540;&#65292;&#21363;&#20351;&#30495;&#23454;&#30340;&#29289;&#29702;&#20449;&#21495;&#20005;&#26684;&#20026;&#27491;&#12290;&#20197;&#24448;&#30340;NMF&#24037;&#20316;&#26410;&#20197;&#32479;&#35745;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#36127;&#25968;&#25454;&#65292;&#36825;&#22312;&#20302;&#20449;&#22122;&#27604;&#25968;&#25454;&#20013;&#20986;&#29616;&#35768;&#22810;&#36127;&#20540;&#26102;&#20250;&#21464;&#24471;&#26840;&#25163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;Shift-NMF&#21644;Nearly-NMF&#65292;&#21487;&#20197;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#22024;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20219;&#20309;&#24341;&#20837;&#30340;&#36127;&#20540;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#20351;&#29992;&#36127;&#25968;&#25454;&#31354;&#38388;&#32780;&#19981;&#36827;&#34892;&#25130;&#21462;&#65292;&#24182;&#19988;&#22312;&#28040;&#38500;&#36127;&#25968;&#25454;&#26102;&#19981;&#20250;&#24341;&#20837;&#27491;&#30340;&#20559;&#31227;&#37327;&#12290;&#25105;&#20204;&#22312;&#31616;&#21333;&#21644;&#26356;&#29616;&#23454;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#20855;&#26377;&#21333;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04855v2 Announce Type: replace-cross  Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotoni
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2201.11653</link><description>&lt;p&gt;
&#12298;SGD&#21644;&#33258;&#36866;&#24212;&#23398;&#20064;&#35268;&#21017;&#23398;&#21040;&#30340;&#34920;&#31034;&#65306;&#21464;&#21270;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26465;&#20214;&#12299;
&lt;/p&gt;
&lt;p&gt;
Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.11653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#22686;&#21152;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36830;&#32493;&#23398;&#20064;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#32780;&#20114;&#19981;&#24178;&#25200;&#12290;&#20943;&#23569;&#20114;&#30456;&#24178;&#25200;&#30340;&#26377;&#25928;&#26041;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#20803;&#30340;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#20013;&#25214;&#21040;&#12290;&#26681;&#25454;Aljundi&#31561;&#20154;&#21644;Hadsell&#31561;&#20154;&#30340;&#35266;&#28857;&#65292;&#22312;&#34920;&#31034;&#27700;&#24179;&#26045;&#21152;&#31232;&#30095;&#24615;&#23545;&#36830;&#32493;&#23398;&#20064;&#26159;&#26377;&#21033;&#30340;&#65292;&#22240;&#20026;&#31232;&#30095;&#30340;&#31070;&#32463;&#20803;&#28608;&#27963;&#40723;&#21169;&#21442;&#25968;&#20043;&#38388;&#30340;&#23569;&#37325;&#21472;&#65292;&#23548;&#33268;&#26356;&#23569;&#30340;&#24178;&#25200;&#12290;&#21516;&#26679;&#65292;&#39640;&#24230;&#36873;&#25321;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#24341;&#36215;&#36739;&#23569;&#30340;&#24178;&#25200;&#65292;&#22240;&#20026;&#31070;&#32463;&#20803;&#20013;&#30340;&#29305;&#23450;&#21709;&#24212;&#23558;&#20943;&#23569;&#19982;&#20854;&#20182;&#21442;&#25968;&#30340;&#37325;&#21472;&#26426;&#20250;&#12290;&#32771;&#34385;&#21040;&#20154;&#33041;&#22312;&#19968;&#29983;&#20013;&#25191;&#34892;&#36830;&#32493;&#23398;&#20064;&#65292;&#25214;&#21040;&#33258;&#28982;&#22686;&#21152;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#26465;&#20214;&#21487;&#33021;&#20026;&#20102;&#35299;&#22823;&#33041;&#21151;&#33021;&#25552;&#20379;&#35265;&#35299;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#31232;&#30095;&#24615;&#21644;&#36873;&#25321;&#24615;&#30340;&#21508;&#31181;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
&lt;/p&gt;</description></item><item><title>&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.13858</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Molecular Design with Multi-Conditional Diffusion Guidance. (arXiv:2401.13858v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13858
&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#30340;&#36870;&#20998;&#23376;&#35774;&#35745;&#27169;&#22411;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;Transformer-based&#21435;&#22122;&#27169;&#22411;&#21644;&#22270;&#20381;&#36182;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#22810;&#20010;&#26465;&#20214;&#32422;&#26463;&#19979;&#20934;&#30830;&#22320;&#29983;&#25104;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#36870;&#20998;&#23376;&#35774;&#35745;&#22312;&#26448;&#26009;&#21644;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#22312;&#26080;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23558;&#21512;&#25104;&#35780;&#20998;&#21644;&#27668;&#20307;&#28183;&#36879;&#24615;&#31561;&#22810;&#20010;&#23646;&#24615;&#20316;&#20026;&#26465;&#20214;&#32422;&#26463;&#38598;&#25104;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26465;&#20214;&#25193;&#25955;&#24341;&#23548;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;Transformer&#30340;&#21435;&#22122;&#27169;&#22411;&#20855;&#26377;&#19968;&#20010;&#26465;&#20214;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#25968;&#20540;&#21644;&#20998;&#31867;&#26465;&#20214;&#30340;&#34920;&#31034;&#12290;&#32452;&#25104;&#32467;&#26500;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;&#21435;&#22122;&#27169;&#22411;&#22312;&#26465;&#20214;&#34920;&#31034;&#19979;&#36827;&#34892;&#21435;&#22122;&#35757;&#32451;&#12290;&#25193;&#25955;&#36807;&#31243;&#21464;&#24471;&#20381;&#36182;&#20110;&#22270;&#26469;&#20934;&#30830;&#20272;&#35745;&#20998;&#23376;&#20013;&#19982;&#22270;&#30456;&#20851;&#30340;&#22122;&#22768;&#65292;&#32780;&#19981;&#20687;&#20197;&#21069;&#30340;&#27169;&#22411;&#20165;&#20851;&#27880;&#21407;&#23376;&#25110;&#38190;&#30340;&#36793;&#32536;&#20998;&#24067;&#12290;&#25105;&#20204;&#24191;&#27867;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#26465;&#20214;&#32858;&#21512;&#29289;&#21644;&#23567;&#20998;&#23376;&#29983;&#25104;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#22312;&#20998;&#24067;&#24230;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse molecular design with diffusion models holds great potential for advancements in material and drug discovery. Despite success in unconditional molecule generation, integrating multiple properties such as synthetic score and gas permeability as condition constraints into diffusion models remains unexplored. We introduce multi-conditional diffusion guidance. The proposed Transformer-based denoising model has a condition encoder that learns the representations of numerical and categorical conditions. The denoising model, consisting of a structure encoder-decoder, is trained for denoising under the representation of conditions. The diffusion process becomes graph-dependent to accurately estimate graph-related noise in molecules, unlike the previous models that focus solely on the marginal distributions of atoms or bonds. We extensively validate our model for multi-conditional polymer and small molecule generation. Results demonstrate our superiority across metrics from distribution
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07961</link><description>&lt;p&gt;
&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Solution of the Probabilistic Lambert Problem: Connections with Optimal Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs. (arXiv:2401.07961v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07961
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#27010;&#29575;Lambert&#38382;&#39064;&#19982;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#12289;Schr\"odinger&#26725;&#21644;&#21453;&#24212;-&#25193;&#25955;&#20559;&#24494;&#20998;&#26041;&#31243;&#31561;&#39046;&#22495;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#20540;&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lambert&#38382;&#39064;&#28041;&#21450;&#36890;&#36807;&#36895;&#24230;&#25511;&#21046;&#22312;&#35268;&#23450;&#30340;&#39134;&#34892;&#26102;&#38388;&#20869;&#23558;&#33322;&#22825;&#22120;&#20174;&#32473;&#23450;&#30340;&#21021;&#22987;&#20301;&#32622;&#36716;&#31227;&#21040;&#32473;&#23450;&#30340;&#32456;&#31471;&#20301;&#32622;&#65292;&#21463;&#21040;&#37325;&#21147;&#21147;&#22330;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;Lambert&#38382;&#39064;&#30340;&#27010;&#29575;&#21464;&#31181;&#65292;&#20854;&#20013;&#20301;&#32622;&#21521;&#37327;&#30340;&#31471;&#28857;&#32422;&#26463;&#30340;&#30693;&#35782;&#34987;&#23427;&#20204;&#21508;&#33258;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#25152;&#26367;&#20195;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#31471;&#28857;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#32422;&#26463;&#30340;Lambert&#38382;&#39064;&#26159;&#19968;&#20010;&#24191;&#20041;&#30340;&#26368;&#20248;&#36136;&#37327;&#20256;&#36755;&#65288;OMT&#65289;&#38382;&#39064;&#65292;&#20174;&#32780;&#23558;&#36825;&#20010;&#32463;&#20856;&#30340;&#22825;&#20307;&#21160;&#21147;&#23398;&#38382;&#39064;&#19982;&#29616;&#20195;&#38543;&#26426;&#25511;&#21046;&#21644;&#38543;&#26426;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20010;&#26032;&#21457;&#29616;&#30340;&#36830;&#25509;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#24314;&#31435;&#27010;&#29575;Lambert&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#21516;&#26679;&#30340;&#36830;&#25509;&#36824;&#24110;&#21161;&#36890;&#36807;&#25193;&#25955;&#27491;&#35268;&#21270;&#25968;&#20540;&#27714;&#35299;&#27010;&#29575;Lambert&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#36830;&#25509;&#26469;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lambert's problem concerns with transferring a spacecraft from a given initial to a given terminal position within prescribed flight time via velocity control subject to a gravitational force field. We consider a probabilistic variant of the Lambert problem where the knowledge of the endpoint constraints in position vectors are replaced by the knowledge of their respective joint probability density functions. We show that the Lambert problem with endpoint joint probability density constraints is a generalized optimal mass transport (OMT) problem, thereby connecting this classical astrodynamics problem with a burgeoning area of research in modern stochastic control and stochastic machine learning. This newfound connection allows us to rigorously establish the existence and uniqueness of solution for the probabilistic Lambert problem. The same connection also helps to numerically solve the probabilistic Lambert problem via diffusion regularization, i.e., by leveraging further connection 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20855;&#26377;&#31934;&#30830;&#35268;&#33539;&#19981;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#20102;&#22235;&#32500;SU&#65288;3&#65289;&#35268;&#33539;&#29702;&#35770;&#30340;&#20248;&#31168;&#22266;&#23450;&#28857;&#20316;&#29992;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#25171;&#19979;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.06481</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#35268;&#33539;&#31561;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;SU&#65288;3&#65289;&#35268;&#33539;&#29702;&#35770;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine learning a fixed point action for SU(3) gauge theory with a gauge equivariant convolutional neural network. (arXiv:2401.06481v1 [hep-lat] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20855;&#26377;&#31934;&#30830;&#35268;&#33539;&#19981;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25214;&#21040;&#20102;&#22235;&#32500;SU&#65288;3&#65289;&#35268;&#33539;&#29702;&#35770;&#30340;&#20248;&#31168;&#22266;&#23450;&#28857;&#20316;&#29992;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#20026;&#26410;&#26469;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#25171;&#19979;&#20102;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22266;&#23450;&#28857;&#30340;&#26684;&#23376;&#20316;&#29992;&#34987;&#35774;&#35745;&#25104;&#20855;&#26377;&#19981;&#21463;&#31163;&#25955;&#21270;&#25928;&#24212;&#24433;&#21709;&#30340;&#36830;&#32493;&#32463;&#20856;&#24615;&#36136;&#65292;&#24182;&#22312;&#37327;&#23376;&#23618;&#38754;&#19978;&#20943;&#23569;&#26684;&#23376;&#25928;&#24212;&#12290;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#36739;&#31895;&#30340;&#26684;&#23376;&#26469;&#25552;&#21462;&#36830;&#32493;&#29289;&#29702;&#30340;&#21487;&#33021;&#26041;&#27861;&#65292;&#20174;&#32780;&#32469;&#36807;&#19982;&#36830;&#32493;&#26497;&#38480;&#30456;&#20851;&#30340;&#20020;&#30028;&#20943;&#24930;&#21644;&#25299;&#25169;&#20923;&#32467;&#38382;&#39064;&#12290;&#23454;&#38469;&#24212;&#29992;&#30340;&#20851;&#38190;&#26159;&#25214;&#21040;&#19968;&#20010;&#31934;&#30830;&#19988;&#32039;&#20945;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#35768;&#22810;&#24615;&#36136;&#21482;&#26159;&#38544;&#21547;&#23450;&#20041;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#24605;&#32771;&#20102;&#22914;&#20309;&#21442;&#25968;&#21270;&#22266;&#23450;&#28857;&#20316;&#29992;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#31934;&#30830;&#35268;&#33539;&#19981;&#21464;&#24615;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#22235;&#32500;SU&#65288;3&#65289;&#35268;&#33539;&#29702;&#35770;&#30340;&#22266;&#23450;&#28857;&#20316;&#29992;&#12290;&#22823;&#30340;&#31639;&#23376;&#31354;&#38388;&#20351;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#27604;&#20043;&#21069;&#30740;&#31350;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#26410;&#26469;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fixed point lattice actions are designed to have continuum classical properties unaffected by discretization effects and reduced lattice artifacts at the quantum level. They provide a possible way to extract continuum physics with coarser lattices, thereby allowing to circumvent problems with critical slowing down and topological freezing toward the continuum limit. A crucial ingredient for practical applications is to find an accurate and compact parametrization of a fixed point action, since many of its properties are only implicitly defined. Here we use machine learning methods to revisit the question of how to parametrize fixed point actions. In particular, we obtain a fixed point action for four-dimensional SU(3) gauge theory using convolutional neural networks with exact gauge invariance. The large operator space allows us to find superior parametrizations compared to previous studies, a necessary first step for future Monte Carlo simulations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10224</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#25512;&#24191;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26085;&#30410;&#22686;&#21152;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#24403;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#24191;&#27867;&#65292;&#22240;&#20026;&#32570;&#20047;&#26041;&#27861;&#35770;&#26631;&#20934;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#20013;&#24515;&#25110;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#36741;&#21161;&#22240;&#32032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23384;&#22312;&#36739;&#22823;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26222;&#36866;&#30340;&#12289;&#25968;&#25454;-&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65288;QUAVE&#65289;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#32467;&#21512;&#23454;&#38469;&#12289;&#22235;&#20803;&#25968;&#25110;&#36229;&#22797;&#20540;&#27169;&#22411;&#65292;&#25512;&#24191;&#23427;&#20204;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;QUAVE&#39318;&#20808;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#19981;&#21516;&#30340;&#23376;&#24102;&#65292;&#24471;&#21040;&#20302;&#39057;/&#36817;&#20284;&#39057;&#24102;&#21644;&#39640;&#39057;/&#32454;&#31890;&#24230;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#23545;&#26368;&#26377;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#19981;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.02635</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22522;&#30784;&#65306;&#26397;&#21521;&#20855;&#26377;&#22522;&#30784;&#20808;&#39564;&#36741;&#21161;&#30340;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance. (arXiv:2310.02635v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#30340;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#20204;&#24050;&#32463;&#34920;&#26126;&#65292;&#20174;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#26159;&#26500;&#24314;&#36890;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#27491;&#22914;&#22312;NLP&#20013;&#25152;&#35265;&#12290;&#20026;&#20102;&#26500;&#24314;&#20855;&#36523;&#36890;&#29992;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21644;&#35768;&#22810;&#20854;&#20182;&#30740;&#31350;&#32773;&#20551;&#35774;&#36825;&#31181;&#22522;&#30784;&#20808;&#39564;&#20063;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#20855;&#20307;&#24418;&#24335;&#34920;&#31034;&#36825;&#20123;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#65292;&#20197;&#21450;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#30452;&#35266;&#26377;&#25928;&#30340;&#20855;&#36523;&#20808;&#39564;&#65292;&#21253;&#25324;&#22522;&#30784;&#31574;&#30053;&#12289;&#20215;&#20540;&#21644;&#25104;&#21151;&#22870;&#21169;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#26159;&#22522;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;MDP&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#30001;&#36825;&#20123;&#20808;&#39564;&#36741;&#21161;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#22522;&#30784;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;FAC&#65289;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#21629;&#21517;&#20026;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#65288;FRL&#65289;&#65292;&#22240;&#20026;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#20855;&#36523;&#22522;&#30784;&#20808;&#39564;&#26469;&#36827;&#34892;&#25506;&#32034;&#12289;&#23398;&#20064;&#21644;&#24378;&#21270;&#12290;FRL&#30340;&#22909;&#22788;&#26377;&#19977;&#20010;&#12290;(1)&#26679;&#26412;&#25928;&#29575;&#39640;&#12290;&#36890;&#36807;&#22522;&#30784;&#20808;&#39564;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#20943;&#23569;&#26679;&#26412;&#20351;&#29992;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, people have shown that large-scale pre-training from internet-scale data is the key to building generalist models, as witnessed in NLP. To build embodied generalist agents, we and many other researchers hypothesize that such foundation prior is also an indispensable component. However, it is unclear what is the proper concrete form to represent those embodied foundation priors and how they should be used in the downstream task. In this paper, we propose an intuitive and effective set of embodied priors that consist of foundation policy, value, and success reward. The proposed priors are based on the goal-conditioned MDP. To verify their effectiveness, we instantiate an actor-critic method assisted by the priors, called Foundation Actor-Critic (FAC). We name our framework as Foundation Reinforcement Learning (FRL), since it completely relies on embodied foundation priors to explore, learn and reinforce. The benefits of FRL are threefold. (1) Sample efficient. With foundation p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16519</link><description>&lt;p&gt;
AtomSurf&#65306;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#30340;&#23398;&#20064;&#30340;&#34920;&#38754;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;Cryo-EM&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#32467;&#26500;&#21487;&#33719;&#24471;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21151;&#33021;&#27880;&#37322;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20851;&#27880;&#21019;&#24314;&#36866;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20174;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#23558;&#36825;&#20123;&#32467;&#26500;&#34920;&#31034;&#20026;&#20960;&#20309;&#23545;&#35937;&#65288;&#22914;&#32593;&#26684;&#12289;&#22270;&#25110;&#34920;&#38754;&#65289;&#24182;&#24212;&#29992;&#36866;&#21512;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#23558;&#21462;&#20915;&#20110;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#34507;&#30333;&#36136;&#34920;&#31034;&#20026;$\textit{3D mesh surfaces}$&#24182;&#23558;&#20854;&#32435;&#20837;&#24050;&#24314;&#31435;&#30340;&#34920;&#31034;&#22522;&#20934;&#20013;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21457;&#29616;&#26159;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#20165;&#21333;&#29420;&#34920;&#38754;&#34920;&#31034;&#20284;&#20046;&#26080;&#27861;&#19982;3D&#32593;&#26684;&#31454;&#20105;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#26041;&#27861;&#65292;&#23558;&#34920;&#38754;&#34920;&#31034;&#19982;&#22270;&#34920;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#20855;&#20307;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968;&#19982;&#19987;&#38376;&#21442;&#25968;&#20043;&#38388;&#30340;&#32047;&#31215;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.04557</link><description>&lt;p&gt;
&#29992;&#20110;&#26680;&#22238;&#24402;&#30340;&#36951;&#25022;&#26368;&#20248;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#21450;&#20854;&#22312;&#32654;&#24335;&#26399;&#26435;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing. (arXiv:2309.04557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#22312;&#26680;&#22238;&#24402;&#27169;&#22411;&#20013;&#20855;&#20307;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968;&#19982;&#19987;&#38376;&#21442;&#25968;&#20043;&#38388;&#30340;&#32047;&#31215;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#30340;&#36845;&#20195;&#26041;&#26696;&#65292;&#29992;&#20110;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#20013;&#24515;&#35745;&#21010;&#32773;&#21487;&#20197;&#35775;&#38382;&#21516;&#19968;&#23398;&#20064;&#27169;&#22411; $f_{\theta}$ &#30340;&#25968;&#25454;&#38598; ${\cal D}_1,\dots,{\cal D}_N$&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23562;&#37325;&#27169;&#22411; $f_{\theta(T)}$ &#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#37327;&#20943;&#23567;&#29983;&#25104;&#21442;&#25968; $\{\theta_i(t)\}_{t=0}^T$ &#22312;&#25152;&#26377; $T$ &#27425;&#36845;&#20195;&#20013;&#19982;&#27599;&#20010;&#25968;&#25454;&#38598;&#24471;&#21040;&#30340;&#19987;&#38376;&#21442;&#25968;$\theta^\star_{1},\ldots,\theta^\star_N$ &#30340;&#32047;&#31215;&#20559;&#24046;&#12290;&#25105;&#20204;&#20165;&#20801;&#35768;&#27599;&#20010;&#19987;&#38376;&#27169;&#22411;&#65288;&#33410;&#28857;/&#20195;&#29702;&#65289;&#21644;&#20013;&#24515;&#35745;&#21010;&#32773;&#65288;&#26381;&#21153;&#22120;&#65289;&#22312;&#27599;&#27425;&#36845;&#20195;&#65288;&#36718;&#65289;&#20043;&#38388;&#36827;&#34892;&#25345;&#32493;&#36890;&#20449;&#12290;&#23545;&#20110;&#27169;&#22411; $f_{\theta}$ &#26159;&#26377;&#38480;&#31209;&#26680;&#22238;&#24402;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#30340;&#26174;&#24335;&#26356;&#26032;&#20844;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36951;&#25022;&#26368;&#20248;&#31639;&#27861;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#20960;&#20046;&#36951;&#25022;&#26368;&#20248;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#38656;&#35201;&#36739;&#23569;&#30340; $\mathcal{O}(Np^2)$ &#20010;&#22522;&#26412;&#36816;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21387;&#32553;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#19982;&#32463;&#20856;&#30340;&#31163;&#32447;&#21387;&#32553;&#31639;&#27861;&#19981;&#21516;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21387;&#32553;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#12290;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2308.14962</link><description>&lt;p&gt;
&#36890;&#36807;&#24369;SINDy&#31639;&#27861;&#36827;&#34892;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Streaming Compression of Scientific Data via weak-SINDy. (arXiv:2308.14962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21387;&#32553;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#19982;&#32463;&#20856;&#30340;&#31163;&#32447;&#21387;&#32553;&#31639;&#27861;&#19981;&#21516;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#21487;&#20197;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#21387;&#32553;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#12290;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38024;&#23545;&#27969;&#24335;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#30340;&#27969;&#24335;&#24369;SINDy&#31639;&#27861;&#12290;&#31185;&#23398;&#25968;&#25454;&#30340;&#20135;&#29983;&#26080;&#35770;&#26159;&#36890;&#36807;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#65292;&#37117;&#27491;&#22312;&#32463;&#21382;&#19968;&#27573;&#25351;&#25968;&#22686;&#38271;&#30340;&#38454;&#27573;&#65292;&#36825;&#20351;&#24471;&#25968;&#25454;&#21387;&#32553;&#23545;&#20110;&#23384;&#20648;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#31185;&#23398;&#25968;&#25454;&#38598;&#26469;&#35828;&#21464;&#24471;&#37325;&#35201;&#19988;&#24120;&#24120;&#26159;&#24517;&#35201;&#30340;&#12290;&#19982;&#32463;&#20856;&#30340;&#8220;&#31163;&#32447;&#8221;&#21387;&#32553;&#31639;&#27861;&#30456;&#21453;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#21387;&#32553;&#65292;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#22312;&#27169;&#25311;&#25110;&#23454;&#39564;&#29983;&#25104;&#30340;&#25968;&#25454;&#20173;&#22312;&#31995;&#32479;&#20013;&#27969;&#21160;&#26102;&#36827;&#34892;&#25968;&#25454;&#21387;&#32553;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#24471;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#22240;&#20026;&#22312;&#31163;&#32447;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#21387;&#32553;&#31639;&#27861;&#65292;&#27969;&#24335;&#24369;SINDy&#65292;&#23427;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#24213;&#23618;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper a streaming weak-SINDy algorithm is developed specifically for compressing streaming scientific data. The production of scientific data, either via simulation or experiments, is undergoing an stage of exponential growth, which makes data compression important and often necessary for storing and utilizing large scientific data sets. As opposed to classical ``offline" compression algorithms that perform compression on a readily available data set, streaming compression algorithms compress data ``online" while the data generated from simulation or experiments is still flowing through the system. This feature makes streaming compression algorithms well-suited for scientific data compression, where storing the full data set offline is often infeasible. This work proposes a new streaming compression algorithm, streaming weak-SINDy, which takes advantage of the underlying data characteristics during compression. The streaming weak-SINDy algorithm constructs feature matrices and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13991</link><description>&lt;p&gt;
JL-&#24341;&#29702;&#25512;&#23548;&#30340;&#29992;&#20110;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#30340;&#26368;&#20248;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22823;&#32500;&#24230;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#21033;&#29992;Johnson-Lindenstrauss(JL)&#24341;&#29702;&#65292;&#22312;&#19968;&#20010;&#36716;&#25442;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#23398;&#20064;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#30340;&#21028;&#21035;&#24335;&#23383;&#20856;&#12290;&#19982;&#36890;&#24120;&#20351;&#29992;JL&#36827;&#34892;&#38477;&#32500;&#30340;&#38543;&#26426;&#25237;&#24433;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Modified Supervised PC Analysis (M-SPCA)&#25512;&#23548;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#32500;&#25968;&#36981;&#24490;JL&#30340;&#35268;&#23450;&#12290;JLSPCADL&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25512;&#26029;&#36866;&#24403;&#30340;&#22833;&#30495;&#27700;&#24179;&#21644;&#30456;&#24212;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;&#36866;&#24403;&#25551;&#36848;&#38271;&#24230;(SDL)&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26368;&#20248;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;SDL&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#20174;M-SPCA&#20013;&#19968;&#27493;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#29305;&#24449;-&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;</title><link>http://arxiv.org/abs/2304.05301</link><description>&lt;p&gt;
TACOS: &#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#38598;&#21512;&#31639;&#27861;&#21512;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed Training. (arXiv:2304.05301v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05301
&lt;/p&gt;
&lt;p&gt;
TACOS &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;&#19982;&#22522;&#20934;&#31639;&#27861;&#30456;&#27604;&#65292;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#36895;&#24230;&#25552;&#39640;&#20102; 3.73 &#20493;&#65292;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#21482;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#32676;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#35759;&#26159;&#20998;&#24067;&#24335;&#35757;&#32451;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#36816;&#34892;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#31639;&#27861;&#23545;&#20110;&#20248;&#21270;&#36890;&#35759;&#24615;&#33021;&#20197;&#26368;&#23567;&#21270;&#25317;&#22622;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#65292;&#27492;&#31867;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#19968;&#23567;&#37096;&#20998;&#31616;&#21333;&#25299;&#25169;&#32467;&#26500;&#65292;&#38480;&#21046;&#20102;&#35757;&#32451;&#38598;&#32676;&#20013;&#37319;&#29992;&#30340;&#25299;&#25169;&#32467;&#26500;&#24182;&#22788;&#29702;&#30001;&#20110;&#32593;&#32476;&#25925;&#38556;&#32780;&#20135;&#29983;&#30340;&#19981;&#35268;&#21017;&#25299;&#25169;&#32467;&#26500;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102; TACOS&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#33258;&#21160;&#21512;&#25104;&#20219;&#24847;&#36755;&#20837;&#32593;&#32476;&#25299;&#25169;&#30340;&#38754;&#21521;&#25299;&#25169;&#32467;&#26500;&#30340;&#38598;&#21512;&#21512;&#25104;&#22120;&#12290;TACOS &#21512;&#25104;&#30340; All-Reduce &#31639;&#27861;&#27604;&#22522;&#32447;&#31639;&#27861;&#24555;&#20102; 3.73 &#20493;&#65292;&#24182;&#20026; 512-NPU &#31995;&#32479;&#21512;&#25104;&#38598;&#20307;&#31639;&#27861;&#20165;&#38656; 6.1 &#20998;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective communications are an indispensable part of distributed training. Running a topology-aware collective algorithm is crucial for optimizing communication performance by minimizing congestion. Today such algorithms only exist for a small set of simple topologies, limiting the topologies employed in training clusters and handling irregular topologies due to network failures. In this paper, we propose TACOS, an automated topology-aware collective synthesizer for arbitrary input network topologies. TACOS synthesized 3.73x faster All-Reduce algorithm over baselines, and synthesized collective algorithms for 512-NPU system in just 6.1 minutes.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.15889</link><description>&lt;p&gt;
&#36208;&#21521;&#25968;&#25454;&#21644;&#30693;&#35782;&#39537;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15889
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#26159;&#23558;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#25972;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#25512;&#29702;&#21644;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31561;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#25104;&#21151;&#24212;&#29992;&#26696;&#20363;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#65288;NeSy&#65289;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#36861;&#27714;&#31526;&#21495;&#21644;&#32479;&#35745;&#35748;&#30693;&#33539;&#24335;&#30340;&#25972;&#21512;&#12290;&#30001;&#20110;NeSy&#22312;&#31526;&#21495;&#34920;&#31034;&#30340;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#23398;&#20064;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#23427;&#21487;&#33021;&#25104;&#20026;&#19979;&#19968;&#20195;AI&#30340;&#20652;&#21270;&#21058;&#12290;&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;NeSy&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21382;&#21490;&#65292;&#28085;&#30422;&#20102;&#26089;&#26399;&#24037;&#20316;&#21644;&#22522;&#30784;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#32972;&#26223;&#27010;&#24565;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#21160;NeSy&#21457;&#23637;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25353;&#29031;&#20960;&#20010;&#20027;&#35201;&#29305;&#24449;&#23545;&#36817;&#26399;&#30340;&#37324;&#31243;&#30865;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25972;&#21512;&#12289;&#30693;&#35782;&#34920;&#31034;&#12289;&#30693;&#35782;&#23884;&#20837;&#21644;&#21151;&#33021;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#31616;&#35201;&#35752;&#35770;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#26696;&#20363;&#65292;&#20197;&#21450;NeSy&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-symbolic computing (NeSy), which pursues the integration of the symbolic and statistical paradigms of cognition, has been an active research area of Artificial Intelligence (AI) for many years. As NeSy shows promise of reconciling the advantages of reasoning and interpretability of symbolic representation and robust learning in neural networks, it may serve as a catalyst for the next generation of AI. In the present paper, we provide a systematic overview of the recent developments and important contributions of NeSy research. Firstly, we introduce study history of this area, covering early work and foundations. We further discuss background concepts and identify key driving factors behind the development of NeSy. Afterward, we categorize recent landmark approaches along several main characteristics that underline this research paradigm, including neural-symbolic integration, knowledge representation, knowledge embedding, and functionality. Next, we briefly discuss the successfu
&lt;/p&gt;</description></item></channel></rss>