<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.19588</link><description>&lt;p&gt;
DenseNets&#37325;&#29983;&#65306;&#36229;&#36234;ResNets&#21644;ViTs&#30340;&#33539;&#24335;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#25506;&#35752;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#20854;&#30456;&#23545;&#20110;ResNet&#39118;&#26684;&#26550;&#26500;&#30340;&#34987;&#20302;&#20272;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#12289;&#22359;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#24471;DenseNets&#21487;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#65292;&#24182;&#22312;ImageNet-1K&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22797;&#33487;&#20102;&#23494;&#38598;&#36830;&#25509;&#21367;&#31215;&#32593;&#32476;&#65288;DenseNets&#65289;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#20027;&#23548;&#30340;ResNet&#39118;&#26684;&#26550;&#26500;&#34987;&#20302;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;DenseNets&#30340;&#28508;&#21147;&#34987;&#24573;&#35270;&#65292;&#26159;&#22240;&#20026;&#26410;&#26366;&#35302;&#21450;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#20256;&#32479;&#35774;&#35745;&#20803;&#32032;&#26410;&#33021;&#23436;&#20840;&#23637;&#29616;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36830;&#25509;&#30340;&#23494;&#38598;&#36830;&#25509;&#26159;&#24378;&#22823;&#30340;&#65292;&#34920;&#26126;DenseNets&#21487;&#20197;&#34987;&#37325;&#26032;&#28608;&#27963;&#20197;&#19982;&#29616;&#20195;&#26550;&#26500;&#31454;&#20105;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25913;&#36827;&#20102;&#27425;&#20248;&#32452;&#20214; - &#26550;&#26500;&#35843;&#25972;&#12289;&#22359;&#37325;&#26032;&#35774;&#35745;&#21644;&#25913;&#36827;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#20197;&#25193;&#23637;DenseNets&#24182;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#36830;&#25509;&#24555;&#25463;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#31616;&#21333;&#30340;&#26550;&#26500;&#20803;&#32032;&#65292;&#26368;&#32456;&#36229;&#36234;&#20102;Swin Transformer&#12289;ConvNeXt&#21644;DeiT-III - &#27531;&#24046;&#23398;&#20064;&#35889;&#31995;&#20013;&#30340;&#20851;&#38190;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ImageNet-1K&#19978;&#23637;&#29616;&#20986;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#31454;&#20105;wi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19588v1 Announce Type: cross  Abstract: This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18742</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#21453;&#39304;&#23545;&#40784;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Learning Dynamics of Alignment with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#26174;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#34892;&#20026;&#30340;&#22797;&#26434;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#24847;&#22270;&#23545;&#40784;&#24050;&#25104;&#20026;&#23433;&#20840;&#37096;&#32626;&#27169;&#22411;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#34429;&#28982;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29702;&#35770;&#19978;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#25105;&#20204;&#27491;&#24335;&#23637;&#31034;&#20102;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#26356;&#26032;&#36895;&#24230;&#65292;&#24182;&#23545;&#35757;&#32451;&#20934;&#30830;&#24230;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36824;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#29616;&#35937;&#65292;&#21363;&#20248;&#21270;&#26131;&#20110;&#20248;&#20808;&#32771;&#34385;&#20855;&#26377;&#26356;&#39640;&#20559;&#22909;&#21487;&#21306;&#20998;&#24615;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24403;&#20195;LLMs&#21644;&#23545;&#40784;&#20219;&#21153;&#19978;&#22312;&#23454;&#35777;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24378;&#21270;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#23545;&#40784;&#26041;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#20813;&#36131;&#22768;&#26126;&#65306;&#26412;&#25991;&#21253;&#21547;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNN Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#22823;&#21367;&#31215;&#22359;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#22359;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#21010;&#20998;&#22240;&#23376;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18613</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;CNN&#30340;Lipschitz&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Lipschitz Estimation for CNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNN Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#22823;&#21367;&#31215;&#22359;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#22359;&#65292;&#24182;&#36890;&#36807;&#35843;&#33410;&#21010;&#20998;&#22240;&#23376;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#27491;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#36890;&#29992;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#24456;&#26377;&#29992;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#20851;&#24212;&#29992;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#20272;&#35745;Lipschitz&#24120;&#25968;&#30340;&#26041;&#27861;&#21487;&#33021;&#24456;&#32039;&#33268;&#65292;&#20294;&#24403;&#24212;&#29992;&#20110;CNNs&#26102;&#65292;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;CNNs Lipschitz&#24120;&#25968;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#32852;&#21512;&#23618;&#21644;&#23485;&#24230;&#21010;&#20998;&#23558;&#22823;&#21367;&#31215;&#22359;&#20998;&#21106;&#20026;&#19968;&#31995;&#21015;&#36739;&#23567;&#30340;&#22359;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22359;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#19982;&#36739;&#23567;&#22359;&#30340;Lipschitz&#24120;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#21464;&#21270;&#21010;&#20998;&#22240;&#23376;&#65292;&#24471;&#21040;&#30340;&#26041;&#27861;&#21487;&#20197;&#35843;&#33410;&#20197;&#20248;&#20808;&#32771;&#34385;&#20934;&#30830;&#24615;&#25110;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#25903;&#25345;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18613v1 Announce Type: new  Abstract: Estimating the Lipschitz constant of deep neural networks is of growing interest as it is useful for informing on generalisability and adversarial robustness. Convolutional neural networks (CNNs) in particular, underpin much of the recent success in computer vision related applications. However, although existing methods for estimating the Lipschitz constant can be tight, they have limited scalability when applied to CNNs. To tackle this, we propose a novel method to accelerate Lipschitz constant estimation for CNNs. The core idea is to divide a large convolutional block via a joint layer and width-wise partition, into a collection of smaller blocks. We prove an upper-bound on the Lipschitz constant of the larger block in terms of the Lipschitz constants of the smaller blocks. Through varying the partition factor, the resulting method can be adjusted to prioritise either accuracy or scalability and permits parallelisation. We demonstrate
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14973</link><description>&lt;p&gt;
&#36712;&#36857;&#27491;&#21017;&#21270;&#22686;&#24378;&#33258;&#30417;&#30563;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Trajectory Regularization Enhances Self-Supervised Geometric Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14973
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26032;&#30340;pose-estimation&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#25552;&#20986;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#22312;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#19978;&#21462;&#24471;10-20%&#30340;&#25552;&#21319;&#65292;&#24182;&#25552;&#39640;&#20102;4%&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#34920;&#31034;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23039;&#21183;&#20272;&#35745;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;SSL&#20960;&#20309;&#34920;&#31034;&#65292;&#35813;&#22522;&#20934;&#35201;&#27714;&#22312;&#27809;&#26377;&#35821;&#20041;&#25110;&#23039;&#21183;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#35821;&#20041;&#21644;&#20960;&#20309;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#24378;SSL&#20960;&#20309;&#34920;&#31034;&#32780;&#19981;&#29306;&#29298;&#35821;&#20041;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21033;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#21487;&#20197;&#23558;&#23039;&#21183;&#20272;&#35745;&#24615;&#33021;&#25552;&#39640;10-20&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#36712;&#36857;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#39069;&#22806;&#25552;&#39640;&#20102;4&#65285;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#36234;&#30028;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14973v1 Announce Type: cross  Abstract: Self-supervised learning (SSL) has proven effective in learning high-quality representations for various downstream tasks, with a primary focus on semantic tasks. However, its application in geometric tasks remains underexplored, partially due to the absence of a standardized evaluation method for geometric representations. To address this gap, we introduce a new pose-estimation benchmark for assessing SSL geometric representations, which demands training without semantic or pose labels and achieving proficiency in both semantic and geometric downstream tasks. On this benchmark, we study enhancing SSL geometric representations without sacrificing semantic classification accuracy. We find that leveraging mid-layer representations improves pose-estimation performance by 10-20%. Further, we introduce an unsupervised trajectory-regularization loss, which improves performance by an additional 4% and improves generalization ability on out-of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.11755</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#25552;&#31034;&#33258;&#21160;&#21270;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#35273;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Meta-Prompting for Visual Recognition (MPVR)&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#23569;&#37327;&#20449;&#24687;&#21363;&#21487;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#20013;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#30340;&#25552;&#31034;&#38598;&#25104;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#38646;&#26679;&#26412;&#35782;&#21035;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#36825;&#20123;&#31867;&#21035;&#29305;&#23450;&#25552;&#31034;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#20026;LLMs&#35774;&#35745;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#19979;&#28216;&#20219;&#21153;&#30340;VLM&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#25163;&#21160;&#32534;&#20889;&#36825;&#20123;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#65292;&#32780;&#19988;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#19982;&#24863;&#20852;&#36259;&#31867;&#21035;&#30456;&#20851;&#30340;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#20219;&#21153;&#29305;&#23450;&#39118;&#26684;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#25490;&#38500;&#22312;&#24490;&#29615;&#20043;&#22806;&#65292;&#24182;&#23436;&#20840;&#33258;&#21160;&#21270;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#25552;&#31034;&#29983;&#25104;&#36807;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#35782;&#21035;&#30340;&#20803;&#25552;&#31034;&#65288;MPVR&#65289;&#12290;&#20165;&#20197;&#30446;&#26631;&#20219;&#21153;&#30340;&#23569;&#37327;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#24418;&#24335;&#20197;&#21450;&#19968;&#31995;&#21015;&#30456;&#20851;&#31867;&#21035;&#26631;&#31614;&#20316;&#20026;&#36755;&#20837;&#65292;MPVR&#33258;&#21160;&#20135;&#29983;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#31867;&#21035;&#25552;&#31034;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11755v1 Announce Type: cross  Abstract: Prompt ensembling of Large Language Model (LLM) generated category-specific prompts has emerged as an effective method to enhance zero-shot recognition ability of Vision-Language Models (VLMs). To obtain these category-specific prompts, the present methods rely on hand-crafting the prompts to the LLMs for generating VLM prompts for the downstream tasks. However, this requires manually composing these task-specific prompts and still, they might not cover the diverse set of visual concepts and task-specific styles associated with the categories of interest. To effectively take humans out of the loop and completely automate the prompt generation process for zero-shot recognition, we propose Meta-Prompting for Visual Recognition (MPVR). Taking as input only minimal information about the target task, in the form of its short natural language description, and a list of associated class labels, MPVR automatically produces a diverse set of cat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.10795</link><description>&lt;p&gt;
&#20174;&#21333;&#35789;&#21040;&#36335;&#24452;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
From Words to Routes: Applying Large Language Models to Vehicle Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#24182;&#25552;&#20986;&#19968;&#31181;&#20351;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65288;&#20363;&#22914;&#25805;&#20316;&#21644;&#23548;&#33322;&#65289;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20854;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#12290;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#35753;&#25105;&#20204;&#24605;&#32771;&#65306;LLMs&#22312;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#35299;&#20915;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#19977;&#27493;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;21&#31181;&#21333;&#36710;&#25110;&#22810;&#36710;&#36335;&#24452;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22235;&#31181;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#27599;&#31181;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#30340;&#22522;&#26412;&#25552;&#31034;&#33539;&#20363;&#23545;&#20110;GPT-4&#25928;&#26524;&#26368;&#20339;&#65292;&#23454;&#29616;&#20102;56%&#30340;&#21487;&#34892;&#24615;&#65292;40%&#30340;&#20248;&#21270;&#24615;&#21644;53%&#30340;&#25928;&#29575;&#12290;&#31532;&#19977;&#65292;&#22522;&#20110;&#35266;&#23519;&#21040;LLMs&#21487;&#33021;&#26080;&#27861;&#22312;&#21021;&#22987;&#23581;&#35797;&#20013;&#25552;&#20379;&#27491;&#30830;&#35299;&#20915;&#26041;&#26696;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10795v1 Announce Type: cross  Abstract: LLMs have shown impressive progress in robotics (e.g., manipulation and navigation) with natural language task descriptions. The success of LLMs in these tasks leads us to wonder: What is the ability of LLMs to solve vehicle routing problems (VRPs) with natural language task descriptions? In this work, we study this question in three steps. First, we construct a dataset with 21 types of single- or multi-vehicle routing problems. Second, we evaluate the performance of LLMs across four basic prompt paradigms of text-to-code generation, each involving different types of text input. We find that the basic prompt paradigm, which generates code directly from natural language task descriptions, performs the best for GPT-4, achieving 56% feasibility, 40% optimality, and 53% efficiency. Third, based on the observation that LLMs may not be able to provide correct solutions at the initial attempt, we propose a framework that enables LLMs to refin
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;</title><link>https://arxiv.org/abs/2403.09032</link><description>&lt;p&gt;
CodeUltraFeedback&#65306;&#19968;&#31181;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09032
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#25991;&#26412;LLMs&#30340;&#36755;&#20986;&#12290;&#29616;&#26377;&#22522;&#20934;&#20208;&#36182;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#65292;&#26410;&#33021;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#21644;LLM&#36755;&#20986;&#20013;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#23545;LLM&#20559;&#22909;&#23545;&#40784;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeUltraFeedback&#65292;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#22797;&#26434;&#25351;&#20196;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;AI&#21453;&#39304;&#26469;&#35843;&#25972;&#21644;&#23545;&#40784;LLMs&#19982;&#32534;&#31243;&#20559;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;14&#31181;&#19981;&#21516;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#36827;&#34892;&#27880;&#37322;&#65292;&#20351;&#29992;GPT-3.5&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#26041;&#27861;&#20135;&#29983;&#25968;&#23383;&#21644;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CODAL-Bench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#19982;&#36825;&#20123;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07632</link><description>&lt;p&gt;
CardioGenAI&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#20943;&#23569;hERG&#27602;&#24615;&#30340;&#33647;&#29289;&#20877;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs for Reduced hERG Liability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07632
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CardioGenAI&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#23569;&#33647;&#29289;&#30340;hERG&#27963;&#24615;&#24182;&#20445;&#30041;&#33647;&#29702;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#35825;&#23548;&#30340;&#24515;&#33039;&#27602;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#19981;&#33391;&#21453;&#24212;&#65292;&#21253;&#25324;&#36890;&#36807;&#38459;&#28382;&#30005;&#21387;&#38376;&#25511;&#30340;hERG&#38078;&#31163;&#23376;&#36890;&#36947;&#23548;&#33268;&#29983;&#21629;&#23041;&#32961;&#30340;&#24515;&#24459;&#22833;&#24120;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26089;&#26399;&#38454;&#27573;&#37492;&#23450;hERG&#27963;&#24615;&#21270;&#21512;&#29289;&#30340;&#20808;&#36827;&#26041;&#27861;&#65292;&#20197;&#21450;&#20248;&#21270;&#21830;&#19994;&#21270;&#33647;&#29289;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CardioGenAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20877;&#35774;&#35745;&#24320;&#21457;&#20013;&#21644;&#24050;&#19978;&#24066;&#33647;&#29289;&#65292;&#20197;&#20943;&#23569;hERG&#27963;&#24615;&#21516;&#26102;&#20445;&#30041;&#20854;&#33647;&#29702;&#27963;&#24615;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29992;&#20110;&#39044;&#27979;hERG&#36890;&#36947;&#27963;&#24615;&#30340;&#26368;&#26032;&#21028;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;&#38048;&#31163;&#23376;&#36890;&#36947;NaV1.5&#21644;&#38041;&#31163;&#23376;&#36890;&#36947;CaV1.2&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35843;&#33410;&#30001;hERG&#36890;&#36947;&#38459;&#28382;&#24341;&#36215;&#30340;&#24515;&#24459;&#22833;&#24120;&#28508;&#22312;&#24433;&#21709;&#20013;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#21487;&#20197;se
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07632v1 Announce Type: new  Abstract: Drug-induced cardiotoxicity is a major health concern which can lead to serious adverse effects including life-threatening cardiac arrhythmias via the blockade of the voltage-gated hERG potassium ion channel. It is therefore of tremendous interest to develop advanced methods to identify hERG-active compounds in early stages of drug development, as well as to optimize commercially available drugs for reduced hERG activity. In this work, we present CardioGenAI, a machine learning-based framework for re-engineering both developmental and marketed drugs for reduced hERG activity while preserving their pharmacological activity. The framework incorporates novel state-of-the-art discriminative models for predicting hERG channel activity, as well as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to their potential implications in modulating the arrhythmogenic potential induced by hERG channel blockade. These models can also se
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04202</link><description>&lt;p&gt;
&#24322;&#36136;&#23398;&#20064;&#20195;&#29702;&#32676;&#20307;&#20013;&#36947;&#24503;&#34892;&#20026;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04202
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#35752;&#20102;&#19981;&#21516;&#36947;&#24503;&#31867;&#22411;&#30340;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#36947;&#24503;&#24322;&#36136;&#24615;&#21487;&#33021;&#23545;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#26085;&#30410;&#20851;&#27880;AI&#31995;&#32479;&#23433;&#20840;&#21644;&#23545;&#40784;&#24615;&#30340;&#38382;&#39064;&#31361;&#26174;&#20102;&#22312;&#20154;&#24037;&#20195;&#29702;&#20013;&#23884;&#20837;&#36947;&#24503;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#32463;&#39564;&#23398;&#20064;&#65292;&#21363;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#22810;&#20195;&#29702;&#65288;&#31038;&#20250;&#65289;&#29615;&#22659;&#20013;&#65292;&#20010;&#20307;&#23398;&#20064;&#20195;&#29702;&#20043;&#38388;&#30340;&#20132;&#20114;&#21487;&#33021;&#20135;&#29983;&#22797;&#26434;&#30340;&#32676;&#20307;&#23618;&#38754;&#29616;&#35937;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#27169;&#25311;&#30340;&#31038;&#20250;&#22256;&#22659;&#29615;&#22659;&#26469;&#30740;&#31350;&#29420;&#31435;&#23398;&#20064;&#20195;&#29702;&#30340;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#23454;&#36341;&#20013;&#20195;&#29702;&#31038;&#20250;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#36947;&#24503;&#24322;&#36136;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#65292;&#21333;&#20010;&#23398;&#20064;&#20195;&#29702;&#21487;&#33021;&#38754;&#23545;&#21518;&#26524;&#20027;&#20041;&#32773;&#65288;&#21363;&#20851;&#24515;&#38543;&#26102;&#38388;&#26368;&#22823;&#21270;&#26576;&#31181;&#32467;&#26524;&#65289;&#25110;&#22522;&#20110;&#35268;&#33539;&#30340;&#23545;&#25163;&#65288;&#21363;&#19987;&#27880;&#20110;&#31435;&#21363;&#36981;&#23432;&#29305;&#23450;&#35268;&#33539;&#65289; &#12290;&#20195;&#29702;&#30340;&#20849;&#21516;&#21457;&#23637;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#36825;&#31181;&#36947;&#24503;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#22303;&#26408;&#24037;&#31243;&#39046;&#22495;&#30340;&#37319;&#29992;&#31574;&#30053;&#38656;&#35201;&#37325;&#26032;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#38454;&#27573;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#20998;&#25955;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02426</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#21644;&#22303;&#26408;&#24037;&#31243;&#38454;&#27573;&#65306;&#37325;&#26032;&#23450;&#20301;&#37319;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Digital Twins and Civil Engineering Phases: Reorienting Adoption Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02426
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#22312;&#22303;&#26408;&#24037;&#31243;&#39046;&#22495;&#30340;&#37319;&#29992;&#31574;&#30053;&#38656;&#35201;&#37325;&#26032;&#23450;&#20301;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#38454;&#27573;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#20998;&#25955;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#25216;&#26415;&#22810;&#24180;&#26469;&#21463;&#21040;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20026;&#31185;&#23398;&#21644;&#24037;&#31243;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#20102;&#35768;&#22810;&#25215;&#35834;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#25506;&#35752;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#19981;&#21516;&#20027;&#39064;&#39046;&#22495;&#12290;&#22303;&#26408;&#24037;&#31243;&#31561;&#29305;&#23450;&#39046;&#22495;&#20063;&#19981;&#20363;&#22806;&#65292;&#23548;&#33268;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#24212;&#29992;&#30340;&#30862;&#29255;&#21270;&#26041;&#27861;&#12290;&#22303;&#26408;&#24037;&#31243;&#34892;&#19994;&#22312;&#36825;&#26041;&#38754;&#36827;&#19968;&#27493;&#22788;&#20110;&#19981;&#21033;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20854;&#20182;&#24037;&#31243;&#39046;&#22495;&#30340;&#22806;&#37096;&#25216;&#26415;&#26469;&#36827;&#34892;&#25968;&#23383;&#23402;&#29983;&#30340;&#37319;&#29992;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#21152;&#30340;&#21518;&#26524;&#26159;&#23558;&#25968;&#23383;&#23402;&#29983;&#38598;&#20013;&#24212;&#29992;&#20110;&#36816;&#33829;&#21644;&#32500;&#25252;&#38454;&#27573;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24314;&#31569;&#20449;&#24687;&#24314;&#27169;&#65288;BIM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35268;&#21010;/&#35774;&#35745;&#38454;&#27573;&#65292;&#32780;&#26045;&#24037;&#38454;&#27573;&#30340;&#30636;&#21464;&#24615;&#36136;&#23545;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#37319;&#29992;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#22312;&#24314;&#31569;&#35774;&#35745;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02426v1 Announce Type: cross  Abstract: Digital twin (DT) technology has received immense attention over the years due to the promises it presents to various stakeholders in science and engineering. As a result, different thematic areas of DT have been explored. This is no different in specific fields such as manufacturing, automation, oil and gas, and civil engineering, leading to fragmented approaches for field-specific applications. The civil engineering industry is further disadvantaged in this regard as it relies on external techniques by other engineering fields for its DT adoption. A rising consequence of these extensions is a concentrated application of DT to the operations and maintenance phase. On another spectrum, Building Information Modeling (BIM) are pervasively utilized in the planning/design phase, and the transient nature of the construction phase remains a challenge for its DT adoption. In this paper, we present a phase-based development of DT in the Archit
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14049</link><description>&lt;p&gt;
&#29992;&#20110;&#26497;&#31471;&#25968;&#25454;&#32553;&#25918;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Models for Extreme Downscaling of Climate Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14049
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#25361;&#25112;&#38656;&#35201;&#20934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#22320;&#26144;&#23556;&#27668;&#20505;&#21644;&#22825;&#27668;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#21482;&#33021;&#20197;&#38750;&#24120;&#31895;&#31961;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25552;&#20379;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#26497;&#39640;&#30340;&#35745;&#31639;&#38656;&#27714;&#25152;&#33268;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25552;&#21319;&#33258;&#28982;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#22312;&#25913;&#36827;&#31185;&#23398;&#25968;&#25454;&#38598;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;GAN&#30340;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#32553;&#25918;&#26041;&#27861;&#65292;&#29992;&#20110;&#26497;&#31471;&#32553;&#25918;&#32593;&#26684;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#38750;&#24120;&#20302;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#20934;&#30830;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14049v1 Announce Type: cross  Abstract: Addressing the challenges of climate change requires accurate and high-resolution mapping of climate and weather variables. However, many existing climate datasets, such as the gridded outputs of the state-of-the-art numerical climate models (e.g., general circulation models), are only available at very coarse spatial resolutions due to the model complexity and extremely high computational demand. Deep-learning-based methods, particularly generative adversarial networks (GANs) and their variants, have proved effective for refining natural images, and have shown great promise in improving scientific datasets. In this paper, we describe a conditional GAN-based geospatial downscaling method for extreme downscaling of gridded climate datasets. Compared to most existing methods, the method can generate high-resolution accurate climate datasets from very low-resolution inputs. More importantly, the method explicitly considers the uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07118</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#24110;&#21161;&#36828;&#31243;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#26126;&#21644;&#20854;&#20182;&#30524;&#37096;&#30142;&#30149;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#21360;&#24230;&#31561;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#25104;&#20026;&#19968;&#31181;&#29983;&#21629;&#32447;&#65292;&#24182;&#19988;&#26234;&#33021;&#25163;&#26426;&#30524;&#37096;&#25104;&#20687;&#30340; Grabi &#38468;&#20214;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#25293;&#25668;&#30340;&#22270;&#29255;&#36136;&#37327;&#24448;&#24448;&#19981;&#22815;&#22909;&#65292;&#38656;&#35201;&#21307;&#29983;&#23457;&#26680;&#24182;&#19988;&#20250;&#24310;&#35823;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#33021;&#22815;&#27169;&#25311;&#21307;&#29983;&#30340;&#21028;&#26029;&#24182;&#19988;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#65292;&#25105;&#20204;&#23545;&#24739;&#32773;&#25293;&#25668;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#22797;&#26434;&#38382;&#39064;&#23618;&#27425;&#21270;&#65292;&#25105;&#20204;&#22312;&#27492;&#35299;&#20915;&#20102;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
&lt;/p&gt;</description></item><item><title>LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06859</link><description>&lt;p&gt;
LiRank: &#39046;&#33521;&#30340;&#24037;&#19994;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiRank: Industrial Large Scale Ranking Models at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06859
&lt;/p&gt;
&lt;p&gt;
LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LiRank&#65292;&#36825;&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#23558;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24314;&#27169;&#25913;&#36827;&#65292;&#21253;&#25324;Residual DCN&#65292;&#23427;&#22312;&#33879;&#21517;&#30340;DCNv2&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#23558;SOTA&#26550;&#26500;&#32452;&#21512;&#21644;&#35843;&#20248;&#20197;&#21019;&#24314;&#32479;&#19968;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;Dense Gating&#12289;Transformers&#21644;Residual DCN&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#26657;&#20934;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25506;&#32034;/&#21033;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;&#30340;&#26377;&#25928;&#12289;&#29983;&#20135;&#32423;&#26381;&#21153;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#21270;&#21644;&#35789;&#27719;&#21387;&#32553;&#35757;&#32451;&#21644;&#21387;&#32553;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Feed&#25490;&#21517;&#12289;&#32844;&#20301;&#25512;&#33616;&#21644;&#24191;&#21578;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#31561;&#22823;&#35268;&#27169;&#20351;&#29992;&#26696;&#20363;&#30340;&#37096;&#32626;&#35774;&#32622;&#32454;&#33410;&#12290;&#36890;&#36807;&#38416;&#26126;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#31181;A/B&#27979;&#35797;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;</title><link>https://arxiv.org/abs/2310.05707</link><description>&lt;p&gt;
&#29992;&#35268;&#21010;&#26631;&#35760;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Guiding Language Model Math Reasoning with Planning Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20445;&#25345;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#65292;&#32780;&#22686;&#21152;&#30340;&#35757;&#32451;&#21442;&#25968;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#65288;&#22914;&#24605;&#32500;&#38142;&#25512;&#29702;&#65289;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24573;&#35270;&#20102;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#26500;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;LLMs&#21487;&#20197;&#24456;&#22909;&#22320;&#22788;&#29702;&#20010;&#21035;&#25512;&#29702;&#27493;&#39588;&#65292;&#20294;&#22312;&#25972;&#20010;&#25512;&#29702;&#38142;&#19978;&#20445;&#25345;&#19968;&#33268;&#24615;&#26041;&#38754;&#21364;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#25512;&#29702;&#27493;&#39588;&#30340;&#24320;&#22987;&#22788;&#24341;&#20837;&#35268;&#21010;&#26631;&#35760;&#65292;&#20316;&#20026;&#27169;&#22411;&#30340;&#24341;&#23548;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#23884;&#20837;&#28155;&#21152;&#21040;&#27169;&#22411;&#21442;&#25968;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#22686;&#21152;&#38750;&#24120;&#23567;&#65288;&#20165;&#20026;0.001%&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#23436;&#20840;&#24494;&#35843;&#25110;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#26041;&#26696;&#26469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;LLMs&#65292;&#22312;&#19977;&#20010;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26631;&#20934;&#26041;&#27861;&#65292;&#20934;&#30830;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. We find that while LLMs can manage individual reasoning steps well, they struggle with maintaining consistency across an entire reasoning chain. To solve this, we introduce planning tokens at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (just 0.001%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. standard f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;</title><link>https://arxiv.org/abs/2302.01089</link><description>&lt;p&gt;
Curriculum Learning&#29992;&#20110;&#20174;&#22836;&#24320;&#22987;&#30340;&#28145;&#24230;&#23398;&#20064;&#25240;&#23556;&#20809;&#23398;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning for ab initio Deep Learned Refractive Optics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01089
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Curriculum Learning&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20809;&#23398;&#20248;&#21270;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;&#20351;&#29992;&#36755;&#20986;&#22270;&#20687;&#20316;&#20026;&#30446;&#26631;&#30340;&#35745;&#31639;&#25104;&#20687;&#31995;&#32479;&#35774;&#35745;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#30446;&#21069;&#34987;&#38480;&#21046;&#20110;&#21333;&#20010;&#20803;&#32032;&#65288;&#22914;&#34893;&#23556;&#20809;&#23398;&#20803;&#20214;&#65288;DOE&#65289;&#25110;&#37329;&#23646;&#36879;&#38236;&#65289;&#26500;&#25104;&#30340;&#31616;&#21333;&#20809;&#23398;&#31995;&#32479;&#65292;&#25110;&#32773;&#20174;&#33391;&#22909;&#30340;&#21021;&#22987;&#35774;&#35745;&#24494;&#35843;&#22797;&#21512;&#36879;&#38236;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Curriculum Learning&#30340;DeepLens&#35774;&#35745;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#34920;&#38754;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#22797;&#21512;&#36879;&#38236;&#30340;&#20809;&#23398;&#35774;&#35745;&#65292;&#32780;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#65292;&#22240;&#27492;&#20811;&#26381;&#20102;&#23545;&#33391;&#22909;&#21021;&#22987;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#33258;&#21160;&#35774;&#35745;&#32463;&#20856;&#25104;&#20687;&#36879;&#38236;&#21644;&#19968;&#20010;&#31867;&#20284;&#25163;&#26426;&#39118;&#26684;&#30340;&#22823;&#35270;&#22330;&#24310;&#20280;&#26223;&#28145;&#35745;&#31639;&#36879;&#38236;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#39640;&#24230;&#38750;&#29699;&#38754;&#26354;&#38754;&#21644;&#30701;&#21518;&#28966;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01089v3 Announce Type: replace-cross  Abstract: Deep optical optimization has recently emerged as a new paradigm for designing computational imaging systems using only the output image as the objective. However, it has been limited to either simple optical systems consisting of a single element such as a diffractive optical element (DOE) or metalens, or the fine-tuning of compound lenses from good initial designs. Here we present a DeepLens design method based on curriculum learning, which is able to learn optical designs of compound lenses ab initio from randomly initialized surfaces without human intervention, therefore overcoming the need for a good initial design. We demonstrate the effectiveness of our approach by fully automatically designing both classical imaging lenses and a large field-of-view extended depth-of-field computational lens in a cellphone-style form factor, with highly aspheric surfaces and a short back focal length.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;SpecSTG&#65292;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.08119</link><description>&lt;p&gt;
SpecSTG:&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#30340;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting. (arXiv:2401.08119v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08119
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35889;&#25193;&#25955;&#26694;&#26550;SpecSTG&#65292;&#29992;&#20110;&#27010;&#29575;&#26102;&#31354;&#20132;&#36890;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#20256;&#32479;&#19978;&#20381;&#36182;&#30830;&#23450;&#24615;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#30340;&#28857;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#35782;&#21035;&#26410;&#26469;&#35266;&#27979;&#20013;&#24847;&#22806;&#27874;&#21160;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27010;&#29575;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25193;&#25955;&#27169;&#22411;&#30340;&#21464;&#31181;&#65292;&#24050;&#25104;&#20026;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#26041;&#27861;&#36890;&#24120;&#21482;&#20851;&#27880;&#20026;&#20132;&#36890;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#20256;&#24863;&#22120;&#29983;&#25104;&#21333;&#29420;&#30340;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#65292;&#23548;&#33268;&#31354;&#38388;&#32593;&#32476;&#29305;&#24449;&#22312;&#27010;&#29575;&#23398;&#20064;&#36807;&#31243;&#20013;&#21442;&#19982;&#19981;&#36275;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#20132;&#36890;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#31995;&#32479;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35889;&#25193;&#25955;&#26694;&#26550;&#8212;&#8212;SpecSTG&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#30340;&#20613;&#37324;&#21494;&#34920;&#31034;&#65292;&#23558;&#23398;&#20064;&#36807;&#31243;&#36716;&#21270;&#20026;&#20805;&#28385;&#31354;&#38388;&#20449;&#24687;&#30340;&#35889;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#31354;&#38388;&#32593;&#32476;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting, a crucial application of spatio-temporal graph (STG) learning, has traditionally relied on deterministic models for accurate point estimations. Yet, these models fall short of identifying latent risks of unexpected volatility in future observations. To address this gap, probabilistic methods, especially variants of diffusion models, have emerged as uncertainty-aware solutions. However, existing diffusion methods typically focus on generating separate future time series for individual sensors in the traffic network, resulting in insufficient involvement of spatial network characteristics in the probabilistic learning process. To better leverage spatial dependencies and systematic patterns inherent in traffic data, we propose SpecSTG, a novel spectral diffusion framework. Our method generates the Fourier representation of future time series, transforming the learning process into the spectral domain enriched with spatial information. Additionally, our approach incorp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;</title><link>http://arxiv.org/abs/2312.01185</link><description>&lt;p&gt;
&#26102;&#38388;&#20013;&#30340;&#28063;&#28458;&#65306;&#32654;&#22269;&#21382;&#21490;&#20013;&#30340;&#19981;&#36830;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
A ripple in time: a discontinuity in American history. (arXiv:2312.01185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01185
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#21644;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#65292;&#21457;&#29616;GPT-2&#19982;UMAP&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#32858;&#31867;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#21487;&#29992;&#20110;&#35782;&#21035;&#24635;&#32479;&#21644;&#28436;&#35762;&#30340;&#24180;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Kaggle&#30340;&#22269;&#24773;&#21672;&#25991;&#25968;&#25454;&#38598;&#23545;&#32654;&#22269;&#21382;&#21490;&#30340;&#24635;&#20307;&#26102;&#38388;&#32447;&#21450;&#21672;&#25991;&#26412;&#36523;&#30340;&#29305;&#28857;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#65288;&#20063;&#26377;&#20123;&#19981;&#37027;&#20040;&#20196;&#20154;&#24778;&#35766;&#65289;&#30340;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#65292;&#22914;BERT&#65288;DistilBERT&#65289;&#21644;GPT-2&#12290;&#34429;&#28982;&#24191;&#27867;&#35748;&#20026;BERT&#65288;&#21450;&#20854;&#21464;&#20307;&#65289;&#26368;&#36866;&#21512;NLP&#20998;&#31867;&#20219;&#21153;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;GPT-2&#32467;&#21512;UMAP&#31561;&#38750;&#32447;&#24615;&#38477;&#32500;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31163;&#21644;&#26356;&#24378;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;&#36825;&#20351;&#24471;GPT-2 + UMAP&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#23601;&#36275;&#22815;&#22909;&#29992;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;DistilBERT&#27169;&#22411;&#26469;&#26816;&#27979;&#21738;&#20301;&#24635;&#32479;&#21457;&#34920;&#20102;&#21738;&#31687;&#28436;&#35762;&#65292;&#24182;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#65288;&#20934;&#30830;&#29575;&#20026;93\% - 95\%&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#36816;&#34892;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#30830;&#23450;&#20889;&#20316;&#24180;&#20221;&#65292;&#25105;&#20204;&#36824;&#25191;&#34892;&#20102;&#19968;&#20010;&#31867;&#20284;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note we use the State of the Union Address (SOTU) dataset from Kaggle to make some surprising (and some not so surprising) observations pertaining to the general timeline of American history, and the character and nature of the addresses themselves. Our main approach is using vector embeddings, such as BERT (DistilBERT) and GPT-2.  While it is widely believed that BERT (and its variations) is most suitable for NLP classification tasks, we find out that GPT-2 in conjunction with nonlinear dimension reduction methods such as UMAP provide better separation and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In our case, no model fine-tuning is required, and the pre-trained out-of-the-box GPT-2 model is enough.  We also used a fine-tuned DistilBERT model for classification detecting which President delivered which address, with very good results (accuracy 93\% - 95\% depending on the run). An analogous task was performed to determine the year of writing, an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.12294</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20551;&#35774;&#26377;&#27491;&#24120;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19968;&#20123;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#21152;&#20837;&#26631;&#35760;&#30340;&#24322;&#24120;&#26679;&#26412;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24322;&#24120;&#31867;&#22411;&#23545;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#35828;&#22312;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#30417;&#30563;&#26041;&#27861;&#20165;&#33021;&#26816;&#27979;&#31867;&#20284;&#20110;&#35757;&#32451;&#26399;&#38388;&#23384;&#22312;&#30340;&#24322;&#24120;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#24322;&#24120;&#31867;&#21035;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#30475;&#21040;&#26469;&#33258;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#26088;&#22312;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22810;&#21464;&#37327;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;MOSAD&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three prim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.12754</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#22312;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#36890;&#36807;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Linear Feature Learning in Regression Through Regularisation. (arXiv:2307.12754v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#20110;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#27979;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#25552;&#21319;&#26159;&#38750;&#24120;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#29305;&#24449;&#36873;&#25321;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#65292;&#38750;&#21442;&#25968;&#26041;&#27861;&#24120;&#24120;&#24456;&#38590;&#24212;&#23545;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#30456;&#20851;&#20449;&#24687;&#23384;&#22312;&#20110;&#25968;&#25454;&#30340;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#65292;&#21363;&#22810;&#25351;&#25968;&#27169;&#22411;&#12290;&#22914;&#26524;&#24050;&#30693;&#35813;&#23376;&#31354;&#38388;&#65292;&#23558;&#22823;&#22823;&#22686;&#24378;&#39044;&#27979;&#12289;&#35745;&#31639;&#21644;&#35299;&#37322;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#39044;&#27979;&#30340;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20272;&#35745;&#39044;&#27979;&#20989;&#25968;&#21644;&#32447;&#24615;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65292;&#24182;&#21152;&#19978;&#20989;&#25968;&#23548;&#25968;&#30340;&#24809;&#32602;&#39033;&#65292;&#20197;&#20445;&#35777;&#20854;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;Hermite&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#24615;&#21644;&#26059;&#36716;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;RegFeaL&#12290;&#36890;&#36807;&#21033;&#29992;&#26367;&#20195;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#26059;&#36716;&#25968;&#25454;&#20197;&#25913;&#21892;&#19982;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning plays a crucial role in automated feature selection, particularly in the context of high-dimensional data, where non-parametric methods often struggle. In this study, we focus on supervised learning scenarios where the pertinent information resides within a lower-dimensional linear subspace of the data, namely the multi-index model. If this subspace were known, it would greatly enhance prediction, computation, and interpretation. To address this challenge, we propose a novel method for linear feature learning with non-parametric prediction, which simultaneously estimates the prediction function and the linear subspace. Our approach employs empirical risk minimisation, augmented with a penalty on function derivatives, ensuring versatility. Leveraging the orthogonality and rotation invariance properties of Hermite polynomials, we introduce our estimator, named RegFeaL. By utilising alternative minimisation, we iteratively rotate the data to improve alignment with 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.07350</link><description>&lt;p&gt;
G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
G-invariant diffusion maps. (arXiv:2306.07350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07350
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26500;&#36896;&#20102;&#38024;&#23545;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;G-&#19981;&#21464;&#25193;&#25955;&#22320;&#22270;&#65292;&#33021;&#22815;&#23454;&#29616;&#31561;&#21464;&#19988;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25193;&#25955;&#22320;&#22270;&#22312;&#20174;&#38477;&#32500;&#21644;&#32858;&#31867;&#21040;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#36830;&#32493;&#30697;&#38453;&#32676;&#23553;&#38381;&#19979;&#30340;&#27969;&#24418;&#20013;&#37319;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#26082;&#31561;&#21464;&#21448;&#19981;&#21464;&#30340;&#23884;&#20837;&#65292;&#36825;&#20123;&#23884;&#20837;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26500;&#36896;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diffusion maps embedding of data lying on a manifold have shown success in tasks ranging from dimensionality reduction and clustering, to data visualization. In this work, we consider embedding data sets which were sampled from a manifold which is closed under the action of a continuous matrix group. An example of such a data set are images who's planar rotations are arbitrary. The G-invariant graph Laplacian, introduced in a previous work of the authors, admits eigenfunctions in the form of tensor products between the elements of the irreducible unitary representations of the group and eigenvectors of certain matrices. We employ these eigenfunctions to derive diffusion maps that intrinsically account for the group action on the data. In particular, we construct both equivariant and invariant embeddings which can be used naturally to cluster and align the data points. We demonstrate the effectiveness of our construction with simulated data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18381</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#30719;&#30707;&#20013;&#25552;&#28860;&#40644;&#37329;: &#22522;&#20110;&#20851;&#38190;&#26679;&#26412;&#36873;&#25321;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18381
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#20197;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#25299;&#23637;&#23545;&#26356;&#22823;&#26356;&#22810;&#20803;&#21270;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#24182;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25928;&#29575;&#23398;&#20064;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#22810;&#27169;&#22411;&#30340;&#29616;&#22312;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#21487;&#20197;&#25104;&#20026;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#38598;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#20173;&#28982;&#38750;&#24120;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#29992;&#20449;&#24687;&#29702;&#35770;&#26469;&#24314;&#27169;&#33976;&#39311;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#25968;&#25454;&#20887;&#20313;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25193;&#23637;&#29616;&#26377;&#30340;&#33976;&#39311;&#31639;&#27861;&#65292;&#20197;&#20415;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#26679;&#26412;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26679;&#26412;&#36873;&#25321;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#21270;&#36807;&#31243;&#12290;&#36825;&#31181;&#26032;&#31574;&#30053;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#25193;&#22823;&#29616;&#26377;&#31639;&#27861;&#33539;&#22260;&#20197;&#23545;&#26356;&#24222;&#22823;&#21644;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#38598;&#33976;&#39311;&#65292;&#20363;&#22914;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;0.04&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#23601;&#36275;&#20197;&#20445;&#25345;&#21487;&#27604;&#30340;&#33976;&#39311;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#20854;&#36129;&#29486;&#21487;&#33021;&#20026;&#33976;&#39311;&#36807;&#31243;&#30340;&#21160;&#21147;&#23398;&#24320;&#36767;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-efficient learning has drawn significant attention, especially given the current trend of large multi-modal models, where dataset distillation can be an effective solution. However, the dataset distillation process itself is still very inefficient. In this work, we model the distillation problem with reference to information theory. Observing that severe data redundancy exists in dataset distillation, we argue to put more emphasis on the utility of the training samples. We propose a family of methods to exploit the most valuable samples, which is validated by our comprehensive analysis of the optimal data selection. The new strategy significantly reduces the training cost and extends a variety of existing distillation algorithms to larger and more diversified datasets, e.g. in some cases only 0.04% training data is sufficient for comparable distillation performance. Moreover, our strategy consistently enhances the performance, which may open up new analyses on the dynamics of dist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06237</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Arrhythmia Classification-Guided Segmentation Model for Electrocardiogram Delineation. (arXiv:2304.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#25351;&#23548;&#30340;&#24515;&#30005;&#22270;&#20998;&#21106;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#21010;&#20998;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#21010;&#20998;ECG&#20013;&#30340;&#20851;&#38190;&#27874;&#24418;&#26159;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#20197;&#25903;&#25345;&#35786;&#26029;&#21644;&#27835;&#30103;&#24515;&#33039;&#30142;&#30149;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#34429;&#28982;&#21033;&#29992;&#20998;&#21106;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23450;&#20301;P&#12289;QRS&#21644;T&#27874;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22788;&#29702;&#21576;&#29616;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20855;&#26377;&#24191;&#27867;&#24515;&#24459;&#22833;&#24120;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#65292;&#23558;&#20998;&#21106;&#19982;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#21253;&#21547;&#21508;&#31181;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#38598;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#24191;&#27867;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20934;&#30830;&#21010;&#20998;&#20102;&#24191;&#27867;&#24322;&#24120;&#33410;&#24459;&#31867;&#22411;&#30340;&#20449;&#21495;&#65292;&#21516;&#26102;&#32467;&#21512;&#20998;&#31867;&#25351;&#23548;&#30340;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#34394;&#25253;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate delineation of key waveforms in an ECG is a critical initial step in extracting relevant features to support the diagnosis and treatment of heart conditions. Although deep learning based methods using a segmentation model to locate P, QRS and T waves have shown promising results, their ability to handle signals exhibiting arrhythmia remains unclear. In this study, we propose a novel approach that leverages a deep learning model to accurately delineate signals with a wide range of arrhythmia. Our approach involves training a segmentation model using a hybrid loss function that combines segmentation with the task of arrhythmia classification. In addition, we use a diverse training set containing various arrhythmia types, enabling our model to handle a wide range of challenging cases. Experimental results show that our model accurately delineates signals with a broad range of abnormal rhythm types, and the combined training with classification guidance can effectively reduce fals
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#32467;&#26500;&#22797;&#26434;&#21644;&#38598;&#20307;&#35268;&#27169;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2302.04262</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Collective Action in Machine Learning. (arXiv:2302.04262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#32467;&#26500;&#22797;&#26434;&#21644;&#38598;&#20307;&#35268;&#27169;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22312;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23383;&#24179;&#21488;&#19978;&#36827;&#34892;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#36827;&#34892;&#20102;&#21407;&#21017;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#25551;&#36848;&#20102;&#19968;&#32676;&#20154;&#19982;&#20844;&#21496;&#30340;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20132;&#20114;&#30340;&#24773;&#20917;&#12290;&#38598;&#20307;&#27719;&#32858;&#21442;&#19982;&#20010;&#20307;&#30340;&#25968;&#25454;&#24182;&#36890;&#36807;&#19968;&#31181;&#31639;&#27861;&#31574;&#30053;&#25351;&#23548;&#21442;&#19982;&#32773;&#20462;&#25913;&#33258;&#24049;&#30340;&#25968;&#25454;&#20197;&#23454;&#29616;&#38598;&#20307;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#22522;&#26412;&#30340;&#23398;&#20064;&#29702;&#35770;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#31181;&#27169;&#22411;&#30340;&#32467;&#26524;&#65306;&#38750;&#21442;&#25968;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#21442;&#25968;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21327;&#35843;&#30340;&#31639;&#27861;&#31574;&#30053;&#65292;&#24182;&#26681;&#25454;&#38598;&#20307;&#35268;&#27169;&#30340;&#22823;&#23567;&#26469;&#34920;&#24449;&#33258;&#28982;&#30340;&#25104;&#21151;&#26631;&#20934;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#23545;&#28041;&#21450;&#25968;&#20197;&#19975;&#35745;&#33258;&#30001;&#32844;&#19994;&#24179;&#21488;&#31616;&#21382;&#30340;&#25216;&#33021;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#12290;&#36890;&#36807; BERT &#27169;&#22411;&#30340;&#20004;&#21315;&#22810;&#27425;&#35757;&#32451;&#36816;&#34892;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#27604;&#38598;&#20013;&#24335;&#23398;&#20064;&#31639;&#27861;&#21644;&#29420;&#31435;&#20462;&#25913;&#25968;&#25454;&#30340;&#38750;&#21327;&#35843;&#26041;&#27861;&#35201;&#22909;&#24471;&#22810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#31639;&#27861;&#38598;&#20307;&#34892;&#21160;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#20381;&#36182;&#20110;&#38598;&#20307;&#30340;&#35268;&#27169;&#21644;&#25968;&#25454;&#30340;&#22522;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a principled study of algorithmic collective action on digital platforms that deploy machine learning algorithms. We propose a simple theoretical model of a collective interacting with a firm's learning algorithm. The collective pools the data of participating individuals and executes an algorithmic strategy by instructing participants how to modify their own data to achieve a collective goal. We investigate the consequences of this model in three fundamental learning-theoretic settings: the case of a nonparametric optimal learning algorithm, a parametric risk minimizer, and gradient-based optimization. In each setting, we come up with coordinated algorithmic strategies and characterize natural success criteria as a function of the collective's size. Complementing our theory, we conduct systematic experiments on a skill classification task involving tens of thousands of resumes from a gig platform for freelancers. Through more than two thousand model training runs of a BERT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2106.11760</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Fingerprinting Generative Adversarial Networks. (arXiv:2106.11760v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#25351;&#32441;&#26679;&#26412;&#24182;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#29256;&#26435;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#65292;&#20855;&#26377;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#21830;&#19994;GAN&#30340;&#29983;&#20135;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#65292;&#22240;&#27492;&#36843;&#20999;&#38656;&#35201;&#29256;&#26435;&#20445;&#25252;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20445;&#25252;GAN&#30693;&#35782;&#20135;&#26435;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#26696;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#21069;&#19968;&#31181;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#25351;&#32441;&#35782;&#21035;&#26041;&#27861;&#22312;&#31616;&#21333;&#36716;&#31227;&#33267;GAN&#26102;&#25152;&#36935;&#21040;&#30340;&#38544;&#34109;&#24615;&#21644;&#40065;&#26834;&#24615;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21019;&#36896;&#24615;&#22320;&#20174;&#30446;&#26631;GAN&#21644;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#22797;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#22797;&#21512;&#27169;&#22411;&#20013;&#20135;&#29983;&#25351;&#32441;&#26679;&#26412;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#20998;&#31867;&#22120;&#20013;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#29256;&#26435;&#39564;&#35777;&#12290;&#36825;&#31181;&#26041;&#26696;&#21551;&#21457;&#20102;&#19968;&#20123;&#20855;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#38469;&#20445;&#25252;&#29616;&#20195;GAN&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#28385;&#36275;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#25152;&#38656;&#35201;&#30340;&#19981;&#21516;&#23433;&#20840;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#26696;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) have been widely used in various application scenarios. Since the production of a commercial GAN requires substantial computational and human resources, the copyright protection of GANs is urgently needed. In this paper, we present the first fingerprinting scheme for the Intellectual Property (IP) protection of GANs. We break through the stealthiness and robustness bottlenecks suffered by previous fingerprinting methods for classification models being naively transferred to GANs. Specifically, we innovatively construct a composite deep learning model from the target GAN and a classifier. Then we generate fingerprint samples from this composite model, and embed them in the classifier for effective ownership verification. This scheme inspires some concrete methodologies to practically protect the modern GAN models. Theoretical analysis proves that these methods can satisfy different security requirements necessary for IP protection. We also conduct 
&lt;/p&gt;</description></item></channel></rss>