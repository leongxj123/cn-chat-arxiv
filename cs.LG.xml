<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;</title><link>https://arxiv.org/abs/2404.02254</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#27169;&#24577;&#19982;&#21333;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#26356;&#24378;&#30340;&#35745;&#31639;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Stronger Computational Separations Between Multimodal and Unimodal Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02254
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26356;&#24378;&#30340;&#24179;&#22343;&#24773;&#20917;&#35745;&#31639;&#20998;&#31163;&#65292;&#23545;&#20110;&#8220;&#20856;&#22411;&#8221;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#20219;&#21153;&#23454;&#20363;&#65292;&#21333;&#27169;&#24577;&#23398;&#20064;&#22312;&#35745;&#31639;&#19978;&#26159;&#22256;&#38590;&#30340;&#65292;&#20294;&#22810;&#27169;&#24577;&#23398;&#20064;&#21364;&#24456;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#22810;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#32467;&#21512;&#36215;&#26469;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#36825;&#20173;&#28982;&#36866;&#29992;&#20110;&#30456;&#24212;&#30340;&#21333;&#27169;&#24577;&#20219;&#21153;&#65288;&#20363;&#22914;&#25991;&#26412;&#29983;&#25104;&#65289;&#12290;&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#32463;&#39564;&#25104;&#21151;&#65288;&#20363;&#22914;GPT-4&#65289;&#12290;&#21463;&#21040;&#20026;&#36825;&#31181;&#32463;&#39564;&#25104;&#21151;&#24320;&#21457;&#29702;&#35770;&#22522;&#30784;&#30340;&#21160;&#26426;&#65292;Lu&#65288;NeurIPS '23&#65292;ALT '24&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#27169;&#24577;&#21644;&#21333;&#27169;&#24577;&#23398;&#20064;&#30340;&#29702;&#35770;&#27169;&#22411;&#20043;&#38388;&#21487;&#33021;&#30340;&#20998;&#31163;&#12290;&#29305;&#21035;&#26159;Lu&#65288;ALT '24&#65289;&#23637;&#31034;&#20102;&#19968;&#31181;&#35745;&#31639;&#20998;&#31163;&#65292;&#36825;&#23545;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#22351;&#24773;&#20917;&#23454;&#20363;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02254v1 Announce Type: cross  Abstract: In multimodal machine learning, multiple modalities of data (e.g., text and images) are combined to facilitate the learning of a better machine learning model, which remains applicable to a corresponding unimodal task (e.g., text generation). Recently, multimodal machine learning has enjoyed huge empirical success (e.g. GPT-4). Motivated to develop theoretical justification for this empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal learning, and considers possible separations between theoretical models of multimodal and unimodal learning. In particular, Lu (ALT '24) shows a computational separation, which is relevant to worst-case instances of the learning task.   In this paper, we give a stronger average-case computational separation, where for "typical" instances of the learning task, unimodal learning is computationally hard, but multimodal learning is easy. We then question how "organic" the average-cas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00522</link><description>&lt;p&gt;
&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minimum-Norm Interpolation Under Covariate Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#29616;&#23454;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#31616;&#21333;&#35774;&#32622;&#20013;&#65292;&#22312;&#23545;&#36716;&#31227;&#23398;&#20064;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#20998;&#24067;&#30740;&#31350;&#20013;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#29616;&#35937;&#30340;&#29616;&#35937;&#65292;&#21363;&#32447;&#24615;&#25554;&#20540;&#22120;&#20250;&#23545;&#22122;&#22768;&#35757;&#32451;&#26631;&#31614;&#36807;&#25311;&#21512;&#65292;&#20294;&#20173;&#28982;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#21457;&#29983;&#22312;&#28304;&#21327;&#26041;&#24046;&#30697;&#38453;&#21644;&#36755;&#20837;&#25968;&#25454;&#32500;&#24230;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#12290;&#22240;&#27492;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#36825;&#26679;&#30340;&#39640;&#32500;&#32447;&#24615;&#27169;&#22411;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#22914;&#20309;&#34892;&#20026;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#35774;&#32622;&#20013;&#33391;&#24615;&#36807;&#25311;&#21512;&#32447;&#24615;&#25554;&#20540;&#22120;&#30340;&#31532;&#19968;&#20010;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36716;&#31227;&#23398;&#20064;&#20013;&#30340;\textit {b&#36827;&#34892;&#20998;&#31867;}}&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00522v1 Announce Type: new  Abstract: Transfer learning is a critical part of real-world machine learning deployments and has been extensively studied in experimental works with overparameterized neural networks. However, even in the simplest setting of linear regression a notable gap still exists in the theoretical understanding of transfer learning. In-distribution research on high-dimensional linear regression has led to the identification of a phenomenon known as \textit{benign overfitting}, in which linear interpolators overfit to noisy training labels and yet still generalize well. This behavior occurs under specific conditions on the source covariance matrix and input data dimension. Therefore, it is natural to wonder how such high-dimensional linear models behave under transfer learning. We prove the first non-asymptotic excess risk bounds for benignly-overfit linear interpolators in the transfer learning setting. From our analysis, we propose a taxonomy of \textit{b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.09857</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09857
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASP&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#27169;&#22411;&#26088;&#22312;&#22312;&#20445;&#30041;&#26087;&#31867;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#36880;&#27493;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#31232;&#32570;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;FSCIL&#26041;&#27861;&#36890;&#24120;&#23545;&#25972;&#20010;&#39592;&#24178;&#36827;&#34892;&#24494;&#35843;&#65292;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38459;&#30861;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#28508;&#21147;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#22522;&#20110;&#25552;&#31034;&#30340;CIL&#26041;&#27861;&#36890;&#36807;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#29992;&#36275;&#22815;&#30340;&#25968;&#25454;&#35757;&#32451;&#25552;&#31034;&#26469;&#20943;&#36731;&#36951;&#24536;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#27880;&#24847;&#21147;&#24863;&#30693;&#33258;&#36866;&#24212;&#25552;&#31034;&#65288;ASP&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;ASP&#36890;&#36807;&#20174;&#27880;&#24847;&#21147;&#26041;&#38754;&#20943;&#23569;&#29305;&#23450;&#20449;&#24687;&#65292;&#40723;&#21169;&#20219;&#21153;&#19981;&#21464;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#20849;&#20139;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;ASP&#20013;&#30340;&#33258;&#36866;&#24212;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#25552;&#20379;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#23398;&#20064;&#30446;&#26631;&#20174;&#26087;&#31867;&#21040;&#26032;&#31867;&#20256;&#36882;&#30693;&#35782;&#12290;&#24635;&#20043;&#65292;ASP&#38450;&#27490;&#20102;&#22312;&#22522;&#30784;&#20219;&#21153;&#19978;&#30340;&#36807;&#25311;&#21512;&#65292;&#24182;&#19981;&#38656;&#35201;&#22312;&#23569;&#26679;&#26412;&#22686;&#37327;&#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.03827</link><description>&lt;p&gt;
&#22312;L-BFGS-B&#31639;&#27861;&#30340;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#36827;&#34892;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21487;&#33021;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#38024;&#23545;&#32447;&#24615;&#27169;&#22411;&#30340;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#32463;&#20856;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22312;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20351;&#29992;&#26041;&#38754;&#26356;&#21152;&#36890;&#29992;&#65292;&#20063;&#22312;&#25968;&#20540;&#19978;&#26356;&#21152;&#31283;&#23450;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#20016;&#23500;&#20102;&#29616;&#26377;&#30340;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#38598;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#35782;&#21035;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38750;&#24120;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;Weigand&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#22522;&#20934;&#30340;&#38750;&#32447;&#24615;&#22810;&#36755;&#20837;/&#22810;&#36755;&#20986;&#31995;&#32479;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03827v1 Announce Type: cross  Abstract: In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python impleme
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.03281</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Credibility-Aware Multi-Modal Fusion Using Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#36827;&#34892;&#21487;&#20449;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65292;&#22312;&#32500;&#25345;&#31454;&#20105;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#21487;&#38752;&#25512;&#26029;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#38024;&#23545;&#36776;&#21035;&#23398;&#20064;&#30340;&#36831;&#21040;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#12290;&#21463;&#21040;&#38656;&#35201;&#29702;&#35299;&#27599;&#20010;&#25968;&#25454;&#28304;&#21487;&#38752;&#24615;&#30340;&#22024;&#26434;&#30340;&#22810;&#28304;&#39046;&#22495;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#21487;&#20449;&#24230;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#26469;&#32467;&#21512;&#20010;&#20307;&#27169;&#24577;&#19978;&#30340;&#39044;&#27979;&#20998;&#24067;&#30340;&#32452;&#21512;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#19968;&#31181;&#27010;&#29575;&#24230;&#37327;&#26469;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#30340;&#21487;&#20449;&#24230;&#65292;&#36890;&#36807;PC&#19978;&#30340;&#25512;&#29702;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34701;&#21512;&#26041;&#27861;&#33021;&#22815;&#21487;&#38752;&#22320;&#25512;&#26029;&#21487;&#20449;&#24230;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#20445;&#25345;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03281v1 Announce Type: cross  Abstract: We consider the problem of late multi-modal fusion for discriminative learning. Motivated by noisy, multi-source domains that require understanding the reliability of each data source, we explore the notion of credibility in the context of multi-modal fusion. We propose a combination function that uses probabilistic circuits (PCs) to combine predictive distributions over individual modalities. We also define a probabilistic measure to evaluate the credibility of each modality via inference queries over the PC. Our experimental evaluation demonstrates that our fusion method can reliably infer credibility while maintaining competitive performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.02920</link><description>&lt;p&gt;
TaylorShift&#65306;&#21033;&#29992;TaylorSoftmax&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#32423;&#36716;&#21464;&#20026;&#32447;&#24615;&#32423;&#65288;&#20877;&#36716;&#22238;&#21435;&#65289;
&lt;/p&gt;
&lt;p&gt;
TaylorShift: Shifting the Complexity of Self-Attention from Squared to Linear (and Back) using Taylor-Softmax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02920
&lt;/p&gt;
&lt;p&gt;
TaylorShift&#36890;&#36807;&#24341;&#20837;TaylorSoftmax&#37325;&#26032;&#35745;&#31639;&#20840;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#30001;&#24179;&#26041;&#32423;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26159;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31232;&#30095;&#34920;&#31034;&#25110;&#26377;&#29366;&#24577;&#30340;&#24490;&#29615;&#65292;&#29306;&#29298;&#20102;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#26368;&#32456;&#23548;&#33268;&#24615;&#33021;&#19978;&#30340;&#22949;&#21327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TaylorShift&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Taylor softmax &#37325;&#26500;&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#21644;&#31354;&#38388;&#20869;&#35745;&#31639;&#20840;&#20307;&#35760;&#21495;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#30830;&#23450;&#20102;&#20351;&#29992;TaylorShift&#27604;&#20256;&#32479;&#27880;&#24847;&#21147;&#26356;&#21152;&#39640;&#25928;&#30340;&#20132;&#21449;&#28857;&#65292;&#36825;&#19982;&#23454;&#35777;&#27979;&#37327;&#32467;&#26524;&#23494;&#20999;&#21305;&#37197;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;TaylorShift&#25552;&#39640;&#20102;&#23545;&#30701;&#33267;800&#20010;&#35760;&#21495;&#30340;&#24207;&#21015;&#30340;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21152;&#36895;&#20102;&#23545;&#38271;&#36798;&#32422;1700&#20010;&#35760;&#21495;&#21450;&#20197;&#19978;&#36755;&#20837;&#30340;&#25512;&#26029;&#12290;&#23545;&#20110;&#36739;&#30701;&#30340;&#24207;&#21015;&#65292;TaylorShift&#19982;&#21407;&#22987;&#27880;&#24847;&#21147;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#20998;&#31867;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02920v1 Announce Type: cross  Abstract: The quadratic complexity of the attention mechanism represents one of the biggest hurdles for processing long sequences using Transformers. Current methods, relying on sparse representations or stateful recurrence, sacrifice token-to-token interactions, which ultimately leads to compromises in performance. This paper introduces TaylorShift, a novel reformulation of the Taylor softmax that enables computing full token-to-token interactions in linear time and space. We analytically determine the crossover points where employing TaylorShift becomes more efficient than traditional attention, aligning closely with empirical measurements. Specifically, our findings demonstrate that TaylorShift enhances memory efficiency for sequences as short as 800 tokens and accelerates inference for inputs of approximately 1700 tokens and beyond. For shorter sequences, TaylorShift scales comparably with the vanilla attention. Furthermore, a classification
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17317</link><description>&lt;p&gt;
&#22914;&#20309;&#36194;&#24471;BraTS 2023&#25104;&#24180;&#33014;&#36136;&#30244;&#25361;&#25112;&#65311;&#20551;&#35013;&#32780;&#24050;&#65281;&#22686;&#24378;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#38598;&#25104;&#29992;&#20110;&#33041;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17317
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#26469;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#25104;&#21151;&#35757;&#32451;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#25216;&#26415;&#22635;&#34917;&#20102;&#30693;&#35782;&#24046;&#36317;&#65292;&#21462;&#24471;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#20351;&#29992;&#38750;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#37197;&#20934;&#34987;&#29992;&#26469;&#22823;&#37327;&#22686;&#21152;&#21487;&#29992;&#26679;&#26412;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#19977;&#20010;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20998;&#21035;&#29992;&#20110;&#39045;&#20869;&#32959;&#30244;&#20998;&#21106;&#30340;BraTS2023&#25361;&#25112;&#30340;&#31532;&#19968;&#20010;&#20219;&#21153;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#26159;&#26631;&#20934;nnU-Net&#65292;&#31532;&#20108;&#20010;&#26159;Swin UNETR&#65292;&#31532;&#19977;&#20010;&#26159;BraTS 2021&#25361;&#25112;&#30340;&#33719;&#32988;&#26041;&#26696;&#12290;&#25972;&#20010;&#27969;&#31243;&#22522;&#20110;nnU-Net&#23454;&#29616;&#65292;&#38500;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#29983;&#25104;&#12290;&#21367;&#31215;&#31639;&#27861;&#21644;transformers&#30340;&#20351;&#29992;&#33021;&#22815;&#22635;&#34917;&#24444;&#27492;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20351;&#29992;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#36798;&#21040;&#20102;0.9005&#30340;dice&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17317v1 Announce Type: cross  Abstract: Deep Learning is the state-of-the-art technology for segmenting brain tumours. However, this requires a lot of high-quality data, which is difficult to obtain, especially in the medical field. Therefore, our solutions address this problem by using unconventional mechanisms for data augmentation. Generative adversarial networks and registration are used to massively increase the amount of available samples for training three different deep learning models for brain tumour segmentation, the first task of the BraTS2023 challenge. The first model is the standard nnU-Net, the second is the Swin UNETR and the third is the winning solution of the BraTS 2021 Challenge. The entire pipeline is built on the nnU-Net implementation, except for the generation of the synthetic data. The use of convolutional algorithms and transformers is able to fill each other's knowledge gaps. Using the new metric, our best solution achieves the dice results 0.9005
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#23558;&#20854;&#25299;&#23637;&#21040;&#20855;&#26377;&#27425;&#25351;&#25968;&#23614;&#37096;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#36825;&#20123;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#35823;&#24046;&#29575;</title><link>https://arxiv.org/abs/2402.15432</link><description>&lt;p&gt;
&#22312;&#27425;&#25351;&#25968;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#26497;&#23567;&#21270;&#32858;&#31867;&#35823;&#24046;&#65306;&#36890;&#29992;&#19979;&#30028;&#21644;&#26368;&#20339;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Universal Lower Bounds and Optimal Rates: Achieving Minimax Clustering Error in Sub-Exponential Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#23558;&#20854;&#25299;&#23637;&#21040;&#20855;&#26377;&#27425;&#25351;&#25968;&#23614;&#37096;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#36825;&#20123;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#35823;&#24046;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#30340;&#35270;&#35282;&#26469;&#30740;&#31350;&#12290;&#22312;&#39640;&#26031;&#21644;&#27425;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#24674;&#22797;&#32858;&#31867;&#26631;&#31614;&#30340;&#26368;&#20339;&#35823;&#24046;&#29575;&#28041;&#21450;&#21040;&#29305;&#23450;&#30340;&#20449;&#22122;&#27604;&#12290;&#31616;&#21333;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#22914;Lloyd&#31639;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#36825;&#20010;&#26368;&#20339;&#35823;&#24046;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#20219;&#20309;&#28151;&#21512;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#29575;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#19979;&#30028;&#65292;&#36890;&#36807;Chernoff&#25955;&#24230;&#26469;&#34920;&#36798;&#65292;&#36825;&#26159;&#19968;&#20010;&#27604;&#20449;&#22122;&#27604;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#20449;&#24687;&#24230;&#37327;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#36845;&#20195;&#31639;&#27861;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#19979;&#30028;&#65292;&#29305;&#21035;&#24378;&#35843;&#20102;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#35823;&#24046;&#30340;&#20301;&#32622;-&#23610;&#24230;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26356;&#36866;&#21512;&#30001;&#27850;&#26494;&#25110;&#36127;&#20108;&#39033;&#28151;&#21512;&#27169;&#22411;&#24314;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20854;&#20998;&#24067;&#23646;&#20110;&#25351;&#25968;&#26063;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15432v1 Announce Type: cross  Abstract: Clustering is a pivotal challenge in unsupervised machine learning and is often investigated through the lens of mixture models. The optimal error rate for recovering cluster labels in Gaussian and sub-Gaussian mixture models involves ad hoc signal-to-noise ratios. Simple iterative algorithms, such as Lloyd's algorithm, attain this optimal error rate. In this paper, we first establish a universal lower bound for the error rate in clustering any mixture model, expressed through a Chernoff divergence, a more versatile measure of model information than signal-to-noise ratios. We then demonstrate that iterative algorithms attain this lower bound in mixture models with sub-exponential tails, notably emphasizing location-scale mixtures featuring Laplace-distributed errors. Additionally, for datasets better modelled by Poisson or Negative Binomial mixtures, we study mixture models whose distributions belong to an exponential family. In such m
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08018</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbour Score Estimators for Diffusion Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#12289;&#26679;&#26412;&#36136;&#37327;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#26159;&#35757;&#32451;&#21644;&#37319;&#26679;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#24120;&#29992;&#30340;&#20272;&#35745;&#22120;&#35201;&#20040;&#26159;&#26377;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65292;&#35201;&#20040;&#26159;&#22522;&#20110;&#26465;&#20214;&#35780;&#20998;&#30340;&#39640;&#26041;&#24046;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26368;&#36817;&#37051;&#35780;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#35757;&#32451;&#38598;&#20013;&#30340;&#22810;&#20010;&#26679;&#26412;&#22823;&#22823;&#38477;&#20302;&#20102;&#20272;&#35745;&#22120;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#20013;&#21033;&#29992;&#20102;&#20302;&#26041;&#24046;&#20272;&#35745;&#22120;&#12290;&#22312;&#20351;&#29992;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#35757;&#32451;&#19968;&#33268;&#24615;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#25910;&#25947;&#36895;&#24230;&#21644;&#26679;&#26412;&#36136;&#37327;&#26174;&#33879;&#25552;&#39640;&#12290;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#26367;&#20195;&#23398;&#20064;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#27969;ODE&#31215;&#20998;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26377;&#21069;&#26223;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.05050</link><description>&lt;p&gt;
Federated Learning&#33021;&#22815;&#25214;&#21040;&#26377;&#30410;&#30340;&#22909;&#21451;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Can Find Friends That Are Beneficial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;Federated Learning&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#32858;&#21512;&#26435;&#37325;&#26469;&#35782;&#21035;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#32463;&#36807;&#23454;&#35777;&#35780;&#20272;&#21457;&#29616;&#65292;&#20351;&#29992;&#35813;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20026;&#26356;&#21152;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;Federated Learning&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Federated Learning (FL)&#20013;&#65292;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#23458;&#25143;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26082;&#24102;&#26469;&#20102;&#26426;&#20250;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#34429;&#28982;&#23458;&#25143;&#20043;&#38388;&#30340;&#21512;&#20316;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#23398;&#20064;&#36807;&#31243;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#21512;&#20316;&#37117;&#26159;&#26377;&#30410;&#30340;&#65307;&#26377;&#20123;&#29978;&#33267;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#20026;&#21442;&#19982;FL&#35757;&#32451;&#30340;&#23458;&#25143;&#20998;&#37197;&#33258;&#36866;&#24212;&#30340;&#32858;&#21512;&#26435;&#37325;&#65292;&#35782;&#21035;&#20986;&#25968;&#25454;&#20998;&#24067;&#23545;&#29305;&#23450;&#23398;&#20064;&#30446;&#26631;&#26368;&#26377;&#30410;&#30340;&#23458;&#25143;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32858;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#19982;&#20165;&#32858;&#21512;&#20855;&#26377;&#30456;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#25509;&#25910;&#30340;&#26356;&#26032;&#30340;&#26041;&#27861;&#19981;&#30456;&#19978;&#19979;&#12290;&#27492;&#22806;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;&#30001;&#25105;&#20204;&#30340;&#31639;&#27861;&#24341;&#23548;&#30340;&#21512;&#20316;&#20248;&#20110;&#20256;&#32479;&#30340;FL&#26041;&#27861;&#12290;&#36825;&#24378;&#35843;&#20102;&#23457;&#24910;&#36873;&#25321;&#23458;&#25143;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#20026;&#26410;&#26469;&#26356;&#31616;&#21270;&#21644;&#26377;&#25928;&#30340;FL&#23454;&#29616;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02868</link><description>&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26263;&#22320;&#37324;&#26159;&#19968;&#31181;&#36951;&#24536;&#32531;&#35299;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02868
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#20801;&#35768;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36716;&#31227;&#33021;&#21147;&#65292;&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#23601;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20174;&#21160;&#20316;&#21644;&#35266;&#23519;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#35282;&#24230;&#65292;&#23558;&#32454;&#35843;&#38454;&#27573;&#26410;&#35775;&#38382;&#21040;&#30340;&#19979;&#28216;&#20219;&#21153;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#39044;&#35757;&#32451;&#33021;&#21147;&#36951;&#24536;&#38382;&#39064;&#20316;&#20026;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#30340;&#19968;&#20010;&#20855;&#20307;&#21407;&#22240;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#26410;&#35775;&#38382;&#21040;&#30340;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#39044;&#35757;&#32451;&#20351;&#20854;&#22833;&#21435;&#20102;&#26399;&#26395;&#30340;&#36716;&#31227;&#20248;&#21183;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#38382;&#39064;&#21457;&#29983;&#30340;&#26465;&#20214;&#65292;&#34920;&#26126;&#23427;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;NetHack&#21644;Montezuma's Revenge&#29615;&#22659;&#36827;&#34892;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#20805;&#20998;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#24314;&#31435;&#20102;&#26377;&#25928;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#24182;&#26356;&#26131;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2312.17173</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Non-Vacuous Generalization Bounds for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17173
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#24314;&#31435;&#20102;&#26377;&#25928;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#35777;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#24182;&#26356;&#26131;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#36827;&#34892;&#27867;&#21270;&#65292;&#25110;&#32773;&#21482;&#26159;&#37325;&#22797;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#39044;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21457;&#29616;&#36866;&#29992;&#20110;&#26410;&#35265;&#25968;&#25454;&#30340;&#35268;&#24459;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#27979;&#24179;&#28369;&#23548;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#26080;&#30028;&#23545;&#25968;&#20284;&#28982;&#25439;&#22833;&#30340;&#21387;&#32553;&#30028;&#38480;&#65292;&#24182;&#19988;&#25105;&#20204;&#25193;&#23637;&#20102;&#35813;&#30028;&#38480;&#20197;&#22788;&#29702;&#23376;&#37319;&#26679;&#65292;&#21152;&#36895;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#30028;&#38480;&#35745;&#31639;&#12290;&#20026;&#20102;&#23454;&#29616;&#38750;&#24179;&#20961;&#27867;&#21270;&#30028;&#38480;&#25152;&#38656;&#30340;&#26497;&#31471;&#21387;&#32553;&#31243;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;SubLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#20302;&#32500;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36739;&#22823;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#27604;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#26131;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can contain billions of parameters, raising the question of whether they can generalize beyond the training data or simply regurgitate their training corpora. We provide the first non-vacuous generalization bounds for pretrained large language models (LLMs), indicating that language models are capable of discovering regularities that generalize to unseen data. In particular, we derive a compression bound that is valid for the unbounded log-likelihood loss using prediction smoothing, and we extend the bound to handle subsampling, accelerating bound computation on massive datasets. To achieve the extreme level of compression required for non-vacuous generalization bounds, we devise SubLoRA, a low-dimensional non-linear parameterization. Using this approach, we find that larger models have better generalization bounds and are more compressible than smaller models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#27979;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#20449;&#21495;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.14440</link><description>&lt;p&gt;
&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#19981;&#23545;&#31216;&#20559;&#24046;&#30340;&#23545;&#25239;&#25915;&#20987;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#25506;&#27979;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#20449;&#21495;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#20869;&#23481;&#29983;&#25104;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#65292;&#21253;&#25324;&#23427;&#20204;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#23545;T2I&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#37325;&#28857;&#20998;&#26512;&#20102;&#19982;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#30446;&#26631; - &#20351;&#29992;&#23545;&#25239;&#24615;&#21518;&#32512;&#36827;&#34892;&#23454;&#20307;&#26367;&#25442;&#65292;&#20197;&#21450;&#20004;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#31639;&#27861;&#12290;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#25581;&#31034;&#20102;&#23454;&#20307;&#20132;&#25442;&#20013;ASR&#30340;&#19981;&#23545;&#31216;&#24615;&#36136;&#65306;&#20363;&#22914;&#65292;&#23545;&#20110;&#22312;&#25552;&#31034;&#8220;&#22312;&#38632;&#20013;&#36339;&#33310;&#30340;&#20154;&#31867;&#8221;&#20013;&#26367;&#25442;&#8220;&#20154;&#31867;&#8221;&#20026;&#8220;&#26426;&#22120;&#20154;&#8221;&#30340;&#23545;&#25239;&#24615;&#21518;&#32512;&#65292;&#36739;&#23481;&#26131;&#23454;&#29616;&#65292;&#32780;&#21453;&#21521;&#26367;&#25442;&#21017;&#26126;&#26174;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25506;&#27979;&#25351;&#26631;&#65292;&#20197;&#30830;&#23450;&#27169;&#22411;&#23545;&#25239;ASR&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14440v2 Announce Type: replace Abstract: The widespread use of Text-to-Image (T2I) models in content generation requires careful examination of their safety, including their robustness to adversarial attacks. Despite extensive research on adversarial attacks, the reasons for their effectiveness remain underexplored. This paper presents an empirical study on adversarial attacks against T2I models, focusing on analyzing factors associated with attack success rates (ASR). We introduce a new attack objective - entity swapping using adversarial suffixes and two gradient-based attack algorithms. Human and automatic evaluations reveal the asymmetric nature of ASRs on entity swap: for example, it is easier to replace "human" with "robot" in the prompt "a human dancing in the rain." with an adversarial suffix, but the reverse replacement is significantly harder. We further propose probing metrics to establish indicative signals from the model's beliefs to the adversarial ASR. We iden
&lt;/p&gt;</description></item><item><title>TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.04142</link><description>&lt;p&gt;
TimeDRL&#65306;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TimeDRL: Disentangled Representation Learning for Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04142
&lt;/p&gt;
&lt;p&gt;
TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#24037;&#19994;&#65289;&#38750;&#24120;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#21644;&#39640;&#32500;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#26174;&#31034;&#20102;&#22312;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23398;&#20064;&#35299;&#32544;&#23884;&#20837;&#21644;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#21464;&#25442;&#19981;&#21464;&#24615;&#65289;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeDRL&#65292;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;TimeDRL&#30340;&#19977;&#20010;&#26032;&#39062;&#29305;&#24449;&#20026;&#65306;&#65288;i&#65289;&#20351;&#29992;[CLS]&#20196;&#29260;&#31574;&#30053;&#20174;&#25171;&#34917;&#19969;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35299;&#32544;&#26102;&#38388;&#25139;&#32423;&#21644;&#23454;&#20363;&#32423;&#23884;&#20837;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#26102;&#38388;&#25139;&#39044;&#27979;&#21644;&#23454;&#20363;&#23545;&#27604;&#20219;&#21153;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21069;&#32773;&#20248;&#21270;&#26102;&#38388;&#25139;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04142v2 Announce Type: replace-cross  Abstract: Multivariate time-series data in numerous real-world applications (e.g., healthcare and industry) are informative but challenging due to the lack of labels and high dimensionality. Recent studies in self-supervised learning have shown their potential in learning rich representations without relying on labels, yet they fall short in learning disentangled embeddings and addressing issues of inductive bias (e.g., transformation-invariance). To tackle these challenges, we propose TimeDRL, a generic multivariate time-series representation learning framework with disentangled dual-level embeddings. TimeDRL is characterized by three novel features: (i) disentangled derivation of timestamp-level and instance-level embeddings from patched time-series data using a [CLS] token strategy; (ii) utilization of timestamp-predictive and instance-contrastive tasks for disentangled representation learning, with the former optimizing timestamp-lev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2311.14649</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#30340;&#28145;&#24230;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning in Deep Factor Graphs with Gaussian Belief Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14649
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32622;&#20449;&#20256;&#25773;&#35299;&#20915;&#35757;&#32451;&#21644;&#39044;&#27979;&#38382;&#39064;&#65292;&#25903;&#25345;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#65292;&#21487;&#25193;&#23637;&#33267;&#28145;&#24230;&#32593;&#32476;&#65292;&#25552;&#20379;&#25345;&#32493;&#23398;&#20064;&#30340;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35270;&#39057;&#21435;&#22122;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#26031;&#22240;&#23376;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#30456;&#20851;&#25968;&#37327;&#65288;&#36755;&#20837;&#12289;&#36755;&#20986;&#12289;&#21442;&#25968;&#12289;&#28508;&#21464;&#37327;&#65289;&#35270;&#20026;&#22270;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#24182;&#23558;&#35757;&#32451;&#21644;&#39044;&#27979;&#37117;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#35266;&#23519;&#33410;&#28857;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#26377;&#25928;&#22320;&#35299;&#20915;&#65292;&#20854;&#26356;&#26032;&#26412;&#36136;&#19978;&#26159;&#26412;&#22320;&#30340;&#65292;&#20026;&#20998;&#24067;&#24335;&#21644;&#24322;&#27493;&#35757;&#32451;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#28145;&#23618;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#24335;&#65306;&#20351;&#29992;&#24403;&#21069;&#20219;&#21153;&#30340;BP&#20272;&#35745;&#21442;&#25968;&#36793;&#38469;&#20316;&#20026;&#19979;&#19968;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#20808;&#39564;&#12290;&#22312;&#35270;&#39057;&#21435;&#22122;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#23398;&#20064;&#21442;&#25968;&#30456;&#23545;&#20110;&#20256;&#32479;&#22240;&#23376;&#22270;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#28145;&#24230;&#22240;&#23376;&#22270;&#22312;&#25345;&#32493;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#40723;&#33310;&#20154;&#24515;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14649v2 Announce Type: replace  Abstract: We propose an approach to do learning in Gaussian factor graphs. We treat all relevant quantities (inputs, outputs, parameters, latents) as random variables in a graphical model, and view both training and prediction as inference problems with different observed nodes. Our experiments show that these problems can be efficiently solved with belief propagation (BP), whose updates are inherently local, presenting exciting opportunities for distributed and asynchronous training. Our approach can be scaled to deep networks and provides a natural means to do continual learning: use the BP-estimated parameter marginals of the current task as parameter priors for the next. On a video denoising task we demonstrate the benefit of learnable parameters over a classical factor graph approach and we show encouraging performance of deep factor graphs for continual image classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#22635;&#20805;&#36317;&#31163;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#35813;&#31639;&#27861;&#22312;&#23454;&#29616;&#30340;&#26131;&#29992;&#24615;&#21644;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#30340;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;GP-UCB&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;&#20854;&#20182;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.17037</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65306;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#25913;&#21892;&#36951;&#25022;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration. (arXiv:2401.17037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#22635;&#20805;&#36317;&#31163;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#35813;&#31639;&#27861;&#22312;&#23454;&#29616;&#30340;&#26131;&#29992;&#24615;&#21644;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#30340;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;GP-UCB&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;&#20854;&#20182;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#30830;&#20445;&#26597;&#35810;&#28857;&#30340;&#22635;&#20805;&#36317;&#31163;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#30041;&#20102;&#32463;&#20856;&#30340;GP-UCB&#31639;&#27861;&#30340;&#26131;&#23454;&#29616;&#24615;&#65292;&#24182;&#28385;&#36275;&#20102;&#20960;&#20046;&#19982;arXiv:2002.05096&#20013;&#30340;&#29468;&#24819;&#30456;&#21305;&#37197;&#30340;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;COLT&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26032;&#31639;&#27861;&#22312;&#20960;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;GP-UCB&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2401.10451</link><description>&lt;p&gt;
&#23398;&#20064;&#36741;&#21161;&#30340;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#35268;&#21010;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian Optimization Approach. (arXiv:2401.10451v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#21487;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#23545;&#20110;&#21306;&#22495;&#33021;&#28304;&#31995;&#32479;&#30340;&#25104;&#26412;&#25928;&#30410;&#20302;&#30899;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#30830;&#20445;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#39044;&#26399;&#32467;&#26524;&#65292;&#24314;&#27169;&#32771;&#34385;&#21040;&#22825;&#27668;&#30456;&#20851;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#20379;&#24212;&#21644;&#33021;&#28304;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38543;&#26426;&#20248;&#21270;&#27169;&#22411;&#36890;&#24120;&#27604;&#30830;&#23450;&#24615;&#27169;&#22411;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36741;&#21161;&#30340;&#36817;&#20284;&#35299;&#27861;&#26469;&#21487;&#34892;&#22320;&#35299;&#20915;&#20004;&#38454;&#27573;&#38543;&#26426;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#21644;&#27714;&#35299;&#19968;&#31995;&#21015;&#21487;&#34892;&#30340;&#26102;&#38388;&#32858;&#21512;&#20195;&#29702;&#38382;&#39064;&#65292;&#35782;&#21035;&#20986;&#20302;&#25104;&#26412;&#30340;&#35268;&#21010;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#25628;&#32034;&#26102;&#38388;&#24207;&#21015;&#32858;&#21512;&#36229;&#21442;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#35745;&#31639;&#22312;&#20379;&#38656;&#39044;&#27979;&#30340;&#39564;&#35777;&#38598;&#19978;&#26368;&#23567;&#21270;&#25104;&#26412;&#30340;&#36817;&#20284;&#35299;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#32452;&#20445;&#30041;&#30340;&#27979;&#35797;&#39044;&#27979;&#19978;&#35780;&#20272;&#35299;&#20915;&#30340;&#35268;&#21010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving large-scale capacity expansion problems (CEPs) is central to cost-effective decarbonization of regional-scale energy systems. To ensure the intended outcomes of CEPs, modeling uncertainty due to weather-dependent variable renewable energy (VRE) supply and energy demand becomes crucially important. However, the resulting stochastic optimization models are often less computationally tractable than their deterministic counterparts. Here, we propose a learning-assisted approximate solution method to tractably solve two-stage stochastic CEPs. Our method identifies low-cost planning decisions by constructing and solving a sequence of tractable temporally aggregated surrogate problems. We adopt a Bayesian optimization approach to searching the space of time series aggregation hyperparameters and compute approximate solutions that minimize costs on a validation set of supply-demand projections. Importantly, we evaluate solved planning outcomes on a held-out set of test projections. We 
&lt;/p&gt;</description></item><item><title>Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06187</link><description>&lt;p&gt;
Scissorhands: &#36890;&#36807;&#32593;&#32476;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#25968;&#25454;&#24433;&#21709;&#20013;&#36827;&#34892;&#25968;&#25454;&#25830;&#38500;
&lt;/p&gt;
&lt;p&gt;
Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06187
&lt;/p&gt;
&lt;p&gt;
Scissorhands &#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#20462;&#21098;&#30340;&#27169;&#22411;&#26469;&#25830;&#38500;&#25968;&#25454;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#25830;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#24433;&#21709;&#12290;&#23427;&#31526;&#21512;&#26368;&#26032;&#30340;&#25968;&#25454;&#30417;&#31649;&#26631;&#20934;&#65292;&#22686;&#24378;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35775;&#38382;&#20854;&#20313;&#25968;&#25454;&#30340;&#20840;&#37096;&#20869;&#23481;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;&#26041;&#27861;&#8220;Scissorhands&#8221;&#65292;&#23427;&#21482;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#26469;&#26377;&#25928;&#36816;&#34892;&#12290;&#21021;&#22987;&#38454;&#27573;&#65292;Scissorhands&#36890;&#36807;&#36830;&#25509;&#25935;&#24863;&#24615;&#22312;&#32473;&#23450;&#27169;&#22411;&#20013;&#35782;&#21035;&#19982;&#36951;&#24536;&#25968;&#25454;&#30456;&#20851;&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#12290;&#35813;&#36807;&#31243;&#36890;&#36807;&#37325;&#26032;&#21021;&#22987;&#21270;&#36825;&#20123;&#21442;&#25968;&#20013;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#21069;k%&#30340;&#26368;&#30456;&#20851;&#21442;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#29992;&#20110;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#24433;&#21709;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;Scissorhands&#36890;&#36807;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#36807;&#31243;&#23545;&#20462;&#21098;&#30340;&#27169;&#22411;&#36827;&#34892;&#20877;&#35757;&#32451;&#65292;&#23547;&#25214;&#20445;&#30041;&#20449;&#24687;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.06942</link><description>&lt;p&gt;
AI &#25511;&#21046;&#65306;&#23613;&#31649;&#23384;&#22312;&#24847;&#22270;&#24615;&#30772;&#22351;&#65292;&#20294;&#25552;&#39640;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI Control: Improving Safety Despite Intentional Subversion. (arXiv:2312.06942v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#25506;&#32034;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#25216;&#26415;&#27969;&#31243;&#65292;&#33021;&#22815;&#23545;&#25239;&#27169;&#22411;&#26377;&#24847;&#30772;&#22351;&#30340;&#24773;&#20917;&#65292;&#20026;&#35299;&#20915;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#33258;&#20027;&#37096;&#32626;&#65292;&#38450;&#27490;&#23427;&#20204;&#23548;&#33268;&#26377;&#23475;&#32467;&#26524;&#23558;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#23433;&#20840;&#25216;&#26415;&#65292;&#20363;&#22914;&#20351;&#29992;&#27169;&#22411;&#26469;&#23457;&#26680;&#20854;&#20182;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#25110;&#20351;&#29992;&#32418;&#38431;&#25216;&#26415;&#25581;&#31034;&#24494;&#22937;&#30340;&#22833;&#25928;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#23578;&#26410;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#22312;&#27169;&#22411;&#26377;&#24847;&#23581;&#35797;&#30772;&#22351;&#23427;&#20204;&#26102;&#26159;&#21542;&#20173;&#28982;&#30830;&#20445;&#23433;&#20840;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#23545;&#26377;&#24847;&#30772;&#22351;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23433;&#20840;&#25216;&#26415;&#27969;&#31243;&#65288;&#8220;&#21327;&#35758;&#8221;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#20294;&#19981;&#21487;&#20449;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-4&#65289;&#12289;&#20351;&#29992;&#36739;&#24369;&#30340;&#21487;&#20449;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#26159;GPT-3.5&#65289;&#20197;&#21450;&#26377;&#38480;&#30340;&#39640;&#36136;&#37327;&#21487;&#20449;&#21171;&#21160;&#21147;&#35775;&#38382;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#19968;&#31995;&#21015;&#32534;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26088;&#22312;&#27704;&#36828;&#19981;&#25552;&#20132;&#21253;&#21547;&#21518;&#38376;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#21327;&#35758;&#65292;&#20854;&#20013;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques ("protocols") that are robust to intentional subversion.  We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16867</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#21644;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#30340;&#31934;&#31070;&#20998;&#35010;&#30151;&#35786;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#21033;&#29992;WGAN-GP&#21644;VAE&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#33041;&#37096;&#35760;&#24405;&#65292;&#36827;&#34892;&#31934;&#31070;&#20998;&#35010;&#30151;&#30340;&#33258;&#21160;&#35786;&#26029;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#33021;&#22815;&#21033;&#29992;&#26102;&#39057;&#29305;&#24449;&#65292;&#20174;&#21407;&#22987;&#20449;&#21495;&#20013;&#25552;&#21462;&#20102;&#39057;&#35889;&#22270;&#12290;&#22312;&#25506;&#32034;&#20102;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#35774;&#32622;&#21518;&#65292;&#36873;&#25321;&#20102;&#36866;&#21512;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;Wasserstein GAN with Gradient Penalty(WGAN-GP)&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#29983;&#25104;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20351;&#29992;VAE&#29983;&#25104;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#22312;&#20934;&#30830;&#24230;&#19978;&#25552;&#39640;&#20102;3.0&#65285;&#65292;&#36798;&#21040;&#20102;99.0&#65285;&#65292;&#21516;&#26102;&#24471;&#21040;&#20102;&#26356;&#20302;&#30340;&#25439;&#22833;&#20540;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;(LIME)&#26041;&#27861;&#35299;&#20915;&#20102;&#23545;&#20110;&#40657;&#30418;&#27169;&#22411;&#30340;&#20449;&#20219;&#32570;&#20047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we leverage a deep learning-based method for the automatic diagnosis of schizophrenia using EEG brain recordings. This approach utilizes generative data augmentation, a powerful technique that enhances the accuracy of the diagnosis. To enable the utilization of time-frequency features, spectrograms were extracted from the raw signals. After exploring several neural network architectural setups, a proper convolutional neural network (CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two different synthetic datasets were generated in order to augment the initial dataset and address the over-fitting issue. The augmented dataset using VAE achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a lower loss value as well as a faster convergence. Finally, we addressed the lack of trust in black-box models using the Local Interpretable Model-agnostic Explanations (LI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IW-GAE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#26435;&#32676;&#20934;&#30830;&#29575;&#20272;&#35745;&#22120;&#26469;&#35299;&#20915;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#26657;&#20934;&#21644;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10611</link><description>&lt;p&gt;
IW-GAE: &#29992;&#20110;&#25552;&#39640;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#26657;&#20934;&#21644;&#27169;&#22411;&#36873;&#25321;&#30340;&#21152;&#26435;&#32676;&#20934;&#30830;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
IW-GAE: Importance weighted group accuracy estimation for improved calibration and model selection in unsupervised domain adaptation. (arXiv:2310.10611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IW-GAE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#26435;&#32676;&#20934;&#30830;&#29575;&#20272;&#35745;&#22120;&#26469;&#35299;&#20915;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#26657;&#20934;&#21644;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#32463;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#27169;&#22411;&#22312;&#27979;&#35797;&#26679;&#26412;&#19978;&#30340;&#20934;&#30830;&#29575;&#24182;&#20174;&#20013;&#25512;&#26029;&#20854;&#32622;&#20449;&#24230;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#19982;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#25506;&#32034;&#31561;&#37325;&#35201;&#24212;&#29992;&#23494;&#20999;&#30456;&#20851;&#12290;&#34429;&#28982;&#36825;&#20123;&#36830;&#25509;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#35774;&#32622;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#32473;&#20256;&#32479;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#38750;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#20013;&#65292;&#27169;&#22411;&#26657;&#20934;&#21644;&#27169;&#22411;&#36873;&#25321;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#20559;&#31227;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#21152;&#26435;&#32676;&#20934;&#30830;&#29575;&#20272;&#35745;&#22120;&#26469;&#35299;&#20915;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#20559;&#31227;&#32780;&#24102;&#26469;&#30340;&#22256;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#25214;&#21040;&#23548;&#33268;&#22312;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#39046;&#22495;&#20013;&#20934;&#30830;&#20272;&#35745;&#32676;&#20934;&#30830;&#29575;&#30340;&#37325;&#35201;&#26435;&#37325;&#65292;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#32676;&#20934;&#30830;&#29575;&#20272;&#35745;&#22312;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about a model's accuracy on a test sample from its confidence is a central problem in machine learning, being connected to important applications such as uncertainty representation, model selection, and exploration. While these connections have been well-studied in the i.i.d. settings, distribution shifts pose significant challenges to the traditional methods. Therefore, model calibration and model selection remain challenging in the unsupervised domain adaptation problem--a scenario where the goal is to perform well in a distribution shifted domain without labels. In this work, we tackle difficulties coming from distribution shifts by developing a novel importance weighted group accuracy estimator. Specifically, we formulate an optimization problem for finding an importance weight that leads to an accurate group accuracy estimation in the distribution shifted domain with theoretical analyses. Extensive experiments show the effectiveness of group accuracy estimation on model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.05506</link><description>&lt;p&gt;
&#26597;&#35810;&#21644;&#24212;&#31572;&#22686;&#24378;&#19981;&#33021;&#24110;&#21161;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization. (arXiv:2310.05506v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#25968;&#23398;&#25512;&#29702;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25968;&#23398;&#25512;&#29702;&#26102;&#65292;&#36890;&#36807;&#26597;&#35810;&#28436;&#21270;&#21644;&#22810;&#26679;&#21270;&#25512;&#29702;&#36335;&#24452;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#32463;&#39564;&#19978;&#34987;&#39564;&#35777;&#20026;&#26377;&#25928;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#24320;&#28304;LLMs&#21644;&#39030;&#23574;&#19987;&#26377;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;&#25968;&#23398;&#25512;&#29702;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#65306;&#65288;1&#65289;&#21738;&#20123;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26356;&#26377;&#25928;&#65307;&#65288;2&#65289;&#22686;&#24378;&#25968;&#25454;&#37327;&#19982;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#32553;&#25918;&#20851;&#31995;&#22914;&#20309;&#65307;&#65288;3&#65289;&#25968;&#25454;&#22686;&#24378;&#33021;&#21542;&#28608;&#21169;&#39046;&#22495;&#22806;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#30340;&#27867;&#21270;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22686;&#21152;GSM8K&#26597;&#35810;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#20197;&#21450;&#37319;&#26679;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AugGSM8K&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;AugGSM8K&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#31216;&#20026;MuggleMath&#12290;MuggleMath&#22312;GSM8K&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65288;&#22312;7B&#35268;&#27169;&#19978;&#20174;54%&#25552;&#39640;&#21040;68.4%&#65292;&#22312;&#25193;&#25918;&#21040;63.9%&#21040;74.0%&#20043;&#38388;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create a new dataset, AugGSM8K, by complicating and diversifying the queries from GSM8K and sampling multiple reasoning paths. We obtained a series of LLMs called MuggleMath by fine-tuning on subsets of AugGSM8K. MuggleMath substantially achieves new state-of-the-art on GSM8K (from 54% to 68.4% at the scale of 7B, and from 63.9% to 74.0% at the scal
&lt;/p&gt;</description></item><item><title>&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06979</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#26159;&#36890;&#29992;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auto-Regressive Next-Token Predictors are Universal Learners. (arXiv:2309.06979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06979
&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#36923;&#36753;&#21644;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#38750;&#20961;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#33021;&#21147;&#22312;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#31616;&#21333;&#20219;&#21153;&#19978;&#30340;&#32593;&#32476;&#20013;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#33258;&#22238;&#24402;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#22914;&#32447;&#24615;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#24403;&#20854;&#22312;&#24605;&#32500;&#38142;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#22270;&#28789;&#26426;&#35745;&#31639;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#38271;&#24230;&#22797;&#26434;&#24230;&#65292;&#23427;&#34913;&#37327;&#20102;&#22312;&#36817;&#20284;&#26576;&#20010;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#24605;&#32500;&#38142;&#24207;&#21015;&#20013;&#25152;&#38656;&#30340;&#20013;&#38388;&#26631;&#35760;&#30340;&#25968;&#37327;&#65292;&#24182;&#20998;&#26512;&#20102;&#38271;&#24230;&#22797;&#26434;&#24230;&#21644;&#20854;&#20182;&#22797;&#26434;&#24615;&#27010;&#24565;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#31616;&#21333;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#22120;&#65292;&#22914;&#32447;&#24615;&#32593;&#32476;&#21644;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#31639;&#26415;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models display remarkable capabilities in logical and mathematical reasoning, allowing them to solve complex tasks. Interestingly, these abilities emerge in networks trained on the simple task of next-token prediction. In this work, we present a theoretical framework for studying auto-regressive next-token predictors. We demonstrate that even simple models such as linear next-token predictors, trained on Chain-of-Thought (CoT) data, can approximate any function efficiently computed by a Turing machine. We introduce a new complexity measure -- length complexity -- which measures the number of intermediate tokens in a CoT sequence required to approximate some target function, and analyze the interplay between length complexity and other notions of complexity. Finally, we show experimentally that simple next-token predictors, such as linear networks and shallow Multi-Layer Perceptrons (MLPs), display non-trivial performance on text generation and arithmetic tasks. Our resul
&lt;/p&gt;</description></item><item><title>EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03244</link><description>&lt;p&gt;
EGIC:&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#22312;&#35821;&#20041;&#20998;&#21106;&#30340;&#25351;&#23548;&#19979;
&lt;/p&gt;
&lt;p&gt;
EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03244
&lt;/p&gt;
&lt;p&gt;
EGIC&#26159;&#19968;&#31181;&#22686;&#24378;&#30340;&#20302;&#20301;&#36895;&#29575;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#20998;&#21106;&#25552;&#20379;&#25351;&#23548;&#12290;&#23427;&#22312;&#22833;&#30495;&#24863;&#30693;&#21644;&#22833;&#30495;&#26041;&#21521;&#22522;&#32447;&#26041;&#27861;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#20248;&#31168;&#30340;&#25554;&#20540;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#22270;&#20687;&#21387;&#32553;&#26041;&#27861;EGIC&#65292;&#23427;&#20801;&#35768;&#20174;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26377;&#25928;&#22320;&#36941;&#21382;&#22833;&#30495;&#24863;&#30693;&#26354;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#32534;&#30721;&#30340;&#22270;&#20687;&#25554;&#20540;&#21464;&#20307;&#65292;&#29992;&#20110;&#39044;&#27979;&#22312;MSE&#20248;&#21270;&#21644;GAN&#20248;&#21270;&#35299;&#30721;&#22120;&#36755;&#20986;&#20043;&#38388;&#30340;&#27531;&#24046;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#27531;&#24046;&#23545;&#22522;&#20110;GAN&#30340;&#37325;&#24314;&#30340;&#24433;&#21709;&#12290;&#32467;&#21512;&#25913;&#36827;&#30340;&#22522;&#20110;GAN&#30340;&#26500;&#24314;&#22359;&#65292;EGIC&#22312;&#24863;&#30693;&#23548;&#21521;&#21644;&#22833;&#30495;&#23548;&#21521;&#30340;&#22522;&#32447;&#26041;&#27861;&#65288;&#21253;&#25324;HiFiC&#65292;MRIC&#21644;DIRAC&#65289;&#19978;&#34920;&#29616;&#20248;&#20110;&#22823;&#22810;&#25968;&#26041;&#27861;&#65292;&#22312;&#22833;&#30495;&#31471;&#19982;VTM-20.0&#20960;&#20046;&#30456;&#24403;&#12290;EGIC&#23454;&#29616;&#31616;&#21333;&#65292;&#38750;&#24120;&#36731;&#37327;&#32423;&#65288;&#19982;HiFiC&#30456;&#27604;&#65292;&#27169;&#22411;&#21442;&#25968;&#21482;&#26377;0.18&#20493;&#65289;&#65292;&#24182;&#25552;&#20379;&#20248;&#24322;&#30340;&#25554;&#20540;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#38024;&#23545;&#20302;&#20301;&#33539;&#22260;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#24076;&#26395;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.05731</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Rethinking Integration of Prediction and Planning in Deep Learning-Based Automated Driving Systems: A Review. (arXiv:2308.05731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05731
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#37325;&#26032;&#24605;&#32771;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#39044;&#27979;&#21644;&#35268;&#21010;&#30340;&#25972;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;&#20854;&#20316;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26469;&#25552;&#39640;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#20010;&#20154;&#12289;&#20844;&#20849;&#21644;&#36135;&#36816;&#20132;&#36890;&#30340;&#26041;&#24335;&#12290;&#38500;&#20102;&#24863;&#30693;&#29615;&#22659;&#30340;&#24040;&#22823;&#25361;&#25112;&#22806;&#65292;&#21363;&#20934;&#30830;&#22320;&#20351;&#29992;&#21487;&#29992;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#24863;&#30693;&#29615;&#22659;&#65292;&#33258;&#21160;&#39550;&#39542;&#36824;&#21253;&#25324;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#39640;&#25928;&#30340;&#36816;&#21160;&#36712;&#36857;&#12290;&#20026;&#20102;&#20419;&#36827;&#23433;&#20840;&#21644;&#36827;&#27493;&#65292;&#35768;&#22810;&#24037;&#20316;&#20381;&#36182;&#20110;&#27169;&#22359;&#21270;&#30340;&#20132;&#36890;&#26410;&#26469;&#36816;&#21160;&#30340;&#39044;&#27979;&#12290;&#27169;&#22359;&#21270;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#20316;&#20026;&#39034;&#24207;&#30340;&#29420;&#31435;&#20219;&#21153;&#22788;&#29702;&#12290;&#34429;&#28982;&#36825;&#32771;&#34385;&#20102;&#21608;&#22260;&#20132;&#36890;&#23545;&#33258;&#36710;&#30340;&#24433;&#21709;&#65292;&#20294;&#23427;&#26410;&#33021;&#39044;&#27979;&#20132;&#36890;&#21442;&#19982;&#32773;&#23545;&#33258;&#36710;&#34892;&#20026;&#30340;&#21453;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#39044;&#27979;&#21644;&#35268;&#21010;&#25972;&#21512;&#20026;&#30456;&#20114;&#20381;&#36182;&#30340;&#32852;&#21512;&#27493;&#39588;&#26159;&#23454;&#29616;&#23433;&#20840;&#12289;&#39640;&#25928;&#21644;&#33298;&#36866;&#39550;&#39542;&#25152;&#24517;&#38656;&#30340;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#20294;&#23545;&#19981;&#21516;&#21407;&#29702;&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated driving has the potential to revolutionize personal, public, and freight mobility. Besides the enormous challenge of perception, i.e. accurately perceiving the environment using available sensor data, automated driving comprises planning a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential separate tasks. While this accounts for the influence of surrounding traffic on the ego-vehicle, it fails to anticipate the reactions of traffic participants to the ego-vehicle's behavior. Recent works suggest that integrating prediction and planning in an interdependent joint step is necessary to achieve safe, efficient, and comfortable driving. While various models implement such integrated systems, a comprehensive overview and theoretical understanding of different principles are lacking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04553</link><description>&lt;p&gt;
&#20174;&#20551;&#21040;&#30495;&#65288;FFR&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#19982;&#21512;&#25104;&#25968;&#25454;&#30456;&#20851;&#24615;&#38169;&#35823;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#27969;&#31243;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20943;&#23569;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23398;&#20064;&#21040;&#19982;&#25968;&#25454;&#38598;&#20559;&#24046;&#30456;&#20851;&#30340;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#23481;&#26131;&#23398;&#20064;&#21040;&#30001;&#20110;&#35757;&#32451;&#38598;&#30340;&#19981;&#24179;&#34913;&#23548;&#33268;&#30340;&#30456;&#20851;&#24615;&#38169;&#35823;&#65292;&#20854;&#20013;&#26576;&#20123;&#32676;&#20307;&#65288;&#22914;&#22899;&#24615;&#65289;&#22312;&#26576;&#20123;&#31867;&#21035;&#65288;&#22914;&#31243;&#24207;&#21592;&#65289;&#20013;&#20195;&#34920;&#24615;&#19981;&#36275;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#20026;&#23569;&#25968;&#26679;&#26412;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#23569;&#36825;&#31181;&#20559;&#24046;&#65292;&#20174;&#32780;&#24179;&#34913;&#35757;&#32451;&#38598;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#24448;&#24448;&#33021;&#22815;&#23398;&#20064;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#28040;&#38500;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#26469;&#20943;&#23569;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;1&#65289;&#25105;&#20204;&#22312;&#24179;&#34913;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;2&#65289;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#31243;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#22312;&#31532;&#19968;&#27493;&#20013;&#25105;&#20204;&#23398;&#20064;&#21040;&#20102;&#25269;&#25239;&#20559;&#24046;&#30340;&#31283;&#20581;&#29305;&#24449;&#65292;&#22312;&#31532;&#20108;&#27493;&#20013;&#20943;&#36731;&#20102;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.01097</link><description>&lt;p&gt;
&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36816;&#21160;&#22686;&#37327;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#35299;&#32806;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#36816;&#21160;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#30001;&#20110;&#26410;&#26469;&#23039;&#21183;&#30340;&#38543;&#26426;&#21644;&#19981;&#35268;&#21017;&#24615;&#36136;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#24037;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24448;&#24448;&#38590;&#20197;&#24314;&#27169;&#20154;&#20307;&#36816;&#21160;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#12290;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#30340;&#26102;&#31354;&#34920;&#31034;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#36816;&#21160;&#25968;&#25454;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#39592;&#26550;&#33410;&#28857;&#30340;&#26102;&#22495;&#21644;&#31354;&#22495;&#20381;&#36182;&#24615;&#26159;&#19981;&#21516;&#30340;&#12290;&#26102;&#22495;&#20851;&#31995;&#25429;&#25417;&#21040;&#38543;&#26102;&#38388;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#31354;&#22495;&#20851;&#31995;&#25551;&#36848;&#20102;&#36523;&#20307;&#32467;&#26500;&#21644;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#22686;&#37327;&#20449;&#24687;&#36827;&#34892;&#26102;&#31354;&#20998;&#25903;&#30340;&#36816;&#21160;&#39044;&#27979;&#32593;&#32476;&#65292;&#23427;&#35299;&#32806;&#20102;&#26102;&#22495;&#21644;&#31354;&#22495;&#29305;&#24449;&#30340;&#23398;&#20064;&#65292;&#25552;&#21462;&#20102;&#26356;&#22810;&#30340;&#36816;&#21160;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human motion prediction (HMP) has emerged as a popular research topic due to its diverse applications, but it remains a challenging task due to the stochastic and aperiodic nature of future poses. Traditional methods rely on hand-crafted features and machine learning techniques, which often struggle to model the complex dynamics of human motion. Recent deep learning-based methods have achieved success by learning spatio-temporal representations of motion, but these models often overlook the reliability of motion data. Additionally, the temporal and spatial dependencies of skeleton nodes are distinct. The temporal relationship captures motion information over time, while the spatial relationship describes body structure and the relationships between different nodes. In this paper, we propose a novel spatio-temporal branching network using incremental information for HMP, which decouples the learning of temporal-domain and spatial-domain features, extracts more motion information, and ac
&lt;/p&gt;</description></item><item><title>DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09124</link><description>&lt;p&gt;
DIFFender&#65306;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#29992;&#20110;&#25269;&#24481;Patch&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks. (arXiv:2306.09124v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09124
&lt;/p&gt;
&lt;p&gt;
DIFFender&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20301;&#21644;&#24674;&#22797;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#65292;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25972;&#20307;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;Patch&#25915;&#20987;&#65292;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;&#24320;&#21457;&#21487;&#38752;&#30340;&#38450;&#24481;&#26041;&#27861;&#20197;&#25269;&#24481;Patch&#25915;&#20987;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#24403;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DIFFender&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#38450;&#24481;&#23545;&#25239;&#24615;Patch&#12290;DIFFender&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;Patch&#23450;&#20301;&#21644;Patch&#24674;&#22797;&#12290;&#22312;&#23450;&#20301;&#38454;&#27573;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#21033;&#29992;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#36259;&#29305;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#23545;&#25239;&#24615;Patch&#30340;&#20301;&#32622;&#12290;&#22312;&#24674;&#22797;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22270;&#20687;&#20013;&#30340;&#23545;&#25239;&#24615;&#21306;&#22495;&#21516;&#26102;&#20445;&#25345;&#35270;&#35273;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#21463;&#21040;&#32479;&#19968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#31934;&#24515;&#24341;&#23548;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#32039;&#23494;&#30456;&#20114;&#20316;&#29992;&#26469;&#25552;&#39640;&#25972;&#20010;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks, particularly patch attacks, pose significant threats to the robustness and reliability of deep learning models. Developing reliable defenses against patch attacks is crucial for real-world applications, yet current research in this area is not satisfactory. In this paper, we propose DIFFender, a novel defense method that leverages a text-guided diffusion model to defend against adversarial patches. DIFFender includes two main stages: patch localization and patch restoration. In the localization stage, we find and exploit an intriguing property of the diffusion model to effectively identify the locations of adversarial patches. In the restoration stage, we employ the diffusion model to reconstruct the adversarial regions in the images while preserving the integrity of the visual content. Importantly, these two stages are carefully guided by a unified diffusion model, thus we can utilize the close interaction between them to improve the whole defense performance. Mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01276</link><description>&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#20013;&#23545;&#31216;&#25506;&#32034;&#26159;&#20813;&#36153;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Symmetric Exploration in Combinatorial Optimization is Free!. (arXiv:2306.01276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20813;&#36153;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#25552;&#39640;&#20102;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20813;&#36153;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#31216;&#24615;&#26469;&#22686;&#24378;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20445;&#30041;&#22870;&#21169;&#30340;&#21464;&#25442;&#26469;&#22686;&#24378;&#22522;&#20110;DRL&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#12290;&#35813;&#31639;&#27861;&#21487;&#33021;&#20855;&#26377;&#24433;&#21709;&#21147;&#65292;&#22240;&#20026;&#23427;&#31616;&#21333;&#65292;&#26131;&#20110;&#19982;&#29616;&#26377;&#27714;&#35299;&#22120;&#38598;&#25104;&#65292;&#24182;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;NP&#38590;&#30340;&#36335;&#32447;&#20248;&#21270;&#65292;&#35843;&#24230;&#20248;&#21270;&#21644;&#26032;&#22411;&#20998;&#23376;&#20248;&#21270;&#30340;&#24191;&#27867;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36731;&#26494;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (DRL) has shown promise in solving combinatorial optimization (CO) problems. However, they often require a large number of evaluations on the objective function, which can be time-consuming in real-world scenarios. To address this issue, we propose a "free" technique to enhance the performance of any deep reinforcement learning (DRL) solver by exploiting symmetry without requiring additional objective function evaluations. Our key idea is to augment the training of DRL-based combinatorial optimization solvers by reward-preserving transformations. The proposed algorithm is likely to be impactful since it is simple, easy to integrate with existing solvers, and applicable to a wide range of combinatorial optimization tasks. Extensive empirical evaluations on NP-hard routing optimization, scheduling optimization, and de novo molecular optimization confirm that our method effortlessly improves the sample efficiency of state-of-the-art DRL algorithms. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15203</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#32500;&#24230;&#23558;&#38544;&#24615;&#20559;&#35265;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30456;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension. (arXiv:2305.15203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#26159;&#38024;&#23545;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#30340;&#23567;&#24178;&#25200;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#35774;&#32622;&#25110;&#23646;&#24615;&#19982;&#25915;&#20987;&#30340;&#24615;&#36136;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#32852;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#20854;&#22266;&#26377;&#20542;&#21521;&#20110;&#25903;&#25345;&#29305;&#23450;&#27169;&#24335;&#25110;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38544;&#24615;&#20559;&#24046;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#21253;&#25324;&#36827;&#34892;&#20934;&#30830;&#22270;&#20687;&#20998;&#31867;&#25152;&#38656;&#30340;&#22522;&#26412;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#27979;&#35797;&#20197;&#35780;&#20272;&#36825;&#20123;&#39057;&#29575;&#19982;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#39057;&#29575;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#20026;&#20102;&#28145;&#20837;&#25506;&#35752;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#22352;&#26631;&#38598;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22352;&#26631;&#38598;&#23601;&#26159;&#21069;&#36848;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25293;&#21334;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11005</link><description>&lt;p&gt;
&#25293;&#21334;&#35774;&#35745;&#20013;&#30340;&#27169;&#24335;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mode Connectivity in Auction Design. (arXiv:2305.11005v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25293;&#21334;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#38750;&#24120;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#24456;&#38590;&#12290;&#26368;&#36817;&#19981;&#21516;&#30340;&#32463;&#27982;&#23398;&#21487;&#24494;&#20998;&#29702;&#35770;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24050;&#30693;&#30340;&#26368;&#20248;&#25293;&#21334;&#26426;&#21046;&#65292;&#21457;&#29616;&#26377;&#36259;&#30340;&#26032;&#26426;&#21046;&#12290;&#20026;&#20102;&#29702;&#35770;&#19978;&#35777;&#26126;&#23427;&#20204;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;RochetNet&#65292;&#24182;&#30740;&#31350;&#25152;&#35859;&#30340;&#20223;&#23556;&#26497;&#22823;&#21270;&#25293;&#21334;&#30340;&#24191;&#20041;&#29256;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#28385;&#36275;&#27169;&#24335;&#36830;&#36890;&#24615;&#65292;&#21363;&#23616;&#37096;&#26368;&#20248;&#35299;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#65292;&#36335;&#24452;&#19978;&#30340;&#27599;&#20010;&#35299;&#37117;&#20960;&#20046;&#21644;&#20004;&#20010;&#23616;&#37096;&#26368;&#20248;&#35299;&#20043;&#19968;&#19968;&#26679;&#22909;&#12290;&#27169;&#24335;&#36830;&#36890;&#24615;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#21487;&#24494;&#20998;&#32463;&#27982;&#23398;&#39046;&#22495;&#20013;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#35774;&#35745;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal auction design is a fundamental problem in algorithmic game theory. This problem is notoriously difficult already in very simple settings. Recent work in differentiable economics showed that neural networks can efficiently learn known optimal auction mechanisms and discover interesting new ones. In an attempt to theoretically justify their empirical success, we focus on one of the first such networks, RochetNet, and a generalized version for affine maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally optimal solutions are connected by a simple, piecewise linear path such that every solution on the path is almost as good as one of the two local optima. Mode connectivity has been recently investigated as an intriguing empirical and theoretically justifiable property of neural networks used for prediction problems. Our results give the first such analysis in the context of differentiable economics, where neural networks are used directly for solving non-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07408</link><description>&lt;p&gt;
&#38754;&#21521;&#20989;&#25968;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Gradient Descent for Functional Learning. (arXiv:2305.07408v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#36890;&#36807;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#24471;&#21040;&#20102;&#35813;&#31639;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#39281;&#21644;&#36793;&#30028;&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#22240;&#20854;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#20449;&#24687;&#26041;&#38754;&#30340;&#24040;&#22823;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#38024;&#23545;&#26368;&#36817;&#20174;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#20135;&#29983;&#30340;&#22823;&#25968;&#25454;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;DGDFL&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20247;&#22810;&#26412;&#22320;&#26426;&#22120;&#65288;&#22788;&#29702;&#22120;&#65289;&#30340;&#20989;&#25968;&#25968;&#25454;&#12290;&#22522;&#20110;&#31215;&#20998;&#31639;&#23376;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;DGDFL&#31639;&#27861;&#22312;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#26041;&#38754;&#30340;&#31532;&#19968;&#20010;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#29702;&#35299;DGDFL&#30340;&#36807;&#31243;&#20013;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20840;&#38754;&#30740;&#31350;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#28176;&#36827;&#24335;&#19979;&#38477;&#20989;&#25968;&#23398;&#20064;&#65288;GDFL&#65289;&#31639;&#27861;&#19982;&#21333;&#26426;&#27169;&#22411;&#30456;&#20851;&#32852;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#24471;&#21040;&#20102;DGDFL&#30340;&#32622;&#20449;&#24230;&#26368;&#20248;&#23398;&#20064;&#29575;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#22312;&#27491;&#21017;&#24615;&#32034;&#24341;&#19978;&#36973;&#21463;&#30340;&#39281;&#21644;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, different types of distributed learning schemes have received increasing attention for their strong advantages in handling large-scale data information. In the information era, to face the big data challenges which stem from functional data analysis very recently, we propose a novel distributed gradient descent functional learning (DGDFL) algorithm to tackle functional data across numerous local machines (processors) in the framework of reproducing kernel Hilbert space. Based on integral operator approaches, we provide the first theoretical understanding of the DGDFL algorithm in many different aspects in the literature. On the way of understanding DGDFL, firstly, a data-based gradient descent functional learning (GDFL) algorithm associated with a single-machine model is proposed and comprehensively studied. Under mild conditions, confidence-based optimal learning rates of DGDFL are obtained without the saturation boundary on the regularity index suffered in previous w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.00386</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#65292;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#29983;&#29289;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20195;&#29702;&#24207;&#21015;-&#21151;&#33021;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#30340;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21463;&#21040;&#21478;&#22806;&#19968;&#20010;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32467;&#26500;&#29305;&#24449;&#30340;&#22686;&#24378;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;MCEM&#65289;&#26469;&#23398;&#20064;&#36825;&#20010;&#27169;&#22411;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20854;MRF&#29305;&#24449;&#21017;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#22312;&#20843;&#39033;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#39640;55&#65285;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11858</link><description>&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#36923;&#36753;&#26597;&#35810;&#24212;&#31572;&#30340;&#20851;&#31995;&#27169;&#24335;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Relational Patterns for Logical Query Answering over Knowledge Graphs. (arXiv:2303.11858v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#24182;&#25552;&#39640;&#20102;&#36923;&#36753;&#26597;&#35810;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36827;&#34892;&#19968;&#38454;&#36923;&#36753;&#65288;FOL&#65289;&#26597;&#35810;&#30340;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;KG&#19981;&#23436;&#25972;&#24615;&#32780;&#23548;&#33268;&#30340;&#12290;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#36923;&#36753;&#26597;&#35810;&#30340;&#20302;&#32500;&#24230;&#21521;&#37327;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;KG&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#21644;&#32452;&#21512;&#24615;&#31561;&#20851;&#31995;&#27169;&#24335;&#65292;&#24314;&#27169;&#36825;&#20123;&#27169;&#24335;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#24335;&#22312;&#26597;&#35810;&#23884;&#20837;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#36827;&#34892;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20801;&#35768;&#23398;&#20064;&#20851;&#31995;&#27169;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#21152;&#24378;FOL&#26597;&#35810;&#25512;&#29702;&#30340;&#27169;&#24335;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;RoConE&#65292;&#23427;&#23558;&#26597;&#35810;&#21306;&#22495;&#23450;&#20041;&#20026;&#20960;&#20309;&#38181;&#20307;&#65292;&#24182;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#12290;RoConE&#32467;&#21512;&#20102;&#20960;&#20309;&#38181;&#20307;&#20316;&#20026;&#21487;&#20197;&#26126;&#30830;&#23450;&#20041;&#26597;&#35810;&#34920;&#31034;&#30340;&#20960;&#20309;&#34920;&#31034;&#21644;&#26059;&#36716;&#20195;&#25968;&#26597;&#35810;&#31639;&#23376;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering first-order logical (FOL) queries over knowledge graphs (KG) remains a challenging task mainly due to KG incompleteness. Query embedding approaches this problem by computing the low-dimensional vector representations of entities, relations, and logical queries. KGs exhibit relational patterns such as symmetry and composition and modeling the patterns can further enhance the performance of query embedding models. However, the role of such patterns in answering FOL queries by query embedding models has not been yet studied in the literature. In this paper, we fill in this research gap and empower FOL queries reasoning with pattern inference by introducing an inductive bias that allows for learning relation patterns. To this end, we develop a novel query embedding method, RoConE, that defines query regions as geometric cones and algebraic query operators by rotations in complex space. RoConE combines the advantages of Cone as a well-specified geometric representation for query e
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.09079</link><description>&lt;p&gt;
SSL&#28165;&#29702;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
SSL-Cleanse: Trojan Detection and Mitigation in Self-Supervised Learning. (arXiv:2303.09079v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#38382;&#39064;&#12290;&#30001;&#20110;&#36825;&#31181;&#25915;&#20987;&#21361;&#38505;&#38544;&#21311;&#65292;&#19988;&#22312;&#19979;&#28216;&#20998;&#31867;&#22120;&#20013;&#24456;&#38590;&#26816;&#27979;&#20986;&#26469;&#12290;&#30446;&#21069;&#22312;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;SSL&#22270;&#20687;&#32534;&#30721;&#22120;&#24182;&#22312;&#20854;&#39030;&#37096;&#35757;&#32451;&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#32780;&#21482;&#38656;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SSL&#30340;&#22686;&#21152;&#20351;&#29992;&#23548;&#33268;&#20102;&#19982;SSL&#32534;&#30721;&#22120;&#30456;&#20851;&#30340;&#23433;&#20840;&#30740;&#31350;&#21644;&#21508;&#31181;&#26408;&#39532;&#25915;&#20987;&#30340;&#21457;&#23637;&#12290;&#22312;SSL&#32534;&#30721;&#22120;&#20013;&#25554;&#20837;&#26408;&#39532;&#25915;&#20987;&#30340;&#21361;&#38505;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#38544;&#34109;&#22320;&#25805;&#20316;&#24182;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#35774;&#22791;&#20043;&#38388;&#24191;&#27867;&#20256;&#25773;&#12290;Trojaned&#32534;&#30721;&#22120;&#20013;&#30340;&#21518;&#38376;&#34892;&#20026;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#34987;&#19979;&#28216;&#20998;&#31867;&#22120;&#24847;&#22806;&#32487;&#25215;&#65292;&#20351;&#26816;&#27979;&#21644;&#32531;&#35299;&#23041;&#32961;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#36229;&#30417;&#30563;&#23398;&#20064;&#20013;&#24403;&#21069;&#30340;&#26408;&#39532;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#28508;&#22312;&#22320;&#20445;&#25252;SSL&#19979;&#28216;&#20998;&#31867;&#22120;&#65292;&#20294;&#22312;&#20854;&#24191;&#27867;&#20256;&#25773;&#20043;&#21069;&#35782;&#21035;&#21644;&#22788;&#29702;SSL&#32534;&#30721;&#22120;&#20013;&#30340;&#35302;&#21457;&#22120;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a commonly used approach to learning and encoding data representations. By using a pre-trained SSL image encoder and training a downstream classifier on top of it, impressive performance can be achieved on various tasks with very little labeled data. The increasing usage of SSL has led to an uptick in security research related to SSL encoders and the development of various Trojan attacks. The danger posed by Trojan attacks inserted in SSL encoders lies in their ability to operate covertly and spread widely among various users and devices. The presence of backdoor behavior in Trojaned encoders can inadvertently be inherited by downstream classifiers, making it even more difficult to detect and mitigate the threat. Although current Trojan detection methods in supervised learning can potentially safeguard SSL downstream classifiers, identifying and addressing triggers in the SSL encoder before its widespread dissemination is a challenging task. This is be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07068</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#26368;&#20248;n&#30340;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;n&#27493;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20013;&#25214;&#21040;&#26368;&#20248;n&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#30001;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;n&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;SPSA&#31243;&#24207;&#65292;&#23558;&#20854;&#21407;&#22987;&#36830;&#32493;&#20248;&#21270;&#26694;&#26550;&#24341;&#20837;&#31163;&#25955;&#20248;&#21270;&#26694;&#26550;&#65292;&#20294;&#24182;&#32467;&#21512;&#20102;&#24490;&#29615;&#25200;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;SDPSA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#20540;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;n&#27493;TD&#20013;&#30340;&#26368;&#20248;n&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SDPSA&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;n&#20540;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#21644;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#30340;V2X&#36890;&#20449;&#22330;&#26223;DT&#21019;&#24314;&#21644;&#20223;&#30495;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#31227;&#21160;&#24615;V2X&#36890;&#20449;&#29615;&#22659;&#19979;&#20934;&#30830;&#27169;&#25311;&#20986;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20449;&#36947;&#12290;</title><link>http://arxiv.org/abs/2303.06947</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#65292;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#22522;&#20110;&#21160;&#24577;&#29615;&#22659;&#30340;V2X&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
A Multi-Modal Simulation Framework to Enable Digital Twin-based V2X Communications in Dynamic Environments. (arXiv:2303.06947v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#21644;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#30340;V2X&#36890;&#20449;&#22330;&#26223;DT&#21019;&#24314;&#21644;&#20223;&#30495;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#31227;&#21160;&#24615;V2X&#36890;&#20449;&#29615;&#22659;&#19979;&#20934;&#30830;&#27169;&#25311;&#20986;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20449;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#29289;&#29702;&#26080;&#32447;&#29615;&#22659;&#30340;&#31934;&#30830;&#34394;&#25311;&#34920;&#31034;&#65292;&#21487;&#22312;&#29289;&#29702;&#36890;&#20449;&#35774;&#22791;&#19978;&#23454;&#29616;&#22810;&#23618;&#20915;&#31574;&#12290;&#22312;&#39640;&#39057;&#27573;&#65292;DT&#21487;&#24110;&#21161;&#20811;&#26381;V2X&#36890;&#20449;&#39640;&#31227;&#21160;&#24615;&#29615;&#22659;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;V2X&#36890;&#20449;&#22330;&#26223;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#24037;&#20316;&#27969;&#29992;&#20110;&#21019;&#24314;DT&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20223;&#30495;&#26694;&#26550;&#29992;&#20110;&#20135;&#29983;&#36924;&#30495;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#20934;&#30830;&#30340;&#27627;&#31859;&#27874;/&#20122;&#27627;&#31859;&#27874;&#26080;&#32447;&#20449;&#36947;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#34394;&#24187;&#24341;&#25806;&#28216;&#25103;&#24341;&#25806;&#30340;&#27773;&#36710;&#20223;&#30495;&#21644;&#27979;&#35797;&#26694;&#26550;&#20197;&#21450;&#20934;&#30830;&#30340;&#23556;&#32447;&#36319;&#36394;&#20449;&#36947;&#27169;&#25311;&#22120;&#12290;&#22312;&#22478;&#24066;&#22330;&#26223;&#19979;&#30340;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#30784;&#35774;&#26045;&#21644;&#33258;&#36710;&#30340;&#36924;&#30495;&#20256;&#24863;&#22120;&#21644;&#20449;&#36947;&#24314;&#27169;&#22343;&#21487;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins (DTs) for physical wireless environments have been recently proposed as accurate virtual representations of the propagation environment that can enable multi-layer decisions at the physical communication equipment. At high frequency bands, DTs can help to overcome the challenges emerging in the high mobility conditions featuring vehicular environments. In this paper, we propose a novel data-driven workflow for the creation of the DT of a Vehicle-to-Everything (V2X) communication scenario and a multi-modal simulation framework for the generation of realistic sensor data and accurate mmWave/sub-THz wireless channels. The proposed method leverages an automotive simulation and testing framework based on the Unreal Engine game engine and an accurate ray-tracing channel simulator. Simulations over an urban scenario show the achievable realistic sensor and channel modelling both at the infrastructure and at an ego-vehicle.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11407</link><description>&lt;p&gt;
&#22522;&#20110;&#36755;&#20837;&#26799;&#24230;&#20256;&#36882;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Architectures Based on Input Gradient Transferability. (arXiv:2210.11407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#65292;&#20998;&#26512;&#20102;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#65292;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#32780;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#30456;&#20284;&#25110;&#19981;&#21516;&#65292;&#20197;&#21450;&#20160;&#20040;&#22240;&#32032;&#24433;&#21709;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#25110;&#19981;&#21516;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#37327;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#30456;&#20284;&#24230;&#20989;&#25968;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#25915;&#20987;&#20256;&#36882;&#24230;&#37327;&#65292;&#35813;&#24230;&#37327;&#20855;&#26377;&#19982;&#36755;&#20837;&#26799;&#24230;&#21644;&#20915;&#31574;&#36793;&#30028;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#23545;69&#20010;&#26368;&#20808;&#36827;&#30340;ImageNet&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#20174;&#32780;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19982;&#31070;&#32463;&#26550;&#26500;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#22810;&#26679;&#24615;&#21487;&#20197;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#23545;&#27169;&#22411;&#38598;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20026;&#20160;&#20040;&#24320;&#21457;&#20855;&#26377;&#19981;&#21516;&#32452;&#20214;&#30340;&#22810;&#26679;&#21270;&#31070;&#32463;&#26550;&#26500;&#26159;&#24517;&#35201;&#30340;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a huge amount of deep neural architectures have been developed for image classification. It remains curious whether these models are similar or different and what factors contribute to their similarities or differences. To address this question, we aim to design a quantitative and scalable similarity function between neural architectures. We utilize adversarial attack transferability, which has information related to input gradients and decision boundaries that are widely used to understand model behaviors. We conduct a large-scale analysis on 69 state-of-the-art ImageNet classifiers using our proposed similarity function to answer the question. Moreover, we observe neural architecture-related phenomena using model similarity that model diversity can lead to better performance on model ensembles and knowledge distillation under specific conditions. Our results provide insights into why the development of diverse neural architectures with distinct components is necessar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36816;&#29992;&#25968;&#23398;&#21644;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#23601; ReLU &#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#19979;&#30028;&#20570;&#20102;&#25506;&#31350;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#32943;&#23450;&#20102;&#19968;&#39033;&#26087;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#29468;&#24819;&#12290;</title><link>http://arxiv.org/abs/2105.14835</link><description>&lt;p&gt;
&#20851;&#20110; ReLU &#31070;&#32463;&#32593;&#32476;&#28145;&#24230;&#19979;&#30028;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Lower Bounds on the Depth of ReLU Neural Networks. (arXiv:2105.14835v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36816;&#29992;&#25968;&#23398;&#21644;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#23601; ReLU &#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#19979;&#30028;&#20570;&#20102;&#25506;&#31350;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#31181;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#32943;&#23450;&#20102;&#19968;&#39033;&#26087;&#30340;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#29468;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36816;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#12289;&#22810;&#38754;&#20307;&#29702;&#35770;&#21644;&#28909;&#24102;&#20960;&#20309;&#23398;&#31561;&#25216;&#26415;&#65292;&#20026;&#29702;&#35299;&#20855;&#26377; ReLU &#28608;&#27963;&#21644;&#32473;&#23450;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#20570;&#20986;&#20102;&#26356;&#22909;&#30340;&#36129;&#29486;&#12290;&#23613;&#31649;&#26222;&#36866;&#36924;&#36817;&#23450;&#29702;&#35748;&#20026;&#21333;&#23618;&#38544;&#34255;&#23618;&#23601;&#36275;&#20197;&#23398;&#20064;&#20219;&#20309;&#20989;&#25968;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#28155;&#21152;&#26356;&#22810;&#23618;&#65288;&#26080;&#22823;&#23567;&#38480;&#21046;&#65289;&#26102;&#26159;&#21542;&#20005;&#26684;&#22686;&#21152;&#20102;&#21487;&#34920;&#31034;&#20989;&#25968;&#30340;&#31867;&#12290;&#20316;&#20026;&#30740;&#31350;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#32943;&#23450;&#20102; Wang &#21644; Sun&#65288;2005&#65289;&#26377;&#20851;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#19968;&#20010;&#26087;&#29468;&#24819;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#20989;&#25968;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We contribute to a better understanding of the class of functions that can be represented by a neural network with ReLU activations and a given architecture. Using techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we provide a mathematical counterbalance to the universal approximation theorems which suggest that a single hidden layer is sufficient for learning any function. In particular, we investigate whether the class of exactly representable functions strictly increases by adding more layers (with no restrictions on size). As a by-product of our investigations, we settle an old conjecture about piecewise linear functions by Wang and Sun (2005) in the affirmative. We also present upper bounds on the sizes of neural networks required to represent functions with logarithmic depth.
&lt;/p&gt;</description></item></channel></rss>