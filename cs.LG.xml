<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2404.01300</link><description>&lt;p&gt;
NeRF-MAE: &#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;Masked AutoEncoders
&lt;/p&gt;
&lt;p&gt;
NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation Learning for Neural Radiance Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Masked AutoEncoders&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;NeRF-MAE&#29992;&#20110;&#33258;&#30417;&#30563;&#19977;&#32500;&#34920;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#65292;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;&#23494;&#38598;&#36755;&#20837;&#65292;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#19977;&#32500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#22330;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#33021;&#22815;&#29702;&#35299;&#19977;&#32500;&#35270;&#35273;&#19990;&#30028;&#65292;&#22914;&#25512;&#26029;&#35821;&#20041;&#12289;&#20960;&#20309;&#21644;&#21160;&#24577;&#31561;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#22330;&#22312;&#20174;&#20108;&#32500;&#22270;&#20687;&#20013;&#23494;&#38598;&#34920;&#31034;&#19977;&#32500;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20855;&#20307;&#20351;&#29992;Masked AutoEncoders&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23558;transformers&#25193;&#23637;&#21040;&#26032;&#25968;&#25454;&#27169;&#24577;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#25104;&#21151;&#65292;&#21033;&#29992;&#26631;&#20934;&#30340;&#19977;&#32500;Vision Transformers&#26469;&#36866;&#24212;NeRF&#30340;&#29420;&#29305;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;NeRF&#30340;&#20307;&#31215;&#32593;&#26684;&#20316;&#20026;transformer&#30340;&#23494;&#38598;&#36755;&#20837;&#65292;&#19982;&#20854;&#20182;&#19977;&#32500;&#34920;&#31034;&#65288;&#22914;&#28857;&#20113;&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#20854;&#20449;&#24687;&#23494;&#24230;&#21487;&#33021;&#19981;&#22343;&#21248;&#65292;&#32780;&#34920;&#31034;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#30001;&#20110;&#23558;masked autoencoders&#24212;&#29992;&#20110;&#31867;&#20284;NeRF&#36825;&#26679;&#30340;&#38544;&#24335;&#34920;&#31034;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#36873;&#25321;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01300v1 Announce Type: cross  Abstract: Neural fields excel in computer vision and robotics due to their ability to understand the 3D visual world such as inferring semantics, geometry, and dynamics. Given the capabilities of neural fields in densely representing a 3D scene from 2D images, we ask the question: Can we scale their self-supervised pretraining, specifically using masked autoencoders, to generate effective 3D representations from posed RGB images. Owing to the astounding success of extending transformers to novel data modalities, we employ standard 3D Vision Transformers to suit the unique formulation of NeRFs. We leverage NeRF's volumetric grid as a dense input to the transformer, contrasting it with other 3D representations such as pointclouds where the information density can be uneven, and the representation is irregular. Due to the difficulty of applying masked autoencoders to an implicit representation, such as NeRF, we opt for extracting an explicit repres
&lt;/p&gt;</description></item><item><title>NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.18211</link><description>&lt;p&gt;
NeuroPictor: &#36890;&#36807;&#22810;&#20010;&#20010;&#20307;&#30340;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#35843;&#21046;&#20248;&#21270;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual Pretraining and Multi-level Modulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18211
&lt;/p&gt;
&lt;p&gt;
NeuroPictor&#36890;&#36807;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;fMRI&#21040;&#22270;&#20687;&#30340;&#37325;&#24314;&#65292;&#22312;&#22810;&#20010;&#20010;&#20307;&#39044;&#35757;&#32451;&#21644;&#22810;&#23618;&#27425;&#30340;&#24341;&#23548;&#26465;&#20214;&#19979;&#65292;&#23454;&#29616;&#20102;&#26356;&#35814;&#32454;&#30340;&#22270;&#20687;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;fMRI&#21040;&#22270;&#20687;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;fMRI&#20449;&#21495;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#23450;&#26465;&#20214;&#20851;&#32852;&#36215;&#26469;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#30452;&#25509;&#35843;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23558;fMRI&#21040;&#22270;&#20687;&#36807;&#31243;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;i) fMRI&#26657;&#20934;&#32534;&#30721;&#65292;&#29992;&#20110;&#22788;&#29702;&#20849;&#20139;&#28508;&#22312;&#31354;&#38388;&#30340;&#22810;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#20197;&#26368;&#23567;&#21270;&#20010;&#20307;&#24046;&#24322;&#24182;&#23454;&#29616;&#21518;&#32493;&#30340;&#36328;&#20027;&#20307;&#35757;&#32451;&#65307;ii) fMRI&#21040;&#22270;&#20687;&#36328;&#20010;&#20307;&#39044;&#35757;&#32451;&#65292;&#24863;&#30693;&#22320;&#23398;&#20064;&#22914;&#20309;&#24341;&#23548;&#19981;&#21516;&#20010;&#20307;&#20043;&#38388;&#39640;&#20302;&#23618;&#27425;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#65307;iii) fMRI&#21040;&#22270;&#20687;&#21333;&#20010;&#20307;&#32454;&#21270;&#65292;&#31867;&#20284;&#20110;&#27493;&#39588;ii&#65292;&#20294;&#20391;&#37325;&#20110;&#36866;&#24212;&#29305;&#23450;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18211v1 Announce Type: cross  Abstract: Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particula
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14951</link><description>&lt;p&gt;
&#31616;&#21333;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Simple Graph Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22270;&#31070;&#32463;&#32593;&#32476;&#25152;&#24102;&#26469;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#19978;&#32321;&#37325;&#30340;&#35757;&#32451;&#25104;&#26412;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#22270;&#21387;&#32553;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#28041;&#21450;&#35843;&#25972;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23567;&#23610;&#24230;&#21387;&#32553;&#22270;&#19978;&#30340;&#35757;&#32451;&#20197;&#22312;&#22823;&#35268;&#27169;&#21407;&#22987;&#22270;&#19978;&#20351;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#21387;&#32553;&#22270;&#21644;&#21407;&#22987;&#22270;&#20043;&#38388;&#30340;&#20851;&#38190;&#25351;&#26631;&#65292;&#22914;&#26799;&#24230;&#12289;GNNs&#30340;&#20998;&#24067;&#21644;&#36712;&#36857;&#65292;&#20174;&#32780;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#25351;&#26631;&#38656;&#35201;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#21487;&#33021;&#20250;&#24178;&#25200;&#21387;&#32553;&#22270;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#21387;&#32553;&#36807;&#31243;&#38750;&#24120;&#32321;&#37325;&#21644;&#19981;&#31283;&#23450;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#31616;&#21270;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#22270;&#21387;&#32553;&#20013;&#30340;&#25351;&#26631;&#23545;&#20934;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#20174;GNNs&#32487;&#25215;&#30340;&#19981;&#24517;&#35201;&#22797;&#26434;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#28040;&#38500;&#22806;&#37096;&#21442;&#25968;&#65292;&#20165;&#20445;&#30041;&#30446;&#26631;&#30340;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14951v1 Announce Type: cross  Abstract: The burdensome training costs on large-scale graphs have aroused significant interest in graph condensation, which involves tuning Graph Neural Networks (GNNs) on a small condensed graph for use on the large-scale original graph. Existing methods primarily focus on aligning key metrics between the condensed and original graphs, such as gradients, distribution and trajectory of GNNs, yielding satisfactory performance on downstream tasks. However, these complex metrics necessitate intricate computations and can potentially disrupt the optimization process of the condensation graph, making the condensation process highly demanding and unstable. Motivated by the recent success of simplified models in various fields, we propose a simplified approach to metric alignment in graph condensation, aiming to reduce unnecessary complexity inherited from GNNs. In our approach, we eliminate external parameters and exclusively retain the target conden
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.13808</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Pretraining Data Diversity for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13808
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#36739;&#23567;&#26102;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26356;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#26159;&#21807;&#19968;&#26679;&#26412;&#25968;&#37327;&#65292;&#22312;&#22266;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#22686;&#21152;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;SSL&#24615;&#33021;&#65292;&#23613;&#31649;&#21482;&#26377;&#24403;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#36317;&#31163;&#24456;&#23567;&#30340;&#26102;&#20505;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#36890;&#36807;&#32593;&#32476;&#29228;&#34411;&#25110;&#25193;&#25955;&#29983;&#25104;&#30340;&#25968;&#25454;&#31561;&#26041;&#24335;&#23454;&#29616;&#20102;&#24322;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20998;&#24067;&#36716;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28085;&#30422;&#20102;&#19971;&#31181;SSL&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;ImageNet&#21644;YFCC100M&#31561;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24635;&#35745;&#36229;&#36807;200&#20010;GPU&#22825;&#12290;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#23558;&#22312;https://github.com/hammoudhasan/DiversitySSL &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#36716;&#23548;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;PARMESAN&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#21644;&#36716;&#23548;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#26080;&#38656;&#36830;&#32493;&#35757;&#32451;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.11743</link><description>&lt;p&gt;
PARMESAN: &#29992;&#20110;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#19982;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
PARMESAN: Parameter-Free Memory Search and Transduction for Dense Prediction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11743
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#36716;&#23548;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;PARMESAN&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#21644;&#36716;&#23548;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#26080;&#38656;&#36830;&#32493;&#35757;&#32451;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36716;&#23548;&#25512;&#29702;&#26469;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PARMESAN&#65288;&#26080;&#21442;&#25968;&#20869;&#23384;&#25628;&#32034;&#19982;&#36716;&#23548;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36716;&#23548;&#26041;&#27861;&#65292;&#21033;&#29992;&#20869;&#23384;&#27169;&#22359;&#26469;&#35299;&#20915;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#20869;&#23384;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#34987;&#25628;&#32034;&#20197;&#25214;&#21040;&#30456;&#24212;&#30340;&#31034;&#20363;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;PARMESAN&#36890;&#36807;&#20462;&#25913;&#20869;&#23384;&#20869;&#23481;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36830;&#32493;&#35757;&#32451;&#25110;&#24494;&#35843;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24120;&#29992;&#30340;&#31070;&#32463;&#32467;&#26500;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11743v1 Announce Type: new  Abstract: In this work we address flexibility in deep learning by means of transductive reasoning. For adaptation to new tasks or new data, existing methods typically involve tuning of learnable parameters or even complete re-training from scratch, rendering such approaches unflexible in practice. We argue that the notion of separating computation from memory by the means of transduction can act as a stepping stone for solving these issues. We therefore propose PARMESAN (parameter-free memory search and transduction), a scalable transduction method which leverages a memory module for solving dense prediction tasks. At inference, hidden representations in memory are being searched to find corresponding examples. In contrast to other methods, PARMESAN learns without the requirement for any continuous training or fine-tuning of learnable parameters simply by modifying the memory content. Our method is compatible with commonly used neural architecture
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;</title><link>https://arxiv.org/abs/2403.10663</link><description>&lt;p&gt;
&#19981;&#20165;&#25913;&#21464;&#26631;&#31614;&#65292;&#23398;&#20064;&#29305;&#24449;&#65306;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10663
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#35270;&#35282;&#25968;&#25454;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28155;&#21152;&#27700;&#21360;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#23545;&#28304;&#27169;&#22411;&#21151;&#33021;&#30340;&#31363;&#21462;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#24179;&#21488;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27700;&#21360;&#25216;&#26415;&#12290;&#36825;&#20123;&#26041;&#27861;&#29992;&#20110;&#39564;&#35777;&#30446;&#26631;DNN&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#12290;&#26412;&#25991;&#39318;&#20808;&#20174;&#29305;&#24449;&#23398;&#20064;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35302;&#21457;&#38598;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#36873;&#25321;&#23637;&#31034;&#22810;&#20010;&#29305;&#24449;&#30340;&#25968;&#25454;&#65292;&#20063;&#34987;&#31216;&#20026;$\textit{&#22810;&#35270;&#35282;&#25968;&#25454;}$&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38450;&#24481;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10663v1 Announce Type: cross  Abstract: With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as $\textit{multi-view data}$, it becomes feasible to effectively defend 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.09635</link><description>&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09635
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#20449;&#21495;&#20256;&#25773;&#30340;&#20844;&#24335;&#65292;&#25552;&#20986;&#20102;DeepScaleLM&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#20351;&#24471;&#21487;&#20197;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#28145;&#23618;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#32988;&#36807;&#27973;&#23618;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#28145;&#24230;&#26041;&#38754;&#20173;&#28982;&#24456;&#38590;&#25193;&#23637;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#21495;&#20256;&#25773;&#29702;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#25511;&#21046;transformer&#27169;&#22411;&#21069;&#21521;&#21644;&#21453;&#21521;&#20449;&#21495;&#30697;&#30340;&#20844;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#29992;&#20110;&#29702;&#35299;&#21644;&#32531;&#35299;&#19982;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#30456;&#20851;&#30340;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#31209;&#22349;&#32553;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;DeepScaleLM&#65292;&#19968;&#31181;&#21021;&#22987;&#21270;&#21644;&#32553;&#25918;&#26041;&#26696;&#65292;&#36890;&#36807;&#35813;&#26041;&#26696;&#33021;&#22815;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#21333;&#20301;&#36755;&#20986;/&#26799;&#24230;&#30697;&#65292;&#20174;&#32780;&#20351;&#35757;&#32451;&#20855;&#26377;100&#22810;&#23618;&#30340;&#38750;&#24120;&#28145;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;transformer&#27169;&#22411;&#21487;&#20197;&#26356;&#28145; - &#25105;&#20204;&#30340;&#28145;&#23618;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#12289;&#35821;&#38899;&#32763;&#35793;&#21644;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#20165;&#32534;&#30721;&#22120;&#12289;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#20307;&#65292;&#36866;&#29992;&#20110;Pre-LN&#21644;Post-LN transformers&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.07128</link><description>&lt;p&gt;
FAX: JAX&#20013;&#21487;&#25193;&#23637;&#19988;&#21487;&#24494;&#20998;&#30340;&#32852;&#37030;&#21407;&#35821;
&lt;/p&gt;
&lt;p&gt;
FAX: Scalable and Differentiable Federated Primitives in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07128
&lt;/p&gt;
&lt;p&gt;
FAX&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#23884;&#20837;&#32852;&#37030;&#35745;&#31639;&#21407;&#35821;&#30340;&#24211;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23454;&#29616;&#65292;&#24182;&#21487;&#35299;&#37322;&#33267;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FAX&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;JAX&#35774;&#35745;&#30340;&#24211;&#65292;&#26088;&#22312;&#25903;&#25345;&#25968;&#25454;&#20013;&#24515;&#21644;&#36328;&#35774;&#22791;&#24212;&#29992;&#20013;&#30340;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#21644;&#32852;&#37030;&#35745;&#31639;&#12290;FAX&#21033;&#29992;JAX&#30340;&#20998;&#29255;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#21407;&#29983;&#38024;&#23545;TPU&#21644;&#26368;&#20808;&#36827;&#30340;JAX&#36816;&#34892;&#26102;&#65288;&#21253;&#25324;Pathways&#65289;&#30340;&#23450;&#20301;&#12290;FAX&#23558;&#32852;&#37030;&#35745;&#31639;&#30340;&#22522;&#26412;&#26500;&#20214;&#23884;&#20837;JAX&#20013;&#65292;&#24102;&#26469;&#20102;&#19977;&#20010;&#20851;&#38190;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#36716;&#25442;&#20026;XLA HLO&#12290;&#20854;&#27425;&#65292;FAX&#25552;&#20379;&#20102;&#32852;&#37030;&#33258;&#21160;&#24494;&#20998;&#30340;&#23436;&#25972;&#23454;&#29616;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#32852;&#37030;&#35745;&#31639;&#30340;&#34920;&#36798;&#12290;&#26368;&#21518;&#65292;FAX&#30340;&#35745;&#31639;&#21487;&#20197;&#35299;&#37322;&#25104;&#29616;&#26377;&#30340;&#29983;&#20135;&#36328;&#35774;&#22791;&#32852;&#37030;&#35745;&#31639;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FAX&#20026;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#32852;&#37030;&#35745;&#31639;&#25552;&#20379;&#20102;&#26131;&#32534;&#31243;&#12289;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#12290;FAX&#21487;&#22312;https://github.com/google-research/google-research/tree/master/fax &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07128v1 Announce Type: cross  Abstract: We present FAX, a JAX-based library designed to support large-scale distributed and federated computations in both data center and cross-device applications. FAX leverages JAX's sharding mechanisms to enable native targeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX embeds building blocks for federated computations as primitives in JAX. This enables three key benefits. First, FAX computations can be translated to XLA HLO. Second, FAX provides a full implementation of federated automatic differentiation, greatly simplifying the expression of federated computations. Last, FAX computations can be interpreted out to existing production cross-device federated compute systems. We show that FAX provides an easily programmable, performant, and scalable framework for federated computations in the data center. FAX is available at https://github.com/google-research/google-research/tree/master/fax .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06870</link><description>&lt;p&gt;
&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Semantic Residual Prompts for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06870
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#21097;&#20313;&#25552;&#31034;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#26469;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#25552;&#31034;&#20914;&#31361;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#20923;&#32467;&#20102;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20391;&#37325;&#20110;&#35757;&#32451;&#19968;&#20123;&#31216;&#20026;&#25552;&#31034;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#36825;&#20123;&#26041;&#27861;&#20013;&#30340;&#22823;&#22810;&#25968;&#23558;&#36825;&#20123;&#21521;&#37327;&#32452;&#32455;&#22312;&#19968;&#20010;&#38190;-&#20540;&#23545;&#27744;&#20013;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#25552;&#31034;&#65288;&#20540;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#34892;&#65292;&#30001;&#20110;&#38190;&#26159;&#23398;&#20064;&#30340;&#65292;&#25552;&#31034;&#36873;&#25321;&#31574;&#30053;&#26412;&#36523;&#20063;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20351;&#36873;&#25321;&#31574;&#30053;&#26356;&#21152;&#31283;&#23450;&#65292;&#25105;&#20204;&#35831;&#27714;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#22312;&#20004;&#32423;&#36866;&#24212;&#26426;&#21046;&#20013;&#36873;&#25321;&#25105;&#20204;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#32423;&#21033;&#29992;&#26631;&#20934;&#25991;&#26412;&#25552;&#31034;&#26469;&#35843;&#25972;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#24418;&#25104;&#31283;&#23450;&#30340;&#31867;&#21407;&#22411;&#12290;&#32780;&#31532;&#20108;&#32423;&#21017;&#23558;&#36825;&#20123;&#21407;&#22411;&#19982;&#26597;&#35810;&#22270;&#20687;&#19968;&#36215;&#29992;&#20316;&#38190;&#26469;&#32034;&#24341;&#19968;&#20010;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01071</link><description>&lt;p&gt;
GraphRCG: &#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#24341;&#23548;&#34920;&#31034;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#21033;&#29992;&#22270;&#20998;&#24067;&#65292;&#20248;&#20110;&#20256;&#32479;&#38544;&#24335;&#25429;&#33719;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#36890;&#24120;&#26088;&#22312;&#21019;&#24314;&#19982;&#29305;&#23450;&#22270;&#20998;&#24067;&#23494;&#20999;&#23545;&#40784;&#30340;&#26032;&#22270;&#12290;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#36890;&#36807;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#38544;&#24335;&#25429;&#33719;&#36825;&#31181;&#20998;&#24067;&#65292;&#21487;&#33021;&#24573;&#35270;&#20998;&#24067;&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23398;&#20064;&#21040;&#30340;&#20998;&#24067;&#23545;&#22270;&#29983;&#25104;&#30340;&#35265;&#35299;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#26465;&#20214;&#22270;&#29983;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#26126;&#30830;&#24314;&#27169;&#22270;&#20998;&#24067;&#24182;&#21033;&#29992;&#36825;&#20123;&#20998;&#24067;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#33258;&#26465;&#20214;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#26679;&#26412;&#36716;&#25442;&#20026;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#19968;&#20010;&#34920;&#31034;&#29983;&#25104;&#22120;&#26469;&#25429;&#33719;&#22270;&#20998;&#24067;&#24182;&#29983;&#25104;&#21453;&#26144;&#23398;&#20064;&#20998;&#24067;&#30340;&#26032;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#24341;&#23548;&#34920;&#31034;&#20316;&#20026;&#33258;&#26465;&#20214;&#25351;&#23548;&#26469;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01071v1 Announce Type: cross  Abstract: Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18286</link><description>&lt;p&gt;
&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#36808;&#21521;&#39640;&#32423;&#22270;&#20687;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Electron Microscopy: Towards a Foundation Model for Advanced Image Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#21516;&#26102;&#25351;&#20986;&#36739;&#20302;&#22797;&#26434;&#24230;&#30340;&#27169;&#22411;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22987;&#32456;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#26080;&#26631;&#31614;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#36808;&#20986;&#20102;&#26500;&#24314;&#35813;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22914;&#20309;&#20419;&#36827;&#26377;&#25928;&#30340;&#24494;&#35843;&#65292;&#20197;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#20041;&#20998;&#21106;&#12289;&#21435;&#22122;&#12289;&#22122;&#22768;&#19982;&#32972;&#26223;&#21435;&#38500;&#20197;&#21450;&#36229;&#20998;&#36776;&#29575;&#12290;&#36890;&#36807;&#23454;&#39564;&#19981;&#21516;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#24863;&#21463;&#37326;&#22823;&#23567;&#30340;&#21464;&#21270;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#20010;&#26174;&#33879;&#30340;&#29616;&#35937;&#65292;&#21363;&#24494;&#35843;&#36807;&#30340;&#36739;&#20302;&#22797;&#26434;&#24230;&#27169;&#22411;&#22987;&#32456;&#32988;&#36807;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#26356;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22312;&#30005;&#23376;&#26174;&#24494;&#38236;&#32972;&#26223;&#19979;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#22810;&#25165;&#22810;&#33402;&#65292;&#20351;&#24471;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20652;&#21270;&#21058;&#65292;&#29305;&#21035;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#21487;&#29992;&#26102;&#21644; ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18286v1 Announce Type: cross  Abstract: In this work, we explore the potential of self-supervised learning from unlabeled electron microscopy datasets, taking a step toward building a foundation model in this field. We show how self-supervised pretraining facilitates efficient fine-tuning for a spectrum of downstream tasks, including semantic segmentation, denoising, noise &amp; background removal, and super-resolution. Experimentation with varying model complexities and receptive field sizes reveals the remarkable phenomenon that fine-tuned models of lower complexity consistently outperform more complex models with random weight initialization. We demonstrate the versatility of self-supervised pretraining across various downstream tasks in the context of electron microscopy, allowing faster convergence and better performance. We conclude that self-supervised pretraining serves as a powerful catalyst, being especially advantageous when limited annotated data are available and ef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#37096;&#32626;&#21644;&#24536;&#35760;"&#26041;&#27861;&#65292;&#32467;&#21512;&#24265;&#20215;&#20256;&#24863;&#22120;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#25439;&#20260;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20165;&#38656;&#23398;&#20064;3&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#33258;&#20027;&#35782;&#21035;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#26410;&#30693;&#25439;&#20260;&#12290;</title><link>https://arxiv.org/abs/2402.15492</link><description>&lt;p&gt;
&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#24847;&#22806;&#30340;&#32467;&#26500;&#25439;&#20260;
&lt;/p&gt;
&lt;p&gt;
Mechanics-Informed Autoencoder Enables Automated Detection and Localization of Unforeseen Structural Damage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15492
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#37096;&#32626;&#21644;&#24536;&#35760;"&#26041;&#27861;&#65292;&#32467;&#21512;&#24265;&#20215;&#20256;&#24863;&#22120;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#32467;&#26500;&#25439;&#20260;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20165;&#38656;&#23398;&#20064;3&#23567;&#26102;&#25968;&#25454;&#21363;&#21487;&#33258;&#20027;&#35782;&#21035;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#26410;&#30693;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#23545;&#20110;&#30830;&#20445;&#24314;&#31569;&#29289;&#21644;&#26725;&#26753;&#31561;&#32467;&#26500;&#30340;&#23433;&#20840;&#24615;&#21644;&#38271;&#23551;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#39062;&#30340;&#8220;&#37096;&#32626;&#21644;&#24536;&#35760;&#8221;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#23450;&#20301;&#32467;&#26500;&#20013;&#30340;&#25439;&#20260;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#24265;&#20215;&#20256;&#24863;&#22120;&#30340;&#20840; pass &#23398;&#20064;&#21644;&#26426;&#26800;&#20449;&#24687;&#33258;&#32534;&#30721;&#22120;&#30340;&#21327;&#21516;&#32452;&#21512;&#65292;&#22312;&#20165;&#23398;&#20064;&#20102; 3 &#23567;&#26102;&#30340;&#25968;&#25454;&#21518;&#65292;&#23601;&#33021;&#33258;&#20027;&#26816;&#27979;&#21644;&#23450;&#20301;&#19981;&#21516;&#31867;&#22411;&#30340;&#24847;&#22806;&#25439;&#20260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15492v1 Announce Type: new  Abstract: Structural health monitoring (SHM) is vital for ensuring the safety and longevity of structures like buildings and bridges. As the volume and scale of structures and the impact of their failure continue to grow, there is a dire need for SHM techniques that are scalable, inexpensive, operate passively without human intervention, and customized for each mechanical structure without the need for complex baseline models. We present a novel "deploy-and-forget" approach for automated detection and localization of damages in structures. It is based on a synergistic combination of fully passive measurements from inexpensive sensors and a mechanics-informed autoencoder. Once deployed, our solution continuously learns and adapts a bespoke baseline model for each structure, learning from its undamaged state's response characteristics. After learning from just 3 hours of data, it can autonomously detect and localize different types of unforeseen dam
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15113</link><description>&lt;p&gt;
MSPipe: &#36890;&#36807;&#24847;&#35782;&#21040;&#38472;&#26087;&#24615;&#30340;&#31649;&#36947;&#23454;&#29616;&#39640;&#25928;&#30340;&#26102;&#38388;&#24615;GNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
MSPipe: Efficient Temporal GNN Training via Staleness-aware Pipeline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MSPipe&#65292;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#22411;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MTGNNs&#65289;&#26159;&#19968;&#31867;&#21033;&#29992;&#33410;&#28857;&#35760;&#24518;&#27169;&#22359;&#25429;&#33719;&#21644;&#20445;&#30041;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#26102;&#38388;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#23545;&#20110;&#26080;&#35760;&#24518;&#30340;&#23545;&#24212;&#32593;&#32476;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;MTGNNs&#20013;&#65292;&#20026;&#20102;&#33719;&#21462;&#26368;&#26032;&#30340;&#20449;&#24687;&#65292;&#35760;&#24518;&#27169;&#22359;&#30340;&#36845;&#20195;&#35835;&#21462;&#21644;&#26356;&#26032;&#36807;&#31243;&#38656;&#35201;&#36981;&#24490;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#24341;&#20837;&#20102;&#26174;&#33879;&#30340;&#24320;&#38144;&#24182;&#38480;&#21046;&#20102;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#38745;&#24577;GNNs&#30340;&#20248;&#21270;&#19981;&#36866;&#29992;&#20110;MTGNNs&#65292;&#22240;&#20026;&#20004;&#32773;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#32570;&#20047;&#35760;&#24518;&#27169;&#22359;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24182;&#26410;&#26377;&#25928;&#22320;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#20351;&#20854;&#23545;MTGNN&#35757;&#32451;&#26080;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSPipe&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#32780;&#39640;&#25928;&#30340;MTGNNs&#26694;&#26550;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15113v1 Announce Type: new  Abstract: Memory-based Temporal Graph Neural Networks (MTGNNs) are a class of temporal graph neural networks that utilize a node memory module to capture and retain long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, the iterative reading and updating process of the memory module in MTGNNs to obtain up-to-date information needs to follow the temporal dependencies. This introduces significant overhead and limits training throughput. Existing optimizations for static GNNs are not directly applicable to MTGNNs due to differences in training paradigm, model architecture, and the absence of a memory module. Moreover, they do not effectively address the challenges posed by temporal dependencies, making them ineffective for MTGNN training. In this paper, we propose MSPipe, a general and efficient framework for MTGNNs that maximizes training throughput while maintaining model accuracy. Our design
&lt;/p&gt;</description></item><item><title>QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09739</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;QuRating&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuRating: Selecting High-Quality Data for Training Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09739
&lt;/p&gt;
&lt;p&gt;
QuRating&#26159;&#19968;&#31181;&#36873;&#25321;&#39640;&#36136;&#37327;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#12290;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#24179;&#34913;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#21019;&#24314;&#33021;&#21147;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;QuRating&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#33021;&#22815;&#25429;&#25417;&#20154;&#31867;&#30452;&#35266;&#24863;&#30693;&#30340;&#25991;&#26412;&#30340;&#25277;&#35937;&#29305;&#24449;&#30340;&#39044;&#35757;&#32451;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22235;&#20010;&#29305;&#24449; - &#20889;&#20316;&#39118;&#26684;&#12289;&#25152;&#38656;&#19987;&#19994;&#30693;&#35782;&#12289;&#20107;&#23454;&#21644;&#29712;&#20107;&#20197;&#21450;&#25945;&#32946;&#20215;&#20540;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36776;&#21035;&#36825;&#20123;&#29305;&#24449;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#36827;&#34892;&#25991;&#26412;&#30340;&#37197;&#23545;&#21028;&#26029;&#26041;&#38754;&#27604;&#30452;&#25509;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#26356;&#22909;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;QuRater&#27169;&#22411;&#65292;&#20174;&#37197;&#23545;&#21028;&#26029;&#20013;&#23398;&#20064;&#26631;&#37327;&#35780;&#20998;&#65292;&#24182;&#20351;&#29992;&#23427;&#20026;260B&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#27599;&#20010;&#26631;&#20934;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#27880;&#37322;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#30340;&#36136;&#37327;&#35780;&#32423;&#36873;&#25321;&#20102;30B&#20010;&#20196;&#29260;&#65292;&#24182;&#22312;&#25152;&#36873;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;13&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09739v1 Announce Type: new  Abstract: Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts &amp; trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#35760;&#24518;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#37327;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#65292;&#24182;&#20934;&#30830;&#23450;&#20041;&#20102;&#23398;&#20064;&#31639;&#27861;&#20934;&#30830;&#24615;&#19982;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;CMI&#20043;&#38388;&#30340;&#26368;&#20339;&#36793;&#30028;&#12290;&#36890;&#36807;&#35774;&#35745;&#23545;&#25163;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35760;&#24518;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09327</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;&#20449;&#24687;&#22797;&#26434;&#24230;&#65306;&#27867;&#21270;&#21644;&#35760;&#24518;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#35760;&#24518;&#21644;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36890;&#36807;&#37327;&#21270;&#23398;&#20064;&#31639;&#27861;&#23545;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#65292;&#24182;&#20934;&#30830;&#23450;&#20041;&#20102;&#23398;&#20064;&#31639;&#27861;&#20934;&#30830;&#24615;&#19982;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;CMI&#20043;&#38388;&#30340;&#26368;&#20339;&#36793;&#30028;&#12290;&#36890;&#36807;&#35774;&#35745;&#23545;&#25163;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35760;&#24518;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#30340;&#32972;&#26223;&#19979;&#35760;&#24518;&#21644;&#23398;&#20064;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31639;&#27861;&#23545;&#20854;&#35757;&#32451;&#25968;&#25454;&#28857;&#25581;&#31034;&#30340;&#20449;&#24687;&#26469;&#23450;&#20041;&#35760;&#24518;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Steinke&#21644;Zakynthinou&#65288;2020&#65289;&#25552;&#20986;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;CMI&#65289;&#26694;&#26550;&#26469;&#37327;&#21270;&#36825;&#20123;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#23427;&#30340;CMI&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#31934;&#30830;&#25551;&#36848;&#65292;&#22238;&#31572;&#20102;Livni&#65288;2023&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#26410;&#35299;&#20043;&#38382;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;$L^2$ Lipschitz-&#26377;&#30028;&#30340;&#35774;&#32622;&#21644;&#24378;&#20984;&#24615;&#19979;&#65292;&#27599;&#20010;&#20855;&#26377;&#36229;&#39069;&#38169;&#35823;$\varepsilon$&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;CMI&#19979;&#30028;&#20998;&#21035;&#34987;$\Omega(1/\varepsilon^2)$&#21644;$\Omega(1/\varepsilon)$&#25152;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#20986;&#22823;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25163;&#26469;&#23637;&#31034;&#35760;&#24518;&#22312;SCO&#20013;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09327v1 Announce Type: new Abstract: In this work, we investigate the interplay between memorization and learning in the context of \emph{stochastic convex optimization} (SCO). We define memorization via the information a learning algorithm reveals about its training data points. We then quantify this information using the framework of conditional mutual information (CMI) proposed by Steinke and Zakynthinou (2020). Our main result is a precise characterization of the tradeoff between the accuracy of a learning algorithm and its CMI, answering an open question posed by Livni (2023). We show that, in the $L^2$ Lipschitz--bounded setting and under strong convexity, every learner with an excess error $\varepsilon$ has CMI bounded below by $\Omega(1/\varepsilon^2)$ and $\Omega(1/\varepsilon)$, respectively. We further demonstrate the essential role of memorization in learning problems in SCO by designing an adversary capable of accurately identifying a significant fraction of the
&lt;/p&gt;</description></item><item><title>SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>https://arxiv.org/abs/2402.01832</link><description>&lt;p&gt;
SynthCLIP: &#25105;&#20204;&#20934;&#22791;&#22909;&#24320;&#22987;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#35757;&#32451;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01832
&lt;/p&gt;
&lt;p&gt;
SynthCLIP&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#22270;&#29255;&#21644;&#26631;&#39064;&#25968;&#25454;&#38598;&#65292;&#22312;&#24615;&#33021;&#19978;&#21487;&#20197;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SynthCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#35757;&#32451;&#23436;&#20840;&#21512;&#25104;&#30340;CLIP&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#19982;&#20043;&#21069;&#20381;&#36182;&#30495;&#23454;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#21306;&#21035;&#12290;&#20511;&#21161;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#35268;&#27169;&#30340;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#26631;&#39064;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#65292;SynthCLIP&#23454;&#29616;&#20102;&#19982;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;SynthCI-30M&#65292;&#19968;&#20010;&#32431;&#31929;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;3000&#19975;&#24352;&#24102;&#26631;&#39064;&#30340;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#24050;&#32463;&#22312;https://github.com/hammoudhasan/SynthCLIP&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP
&lt;/p&gt;</description></item><item><title>Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.17717</link><description>&lt;p&gt;
Receler: &#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#21487;&#38752;&#22320;&#25830;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17717
&lt;/p&gt;
&lt;p&gt;
Receler&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#27010;&#24565;&#25830;&#38500;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#23454;&#29616;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#65292;&#27010;&#24565;&#25830;&#38500;&#26088;&#22312;&#31105;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#65292;&#24076;&#26395;&#20855;&#22791;&#40065;&#26834;&#24615;&#21644;&#23616;&#37096;&#24615;&#30340;&#23646;&#24615;&#12290;&#21069;&#32773;&#38459;&#27490;&#27169;&#22411;&#20026;&#20219;&#20309;&#37322;&#20041;&#25110;&#23398;&#20064;&#25552;&#31034;&#29983;&#25104;&#19982;&#30446;&#26631;&#27010;&#24565;&#30456;&#20851;&#30340;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#20445;&#25345;&#20854;&#29983;&#25104;&#20855;&#26377;&#38750;&#30446;&#26631;&#27010;&#24565;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#36731;&#37327;&#32423;&#27233;&#30382;&#25830;&#65288;Receler&#65289;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#27010;&#24565;&#25830;&#38500;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#27233;&#30382;&#25830;&#26469;&#36827;&#34892;&#27010;&#24565;&#25830;&#38500;&#65292;&#21516;&#26102;&#36890;&#36807;&#25552;&#20986;&#30340;&#27010;&#24565;&#23450;&#20301;&#27491;&#21017;&#21270;&#21644;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;&#26041;&#26696;&#28385;&#36275;&#19978;&#36848;&#29702;&#24819;&#29305;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27010;&#24565;&#30340;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;Receler&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#25509;&#21463;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17717v2 Announce Type: replace-cross  Abstract: Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves its ability in generating images with non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler). It learns a lightweight Eraser to perform concept erasing while satisfying the above desirable properties by proposed concept-localized regularization and adversarial prompt learning schemes. Comprehensive experiments with various concepts verify the superiority of Receler over previous methods. Our code will be available upon acceptance.
&lt;/p&gt;</description></item><item><title>LADDER&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23431;&#23449;&#30340;&#8220;&#36317;&#31163;&#26799;&#24230;&#8221;&#65292;&#23454;&#29616;&#20102;&#39044;&#27979;&#23431;&#23449;&#36317;&#31163;&#24182;&#25506;&#32034;&#20102;&#22810;&#20010;&#23431;&#23449;&#23398;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38656;&#35201;&#36827;&#34892;&#26377;&#36259;&#20294;&#35880;&#24910;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2401.17029</link><description>&lt;p&gt;
LADDER: &#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#25506;&#32034;&#23431;&#23449;&#36317;&#31163;&#26799;&#24230;&#24182;&#25506;&#32034;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications. (arXiv:2401.17029v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17029
&lt;/p&gt;
&lt;p&gt;
LADDER&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23431;&#23449;&#30340;&#8220;&#36317;&#31163;&#26799;&#24230;&#8221;&#65292;&#23454;&#29616;&#20102;&#39044;&#27979;&#23431;&#23449;&#36317;&#31163;&#24182;&#25506;&#32034;&#20102;&#22810;&#20010;&#23431;&#23449;&#23398;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38656;&#35201;&#36827;&#34892;&#26377;&#36259;&#20294;&#35880;&#24910;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;LADDER&#65288;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#36317;&#31163;&#20272;&#35745;&#21644;&#37325;&#24314;&#65289;&#30340;&#26032;&#39062;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#8220;&#23431;&#23449;&#36317;&#31163;&#26799;&#24230;&#8221;&#37325;&#24314;&#23431;&#23449;&#30340;&#21069;&#26223;&#12290;LADDER&#20351;&#29992;&#20102;&#26469;&#33258;Pantheon Type Ia&#36229;&#26032;&#26143;&#32534;&#35793;&#30340;&#35270;&#26143;&#31561;&#25968;&#25454;&#65292;&#24182;&#23558;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20840;&#37096;&#21327;&#26041;&#24046;&#20449;&#24687;&#36827;&#34892;&#20102;&#34701;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#30456;&#24212;&#35823;&#24046;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#20010;&#39564;&#35777;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;LADDER&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23431;&#23449;&#23398;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#24037;&#20855;&#65292;&#29992;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#65288;&#22914;&#37325;&#23376;&#22768;&#23398;&#25391;&#33633;&#65289;&#30340;&#26657;&#20934;&#65292;&#29992;&#20110;&#39640;&#32418;&#31227;&#25968;&#25454;&#38598;&#65288;&#22914;&#20285;&#29595;&#23556;&#32447;&#26292;&#65289;&#30340;&#26657;&#20934;&#65292;&#20197;&#21450;&#29992;&#20316;&#26410;&#26469;&#25506;&#27979;&#30340;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#27169;&#25311;&#30446;&#24405;&#29983;&#25104;&#22120;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32473;&#20986;&#20102;&#20851;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#35201;&#36827;&#34892;&#26377;&#36259;&#32780;&#35880;&#24910;&#30340;&#32771;&#34385;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the prospect of reconstructing the ``cosmic distance ladder'' of the Universe using a novel deep learning framework called LADDER - Learning Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on the apparent magnitude data from the Pantheon Type Ia supernovae compilation, incorporating the full covariance information among data points, to produce predictions along with corresponding errors. After employing several validation tests with a number of deep learning models, we pick LADDER as the best performing one. We then demonstrate applications of our method in the cosmological context, that include serving as a model-independent tool for consistency checks for other datasets like baryon acoustic oscillations, calibration of high-redshift datasets such as gamma ray bursts, use as a model-independent mock catalog generator for future probes, etc. Our analysis advocates for interesting yet cautious consideration of machine learning applications in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;</title><link>http://arxiv.org/abs/2312.00024</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20462;&#22797;&#23433;&#20840;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24320;&#21457;&#32773;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#21644;&#32570;&#38519;&#30340;&#20195;&#30721;&#12290;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#28431;&#27934;&#36890;&#24120;&#22312;&#31243;&#24207;&#19982;&#22806;&#37096;&#31995;&#32479;&#25110;&#26381;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#21644;&#25805;&#20316;&#31995;&#32479;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#20013;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#26696;&#21512;&#25104;&#65288;FDSS&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLMs&#25509;&#25910;&#26469;&#33258;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#30340;&#21453;&#39304;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#23433;&#20840;&#28431;&#27934;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#38543;&#21518;&#34987;&#36865;&#22238;LLMs&#36827;&#34892;&#20195;&#30721;&#23436;&#21892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;Stack Overflow&#30340;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08459</link><description>&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Heterogeneous Transfer Learning. (arXiv:2310.08459v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08459
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#36866;&#29992;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#22788;&#29702;&#36825;&#20123;&#24046;&#24322;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;&#24212;&#29992;&#22312;&#24456;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#23427;&#21033;&#29992;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#30446;&#26631;&#39046;&#22495;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#21644;&#26631;&#31614;&#31354;&#38388;&#65292;&#21363;&#21516;&#36136;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#36825;&#24182;&#19981;&#24635;&#26159;&#29616;&#23454;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;&#36890;&#24120;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#22312;&#29305;&#24449;&#31354;&#38388;&#12289;&#25968;&#25454;&#20998;&#24067;&#21644;&#26631;&#31614;&#31354;&#38388;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20351;&#24471;&#33719;&#21462;&#20855;&#26377;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#28304;&#39046;&#22495;&#25968;&#25454;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#26114;&#36149;&#12290;&#23545;&#36825;&#20123;&#24046;&#24322;&#36827;&#34892;&#38543;&#24847;&#30340;&#28040;&#38500;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#25110;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#24322;&#26500;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#24212;&#23545;&#36825;&#31181;&#24046;&#24322;&#30340;&#26041;&#27861;&#24050;&#32463;&#23853;&#38706;&#22836;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of transfer learning, an approach utilizing knowledge from a source domain to enhance model performance in a target domain, has seen a tremendous rise in recent years, underpinning many real-world scenarios. The key to its success lies in the shared common knowledge between the domains, a prerequisite in most transfer learning methodologies. These methods typically presuppose identical feature spaces and label spaces in both domains, known as homogeneous transfer learning, which, however, is not always a practical assumption. Oftentimes, the source and target domains vary in feature spaces, data distributions, and label spaces, making it challenging or costly to secure source domain data with identical feature and label spaces as the target domain. Arbitrary elimination of these differences is not always feasible or optimal. Thus, heterogeneous transfer learning, acknowledging and dealing with such disparities, has emerged as a promising approach for a variety of tasks.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2310.07219</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Improved Membership Inference Attacks Against Language Classification Models. (arXiv:2310.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07219
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#22810;&#20010;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20855;&#26377;&#38646;&#21806;&#12289;&#21046;&#36896;&#12289;&#20581;&#24247;&#31561;&#35768;&#22810;&#39046;&#22495;&#30340;&#29992;&#20363;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#37319;&#29992;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#21457;&#29616;&#20102;&#30456;&#20851;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#23545;&#20351;&#29992;&#20854;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#30340;&#20154;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#39118;&#38505;&#23545;&#20110;&#26159;&#21542;&#20351;&#29992;&#12289;&#37096;&#32626;&#25110;&#20849;&#20139;&#27169;&#22411;&#20570;&#20986;&#30693;&#24773;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#38544;&#31169;&#39118;&#38505;&#35780;&#20272;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#23545;&#27169;&#22411;&#36827;&#34892;&#19968;&#20010;&#25110;&#22810;&#20010;&#24050;&#30693;&#25915;&#20987;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#25968;&#25454;&#23376;&#38598;&#29983;&#25104;&#35768;&#22810;&#19987;&#38376;&#30340;&#25915;&#20987;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#32463;&#20856;&#21644;&#35821;&#35328;&#20998;&#31867;&#20219;&#21153;&#19978;&#27604;&#21333;&#20010;&#25915;&#20987;&#27169;&#22411;&#25110;&#27599;&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#25915;&#20987;&#27169;&#22411;&#37117;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence systems are prevalent in everyday life, with use cases in retail, manufacturing, health, and many other fields. With the rise in AI adoption, associated risks have been identified, including privacy risks to the people whose data was used to train models. Assessing the privacy risks of machine learning models is crucial to enabling knowledgeable decisions on whether to use, deploy, or share a model. A common approach to privacy risk assessment is to run one or more known attacks against the model and measure their success rate. We present a novel framework for running membership inference attacks against classification models. Our framework takes advantage of the ensemble method, generating many specialized attack models for different subsets of the data. We show that this approach achieves higher accuracy than either a single attack model or an attack model per class label, both on classical and language classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.00508</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. (arXiv:2309.00508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#32467;&#26500;&#65292;&#30830;&#23450;&#20102;&#33021;&#22815;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#30340;&#21442;&#25968;&#38598;&#65292;&#24182;&#23436;&#25972;&#25551;&#36848;&#20102;&#20854;&#21608;&#22260;&#30340;&#26799;&#24230;&#27969;&#21160;&#24577;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#19968;&#20123;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#12289;&#30446;&#26631;&#20989;&#25968;&#12289;&#26679;&#26412;&#21644;&#21021;&#22987;&#21270;&#23545;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#65288;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65289;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#30340;&#26816;&#27979;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#20132;&#26131;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#24222;&#27663;&#39575;&#23616;&#65292;&#22240;&#20026;&#20132;&#26131;&#26356;&#38590;&#20266;&#35013;&#12290;</title><link>http://arxiv.org/abs/2308.16391</link><description>&lt;p&gt;
&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Accuracy of Ponzi Scheme Detection on Ethereum Using Time-Dependent Features. (arXiv:2308.16391v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16391
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20197;&#22826;&#22346;&#19978;&#24222;&#27663;&#39575;&#23616;&#30340;&#26816;&#27979;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#20132;&#26131;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#35782;&#21035;&#24222;&#27663;&#39575;&#23616;&#65292;&#22240;&#20026;&#20132;&#26131;&#26356;&#38590;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#36164;&#37329;&#28044;&#20837;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#65292;&#20063;&#21560;&#24341;&#20102;&#36817;&#24180;&#26469;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#30340;&#20852;&#36259;&#12290;&#24222;&#27663;&#39575;&#23616;&#20316;&#20026;&#19968;&#31181;&#32769;&#24335;&#30340;&#27450;&#35784;&#34892;&#20026;&#65292;&#29616;&#22312;&#20063;&#27969;&#34892;&#20110;&#21306;&#22359;&#38142;&#19978;&#65292;&#32473;&#35768;&#22810;&#21152;&#23494;&#36135;&#24065;&#25237;&#36164;&#32773;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#36130;&#21153;&#25439;&#22833;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#24222;&#27663;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#25110;&#25805;&#20316;&#30721;&#36827;&#34892;&#26816;&#27979;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#21512;&#32422;&#20195;&#30721;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#32570;&#20047;&#40065;&#26834;&#24615;&#65306;&#39318;&#20808;&#65292;&#22823;&#37096;&#20998;&#20197;&#22826;&#22346;&#19978;&#30340;&#21512;&#32422;&#28304;&#20195;&#30721;&#24182;&#19981;&#20844;&#24320;&#21487;&#29992;&#65307;&#20854;&#27425;&#65292;&#24222;&#27663;&#39575;&#23616;&#24320;&#21457;&#32773;&#21487;&#20197;&#36890;&#36807;&#28151;&#28102;&#25805;&#20316;&#30721;&#25110;&#32773;&#21019;&#36896;&#26032;&#30340;&#20998;&#37197;&#36923;&#36753;&#26469;&#27450;&#39575;&#22522;&#20110;&#21512;&#32422;&#20195;&#30721;&#30340;&#26816;&#27979;&#27169;&#22411;&#65288;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#20165;&#22312;&#29616;&#26377;&#30340;&#24222;&#27663;&#36923;&#36753;&#19978;&#36827;&#34892;&#35757;&#32451;&#65289;&#12290;&#22522;&#20110;&#20132;&#26131;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#20026;&#19982;&#26234;&#33021;&#21512;&#32422;&#19981;&#21516;&#65292;&#20132;&#26131;&#26356;&#21152;&#38590;&#20197;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of blockchain has led to more and more funding pouring into the cryptocurrency market, which also attracted cybercriminals' interest in recent years. The Ponzi scheme, an old-fashioned fraud, is now popular on the blockchain, causing considerable financial losses to many crypto-investors. A few Ponzi detection methods have been proposed in the literature, most of which detect a Ponzi scheme based on its smart contract source code or opcode. The contract-code-based approach, while achieving very high accuracy, is not robust: first, the source codes of a majority of contracts on Ethereum are not available, and second, a Ponzi developer can fool a contract-code-based detection model by obfuscating the opcode or inventing a new profit distribution logic that cannot be detected (since these models were trained on existing Ponzi logics only). A transaction-based approach could improve the robustness of detection because transactions, unlike smart contracts, are harder t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12919</link><description>&lt;p&gt;
&#29992;CLIP&#23454;&#29616;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Unsupervised Fine-tuning with CLIP. (arXiv:2308.12919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24494;&#35843;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#26410;&#30693;&#31867;&#21035;&#21644;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;UEO&#30340;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26679;&#26412;&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#39044;&#23450;&#20041;&#31867;&#21035;&#23454;&#20363;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22914;CLIP&#30340;&#20986;&#29616;&#25512;&#21160;&#20102;&#20154;&#20204;&#22312;&#19979;&#28216;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;&#12290;&#23613;&#31649;&#19968;&#20123;&#20043;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;CLIP&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#20381;&#36182;&#20110;&#19982;&#30495;&#23454;&#26631;&#31614;&#30456;&#20851;&#30340;&#31867;&#21517;&#31561;&#20808;&#39564;&#30693;&#35782;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#30495;&#23454;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#24773;&#26223;&#65292;&#20551;&#35774;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#26469;&#33258;&#26410;&#30693;&#31867;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#39044;&#23450;&#20041;&#31867;&#26631;&#31614;&#30340;&#35782;&#21035;&#20043;&#22806;&#65292;&#21516;&#26102;&#25552;&#39640;&#23545;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Universal Entropy Optimization (UEO)&#12290;UEO&#21033;&#29992;&#26679;&#26412;&#32423;&#32622;&#20449;&#24230;&#65292;&#20197;&#36817;&#20284;&#26041;&#24335;&#26368;&#23567;&#21270;&#32622;&#20449;&#23454;&#20363;&#30340;&#26465;&#20214;&#29109;&#24182;&#26368;&#22823;&#21270;&#36793;&#32536;&#29109;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of vision-language models (VLMs), such as CLIP, has spurred a significant research effort towards their application for downstream supervised learning tasks. Although some previous studies have explored the unsupervised fine-tuning of CLIP, they often rely on prior knowledge in the form of class names associated with ground truth labels. In this paper, we delve into a realistic unsupervised fine-tuning scenario by assuming that the unlabeled data might contain out-of-distribution samples from unknown classes. Furthermore, we emphasize the importance of simultaneously enhancing out-of-distribution detection capabilities alongside the recognition of instances associated with predefined class labels.  To tackle this problem, we present a simple, efficient, and effective fine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages sample-level confidence to approximately minimize the conditional entropy of confident instances and maximize the marginal entro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2308.02121</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;DNA&#30340;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#30340;&#26032;&#27010;&#24565;&#27169;&#22411;DNA&#65292;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#20316;&#20026;&#32039;&#20945;&#20840;&#38754;&#30340;&#34920;&#31034;&#65292;&#26469;&#30830;&#23450;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#26159;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#26469;&#28304;&#65292;&#35757;&#32451;&#26041;&#24335;&#20197;&#21450;&#20351;&#29992;&#26041;&#24335;&#65289;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#36825;&#19968;&#39046;&#22495;&#20869;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#26469;&#28304;&#35777;&#26126;&#65288;MP&#65289;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#30446;&#26631;&#27169;&#22411;&#19982;&#20854;&#39044;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#19968;&#20010;&#28304;&#27169;&#22411;&#26159;&#21542;&#20316;&#20026;&#30446;&#26631;&#27169;&#22411;&#30340;&#26469;&#28304;&#35777;&#26126;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#30693;&#35782;&#20135;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#22312;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#21363;&#27169;&#22411;DNA&#65292;&#23427;&#20195;&#34920;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#36755;&#20986;&#20449;&#24687;&#32534;&#30721;&#20026;&#27169;&#22411;&#30340;&#32039;&#20945;&#19988;&#20840;&#38754;&#30340;&#34920;&#31034;&#65288;&#21363;DNA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the life cycle of the machine learning (ML) model is an intriguing area of research (e.g., understanding where the model comes from, how it is trained, and how it is used). This paper focuses on a novel problem within this field, namely Model Provenance (MP), which concerns the relationship between a target model and its pre-training model and aims to determine whether a source model serves as the provenance for a target model. This is an important problem that has significant implications for ensuring the security and intellectual property of machine learning models but has not received much attention in the literature. To fill in this gap, we introduce a novel concept of Model DNA which represents the unique characteristics of a machine learning model. We utilize a data-driven and model-driven representation learning method to encode the model's training data and input-output information as a compact and comprehensive representation (i.e., DNA) of the model. Using this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14088</link><description>&lt;p&gt;
&#38750;&#21516;&#36136;&#21270;&#38598;&#32676;&#19979;&#30340;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31169;&#26377;&#25968;&#25454;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Private Aggregation in Wireless Federated Learning with Heterogeneous Clusters. (arXiv:2306.14088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#20449;&#24687;&#35770;&#38544;&#31169;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#30340;&#23458;&#25143;&#31471;&#65292;&#22914;&#20309;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25968;&#25454;&#32858;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#36890;&#36807;&#22810;&#20010;&#21442;&#19982;&#23458;&#25143;&#31471;&#31169;&#26377;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#19968;&#31181;&#33879;&#21517;&#24182;&#24191;&#27867;&#20351;&#29992;&#30340;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#27599;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#35745;&#31639;&#23616;&#37096;&#26799;&#24230;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#32852;&#21512;&#22120;&#20197;&#36827;&#34892;&#32858;&#21512;&#12290;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#38544;&#31169;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#23454;&#38469;&#19978;&#65292;&#35266;&#23519;&#21040;&#23616;&#37096;&#26799;&#24230;&#23601;&#36275;&#20197;&#27844;&#38706;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;&#24050;&#30740;&#31350;&#20102;&#29992;&#20110;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#29992;&#25143;&#37117;&#24444;&#27492;&#36830;&#25509;&#24182;&#19982;&#32852;&#21512;&#22120;&#36830;&#25509;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#26080;&#32447;&#31995;&#32479;&#26550;&#26500;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20165;&#36890;&#36807;&#22522;&#31449;&#36830;&#25509;&#21040;&#32852;&#21512;&#22120;&#12290;&#24403;&#38656;&#35201;&#20449;&#24687;&#35770;&#38544;&#31169;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#36890;&#20449;&#25104;&#26412;&#30340;&#22522;&#26412;&#26497;&#38480;&#65292;&#24182;&#24341;&#20837;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#37327;&#36523;&#23450;&#21046;&#30340;&#31169;&#26377;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning collaboratively trains a neural network on privately owned data held by several participating clients. The gradient descent algorithm, a well-known and popular iterative optimization procedure, is run to train the neural network. Every client uses its local data to compute partial gradients and sends it to the federator which aggregates the results. Privacy of the clients' data is a major concern. In fact, observing the partial gradients can be enough to reveal the clients' data. Private aggregation schemes have been investigated to tackle the privacy problem in federated learning where all the users are connected to each other and to the federator. In this paper, we consider a wireless system architecture where clients are only connected to the federator via base stations. We derive fundamental limits on the communication cost when information-theoretic privacy is required, and introduce and analyze a private aggregation scheme tailored for this setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00510</link><description>&lt;p&gt;
&#36890;&#21521;&#33258;&#30001;&#35745;&#31639;&#26550;&#26500;: &#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#20803;&#23431;&#23449;&#34394;&#25311;&#24314;&#31569;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#24418;&#29366;&#29983;&#25104;&#25216;&#26415;&#27491;&#22312;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24314;&#31569;&#35774;&#35745;&#20004;&#26041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#35843;&#26597;&#21644;&#27604;&#36739;&#24403;&#21069;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;3D&#24863;&#30693;&#22270;&#20687;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;187&#31687;&#25991;&#31456;(&#21344;2018-2022&#24180;&#38388;&#21457;&#34920;&#25991;&#31456;&#30340;80.7%)&#65292;&#20197;&#22238;&#39038;&#22312;&#34394;&#25311;&#29615;&#22659;&#19979;&#24314;&#31569;&#29983;&#25104;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#65292;&#38480;&#20110;&#24314;&#31569;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31569;&#30740;&#31350;&#12289;&#34394;&#25311;&#29615;&#22659;&#21644;&#30456;&#20851;&#25216;&#26415;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#25509;&#30528;&#22238;&#39038;&#20102;&#31163;&#25955;&#20307;&#32032;&#29983;&#25104;&#12289;&#30001;2D&#22270;&#20687;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#20197;&#21450;&#26465;&#20214;&#21442;&#25968;&#30340;&#26368;&#36817;&#36235;&#21183;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;3D&#29983;&#25104;&#21644;&#21442;&#25968;&#21270;&#25511;&#21046;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#21253;&#25324;&#29983;&#25104;&#22810;&#26679;&#24615;&#12289;&#26032;&#22411;&#36755;&#20986;&#21644;&#23884;&#20837;&#24335;&#26500;&#24314;&#31561;&#22235;&#20010;&#30740;&#31350;&#35758;&#31243;&#21487;&#33021;&#20250;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item></channel></rss>