<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2404.02325</link><description>&lt;p&gt;
&#38381;&#29615;&#23398;&#20064;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#28909;&#21147;&#23398;&#27515;&#20129;
&lt;/p&gt;
&lt;p&gt;
Heat Death of Generative Models in Closed-Loop Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02325
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#20135;&#29983;&#36864;&#21270;&#29616;&#35937;&#65292;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#25110;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25913;&#36827;&#21644;&#37319;&#32435;&#27491;&#22312;&#36805;&#36895;&#21152;&#36895;&#65292;&#20363;&#22914;&#25991;&#26412;&#20013;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27969;&#34892;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23427;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#25972;&#21512;&#21040;&#20844;&#20849;&#32593;&#32476;&#20013;&#30340;&#20849;&#20139;&#20869;&#23481;&#20013;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24403;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#36865;&#22238;&#21040;&#27169;&#22411;&#36827;&#34892;&#21518;&#32493;&#35757;&#32451;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35757;&#32451;&#36807;&#31243;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#20844;&#20849;&#21487;&#35775;&#38382;&#20869;&#23481;&#30340;&#20998;&#24067;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30693;&#35782;&#8221;&#65289;&#26159;&#21542;&#20445;&#25345;&#31283;&#23450;&#36824;&#26159;&#23849;&#28291;&#12290;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#23567;&#35268;&#27169;&#23454;&#35777;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#31181;&#38381;&#29615;&#35757;&#32451;&#36807;&#31243;&#23481;&#26131;&#36864;&#21270;&#12290;&#27169;&#22411;&#21487;&#33021;&#24320;&#22987;&#29983;&#25104;&#26080;&#24847;&#20041;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#20165;&#20174;&#25152;&#38656;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#20013;&#37319;&#26679;&#65288;&#31216;&#20026;&#27169;&#24335;&#23849;&#28291;&#29616;&#35937;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02325v1 Announce Type: new  Abstract: Improvement and adoption of generative machine learning models is rapidly accelerating, as exemplified by the popularity of LLMs (Large Language Models) for text, and diffusion models for image generation.As generative models become widespread, data they generate is incorporated into shared content through the public web. This opens the question of what happens when data generated by a model is fed back to the model in subsequent training campaigns. This is a question about the stability of the training process, whether the distribution of publicly accessible content, which we refer to as "knowledge", remains stable or collapses.   Small scale empirical experiments reported in the literature show that this closed-loop training process is prone to degenerating. Models may start producing gibberish data, or sample from only a small subset of the desired data distribution (a phenomenon referred to as mode collapse). So far there has been on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#26550;&#26500;&#65292;&#24182;&#23545;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.07585</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#65306;&#26550;&#26500;&#12289;&#36827;&#23637;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Communication Optimization for Distributed Training: Architecture, Advances, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#36890;&#20449;&#20248;&#21270;&#26550;&#26500;&#65292;&#24182;&#23545;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#21442;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#35757;&#32451;&#36825;&#20123;&#22823;&#35268;&#27169;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36229;&#20986;&#20102;&#21333;&#20010;GPU&#30340;&#33539;&#22260;&#65292;&#38656;&#35201;&#36827;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#30001;&#20110;&#36817;&#24180;&#26469;GPU&#24615;&#33021;&#36805;&#36895;&#21457;&#23637;&#65292;&#35745;&#31639;&#26102;&#38388;&#32553;&#30701;&#65292;&#22240;&#27492;&#36890;&#20449;&#22312;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#20013;&#30340;&#27604;&#20363;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#36890;&#20449;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#26412;&#25991;&#31616;&#35201;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#24635;&#20307;&#26550;&#26500;&#65292;&#24182;&#20174;&#36890;&#20449;&#20248;&#21270;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#24182;&#34892;&#21270;&#31574;&#30053;&#12289;&#38598;&#20307;&#36890;&#20449;&#24211;&#21644;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#19977;&#23618;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#36827;&#23637;&#19982;&#36825;&#20010;&#19977;&#23618;&#33539;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;lay
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07585v1 Announce Type: cross  Abstract: The past few years have witnessed the flourishing of large-scale deep neural network models with ever-growing parameter numbers. Training such large-scale models typically requires massive memory and computing resources that exceed those of a single GPU, necessitating distributed training. As GPU performance has rapidly evolved in recent years, computation time has shrunk, thereby increasing the proportion of communication in the overall training time. Therefore, optimizing communication for distributed training has become an urgent issue. In this article, we briefly introduce the general architecture of distributed deep neural network training and analyze relationships among Parallelization Strategy, Collective Communication Library, and Network from the perspective of communication optimization, which forms a three-layer paradigm. We then review current representative research advances with this three-layer paradigm. We find that lay
&lt;/p&gt;</description></item><item><title>GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.05527</link><description>&lt;p&gt;
GEAR: &#19968;&#31181;&#29992;&#20110;&#20960;&#20046;&#26080;&#25439;&#29983;&#25104;&#25512;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;KV&#32531;&#23384;&#21387;&#32553;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
GEAR: An Efficient KV Cache Compression Recipefor Near-Lossless Generative Inference of LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05527
&lt;/p&gt;
&lt;p&gt;
GEAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#27604;&#29575;&#21387;&#32553;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#20013;&#22240;&#32531;&#23384;&#38656;&#27714;&#22686;&#38271;&#32780;&#23548;&#33268;&#30340;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#65288;KV&#65289;&#32531;&#23384;&#24050;&#25104;&#20026;&#21152;&#24555;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#26029;&#29983;&#25104;&#36895;&#24230;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#38271;&#30340;&#32531;&#23384;&#38656;&#27714;&#24050;&#23558;LLM&#25512;&#26029;&#36716;&#21464;&#20026;&#19968;&#20010;&#35760;&#24518;&#32465;&#23450;&#38382;&#39064;&#65292;&#26174;&#33879;&#22320;&#38480;&#21046;&#20102;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#26631;&#35760;&#25110;&#22343;&#21248;&#37327;&#21270;&#25152;&#26377;&#26465;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#20250;&#20135;&#29983;&#36739;&#39640;&#30340;&#36817;&#20284;&#35823;&#24046;&#26469;&#34920;&#31034;&#21387;&#32553;&#21518;&#30340;&#30697;&#38453;&#12290;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27599;&#20010;&#27493;&#39588;&#30340;&#35823;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#37325;&#22823;&#20559;&#24046;&#21644;&#24615;&#33021;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GEAR&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;KV&#32531;&#23384;&#21387;&#32553;&#26694;&#26550;&#65292;&#23454;&#29616;&#20960;&#20046;&#26080;&#25439;&#30340;&#39640;&#21387;&#32553;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05527v1 Announce Type: cross  Abstract: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quant
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;</title><link>https://arxiv.org/abs/2403.04447</link><description>&lt;p&gt;
FRRI&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FRRI: a novel algorithm for fuzzy-rough rule induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04447
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#27169;&#31946;&#19982;&#31895;&#31961;&#38598;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861; FRRI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19979;&#19968;&#20010;&#21069;&#27839;&#12290;&#22312;&#23547;&#25214;&#30333;&#30418;&#27169;&#22411;&#30340;&#36807;&#31243;&#20013;-&#19982;&#38543;&#26426;&#26862;&#26519;&#25110;&#31070;&#32463;&#32593;&#32476;&#31561;&#40657;&#30418;&#27169;&#22411;&#30456;&#23545;&#24212;&#65292;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#26159;&#19968;&#20010;&#21512;&#20046;&#36923;&#36753;&#19988;&#26377;&#24076;&#26395;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#35268;&#21017;&#21487;&#20197;&#34987;&#20154;&#31867;&#36731;&#26494;&#29702;&#35299;&#12290;&#27169;&#31946;&#21644;&#31895;&#31961;&#38598;&#29702;&#35770;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#31181;&#21407;&#22411;&#65292;&#20960;&#20046;&#24635;&#26159;&#20998;&#24320;&#24212;&#29992;&#12290;&#30001;&#20110;&#35268;&#21017;&#24402;&#32435;&#30340;&#20004;&#31181;&#26041;&#27861;&#22343;&#28041;&#21450;&#22522;&#20110;&#31561;&#20215;&#31867;&#27010;&#24565;&#30340;&#31890;&#35745;&#31639;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#26159;&#33258;&#28982;&#30340;&#36873;&#25321;&#12290;QuickRules&#31639;&#27861;&#26159;&#21033;&#29992;&#27169;&#31946;&#31895;&#31961;&#38598;&#29702;&#35770;&#36827;&#34892;&#35268;&#21017;&#24402;&#32435;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#23427;&#22522;&#20110;QuickReduct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#20915;&#31574;&#32422;&#31616;&#30340;&#36138;&#23146;&#31639;&#27861;&#12290;QuickRules &#24050;&#32463;&#23637;&#31034;&#20102;&#30456;&#27604;&#20854;&#20182;&#35268;&#21017;&#24402;&#32435;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#35201;&#35780;&#20272;&#27169;&#31946;-&#31895;&#31961;&#35268;&#21017;&#24402;&#32435;&#31639;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23601;&#38656;&#35201;&#20174;&#22522;&#30784;&#24320;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04447v1 Announce Type: cross  Abstract: Interpretability is the next frontier in machine learning research. In the search for white box models - as opposed to black box models, like random forests or neural networks - rule induction algorithms are a logical and promising option, since the rules can easily be understood by humans. Fuzzy and rough set theory have been successfully applied to this archetype, almost always separately. As both approaches to rule induction involve granular computing based on the concept of equivalence classes, it is natural to combine them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at using fuzzy rough set theory for rule induction. It is based on QuickReduct, a greedy algorithm for building decision reducts. QuickRules already showed an improvement over other rule induction methods. However, to evaluate the full potential of a fuzzy rough rule induction algorithm, one needs to start from the foundations. In this paper,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01942</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;&#20943;&#36731;&#22270;&#20013;&#30340;&#26631;&#31614;&#22122;&#38899;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Noise on Graph via Topological Sample Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22270;&#25968;&#25454;&#30340;&#25299;&#25169;&#20449;&#24687;&#26469;&#22686;&#24378;&#20449;&#24687;&#36873;&#25321;&#36807;&#31243;&#30340;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#8221;&#65288;TSS&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31934;&#24515;&#27880;&#37322;&#30340;&#22522;&#20934;&#27979;&#35797;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#24403;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#26102;&#65292;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26377;&#25928;&#24615;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#20250;&#21463;&#21040;&#30456;&#24403;&#22823;&#30340;&#24433;&#21709;&#12290;&#20197;&#24448;&#22312;&#26679;&#26412;&#36873;&#25321;&#26041;&#38754;&#30340;&#25506;&#32034;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24212;&#23545;&#22122;&#22768;&#26631;&#31614;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#20256;&#32479;&#30740;&#31350;&#20391;&#37325;&#20110;i.i.d&#25968;&#25454;&#65292;&#24403;&#36716;&#21521;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#21644;GNNs&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#25361;&#25112;&#65306;(1) &#20301;&#20110;&#25299;&#25169;&#31867;&#36793;&#30028;&#38468;&#36817;&#30340;&#33410;&#28857;&#23545;&#20998;&#31867;&#38750;&#24120;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#26080;&#27861;&#36890;&#36807;&#21551;&#21457;&#24335;&#26679;&#26412;&#36873;&#25321;&#25104;&#21151;&#21306;&#20998;&#12290;(2) &#27809;&#26377;&#21487;&#29992;&#30340;&#34913;&#37327;&#26631;&#20934;&#32771;&#34385;&#22270;&#30340;&#25299;&#25169;&#20449;&#24687;&#20197;&#20419;&#36827;&#22270;&#20013;&#30340;&#26679;&#26412;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;$&#8220;\textit{&#25299;&#25169;&#26679;&#26412;&#36873;&#25321;}$&#65288;TSS&#65289;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#26469;&#25552;&#21319;&#22270;&#20013;&#20449;&#24687;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01942v1 Announce Type: new  Abstract: Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.15650</link><description>&lt;p&gt;
&#20855;&#26377;&#30446;&#26631;&#25233;&#21046;&#30340;&#22810;&#32422;&#26463;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Constraint Safe RL with Objective Suppression for Safety-Critical Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#25233;&#21046;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#25913;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#34920;&#29616;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#32467;&#21512;&#29616;&#26377;&#31639;&#27861;&#33021;&#22815;&#22312;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19982;&#22522;&#20934;&#32447;&#30456;&#24403;&#30340;&#20219;&#21153;&#22870;&#21169;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#38750;&#24120;&#24120;&#35265;&#65292;&#20294;&#20855;&#26377;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#30446;&#26631;&#25233;&#21046;&#65292;&#26681;&#25454;&#23433;&#20840;&#35780;&#21028;&#22120;&#33258;&#36866;&#24212;&#22320;&#25233;&#21046;&#20219;&#21153;&#22870;&#21169;&#26368;&#22823;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#32422;&#26463;&#23433;&#20840;&#39046;&#22495;&#20013;&#23545;&#30446;&#26631;&#25233;&#21046;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20219;&#20309;&#38169;&#35823;&#30340;&#34892;&#20026;&#37117;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#32422;&#26463;&#36829;&#35268;&#30340;&#24773;&#20917;&#19979;&#21305;&#37197;&#25105;&#20204;&#30340;&#22522;&#20934;&#32447;&#25152;&#36798;&#21040;&#30340;&#20219;&#21153;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15650v1 Announce Type: cross  Abstract: Safe reinforcement learning tasks with multiple constraints are a challenging domain despite being very common in the real world. To address this challenge, we propose Objective Suppression, a novel method that adaptively suppresses the task reward maximizing objectives according to a safety critic. We benchmark Objective Suppression in two multi-constraint safety domains, including an autonomous driving domain where any incorrect behavior can lead to disastrous consequences. Empirically, we demonstrate that our proposed method, when combined with existing safe RL algorithms, can match the task reward achieved by our baselines with significantly fewer constraint violations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.12326</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24515;&#29702;&#23398;&#26234;&#33021;&#20195;&#29702;&#65306;&#19968;&#39033;&#20851;&#20110;&#28216;&#25103;&#21270;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Agents for Psychology: A Study on Gamified Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#27979;&#37327;&#23545;&#20110;&#31934;&#31070;&#20581;&#24247;&#12289;&#33258;&#25105;&#29702;&#35299;&#21644;&#20010;&#20154;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#37327;&#34920;&#21644;&#24515;&#29702;&#23398;&#23478;&#35775;&#35848;&#65292;&#24120;&#24120;&#38754;&#20020;&#21442;&#19982;&#24230;&#21644;&#21487;&#33719;&#24471;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#35752;&#20102;&#22522;&#20110;&#28216;&#25103;&#21644;LLM&#30340;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#20852;&#36259;&#24182;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24179;&#34913;&#21442;&#19982;&#24230;&#21644;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#65292;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#12290;&#20027;&#35201;&#27934;&#23519;&#26159;&#24378;&#22823;&#30340;LLM&#26082;&#21487;&#20197;&#20805;&#24403;&#29087;&#32451;&#30340;&#24515;&#29702;&#23398;&#23478;&#65292;&#20063;&#21487;&#20197;&#26159;&#21019;&#26032;&#30340;&#28216;&#25103;&#35774;&#35745;&#24072;&#12290;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#32435;&#20837;&#25351;&#23450;&#35282;&#33394;&#24182;&#31934;&#24515;&#31649;&#29702;&#23427;&#20204;&#30340;&#20114;&#21160;&#65292;PsychoGAT&#21487;&#20197;&#23558;&#20219;&#20309;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;&#20026;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#24515;&#29702;&#24230;&#37327;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03973</link><description>&lt;p&gt;
&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#65292;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#20987;&#36133;&#20102;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03973
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20960;&#20010;&#29289;&#20307;&#35782;&#21035;&#22522;&#20934;&#19978;&#27491;&#22312;&#32553;&#23567;&#19982;&#20154;&#31867;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#28041;&#21450;&#20174;&#19981;&#23547;&#24120;&#35270;&#35282;&#35266;&#23519;&#29289;&#20307;&#30340;&#25361;&#25112;&#24615;&#22270;&#20687;&#20013;&#23545;&#36825;&#19968;&#24046;&#36317;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65288;EfficientNet&#12289;SWAG&#12289;ViT&#12289;SWIN&#12289;BEiT&#12289;ConvNext&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27492;&#24773;&#20917;&#19979;&#26222;&#36941;&#33030;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#25105;&#20204;&#38480;&#21046;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#19979;&#38477;&#21040;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#38656;&#35201;&#39069;&#22806;&#30340;&#26102;&#38388;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#19982;&#32593;&#32476;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38480;&#21046;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#19982;&#21069;&#39304;&#28145;&#24230;&#32593;&#32476;&#20063;&#26377;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24102;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#29702;&#35299;&#22312;&#22806;&#37096;&#24773;&#20917;&#19979;&#21457;&#29983;&#30340;&#24515;&#29702;&#36807;&#31243;&#30340;&#26412;&#36136;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
&lt;/p&gt;</description></item><item><title>DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.02139</link><description>&lt;p&gt;
DiffiT: &#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#30340;&#25193;&#25955;&#35270;&#35273;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffiT: Diffusion Vision Transformers for Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02139
&lt;/p&gt;
&lt;p&gt;
DiffiT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;Vision Transformer&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#21435;&#22122;&#25511;&#21046;&#21644;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#24378;&#22823;&#34920;&#29616;&#21147;&#21644;&#39640;&#26679;&#26412;&#36136;&#37327;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#24320;&#21019;&#24615;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35782;&#21035;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;ViTs&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;Diffusion Vision Transformers&#65288;DiffiT&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23545;&#21435;&#22122;&#36807;&#31243;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#26102;&#38388;&#20381;&#36182;&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;TMSA&#65289;&#26426;&#21046;&#12290;DiffiT&#22312;&#29983;&#25104;&#39640;&#20445;&#30495;&#22270;&#20687;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#21442;&#25968;&#25928;&#29575;&#20063;&#26174;&#33879;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#31354;&#38388;&#21644;&#22270;&#20687;&#31354;&#38388;&#30340;DiffiT&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#21508;&#31181;&#31867;&#21035;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#32508;&#21512;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28508;&#31354;&#38388;DiffiT&#27169;&#22411;&#36798;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02139v2 Announce Type: replace-cross  Abstract: Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2011.08388</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.08388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#38754;&#37096;&#21644;&#38750;&#38754;&#37096;&#29289;&#20307;&#20197;&#21450;&#38750;&#20154;&#31867;&#32452;&#20214;&#30340;&#36890;&#29992;&#22270;&#20687;&#20013;&#30340;&#24773;&#32490;&#12290;&#23427;&#35299;&#20915;&#20102;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65288;IER&#65289;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#20998;&#31867;&#20026;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#25552;&#20986;&#30340;FER&#31995;&#32479;&#36866;&#24212;&#20110;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#35782;&#21035;&#22270;&#20687;&#25152;&#20256;&#36798;&#30340;&#24773;&#32490;&#12290;&#23427;&#23558;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#20026;&#8220;&#24555;&#20048;&#8221;&#65292;&#8220;&#24754;&#20260;&#8221;&#65292;&#8220;&#20167;&#24680;&#8221;&#21644;&#8220;&#24868;&#24594;&#8221;&#31867;&#21035;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#30340;Shap&#65288;DnCShap&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#39640;&#24230;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial &amp; non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.15274</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#29992;&#20110;AGI&#65306;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Systematic AI Approach for AGI: Addressing Alignment, Energy, and AGI Grand Challenges. (arXiv:2310.15274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#38754;&#20020;&#33021;&#28304;&#12289;&#23545;&#40784;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#19977;&#22823;&#25361;&#25112;&#30340;&#31995;&#32479;&#21270;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#33021;&#28304;&#28040;&#32791;&#12289;&#31995;&#32479;&#35774;&#35745;&#21644;&#23545;&#40784;&#38382;&#39064;&#19978;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#38754;&#20020;&#30528;&#19977;&#22823;&#25361;&#25112;&#65306;&#33021;&#28304;&#22721;&#22418;&#12289;&#23545;&#40784;&#38382;&#39064;&#21644;&#20174;&#29421;&#20041;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#12290;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#26085;&#24120;&#36816;&#34892;&#36807;&#31243;&#20013;&#28040;&#32791;&#30528;&#19981;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#33258;2020&#24180;&#20197;&#26469;&#65292;&#27599;&#20010;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#27599;&#20004;&#20010;&#26376;&#23601;&#32763;&#20493;&#65292;&#30452;&#25509;&#23548;&#33268;&#33021;&#28304;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21040;AGI&#30340;&#39134;&#36291;&#38656;&#35201;&#22810;&#20010;&#21151;&#33021;&#23376;&#31995;&#32479;&#20197;&#24179;&#34913;&#30340;&#26041;&#24335;&#36816;&#20316;&#65292;&#36825;&#38656;&#35201;&#19968;&#20010;&#31995;&#32479;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#32570;&#20047;&#31995;&#32479;&#35774;&#35745;&#65307;&#21363;&#20351;&#31995;&#32479;&#29305;&#24449;&#22312;&#20154;&#33041;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20174;&#23427;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#21040;&#23427;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#12290;&#21516;&#26679;&#65292;&#24403;&#21069;&#30340;&#23545;&#40784;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#31995;&#32479;&#35774;&#35745;&#65292;&#28982;&#32780;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#33041;&#30340;&#31995;&#32479;&#26550;&#26500;&#22312;&#20581;&#24247;&#30340;&#36947;&#24503;&#20915;&#31574;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#31995;&#32479;&#35774;&#35745;&#22312;&#35299;&#20915;&#23545;&#40784;&#12289;&#33021;&#28304;&#21644;AGI&#22823;&#25361;&#25112;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI faces a trifecta of grand challenges the Energy Wall, the Alignment Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume unsustainable amounts of energy during model training and daily operations.Making things worse, the amount of computation required to train each new AI model has been doubling every 2 months since 2020, directly translating to increases in energy consumption.The leap from AI to AGI requires multiple functional subsystems operating in a balanced manner, which requires a system architecture. However, the current approach to artificial intelligence lacks system design; even though system characteristics play a key role in the human brain from the way it processes information to how it makes decisions. Similarly, current alignment and AI ethics approaches largely ignore system design, yet studies show that the brains system architecture plays a critical role in healthy moral decisions.In this paper, we argue that system design is critically im
&lt;/p&gt;</description></item><item><title>&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;&#65292;&#35813;&#30740;&#31350;&#20026;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#21644;&#27169;&#20056;&#27861;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12660</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic. (arXiv:2310.12660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12660
&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;&#65292;&#35813;&#30740;&#31350;&#20026;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#21644;&#27169;&#20056;&#27861;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#19968;&#20123;&#21253;&#21547;&#22823;&#37327;&#36817;&#20284;&#27491;&#20132;&#20803;&#32032;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#21035;&#38590;&#20197;&#34987;&#32479;&#35745;&#26597;&#35810;&#31639;&#27861;&#23398;&#20064;&#21040;&#12290;&#26368;&#36817;&#65292;&#36825;&#19968;&#32463;&#20856;&#20107;&#23454;&#20877;&#27425;&#20986;&#29616;&#22312;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#20248;&#21270;&#30340;&#29702;&#35770;&#20013;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#31867;&#30340;&#38590;&#24230;&#36890;&#24120;&#30001;&#26799;&#24230;&#23545;&#38543;&#26426;&#36873;&#25321;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#24046;&#26469;&#34913;&#37327;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#24418;&#24335;&#20026;$x \to ax \bmod p$&#30340;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;$a$&#21462;&#33258;${\mathbb Z}_p$&#65292;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#23478;&#21644;&#23494;&#30721;&#23398;&#23478;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#31867;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;${\mathbb Z}$&#19978;&#30340;$p$-&#21608;&#26399;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#19982;&#23454;&#25968;&#32447;&#19978;&#30340;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#31867;&#32039;&#23494;&#30456;&#20851;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#20174;&#31034;&#20363;&#20013;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#25110;&#27169;&#20056;&#27861;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#30456;&#20851;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function.  A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from ${\mathbb Z}_p$, has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class of high-frequency periodic functions on the real line.  We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12000</link><description>&lt;p&gt;
Vecchia-Laplace&#36817;&#20284;&#27861;&#22312;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#26159;&#28789;&#27963;&#30340;&#27010;&#29575;&#38750;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#12290;Vecchia&#36817;&#20284;&#26159;&#29992;&#20110;&#20811;&#26381;&#22823;&#25968;&#25454;&#35745;&#31639;&#29942;&#39048;&#30340;&#20934;&#30830;&#36817;&#20284;&#26041;&#27861;&#65292;Laplace&#36817;&#20284;&#26159;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#38750;&#39640;&#26031;&#20284;&#28982;&#20989;&#25968;&#30340;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#30452;&#25509;&#27714;&#35299;&#26041;&#27861;&#65288;&#22914;Cholesky&#20998;&#35299;&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Vecchia-Laplace&#36817;&#20284;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#36229;&#32447;&#24615;&#22320;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#19982;Vecchia-Laplace&#36817;&#20284;&#35745;&#31639;&#30456;&#20851;&#30340;&#36816;&#31639;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#26159;&#26368;&#20934;&#30830;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20250;&#21464;&#24471;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;Vecchia-Laplace&#36817;&#20284;&#25512;&#26029;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Cholesky&#30340;&#35745;&#31639;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06380</link><description>&lt;p&gt;
CAST&#65306;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CAST: Cluster-Aware Self-Training for Tabular Data. (arXiv:2310.06380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;CAST&#65289;&#65292;&#36890;&#36807;&#35268;&#33539;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#65292;&#24357;&#34917;&#20102;&#33258;&#35757;&#32451;&#31639;&#27861;&#20013;&#30340;&#19968;&#20123;&#24369;&#28857;&#65292;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#22810;&#21151;&#33021;&#24615;&#32780;&#21463;&#21040;&#21560;&#24341;&#65292;&#28982;&#32780;&#23427;&#23481;&#26131;&#21463;&#21040;&#26377;&#22122;&#38899;&#30340;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;&#20960;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25104;&#21151;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#21066;&#24369;&#20102;&#33258;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#23545;&#33258;&#35757;&#32451;&#31639;&#27861;&#25110;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#29305;&#23450;&#30340;&#20462;&#25913;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19982;&#22312;&#34920;&#26684;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#19981;&#20860;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#32676;&#38598;&#20551;&#35774;&#65292;&#21363;&#30456;&#20114;&#25509;&#36817;&#30340;&#25968;&#25454;&#26679;&#26412;&#24448;&#24448;&#23646;&#20110;&#21516;&#19968;&#31867;&#12290;&#22312;&#27492;&#20551;&#35774;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#32676;&#38598;&#24863;&#30693;&#33258;&#35757;&#32451;&#65288;CAST&#65289;&#26041;&#27861;&#12290;CAST&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26222;&#36941;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#29616;&#26377;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#24133;&#20462;&#25913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35268;&#33539;&#20102;&#20998;&#31867;&#22120;&#30340;&#32622;&#20449;&#24230;&#65292;&#21363;&#20266;&#26631;&#31614;&#30340;&#20540;&#65292;&#24378;&#21046;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#23545;&#20266;&#26631;&#31614;&#36827;&#34892;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training has gained attraction because of its simplicity and versatility, yet it is vulnerable to noisy pseudo-labels. Several studies have proposed successful approaches to tackle this issue, but they have diminished the advantages of self-training because they require specific modifications in self-training algorithms or model architectures. Furthermore, most of them are incompatible with gradient boosting decision trees, which dominate the tabular domain. To address this, we revisit the cluster assumption, which states that data samples that are close to each other tend to belong to the same class. Inspired by the assumption, we propose Cluster-Aware Self-Training (CAST) for tabular data. CAST is a simple and universally adaptable approach for enhancing existing self-training algorithms without significant modifications. Concretely, our method regularizes the confidence of the classifier, which represents the value of the pseudo-label, forcing the pseudo-labels in low-density r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03103</link><description>&lt;p&gt;
&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#25552;&#31034;&#35843;&#20248;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#20043;&#38388;&#26222;&#36941;&#23384;&#22312;&#39046;&#22495;&#21464;&#21270;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#38590;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#29616;&#23454;&#30340;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#65292;&#20854;&#20013;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#39046;&#22495;&#21464;&#21270;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32852;&#37030;&#21452;&#25552;&#31034;&#35843;&#20248;&#65288;Fed-DPT&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Fed-DPT&#37319;&#29992;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#25552;&#31034;&#35843;&#20248;&#26469;&#20419;&#36827;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;&#22823;&#37327;&#30340;Fed-DPT&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#22312;&#39046;&#22495;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.11375</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Interpretable Distribution-Invariant Fairness Measures for Continuous Scores. (arXiv:2308.11375v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11375
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36830;&#32493;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#37322;&#24230;&#37327;&#32467;&#26524;&#24182;&#36866;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#24230;&#37327;&#36890;&#24120;&#22312;&#20108;&#20803;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#36830;&#32493;&#35780;&#20998;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22522;&#20110;ROC&#30340;&#24230;&#37327;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#27492;&#30446;&#30340;&#12290;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35780;&#20998;&#30340;&#20998;&#24067;&#65292;&#19981;&#36866;&#29992;&#20110;&#25490;&#21517;&#20219;&#21153;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#25928;&#26524;&#22823;&#23567;&#19981;&#21487;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#36830;&#32493;&#35780;&#20998;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#20855;&#26377;&#21512;&#29702;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#26131;&#20110;&#35745;&#31639;&#65292;&#24182;&#36866;&#29992;&#20110;&#37327;&#21270;&#21644;&#35299;&#37322;&#32676;&#20307;&#24046;&#24322;&#30340;&#24378;&#24230;&#65292;&#20197;&#21450;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#25110;&#26102;&#38388;&#28857;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#26377;&#35780;&#20998;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#30340;&#19981;&#21516;&#26063;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#19981;&#21464;&#20844;&#24179;&#24615;&#24230;&#37327;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#26126;&#30830;&#65292;&#24182;&#19988;&#21487;&#20197;&#37327;&#21270;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#32780;ROC-based&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03411</link><description>&lt;p&gt;
&#20174;&#24322;&#36136;&#24615;&#20013;&#23398;&#20064;&#65306;&#29992;&#20110;&#36229;&#22270;&#30340;&#21160;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs. (arXiv:2307.03411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#22312;&#36817;&#24180;&#26469;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#25152;&#26377;&#22270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#36229;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#22270;&#30340;&#23884;&#20837;&#31354;&#38388;&#26102;&#25506;&#32034;&#38544;&#21547;&#30340;&#39640;&#38454;&#20851;&#32852;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#21160;&#24577;&#36229;&#36793;&#26500;&#24314;&#21644;&#20851;&#27880;&#24615;&#23884;&#20837;&#26356;&#26032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#28982;&#21518;&#36890;&#36807;&#38544;&#24335;&#36229;&#36793;&#30340;&#21160;&#24577;&#20998;&#32452;&#26469;&#26500;&#24314;&#36229;&#22270;&#65292;&#24182;&#36827;&#34892;&#31867;&#22411;&#29305;&#23450;&#30340;&#36229;&#22270;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this paper, we propose a hypergraph learning framework named LFH that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterwards, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#20197;&#21450;&#24433;&#21709;&#20998;&#37197;&#30340;&#22240;&#32032;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#36866;&#21512;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.10994</link><description>&lt;p&gt;
&#29702;&#35299;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#38544;&#31169;&#39044;&#31639;
&lt;/p&gt;
&lt;p&gt;
Understanding how Differentially Private Generative Models Spend their Privacy Budget. (arXiv:2305.10994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#20197;&#21450;&#24433;&#21709;&#20998;&#37197;&#30340;&#22240;&#32032;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#36866;&#21512;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#20943;&#23569;&#38544;&#31169;&#39118;&#38505;&#12290;&#20294;&#26159;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#26435;&#34913;&#23427;&#20204;&#20043;&#38388;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20851;&#31995;&#12290;&#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;DP&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#24182;&#25506;&#35752;&#20102;&#24433;&#21709;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#23545;&#22270;&#24418;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models trained with Differential Privacy (DP) are increasingly used to produce synthetic data while reducing privacy risks. Navigating their specific privacy-utility tradeoffs makes it challenging to determine which models would work best for specific settings/tasks. In this paper, we fill this gap in the context of tabular data by analyzing how DP generative models distribute privacy budgets across rows and columns, arguably the main source of utility degradation. We examine the main factors contributing to how privacy budgets are spent, including underlying modeling techniques, DP mechanisms, and data dimensionality.  Our extensive evaluation of both graphical and deep generative models sheds light on the distinctive features that render them suitable for different settings and tasks. We show that graphical models distribute the privacy budget horizontally and thus cannot handle relatively wide datasets while the performance on the task they were optimized for monotonicall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.14326</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#24102;&#38271;&#26399;&#32422;&#26463;&#30340;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Best-of-Both-Worlds Algorithm for Constrained MDPs with Long-Term Constraints. (arXiv:2304.14326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32422;&#26463;MDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29615;&#24418;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#23613;&#21487;&#33021;&#22810;&#30340;&#22870;&#21169;&#30340;&#21516;&#26102;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#35777;&#28385;&#36275;&#19968;&#20123;&#38271;&#26399;&#32422;&#26463;&#12290;&#22870;&#21169;&#21644;&#32422;&#26463;&#21487;&#20197;&#38543;&#26426;&#25110;&#25932;&#23545;&#22320;&#36873;&#25321;&#65292;&#24182;&#19988;&#36716;&#31227;&#20989;&#25968;&#23545;&#23398;&#20064;&#32773;&#26159;&#26410;&#30693;&#30340;&#12290;&#34429;&#28982;&#22312;&#32463;&#20856;&#30340;&#26080;&#32422;&#26463;MDPs&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21463;&#21040;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#20294;CMDP&#30340;&#35774;&#32622;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#19968;&#28857;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#12289;&#33258;&#21160;&#25237;&#26631;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#39069;&#22806;&#30340;&#32422;&#26463;&#21644;&#35268;&#33539;&#65292;&#20195;&#29702;&#24517;&#39035;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36981;&#23432;&#36825;&#20123;&#35268;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#32422;&#26463;&#30340;CMDPs&#30340;&#21452;&#36194;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#22870;&#21169;&#21644;&#32422;&#26463;&#38543;&#26426;&#25110;&#25932;&#23545;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online learning in episodic constrained Markov decision processes (CMDPs), where the goal of the learner is to collect as much reward as possible over the episodes, while guaranteeing that some long-term constraints are satisfied during the learning process. Rewards and constraints can be selected either stochastically or adversarially, and the transition function is not known to the learner. While online learning in classical unconstrained MDPs has received considerable attention over the last years, the setting of CMDPs is still largely unexplored. This is surprising, since in real-world applications, such as, e.g., autonomous driving, automated bidding, and recommender systems, there are usually additional constraints and specifications that an agent has to obey during the learning process. In this paper, we provide the first best-of-both-worlds algorithm for CMDPs with long-term constraints. Our algorithm is capable of handling settings in which rewards and constraints are
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15477</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#30340;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20869;&#22312;&#33021;&#22815;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#30456;&#20851;&#24615;&#65292;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21453;&#26144;SPD&#27969;&#24418;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22266;&#23450;&#24230;&#37327;&#24352;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;SPD&#30697;&#38453;&#23398;&#20064;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;SPD&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#25289;&#22238;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;SPD&#27969;&#24418;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#24230;&#37327;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;SPD&#32593;&#32476;&#21487;&#20197;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11962</link><description>&lt;p&gt;
&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#32479;&#19968;&#25910;&#25947;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Unified Convergence Theory of Stochastic and Variance-Reduced Cubic Newton Methods. (arXiv:2302.11962v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11962
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36741;&#21161;&#26694;&#26550;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#12290;&#35813;&#26694;&#26550;&#22312;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#26102;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20351;&#29992;&#20102;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#20197;&#21450;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#32467;&#21512;&#20102;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#21487;&#33021;&#38750;&#20984;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#36741;&#21161;&#26694;&#26550;&#65292;&#23427;&#25552;&#20379;&#20102;&#20855;&#26377;&#20840;&#23616;&#22797;&#26434;&#24615;&#20445;&#35777;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#20108;&#38454;&#31639;&#27861;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#23427;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#24102;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#36741;&#21161;&#26694;&#26550;&#20026;&#31639;&#27861;&#35774;&#35745;&#32773;&#25552;&#20379;&#20102;&#26500;&#24314;&#21644;&#20998;&#26512;&#38543;&#26426;&#19977;&#27425;&#29275;&#39039;&#26041;&#27861;&#30340;&#39640;&#24230;&#28789;&#27963;&#24615;&#65292;&#20801;&#35768;&#20219;&#24847;&#22823;&#23567;&#30340;&#25209;&#37327;&#65292;&#24182;&#19988;&#20351;&#29992;&#26377;&#22122;&#22768;&#21644;&#21487;&#33021;&#26377;&#20559;&#24046;&#30340;&#26799;&#24230;&#21644;Hessian&#30340;&#20272;&#35745;&#65292;&#23558;&#26041;&#24046;&#20943;&#23569;&#21644;&#24816;&#24615;Hessian&#26356;&#26032;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#22122;&#22768;&#30340;&#24369;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#24050;&#30693;&#30340;&#38543;&#26426;&#21644;&#26041;&#24046;&#20943;&#23569;&#30340;&#19977;&#27425;&#29275;&#39039;&#30340;&#26368;&#20339;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#30452;&#25509;&#32467;&#26524;&#26159;&#26032;&#30340;&#24816;&#24615;&#38543;&#26426;&#20108;&#38454;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#25913;&#36827;&#20102;&#22823;&#32500;&#38382;&#39064;&#30340;&#31639;&#26415;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study stochastic Cubic Newton methods for solving general possibly non-convex minimization problems. We propose a new framework, which we call the helper framework, that provides a unified view of the stochastic and variance-reduced second-order algorithms equipped with global complexity guarantees. It can also be applied to learning with auxiliary information. Our helper framework offers the algorithm designer high flexibility for constructing and analyzing the stochastic Cubic Newton methods, allowing arbitrary size batches, and the use of noisy and possibly biased estimates of the gradients and Hessians, incorporating both the variance reduction and the lazy Hessian updates. We recover the best-known complexities for the stochastic and variance-reduced Cubic Newton, under weak assumptions on the noise. A direct consequence of our theory is the new lazy stochastic second-order method, which significantly improves the arithmetic complexity for large dimension problems. We also esta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2206.02346</link><description>&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22312;&#32422;&#26463;MDP&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. (arXiv:2206.02346v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#21644;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21464;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#23616;&#25910;&#25947;&#20013;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36895;&#29575;&#65292;&#32780;&#19988;&#19981;&#21463;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#22823;&#23567;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#22870;&#21169;&#65292;&#21516;&#26102;&#28385;&#36275;&#23545;&#39044;&#26399;&#24635;&#25928;&#29992;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#26469;&#35299;&#20915;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;&#32422;&#26463;MDP&#65289;&#30340;&#25240;&#25187;&#26080;&#38480;&#26102;&#24207;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21407;&#22987;-&#23545;&#20598;&#65288;NPG-PD&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#65292;&#36890;&#36807;&#25237;&#24433;&#27425;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;&#23613;&#31649;&#24213;&#23618;&#26368;&#22823;&#21270;&#28041;&#21450;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#38750;&#20984;&#32422;&#26463;&#38598;&#65292;&#20294;&#22312;softmax&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#38388;&#38553;&#21644;&#32422;&#26463;&#36829;&#35268;&#26041;&#38754;&#23454;&#29616;&#20840;&#23616;&#25910;&#25947;&#65292;&#24182;&#20855;&#26377;&#27425;&#32447;&#24615;&#36895;&#29575;&#12290;&#27492;&#31867;&#25910;&#25947;&#19982;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#21363;&#26080;&#32500;&#24230;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#25968;&#32447;&#24615;&#21644;&#19968;&#33324;&#24179;&#28369;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#25105;&#20204;&#30830;&#31435;&#20102;&#25910;&#25947;&#24615;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we esta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#25351;&#21335;&#12290;&#20854;&#20013;&#21253;&#25324;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#30340;&#20934;&#22791;&#24037;&#20316;&#12289;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#12289;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#12289;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#20197;&#21450;&#25253;&#21578;&#32467;&#26524;&#31561;&#20116;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2108.02497</link><description>&lt;p&gt;
&#22914;&#20309;&#36991;&#20813;&#26426;&#22120;&#23398;&#20064;&#38519;&#38449;&#65306;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#30340;&#26041;&#27861;&#21644;&#25351;&#21335;&#12290;&#20854;&#20013;&#21253;&#25324;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#30340;&#20934;&#22791;&#24037;&#20316;&#12289;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#12289;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#12289;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#20197;&#21450;&#25253;&#21578;&#32467;&#26524;&#31561;&#20116;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26102;&#24120;&#35265;&#30340;&#19968;&#20123;&#38169;&#35823;&#65292;&#20197;&#21450;&#22914;&#20309;&#36991;&#20813;&#23427;&#20204;&#12290;&#34429;&#28982;&#23545;&#20110;&#20855;&#26377;&#22522;&#26412;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29702;&#35299;&#30340;&#20219;&#20309;&#20154;&#37117;&#24212;&#35813;&#26131;&#20110;&#29702;&#35299;&#65292;&#20294;&#23427;&#26368;&#21021;&#26159;&#20026;&#30740;&#31350;&#29983;&#25776;&#20889;&#30340;&#65292;&#24182;&#20851;&#27880;&#23398;&#26415;&#30740;&#31350;&#20013;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36827;&#34892;&#20005;&#26684;&#27604;&#36739;&#21644;&#24471;&#20986;&#26377;&#25928;&#32467;&#35770;&#30340;&#38656;&#27714;&#12290;&#23427;&#28085;&#30422;&#20102;&#26426;&#22120;&#23398;&#20064;&#36807;&#31243;&#30340;&#20116;&#20010;&#38454;&#27573;&#65306;&#22914;&#20309;&#22312;&#27169;&#22411;&#26500;&#24314;&#20043;&#21069;&#36827;&#34892;&#20934;&#22791;&#65292;&#22914;&#20309;&#21487;&#38752;&#22320;&#26500;&#24314;&#27169;&#22411;&#65292;&#22914;&#20309;&#31283;&#20581;&#22320;&#35780;&#20272;&#27169;&#22411;&#65292;&#22914;&#20309;&#20844;&#24179;&#27604;&#36739;&#27169;&#22411;&#65292;&#20197;&#21450;&#22914;&#20309;&#25253;&#21578;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document outlines some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.
&lt;/p&gt;</description></item></channel></rss>