<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01632</link><description>&lt;p&gt;
&#36229;&#36234;&#23610;&#24230;&#65306;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#26080;&#36951;&#25022;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#38656;&#35201;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#32780;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#25351;&#23450;&#36229;&#21442;&#25968; - &#22823;&#37096;&#20998;&#29702;&#35770;&#25991;&#29486;&#20551;&#35774;&#36825;&#20123;&#36229;&#21442;&#25968;&#26159;&#24050;&#30693;&#30340;&#12290;&#20043;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#22312;&#31354;&#38388;&#20013;&#22343;&#21248;&#22635;&#20805;&#65292;&#32780;&#24120;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#21482;&#26377;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25165;&#26159;&#19968;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25968;&#25454;&#19981;&#19968;&#23450;&#28385;&#36275;&#36825;&#31181;&#22343;&#21248;&#22635;&#20805;&#30340;&#26465;&#20214;&#12290;&#30001;&#20110;&#26080;&#27861;&#20445;&#35777;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36229;&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#65292;&#22240;&#27492;&#23545;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#20165;&#33021;&#22788;&#29702;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26410;&#30693;&#38271;&#24230;&#23610;&#24230;&#12289;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#33539;&#25968;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#39057;&#29575;&#27966;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;HE-GP-UCB&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.11894</link><description>&lt;p&gt;
&#20174;&#21487;&#35299;&#37322;&#21040;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#30103;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#23454;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
From explainable to interpretable deep learning for natural language processing in healthcare: how far from reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#23457;&#26597;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XIAI&#65289;&#27010;&#24565;&#65292;&#24182;&#21457;&#29616;&#27880;&#24847;&#26426;&#21046;&#26159;&#20027;&#35201;&#26032;&#20852;IAI&#65292;&#21516;&#26102;&#38754;&#20020;&#30528;&#32570;&#20047;&#20840;&#23616;&#24314;&#27169;&#12289;&#26368;&#20339;&#23454;&#36341;&#20197;&#21450;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#36890;&#36807;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#21307;&#30103;&#20445;&#20581;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;DL&#30340;NLP&#26041;&#27861;&#26085;&#30410;&#22797;&#26434;&#65292;&#38656;&#35201;&#36879;&#26126;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#25110;&#33267;&#23569;&#26159;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#36827;&#34892;&#21487;&#38752;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#26412;&#25991;&#23545;&#21307;&#30103;&#20581;&#24247;NLP&#20013;&#30340;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;DL&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#33539;&#22260;&#23457;&#26597;&#12290;&#24341;&#20837;&#20102;&#26415;&#35821;&#8220;XIAI&#8221;&#65288;eXplainable&#21644;Interpretable Artificial Intelligence&#65289;&#20197;&#21306;&#20998;XAI&#21644;IAI&#12290;&#26041;&#27861;&#26681;&#25454;&#20854;&#21151;&#33021;&#65288;&#27169;&#22411;&#12289;&#36755;&#20837;&#12289;&#36755;&#20986;&#20026;&#22522;&#30784;&#65289;&#21644;&#33539;&#22260;&#65288;&#23616;&#37096;&#12289;&#20840;&#23616;&#65289;&#36827;&#19968;&#27493;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27880;&#24847;&#26426;&#21046;&#26159;&#26368;&#20027;&#35201;&#30340;&#26032;&#20852;IAI&#12290;&#27492;&#22806;&#65292;IAI&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#23545;&#25239;XAI&#12290;&#30830;&#23450;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;XIAI&#19981;&#25506;&#32034;&#8220;&#20840;&#23616;&#8221;&#24314;&#27169;&#36807;&#31243;&#65292;&#32570;&#20047;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#19988;&#38656;&#35201;&#31995;&#32479;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11894v1 Announce Type: cross  Abstract: Deep learning (DL) has substantially enhanced healthcare research by addressing various natural language processing (NLP) tasks. Yet, the increasing complexity of DL-based NLP methods necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review on explainable and interpretable DL in healthcare NLP. The term "XIAI" (eXplainable and Interpretable Artificial Intelligence) was introduced to distinguish XAI from IAI. Methods were further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms were the most dominant emerging IAI. Moreover, IAI is increasingly used against XAI. The major challenges identified are that most XIAI do not explore "global" modeling processes, the lack of best practices, and the unmet need for systematic evaluation and benchmarks. Importan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.00853</link><description>&lt;p&gt;
&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Momentum Methods Under Biased Gradient Estimations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#28041;&#21450;&#20998;&#24067;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26080;&#20559;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#29702;&#35770;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26799;&#24230;&#20272;&#35745;&#24456;&#23481;&#26131;&#21464;&#24471;&#26377;&#20559;&#65292;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#65292;&#25968;&#25454;&#34987;&#27927;&#29260;&#26102;&#65292;&#20197;&#21450;&#22312;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on 
&lt;/p&gt;</description></item><item><title>DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18567</link><description>&lt;p&gt;
&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Are Versatile Protein Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18567
&lt;/p&gt;
&lt;p&gt;
DPLM&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#34507;&#30333;&#36136;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25193;&#25955;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;DPLM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#25165;&#22810;&#33402;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23545;&#34507;&#30333;&#36136;&#24207;&#21015;&#20855;&#26377;&#24378;&#22823;&#30340;&#29983;&#25104;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#31181;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#31163;&#25955;&#25193;&#25955;&#27010;&#29575;&#26694;&#26550;&#20013;&#20174;&#36827;&#21270;&#35268;&#27169;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#39044;&#35757;&#32451;&#21487;&#25193;&#23637;&#30340;DPLM&#65292;&#36825;&#20026;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;&#24314;&#27169;&#25552;&#20379;&#20102;&#22522;&#26412;&#26041;&#27861;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;DPLM&#23637;&#31034;&#20102;&#29983;&#25104;&#20986;&#31526;&#21512;&#32467;&#26500;&#30340;&#12289;&#26032;&#39062;&#30340;&#12289;&#22810;&#26679;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20351;&#24471;DPLM&#23545;&#34507;&#30333;&#36136;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20248;&#31168;&#30340;&#34920;&#31034;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#19982;ESM2&#65288;Lin et al., 2022&#65289;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;DPLM&#21487;&#20197;&#38024;&#23545;&#21508;&#31181;&#38656;&#27714;&#36827;&#34892;&#23450;&#21046;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#30340;&#23454;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18567v1 Announce Type: new  Abstract: This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionary-scale protein sequences within a generative self-supervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel, and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training makes DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#12290;&#38416;&#26126;&#20102;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09469</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20613;&#31435;&#21494;&#30005;&#36335;&#65306;&#35299;&#38145;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fourier Circuits in Neural Networks: Unlocking the Potential of Large Language Models in Mathematical Reasoning and Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#27169;&#36816;&#31639;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#12290;&#38416;&#26126;&#20102;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#19981;&#26029;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#25152;&#21033;&#29992;&#30340;&#20869;&#37096;&#34920;&#31034;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#22312;&#36817;&#26399;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#23545;&#32593;&#32476;&#37319;&#29992;&#29305;&#23450;&#35745;&#31639;&#31574;&#30053;&#32972;&#21518;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#28041;&#21450;k&#20010;&#36755;&#20837;&#30340;&#22797;&#26434;&#20195;&#25968;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#27169;&#36816;&#31639;&#30340;&#21152;&#27861;&#12290;&#25105;&#20204;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21333;&#23618;Transformer&#22312;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#29702;&#35770;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#26159;&#38416;&#26126;&#36793;&#32536;&#26368;&#22823;&#21270;&#21407;&#21017;&#23545;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#30340;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#65292;p&#34920;&#31034;&#27169;&#25968;&#65292;Dp&#34920;&#31034;k&#20010;&#36755;&#20837;&#30340;&#27169;&#36816;&#31639;&#25968;&#25454;&#38598;&#65292;m&#34920;&#31034;&#32593;&#32476;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09469v1 Announce Type: new  Abstract: In the evolving landscape of machine learning, a pivotal challenge lies in deciphering the internal representations harnessed by neural networks and Transformers. Building on recent progress toward comprehending how networks execute distinct target functions, our study embarks on an exploration of the underlying reasons behind networks adopting specific computational strategies. We direct our focus to the complex algebraic learning task of modular addition involving $k$ inputs. Our research presents a thorough analytical characterization of the features learned by stylized one-hidden layer neural networks and one-layer Transformers in addressing this task.   A cornerstone of our theoretical framework is the elucidation of how the principle of margin maximization shapes the features adopted by one-hidden layer neural networks. Let $p$ denote the modulus, $D_p$ denote the dataset of modular arithmetic with $k$ inputs and $m$ denote the net
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#36335;&#24452;&#38271;&#24230;&#26469;&#37327;&#21270;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09113</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#26368;&#20248;&#36755;&#36816;&#27979;&#37327;&#24378;&#21270;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Measuring Exploration in Reinforcement Learning via Optimal Transport in Policy Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#36335;&#24452;&#38271;&#24230;&#26469;&#37327;&#21270;&#25506;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#20915;&#23450;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23398;&#20064;&#36895;&#24230;&#21644;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37327;&#21270;&#21644;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25152;&#23436;&#25104;&#30340;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#25968;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#25968;&#65292;&#29992;&#20110;&#27604;&#36739;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30456;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#30693;&#35782;&#20256;&#36755;&#65288;&#21487;&#36716;&#31227;&#24615;&#65289;&#26041;&#38754;&#25152;&#20184;&#20986;&#30340;&#30456;&#23545;&#21162;&#21147;&#65292;&#20197;&#21450;&#23558; RL &#30340;&#21021;&#22987;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#26368;&#32456;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#23558;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27604;&#36739; RL &#21644; SL &#31639;&#27861;&#22312;&#25968;&#25454;&#20998;&#24067;&#31354;&#38388;&#20013;&#30340;&#24635;&#36335;&#24452;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#29615;&#22659;&#21644;&#22810;&#31181;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#35777;&#26126;&#25506;&#32034;&#25351;&#25968;&#21487;&#20197;&#25581;&#31034;&#25506;&#32034;&#34892;&#20026;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09113v1 Announce Type: new Abstract: Exploration is the key ingredient of reinforcement learning (RL) that determines the speed and success of learning. Here, we quantify and compare the amount of exploration and learning accomplished by a Reinforcement Learning (RL) algorithm. Specifically, we propose a novel measure, named Exploration Index, that quantifies the relative effort of knowledge transfer (transferability) by an RL algorithm in comparison to supervised learning (SL) that transforms the initial data distribution of RL to the corresponding final data distribution. The comparison is established by formulating learning in RL as a sequence of SL tasks, and using optimal transport based metrics to compare the total path traversed by the RL and SL algorithms in the data distribution space. We perform extensive empirical analysis on various environments and with multiple algorithms to demonstrate that the exploration index yields insights about the exploration behaviour 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.07762</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31232;&#30095;&#29305;&#23450;&#32972;&#26223;&#19979;&#22240;&#26524;&#31995;&#32479;&#30340;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Structure Learning for Sparse Context-Specific Causal Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#34920;&#31034;&#20849;&#21516;&#20998;&#24067;&#20998;&#31867;&#21464;&#37327;&#20043;&#38388;&#29305;&#23450;&#32972;&#26223;&#19979;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#37327;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#30340;&#23384;&#22312;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;&#22522;&#20110;&#32422;&#26463;&#30340;&#26041;&#27861;&#27604;&#32422;&#26463;DAG&#23398;&#20064;&#31639;&#27861;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#22240;&#20026;&#24517;&#39035;&#27979;&#35797;&#26356;&#22810;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#26469;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#21464;&#37327;&#65292;&#24182;&#19988;&#27979;&#35797;&#30340;&#32422;&#26463;&#19981;&#22810;&#20110;&#26631;&#20934;DAG&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31867;&#20284;&#20110;DAG&#27169;&#22411;&#24120;&#29992;&#30340;&#31232;&#30095;&#24615;&#20551;&#35774;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Alon&#21644;Balogh&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#32463;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested. We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms. Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models. To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh. The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#21516;&#26102;&#23398;&#20064;&#20301;&#32622;&#21644;&#35821;&#20041;&#20851;&#27880;&#65292;&#21457;&#29616;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#20174;&#20301;&#32622;&#26426;&#21046;&#21040;&#35821;&#20041;&#26426;&#21046;&#30340;&#30456;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.03902</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#27714;&#35299;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#20301;&#32622;&#23398;&#20064;&#21644;&#35821;&#20041;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
A phase transition between positional and semantic learning in a solvable model of dot-product attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#21516;&#26102;&#23398;&#20064;&#20301;&#32622;&#21644;&#35821;&#20041;&#20851;&#27880;&#65292;&#21457;&#29616;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#20174;&#20301;&#32622;&#26426;&#21046;&#21040;&#35821;&#20041;&#26426;&#21046;&#30340;&#30456;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23398;&#20064;&#20301;&#32622;&#27880;&#24847;&#21147;&#30697;&#38453;&#65288;&#36890;&#36807;&#21508;&#33258;&#30340;&#20301;&#32622;&#20915;&#23450;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#27880;&#65289;&#21644;&#35821;&#20041;&#27880;&#24847;&#21147;&#30697;&#38453;&#65288;&#36890;&#36807;&#24847;&#20041;&#20915;&#23450;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#27880;&#65289;&#12290;&#36890;&#36807;&#31639;&#27861;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21516;&#26679;&#31616;&#21333;&#30340;&#26550;&#26500;&#22914;&#20309;&#20351;&#29992;&#20301;&#32622;&#26426;&#21046;&#25110;&#35821;&#20041;&#26426;&#21046;&#26469;&#23454;&#29616;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#32465;&#23450;&#21644;&#20302;&#31209;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#30340;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#23398;&#20064;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#28176;&#36817;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#20301;&#32622;&#26426;&#21046;&#25110;&#35821;&#20041;&#26426;&#21046;&#65292;&#35777;&#26126;&#20102;&#38543;&#30528;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#20174;&#20301;&#32622;&#26426;&#21046;&#21521;&#35821;&#20041;&#26426;&#21046;&#30340;&#33258;&#21457;&#30456;&#21464;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#19982;&#20854;&#20182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product att
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.02701</link><description>&lt;p&gt;
&#29702;&#35299;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#22240;&#32032;&#65306;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Understanding What Affects Generalization Gap in Visual Reinforcement Learning: Theory and Empirical Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#24433;&#21709;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#26159;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23545;&#36830;&#32493;&#25511;&#21046;&#26377;&#29992;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#65292;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#31574;&#30053;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#27979;&#35797;&#29615;&#22659;&#21487;&#33021;&#19982;&#35757;&#32451;&#29615;&#22659;&#19981;&#21516;&#65292;&#20363;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#24178;&#25200;&#22240;&#32032;&#12290;&#35768;&#22810;&#23454;&#38469;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#20013;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#22240;&#32032;&#20197;&#21450;&#20026;&#20160;&#20040;&#20182;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27979;&#35797;&#29615;&#22659;&#20855;&#26377;&#24178;&#25200;&#22240;&#32032;&#26102;&#29702;&#35770;&#19978;&#22238;&#31572;&#24433;&#21709;&#27867;&#21270;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#26368;&#23567;&#21270;&#35757;&#32451;&#21644;&#27979;&#35797;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#65288;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65289;&#23545;&#20110;&#20943;&#23569;&#27867;&#21270;&#24046;&#36317;&#30340;&#25928;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;DM&#25968;&#25454;&#30340;&#23454;&#35777;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there are many efforts attempting to learn useful policies for continuous control in visual reinforcement learning (RL). In this scenario, it is important to learn a generalizable policy, as the testing environment may differ from the training environment, e.g., there exist distractors during deployment. Many practical algorithms are proposed to handle this problem. However, to the best of our knowledge, none of them provide a theoretical understanding of what affects the generalization gap and why their proposed methods work. In this paper, we bridge this issue by theoretically answering the key factors that contribute to the generalization gap when the testing environment has distractors. Our theories indicate that minimizing the representation distance between training and testing environments, which aligns with human intuition, is the most critical for the benefit of reducing the generalization gap. Our theoretical results are supported by the empirical evidence in the DM
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.00769</link><description>&lt;p&gt;
AnimateLCM: &#20351;&#29992;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#21152;&#36895;&#20010;&#24615;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#30340;&#21160;&#30011;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00769
&lt;/p&gt;
&lt;p&gt;
AnimateLCM&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#20135;&#29983;&#36830;&#36143;&#19988;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#30340;&#21435;&#22122;&#36807;&#31243;&#20351;&#20854;&#35745;&#31639;&#23494;&#38598;&#19988;&#32791;&#26102;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#21463;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#30340;&#27493;&#39588;&#33976;&#39311;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#20197;&#21450;&#20854;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#30340;&#25104;&#21151;&#25193;&#23637;&#8212;&#8212;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCM&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AnimateLCM&#65292;&#20801;&#35768;&#22312;&#26368;&#23567;&#30340;&#27493;&#39588;&#20869;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;&#19968;&#33268;&#24615;&#23398;&#20064;&#31574;&#30053;&#65292;&#23558;&#22270;&#20687;&#29983;&#25104;&#20248;&#20808;&#32423;&#21644;&#21160;&#20316;&#29983;&#25104;&#20248;&#20808;&#32423;&#30340;&#33976;&#39311;&#20998;&#31163;&#24320;&#26469;&#65292;&#36825;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#22686;&#24378;&#20102;&#29983;&#25104;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#31283;&#23450;&#30340;&#25193;&#25955;&#31038;&#21306;&#20013;&#30340;&#21363;&#25554;&#21363;&#29992;&#36866;&#37197;&#22120;&#30340;&#32452;&#21512;&#20197;&#23454;&#29616;&#21508;&#31181;&#20462;&#25913;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#37197;&#22120;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#31181;&#65292;&#21033;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.11929</link><description>&lt;p&gt;
&#8220;&#36234;&#22823;&#36234;&#22909;&#65311;&#8221;&#37325;&#26032;&#24605;&#32771;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26377;&#25928;&#27169;&#22411;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#31181;&#65292;&#21033;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#21069;&#27839;&#65292;&#20854;&#29305;&#28857;&#26159;&#20851;&#27880;&#20110;&#22823;&#37327;&#36755;&#20837;&#24207;&#21015;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#26377;&#38480;&#38271;&#24230;&#30456;&#27604;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#26356;&#38271;&#30340;&#24207;&#21015;&#26412;&#36136;&#19978;&#20256;&#36798;&#20102;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#21487;&#33021;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#24448;&#24448;&#36890;&#36807;&#25552;&#39640;&#27169;&#22411;&#22797;&#26434;&#24615;&#26469;&#24212;&#23545;&#12290;&#36825;&#20123;&#22797;&#26434;&#30340;&#27169;&#22411;&#21487;&#20197;&#33192;&#32960;&#20026;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#32534;&#30721;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#31561;&#21442;&#25968;&#23494;&#38598;&#22411;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#23548;&#33268;&#20102;&#31105;&#27490;&#24615;&#30340;&#27169;&#22411;&#35268;&#27169;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#35821;&#20041;&#31616;&#21333;&#24615;&#12290;&#20986;&#20110;&#36861;&#27714;&#31616;&#27905;&#24615;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#26465;&#20214;&#30456;&#20851;&#24615;&#21644;&#33258;&#30456;&#20851;&#24615;&#20316;&#20026;&#35843;&#26597;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36755;&#20837;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#20887;&#20313;&#12290;&#20511;&#21161;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HDformer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;Transformer&#21464;&#20307;&#65292;&#32463;&#36807;&#22686;&#24378;&#65292;&#20351;&#29992;&#33976;&#39311;&#25216;&#26415;&#21644;&#24555;&#36895;&#32593;&#32476;&#36830;&#25509;&#23618;&#26469;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;</title><link>https://arxiv.org/abs/2311.07454</link><description>&lt;p&gt;
&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discrete Nonparametric Causal Discovery Under Latent Class Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#29992;&#20110;&#24314;&#27169;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;"&#22240;&#26524;&#21457;&#29616;"&#25551;&#36848;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#24403;&#25968;&#25454;&#26159;&#26469;&#33258;&#22810;&#20010;&#28304;&#65288;&#32676;&#20307;&#25110;&#29615;&#22659;&#65289;&#30340;&#32858;&#21512;&#29289;&#26102;&#65292;&#20840;&#23616;&#28151;&#28102;&#20351;&#39537;&#21160;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#29305;&#24615;&#21464;&#24471;&#27169;&#31946;&#12290;&#36825;&#31181;&#24773;&#20917;&#26377;&#26102;&#34987;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#25110;&#28508;&#22312;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#20195;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#33021;&#22815;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#22788;&#29702;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#65292;&#20294;&#26159;&#30446;&#21069;&#25152;&#30693;&#30340;&#22788;&#29702;&#20840;&#23616;&#28151;&#28102;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#19981;&#36866;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#20197;&#31163;&#25955;&#21644;&#38750;&#21442;&#25968;&#35266;&#23519;&#21464;&#37327;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#30001;&#20840;&#23616;&#28151;&#28102;&#30340;&#22522;&#25968;&#12289;&#35266;&#23519;&#21464;&#37327;&#30340;&#22522;&#25968;&#31561;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07454v2 Announce Type: replace Abstract: Directed acyclic graphs are used to model the causal structure of a system. ``Causal discovery'' describes the problem of learning this structure from data. When data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. This setting is sometimes known as a mixture model or a latent class. While some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.Focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. The feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the o
&lt;/p&gt;</description></item><item><title>cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08895</link><description>&lt;p&gt;
cedar&#65306;&#21487;&#32452;&#21512;&#21644;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08895
&lt;/p&gt;
&lt;p&gt;
cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#26159;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#36127;&#36131;&#35835;&#21462;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#21464;&#25442;&#22788;&#29702;&#26679;&#26412;&#25209;&#27425;&#65292;&#24182;&#20197;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#23558;&#20854;&#21152;&#36733;&#21040;&#35757;&#32451;&#33410;&#28857;&#19978;&#12290;&#39640;&#24615;&#33021;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#30340;&#24615;&#33021;&#20248;&#21270;&#65292;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#26497;&#20302;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#22320;&#65292;&#28010;&#36153;&#26114;&#36149;&#30340;&#21152;&#36895;&#22120;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cedar&#65292;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;cedar&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#26469;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#30340;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02300</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02300
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#65292;&#35813;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;PDE&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;PINNs&#20013;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#19981;&#40065;&#26834;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#40065;&#26834;&#29256;&#26412;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;RPINNs&#65289;&#26469;&#36817;&#20284;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#35299;&#12290;&#26631;&#20934;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#30001;PDE&#25551;&#36848;&#30340;&#25511;&#21046;&#29289;&#29702;&#27861;&#21017;&#12290;&#35813;&#32593;&#32476;&#22312;&#30001;&#29289;&#29702;&#22495;&#21644;&#36793;&#30028;&#38543;&#26426;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;PINNs&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#30001;PDE&#21644;&#36793;&#30028;&#26465;&#20214;&#25551;&#36848;&#30340;&#21508;&#31181;&#38382;&#39064;&#12290;&#20256;&#32479;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;PDE&#30340;&#24378;&#27531;&#24046;&#12290;&#36825;&#31181;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#23545;&#30495;&#23454;&#35823;&#24046;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;PINNs&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#19982;&#30495;&#23454;&#35823;&#24046;&#21487;&#33021;&#30456;&#24046;&#24456;&#22823;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#26356;&#21152;&#22256;&#38590;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#65292;&#25105;&#20204;&#23601;&#19981;&#33021;&#20272;&#35745;&#35757;&#32451;&#36807;&#31243;&#26159;&#21542;&#24050;&#32463;&#20197;&#25152;&#38656;&#30340;&#31934;&#24230;&#25910;&#25947;&#21040;&#35299;&#12290;&#36825;&#22312;&#25105;&#20204;&#19981;&#30693;&#36947;&#31934;&#30830;&#35299;&#26102;&#23588;&#20854;&#27491;&#30830;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;</title><link>http://arxiv.org/abs/2312.00024</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#20462;&#22797;&#23433;&#20840;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;, Feedback-Driven Solution Synthesis (FDSS), &#26088;&#22312;&#36890;&#36807;&#23558;LLMs&#19982;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#32467;&#21512;&#65292;&#35299;&#20915;&#20195;&#30721;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#24320;&#21457;&#32773;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29983;&#25104;&#21253;&#21547;&#23433;&#20840;&#28431;&#27934;&#21644;&#32570;&#38519;&#30340;&#20195;&#30721;&#12290;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#28431;&#27934;&#36890;&#24120;&#22312;&#31243;&#24207;&#19982;&#22806;&#37096;&#31995;&#32479;&#25110;&#26381;&#21153;&#65288;&#22914;&#25968;&#25454;&#24211;&#21644;&#25805;&#20316;&#31995;&#32479;&#65289;&#20043;&#38388;&#30340;&#20132;&#20114;&#36807;&#31243;&#20013;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#26696;&#21512;&#25104;&#65288;FDSS&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20351;&#29992;LLMs&#25509;&#25910;&#26469;&#33258;&#38745;&#24577;&#20195;&#30721;&#20998;&#26512;&#24037;&#20855;Bandit&#30340;&#21453;&#39304;&#65292;&#28982;&#21518;LLMs&#29983;&#25104;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#23433;&#20840;&#28431;&#27934;&#12290;&#27599;&#20010;&#35299;&#20915;&#26041;&#26696;&#20197;&#21450;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#38543;&#21518;&#34987;&#36865;&#22238;LLMs&#36827;&#34892;&#20195;&#30721;&#23436;&#21892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PythonSecurityEval&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#26469;&#33258;Stack Overflow&#30340;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
&lt;/p&gt;</description></item><item><title>GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13833</link><description>&lt;p&gt;
GraphMaker: &#25193;&#25955;&#27169;&#22411;&#33021;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?. (arXiv:2310.13833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13833
&lt;/p&gt;
&lt;p&gt;
GraphMaker&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#22270;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#21019;&#24314;&#19982;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#31867;&#20284;&#30340;&#21512;&#25104;&#12289;&#23500;&#23646;&#24615;&#22270;&#23545;&#20110;&#20849;&#20139;&#22270;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#21644;&#24320;&#21457;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#21407;&#22987;&#25968;&#25454;&#38480;&#21046;&#34987;&#20849;&#20139;&#26102;&#12290;&#20256;&#32479;&#30340;&#22270;&#29983;&#25104;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#27809;&#26377;&#23646;&#24615;&#21644;&#36739;&#23567;&#30340;&#20998;&#23376;&#22270;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#23646;&#24615;-&#32467;&#26500;&#30456;&#20851;&#24615;&#21644;&#22270;&#30340;&#22823;&#35268;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#24102;&#23646;&#24615;&#22270;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65306;GraphMaker&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#21457;&#29616;&#24322;&#27493;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#20869;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with node attributes are increasingly common in various real-world applications. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial, especially for sharing graph data for analysis and developing learning models when original data is restricted to be shared. Traditional graph generation methods are limited in their capacity to handle these complex structures. Recent advances in diffusion models have shown potential in generating graph structures without attributes and smaller molecular graphs. However, these models face challenges in generating large attributed graphs due to the complex attribute-structure correlations and the large size of these graphs. This paper introduces a novel diffusion model, GraphMaker, specifically designed for generating large attributed graphs. We explore various combinations of node attribute and graph structure generation processes, finding that an asynchronous approach more effectively captures the intr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.00339</link><description>&lt;p&gt;
FedLPA: &#20351;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLPA: Personalized One-shot Federated Learning with Layer-Wise Posterior Aggregation. (arXiv:2310.00339v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedLPA&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#30340;&#26041;&#24335;&#23454;&#29616;&#20010;&#24615;&#21270;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#12290;FedLPA&#33021;&#22815;&#39640;&#25928;&#22320;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#27425;&#32858;&#21512;&#22312;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39640;&#25928;&#22320;&#32858;&#21512;&#21040;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21463;&#21040;&#38544;&#31169;&#38382;&#39064;&#20943;&#23569;&#12289;&#28508;&#22312;&#25915;&#20987;&#20943;&#24369;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#30340;&#25512;&#21160;&#65292;&#21333;&#27425;&#32852;&#37030;&#23398;&#20064;&#65288;&#21363;&#23558;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#38388;&#30340;&#36890;&#20449;&#38480;&#21046;&#20026;&#19968;&#36718;&#65289;&#22312;&#30740;&#31350;&#32773;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#21333;&#27425;&#32858;&#21512;&#30340;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#38750;&#30456;&#21516;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#22312;&#19968;&#20123;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#27425;&#32858;&#21512;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#21518;&#39564;&#32858;&#21512;&#65288;FedLPA&#65289;&#12290;FedLPA&#33021;&#22815;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#25110;&#26292;&#38706;&#20219;&#20309;&#26426;&#23494;&#30340;&#26412;&#22320;&#20449;&#24687;&#65292;&#27604;&#22914;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently aggregating trained neural networks from local clients into a global model on a server is a widely researched topic in federated learning. Recently, motivated by diminishing privacy concerns, mitigating potential attacks, and reducing the overhead of communication, one-shot federated learning (i.e., limiting client-server communication into a single round) has gained popularity among researchers. However, the one-shot aggregation performances are sensitively affected by the non-identical training data distribution, which exhibits high statistical heterogeneity in some real-world scenarios. To address this issue, we propose a novel one-shot aggregation method with Layer-wise Posterior Aggregation, named FedLPA. FedLPA aggregates local models to obtain a more accurate global model without requiring extra auxiliary datasets or exposing any confidential local information, e.g., label distributions. To effectively capture the statistics maintained in the biased local datasets in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#36890;&#36807;&#29305;&#23450;&#26041;&#24335;&#36941;&#21382;&#35774;&#35745;&#30697;&#38453;&#65292;&#21487;&#20197;&#25552;&#39640;&#35774;&#35745;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.04085</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#22312;&#36890;&#29992;&#31574;&#30053;&#32593;&#32476;&#19978;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Co-Design of Robotic Agents Using Multi-fidelity Training on Universal Policy Network. (arXiv:2309.04085v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04085
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#20849;&#35774;&#35745;&#30340;&#26679;&#26412;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#36890;&#36807;&#29305;&#23450;&#26041;&#24335;&#36941;&#21382;&#35774;&#35745;&#30697;&#38453;&#65292;&#21487;&#20197;&#25552;&#39640;&#35774;&#35745;&#35780;&#20272;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#35774;&#35745;&#28041;&#21450;&#21516;&#26102;&#20248;&#21270;&#25511;&#21046;&#22120;&#21644;&#20195;&#29702;&#29289;&#29702;&#35774;&#35745;&#12290;&#20854;&#22266;&#26377;&#30340;&#21452;&#23618;&#20248;&#21270;&#24418;&#24335;&#35201;&#27714;&#36890;&#36807;&#20869;&#23618;&#25511;&#21046;&#20248;&#21270;&#26469;&#39537;&#21160;&#22806;&#23618;&#35774;&#35745;&#20248;&#21270;&#12290;&#24403;&#35774;&#35745;&#31354;&#38388;&#36739;&#22823;&#19988;&#27599;&#20010;&#35774;&#35745;&#35780;&#20272;&#37117;&#28041;&#21450;&#25968;&#25454;&#23494;&#38598;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hyperband&#30340;&#22810;&#20445;&#30495;&#24230;&#35774;&#35745;&#25506;&#32034;&#31574;&#30053;&#65292;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#36890;&#36807;&#36890;&#29992;&#31574;&#30053;&#23398;&#20064;&#32773;&#23558;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#22120;&#36827;&#34892;&#20851;&#32852;&#65292;&#20197;&#21551;&#21160;&#21518;&#32493;&#25511;&#21046;&#22120;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#33616;&#19968;&#31181;&#29305;&#23450;&#30340;&#36941;&#21382;Hyperband&#29983;&#25104;&#30340;&#35774;&#35745;&#30697;&#38453;&#30340;&#26041;&#24335;&#65292;&#20197;&#30830;&#20445;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#35774;&#35745;&#35780;&#20272;&#65292;&#36890;&#29992;&#31574;&#30053;&#23398;&#20064;&#32773;&#30340;&#22686;&#24378;&#25928;&#26524;&#36234;&#26469;&#36234;&#24378;&#65292;&#20174;&#32780;&#38477;&#20302;Hyperband&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24180;&#40836;&#33539;&#22260;&#20869;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Co-design involves simultaneously optimizing the controller and agents physical design. Its inherent bi-level optimization formulation necessitates an outer loop design optimization driven by an inner loop control optimization. This can be challenging when the design space is large and each design evaluation involves data-intensive reinforcement learning process for control optimization. To improve the sample-efficiency we propose a multi-fidelity-based design exploration strategy based on Hyperband where we tie the controllers learnt across the design spaces through a universal policy learner for warm-starting the subsequent controller learning problems. Further, we recommend a particular way of traversing the Hyperband generated design matrix that ensures that the stochasticity of the Hyperband is reduced the most with the increasing warm starting effect of the universal policy learner as it is strengthened with each new design evaluation. Experiments performed on a wide range of age
&lt;/p&gt;</description></item><item><title>MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;</title><link>http://arxiv.org/abs/2308.06686</link><description>&lt;p&gt;
MDB&#65306;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MDB: Interactively Querying Datasets and Models. (arXiv:2308.06686v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06686
&lt;/p&gt;
&lt;p&gt;
MDB&#26159;&#19968;&#20010;&#35843;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#65292;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#24320;&#21457;&#32773;&#38656;&#35201;&#33021;&#22815;&#31995;&#32479;&#22320;&#35843;&#35797;&#22312;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#20013;&#20986;&#29616;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MDB&#65292;&#19968;&#20010;&#29992;&#20110;&#20114;&#21160;&#26597;&#35810;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#35843;&#35797;&#26694;&#26550;&#12290;MDB&#36890;&#36807;&#23558;&#20989;&#25968;&#24335;&#32534;&#31243;&#19982;&#20851;&#31995;&#20195;&#25968;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#39044;&#27979;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#34920;&#36798;&#24615;&#26597;&#35810;&#30340;&#24037;&#20855;&#12290;&#26597;&#35810;&#21487;&#37325;&#29992;&#19988;&#26131;&#20110;&#20462;&#25913;&#65292;&#20351;&#24471;&#35843;&#35797;&#20154;&#21592;&#33021;&#22815;&#24555;&#36895;&#36845;&#20195;&#21644;&#20248;&#21270;&#26597;&#35810;&#65292;&#20197;&#21457;&#29616;&#21644;&#25551;&#36848;&#38169;&#35823;&#21644;&#27169;&#22411;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#30446;&#26631;&#26816;&#27979;&#12289;&#20559;&#24046;&#21457;&#29616;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#25968;&#25454;&#22635;&#20805;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;MDB&#22312;&#33258;&#21160;&#39550;&#39542;&#35270;&#39057;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21307;&#30103;&#35760;&#24405;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MDB&#27604;&#20854;&#20182;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#39640;10&#20493;&#30340;&#26597;&#35810;&#36895;&#24230;&#21152;&#24555;&#21644;40%&#30340;&#26597;&#35810;&#38271;&#24230;&#32553;&#30701;&#12290;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24320;&#21457;&#32773;&#33021;&#22815;&#25104;&#21151;&#26500;&#24314;&#22797;&#26434;&#26597;&#35810;&#26469;&#25551;&#36848;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
As models are trained and deployed, developers need to be able to systematically debug errors that emerge in the machine learning pipeline. We present MDB, a debugging framework for interactively querying datasets and models. MDB integrates functional programming with relational algebra to build expressive queries over a database of datasets and model predictions. Queries are reusable and easily modified, enabling debuggers to rapidly iterate and refine queries to discover and characterize errors and model behaviors. We evaluate MDB on object detection, bias discovery, image classification, and data imputation tasks across self-driving videos, large language models, and medical records. Our experiments show that MDB enables up to 10x faster and 40\% shorter queries than other baselines. In a user study, we find developers can successfully construct complex queries that describe errors of machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01472</link><description>&lt;p&gt;
&#21453;&#21521;&#31283;&#23450;&#25193;&#25955;&#65306;&#29983;&#25104;&#35813;&#22270;&#20687;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#32467;&#21512;&#20102;&#22810;&#31181;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#65292;&#24182;&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#26368;&#36817;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#22312;&#26356;&#22909;&#22320;&#29702;&#35299;&#29983;&#25104;&#36807;&#31243;&#21644;&#22914;&#20309;&#35774;&#35745;&#25552;&#31034;&#20197;&#33719;&#24471;&#25152;&#38656;&#22270;&#20687;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#30001;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#31995;&#21015;&#30333;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#65288;&#26377;&#21644;&#26080;&#23545;&#25193;&#25955;&#32593;&#32476;&#26435;&#37325;&#36827;&#34892;&#35775;&#38382;&#65289;&#26469;&#22788;&#29702;&#25152;&#25552;&#20986;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#25552;&#31034;&#22238;&#24402;&#21644;&#22810;&#26631;&#31614;&#35789;&#27719;&#20998;&#31867;&#30446;&#26631;&#65292;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#20855;&#26377;&#26356;&#20302;&#26631;&#27880;&#22122;&#22768;&#65288;&#21363;&#26356;&#22909;&#23545;&#40784;&#65289;&#30340;&#22270;&#20687;&#25552;&#31034;&#23545;&#30340;&#23398;&#20064;&#65292;&#24182;&#19988;&#20351;&#29992;&#30456;&#20284;&#24615;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26680;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models such as Stable Diffusion have recently attracted the interest of many researchers, and inverting the diffusion process can play an important role in better understanding the generative process and how to engineer prompts in order to obtain the desired images. To this end, we introduce the new task of predicting the text prompt given an image generated by a generative diffusion model. We combine a series of white-box and black-box models (with and without access to the weights of the diffusion network) to deal with the proposed task. We propose a novel learning framework comprising of a joint prompt regression and multi-label vocabulary classification objective that generates improved prompts. To further improve our method, we employ a curriculum learning procedure that promotes the learning of image-prompt pairs with lower labeling noise (i.e. that are better aligned), and an unsupervised domain-adaptive kernel learning method that uses the similarities b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;</title><link>http://arxiv.org/abs/2307.06541</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effective Horizon of Inverse Reinforcement Learning. (arXiv:2307.06541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#26102;&#38388;&#35270;&#37326;&#30340;&#37325;&#35201;&#24615;&#65292;&#21457;&#29616;&#30701;&#20110;&#23454;&#38469;&#20540;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#21487;&#20197;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21628;&#21505;&#22312;IRL&#20013;&#21516;&#26102;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#32473;&#23450;&#26102;&#38388;&#35270;&#37326;&#30340;&#65288;&#21069;&#21521;&#65289;&#24378;&#21270;&#23398;&#20064;&#25110;&#35268;&#21010;&#26469;&#35745;&#31639;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#19982;&#19987;&#23478;&#28436;&#31034;&#21305;&#37197;&#12290;&#26102;&#38388;&#35270;&#37326;&#22312;&#30830;&#23450;&#22870;&#21169;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;IRL&#31639;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#27604;&#22320;&#38754;&#23454;&#38469;&#20540;&#26356;&#30701;&#30340;&#26377;&#25928;&#26102;&#38388;&#35270;&#37326;&#36890;&#24120;&#33021;&#26356;&#24555;&#22320;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;&#27492;&#29616;&#35937;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#24182;&#32473;&#20986;&#20102;&#35299;&#37322;&#65306;&#26102;&#38388;&#35270;&#37326;&#25511;&#21046;&#20102;&#24341;&#21457;&#31574;&#30053;&#31867;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#36825;&#19968;&#20998;&#26512;&#20026;IRL&#30340;&#26377;&#25928;&#35270;&#37326;&#36873;&#25321;&#25552;&#20379;&#20102;&#21407;&#21017;&#24615;&#25351;&#23548;&#12290;&#23427;&#20063;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#32463;&#20856;&#30340;IRL&#20844;&#24335;&#65306;&#19982;&#20165;&#20855;&#26377;&#32473;&#23450;&#35270;&#37326;&#30340;&#22870;&#21169;&#30456;&#27604;&#65292;&#20849;&#21516;&#23398;&#20064;&#22870;&#21169;&#21644;&#26377;&#25928;&#35270;&#37326;&#26356;&#21152;&#33258;&#28982;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning over a given time horizon to compute an approximately optimal policy for a hypothesized reward function and then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimate and the computational efficiency of IRL algorithms. Interestingly, an effective time horizon shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis leads to a principled choice of the effective horizon for IRL. It also prompts us to reexamine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon together rather than the reward alone with a given horizon. Our experimental re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.14522</link><description>&lt;p&gt;
&#38750;&#20984;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230;&#27861;&#21450;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#38750;&#20984;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;Lipschitz&#24179;&#28369;&#24615;, &#20294;&#36825;&#19968;&#35201;&#27714;&#23545;&#20110;&#21253;&#25324;&#20108;&#27425;&#36870;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38382;&#39064;&#31867;&#21035;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#26063;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230; (SBPG) &#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;&#24179;&#28369;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09946</link><description>&lt;p&gt;
DeepMSS&#65306;&#22522;&#20110;PET/CT&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#39044;&#27979;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39044;&#27979;&#26159;&#30284;&#30151;&#31649;&#29702;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#25191;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25351;&#23548;&#27169;&#22411;&#25552;&#21462;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#25506;&#32034;&#32959;&#30244;&#22806;&#39044;&#21518;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23616;&#37096;&#28107;&#24052;&#32467;&#36716;&#31227;&#21644;&#37051;&#36817;&#32452;&#32455;&#20405;&#34989;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#26041;&#38754;&#27424;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepMSS&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#12290;&#23545;&#20110;&#29983;&#23384;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;MMPAN&#30340;&#29305;&#24449;&#34920;&#31034;&#24182;&#25191;&#34892;&#29983;&#23384;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DeepMSS&#27169;&#22411;&#22312;&#29983;&#23384;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03136</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#30340;&#24191;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03136
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#20989;&#25968;&#23558;&#29983;&#29289;&#24207;&#21015;&#30340;&#22823;&#32452;&#21512;&#31354;&#38388;&#26144;&#23556;&#21040;&#25152;&#20851;&#27880;&#30340;&#29305;&#24615;&#19978;&#12290;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22810;&#27169;&#24577;&#20989;&#25968;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#25928;&#19988;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#31232;&#30095;&#30340;&#28508;&#22312;&#20989;&#25968;&#36890;&#36807;&#21333;&#35843;&#38750;&#32447;&#24615;&#21464;&#25442;&#20197;&#21457;&#23556;&#21487;&#27979;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914; Bradley-Terry &#25439;&#22833;&#65289;&#26159;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#25152;&#38544;&#31034;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#30340;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#36866;&#24212;&#24615;-&#19978;&#20301;&#32852;&#31995;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20105;&#36777;&#65292;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#21487;&#20197;&#20135;&#29983;&#19981;&#20855;&#22791;&#31232;&#30095;&#34920;&#31034;&#30340;&#35266;&#23519;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#36866;&#21512;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#65289;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#25439;&#22833;&#21487;&#29992;&#20110;&#25512;&#26029;&#19981;&#36866;&#21512; MSE &#25439;&#22833;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13431</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Implicit Counterfactual Data Augmentation for Deep Neural Networks. (arXiv:2304.13431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26131;&#20110;&#25429;&#25417;&#38750;&#22240;&#26524;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26159;&#30772;&#38500;&#36825;&#20123;&#34394;&#20551;&#30340;&#32852;&#24819;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26126;&#30830;&#29983;&#25104;&#21453;&#20107;&#23454;&#25968;&#25454;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35757;&#32451;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;Implicit Counterfactual Data Augmentation&#65292;ICDA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#22312;&#35821;&#20041;&#21644;&#21453;&#20107;&#23454;&#24847;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#22686;&#24378;&#24378;&#24230;&#12290;&#20854;&#27425;&#65292;&#24403;&#22686;&#24191;&#26679;&#26412;&#25968;&#21464;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#20110;&#22686;&#24191;&#29305;&#24449;&#38598;&#30340;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#12290;&#31532;&#19977;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#20307;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#30452;&#25509;&#37327;&#21270;&#21644;&#20803;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#40065;&#26834;&#24615;&#25439;&#22833;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#20174;&#23454;&#39564;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;ICDA&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.05294</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#22797;&#26434;&#21644;&#26080;&#20449;&#21495;&#30340;&#20132;&#21449;&#21475;&#20013;&#23398;&#20064;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;
&lt;/p&gt;
&lt;p&gt;
Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#21475;&#26159;&#29616;&#20195;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20107;&#25925;&#25110;&#32570;&#20047;&#20132;&#36890;&#21327;&#35843;&#26426;&#21046;&#65288;&#22914;&#20132;&#36890;&#20449;&#21495;&#28783;&#65289;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#25104;&#20026;&#20132;&#36890;&#27969;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#36229;&#36234;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#20132;&#21449;&#21475;&#20132;&#36890;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#21487;&#39044;&#35265;&#30340;&#21253;&#21547;&#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HVs&#65289;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#65288;RVs&#65289;&#30340;&#28151;&#21512;&#20132;&#36890;&#24050;&#32463;&#20986;&#29616;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#28151;&#21512;&#20132;&#36890;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#36807;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#20132;&#36890;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;5%&#30340;RVs&#65292;&#25105;&#20204;&#21487;&#20197;&#38450;&#27490;&#22797;&#26434;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
&lt;/p&gt;</description></item></channel></rss>