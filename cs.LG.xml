<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01204</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#24635;&#32467;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#26041;&#27861;&#20998;&#20026;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;SSL&#36890;&#36807;&#23450;&#20041;&#22522;&#20110;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#21270;&#21644;&#40065;&#26834;&#30340;&#34920;&#31034;&#12290;&#26368;&#36817;&#65292;SSL&#24050;&#25104;&#20026;&#25506;&#32034;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#20013;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#30340;&#26032;&#36235;&#21183;&#65292;&#36825;&#26159;&#19968;&#39033;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#26126;&#30830;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#25551;&#36848;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#22238;&#39038;&#21644;&#24635;&#32467;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#38750;&#36830;&#32493;&#34920;&#26684;&#25968;&#25454;&#65288;SSL4NS-TD&#65289;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;NS-TD&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#19982;&#30456;&#20851;&#30740;&#31350;&#30340;&#20851;&#32852;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#26041;&#27861;&#34987;&#20998;&#20026;&#19977;&#32452;&#8212;&#8212;&#39044;&#27979;&#24615;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#28151;&#21512;&#23398;&#20064;&#65292;&#24182;&#20171;&#32461;&#20102;&#27599;&#20010;&#26041;&#21521;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#21160;&#26426;&#21644;&#20248;&#28857;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#20171;&#32461;&#20102;SSL4NS-TD&#30340;&#24212;&#29992;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#25968;&#25454;&#24037;&#31243;&#12289;&#36328;&#34920;&#26684;&#26597;&#35810;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been incorporated into many state-of-the-art models in various domains, where SSL defines pretext tasks based on unlabeled datasets to learn contextualized and robust representations. Recently, SSL has been a new trend in exploring the representation learning capability in the realm of tabular data, which is more challenging due to not having explicit relations for learning descriptive representations. This survey aims to systematically review and summarize the recent progress and challenges of SSL for non-sequential tabular data (SSL4NS-TD). We first present a formal definition of NS-TD and clarify its correlation to related studies. Then, these approaches are categorized into three groups -- predictive learning, contrastive learning, and hybrid learning, with their motivations and strengths of representative methods within each direction. On top of this, application issues of SSL4NS-TD are presented, including automatic data engineering, cross-table
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2404.02476</link><description>&lt;p&gt;
&#29992;&#20110;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Traveling Purchaser Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#20013;&#30340;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26053;&#34892;&#36141;&#20080;&#32773;&#38382;&#39064;&#65288;TPP&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#21035;&#35299;&#20915;&#20102;&#36335;&#30001;&#26500;&#24314;&#21644;&#36141;&#20080;&#35268;&#21010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20174;&#20840;&#23616;&#35282;&#24230;&#35780;&#20272;&#21644;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#29992;&#20110;&#25429;&#25417;&#24066;&#22330;-&#20135;&#21697;&#20851;&#31995;&#30340;TPP&#30340;&#20108;&#37096;&#22270;&#34920;&#31034;&#65292;&#20197;&#21450;&#20174;&#20108;&#37096;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#29992;&#20110;&#39034;&#24207;&#26500;&#24314;&#36335;&#30001;&#30340;&#31574;&#30053;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02476v1 Announce Type: cross  Abstract: The traveling purchaser problem (TPP) is an important combinatorial optimization problem with broad applications. Due to the coupling between routing and purchasing, existing works on TPPs commonly address route construction and purchase planning simultaneously, which, however, leads to exact methods with high computational cost and heuristics with sophisticated design but limited performance. In sharp contrast, we propose a novel approach based on deep reinforcement learning (DRL), which addresses route construction and purchase planning separately, while evaluating and optimizing the solution from a global perspective. The key components of our approach include a bipartite graph representation for TPPs to capture the market-product relations, and a policy network that extracts information from the bipartite graph and uses it to sequentially construct the route. One significant benefit of our framework is that we can efficiently const
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.19852</link><description>&lt;p&gt;
&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Graph Neural Networks in Epidemic Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19852
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#34892;&#30149;&#24314;&#27169;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#24037;&#20855;&#22791;&#21463;&#20851;&#27880;&#65292;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;GNN&#22312;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#20197;&#26469;&#65292;&#20154;&#20204;&#23545;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20256;&#32479;&#30340;&#26426;&#26800;&#27169;&#22411;&#25968;&#23398;&#25551;&#36848;&#20102;&#20256;&#26579;&#30149;&#30340;&#20256;&#25773;&#26426;&#21046;&#65292;&#20294;&#22312;&#38754;&#23545;&#24403;&#20170;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#26102;&#24448;&#24448;&#21147;&#19981;&#20174;&#24515;&#12290;&#22240;&#27492;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#27969;&#34892;&#30149;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#35797;&#22270;&#20840;&#38754;&#22238;&#39038;GNN&#22312;&#27969;&#34892;&#30149;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20026;&#27969;&#34892;&#30149;&#20219;&#21153;&#21644;&#26041;&#27861;&#35770;&#21508;&#24341;&#20837;&#20102;&#20998;&#23618;&#20998;&#31867;&#27861;&#65292;&#20026;&#35813;&#39046;&#22495;&#20869;&#30340;&#21457;&#23637;&#36712;&#36857;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#12290;&#23545;&#20110;&#27969;&#34892;&#30149;&#20219;&#21153;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#27969;&#34892;&#30149;&#39046;&#22495;&#36890;&#24120;&#24212;&#29992;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#23545;&#20110;&#26041;&#27861;&#35770;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30740;&#31350;&#20998;&#20026;&#8220;&#31070;&#32463;&#27169;&#22411;&#8221;&#21644;&#8220;&#28151;&#21512;&#27169;&#22411;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19852v1 Announce Type: new  Abstract: Since the onset of the COVID-19 pandemic, there has been a growing interest in studying epidemiological models. Traditional mechanistic models mathematically describe the transmission mechanisms of infectious diseases. However, they often fall short when confronted with the growing challenges of today. Consequently, Graph Neural Networks (GNNs) have emerged as a progressively popular tool in epidemic research. In this paper, we endeavor to furnish a comprehensive review of GNNs in epidemic tasks and highlight potential future directions. To accomplish this objective, we introduce hierarchical taxonomies for both epidemic tasks and methodologies, offering a trajectory of development within this domain. For epidemic tasks, we establish a taxonomy akin to those typically employed within the epidemic domain. For methodology, we categorize existing work into \textit{Neural Models} and \textit{Hybrid Models}. Following this, we perform an exha
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19578</link><description>&lt;p&gt;
&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#23454;&#29616;&#19978;&#19979;&#25991;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19578
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#26694;&#26550;&#65292;&#30740;&#31350;&#23637;&#31034;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#21487;&#23454;&#29616;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#25104;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#65292;&#23601;&#21487;&#20197;&#25191;&#34892;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#20869;&#35270;&#35273;&#27169;&#20223;&#23398;&#20064;&#65292;&#23558;&#35270;&#35273;&#35266;&#27979;&#26144;&#23556;&#20026;&#27169;&#25311;&#31034;&#33539;&#32773;&#34892;&#20026;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35270;&#35273;&#35266;&#27979;&#65288;&#36755;&#20837;&#65289;&#21644;&#21160;&#20316;&#36712;&#36857;&#65288;&#36755;&#20986;&#65289;&#36716;&#25442;&#20026;&#19968;&#31995;&#21015;&#20196;&#29260;&#65292;&#36825;&#20123;&#20196;&#29260;&#21487;&#20197;&#34987;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#65288;GPT-4 Turbo&#65289;&#25509;&#25910;&#21644;&#29983;&#25104;&#65292;&#36890;&#36807;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#38190;&#21160;&#20316;&#20196;&#29260;&#65288;KAT&#65289;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#23613;&#31649;&#20165;&#22312;&#35821;&#35328;&#19978;&#35757;&#32451;&#65292;&#25105;&#20204;&#23637;&#31034;&#36825;&#20123;&#21464;&#24418;&#22120;&#25797;&#38271;&#23558;&#26631;&#35760;&#21270;&#30340;&#35270;&#35273;&#20851;&#38190;&#28857;&#35266;&#23519;&#32763;&#35793;&#20026;&#34892;&#20026;&#36712;&#36857;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#22871;&#20214;&#20013;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;&#25193;&#25955;&#31574;&#30053;&#65289;&#12290;KAT&#19981;&#21516;&#20110;&#36890;&#24120;&#22312;&#35821;&#35328;&#39046;&#22495;&#25805;&#20316;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;&#21464;&#24418;&#22120;&#22312;&#35270;&#35273;&#21644;&#21160;&#20316;&#39046;&#22495;&#20013;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19578v1 Announce Type: cross  Abstract: We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn ge
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2403.19159</link><description>&lt;p&gt;
&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#23558;&#38271;&#24230;&#19982;&#36136;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling Length from Quality in Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19159
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#20013;&#30340;&#38271;&#24230;&#38382;&#39064;&#23637;&#24320;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;DPO&#20013;&#26174;&#33879;&#30340;&#21033;&#29992;&#24773;&#20917;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF)&#26159;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;RLHF&#34987;&#35748;&#20026;&#21033;&#29992;&#20102;&#20154;&#31867;&#20559;&#22909;&#20013;&#30340;&#20559;&#35265;&#65292;&#27604;&#22914;&#20887;&#38271;&#24615;&#12290;&#31934;&#24515;&#26684;&#24335;&#21270;&#21644;&#38596;&#36777;&#30340;&#31572;&#26696;&#36890;&#24120;&#20250;&#34987;&#29992;&#25143;&#26356;&#39640;&#35780;&#20215;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#24110;&#21161;&#24615;&#21644;&#23458;&#35266;&#24615;&#19978;&#36739;&#20302;&#12290;&#19968;&#20123;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#26469;&#25511;&#21046;&#36825;&#20123;&#20559;&#35265;&#65292;&#22312;&#21476;&#20856;RLHF&#25991;&#29486;&#20013;&#36825;&#20010;&#38382;&#39064;&#24050;&#26377;&#25152;&#25506;&#35752;&#65292;&#20294;&#23545;&#20110;&#30452;&#25509;&#23545;&#40784;&#31639;&#27861;&#22914;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36825;&#20010;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#19982;&#21476;&#20856;RLHF&#19981;&#21516;&#65292;DPO&#19981;&#35757;&#32451;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#25110;&#30452;&#25509;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#22240;&#27492;&#20043;&#21069;&#29992;&#26469;&#25511;&#21046;&#20887;&#38271;&#24615;&#30340;&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20570;&#20986;&#20102;&#20960;&#28857;&#36129;&#29486;&#12290;&#39318;&#27425;&#22312;DPO&#29615;&#22659;&#20013;&#30740;&#31350;&#38271;&#24230;&#38382;&#39064;&#65292;&#26174;&#31034;DPO&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#21033;&#29992;&#65292;&#24182;&#23558;&#20854;&#19982;&#20998;&#24067;&#22806;&#24341;&#23548;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19159v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstra
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;</title><link>https://arxiv.org/abs/2403.18103</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#20687;&#21644;&#35270;&#35273;&#25193;&#25955;&#27169;&#22411;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on Diffusion Models for Imaging and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#35752;&#35770;&#20102;&#22270;&#20687;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#65292;&#36866;&#21512;&#23545;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#25110;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#29983;&#25104;&#24037;&#20855;&#30340;&#24778;&#20154;&#22686;&#38271;&#20351;&#24471;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#31561;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#20123;&#29983;&#25104;&#24037;&#20855;&#32972;&#21518;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#25193;&#25955;&#27010;&#24565;&#65292;&#19968;&#31181;&#29305;&#27530;&#30340;&#37319;&#26679;&#26426;&#21046;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#34987;&#35748;&#20026;&#22256;&#38590;&#30340;&#19968;&#20123;&#32570;&#28857;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#26159;&#35752;&#35770;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#24565;&#12290;&#26412;&#25945;&#31243;&#30340;&#30446;&#26631;&#21463;&#20247;&#21253;&#25324;&#23545;&#30740;&#31350;&#25193;&#25955;&#27169;&#22411;&#25110;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35299;&#20915;&#20854;&#20182;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#26412;&#31185;&#29983;&#21644;&#30740;&#31350;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18103v1 Announce Type: new  Abstract: The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.14392</link><description>&lt;p&gt;
&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
A Bag of Tricks for Few-Shot Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#23558;&#20843;&#31181;&#20851;&#38190;&#25216;&#26415;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25913;&#36827;&#20102;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19968;&#25597;&#23376;&#25216;&#24039;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#23398;&#20064;&#24418;&#24335;&#65292;&#28041;&#21450;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#36830;&#32493;&#36866;&#24212;&#65292;&#24182;&#19988;&#26679;&#26412;&#26377;&#38480;&#12290; FSCIL &#38656;&#35201;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21363;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19968;&#25597;&#23376;&#25216;&#24039;&#23558;&#20843;&#31181;&#20851;&#38190;&#19988;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#25216;&#26415;&#27719;&#38598;&#22312;&#19968;&#36215;&#65292;&#38024;&#23545; FSCIL &#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#19979;&#25913;&#36827;&#31283;&#23450;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25216;&#24039;&#32452;&#32455;&#25104;&#19977;&#31867;&#65306;&#31283;&#23450;&#24615;&#25216;&#24039;&#12289;&#36866;&#24212;&#24615;&#25216;&#24039;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#31283;&#23450;&#24615;&#25216;&#24039;&#26088;&#22312;&#36890;&#36807;&#22686;&#24378;&#24050;&#23398;&#20064;&#31867;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#20998;&#31163;&#21644;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#26102;&#26368;&#23567;&#21270;&#24178;&#25200;&#26469;&#20943;&#36731;&#20808;&#21069;&#23398;&#20064;&#31867;&#21035;&#30340;&#36951;&#24536;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36866;&#24212;&#24615;&#25216;&#24039;&#20391;&#37325;&#20110;&#26377;&#25928;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14392v1 Announce Type: cross  Abstract: We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which is a challenging form of continual learning that involves continuous adaptation to new tasks with limited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiency in previously learned tasks while learning new ones. Our proposed bag of tricks brings together eight key and highly influential techniques that improve stability, adaptability, and overall performance under a unified framework for FSCIL. We organize these tricks into three categories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim to mitigate the forgetting of previously learned classes by enhancing the separation between the embeddings of learned classes and minimizing interference when learning new ones. On the other hand, adaptability tricks focus on the effective learning of new classes. Finally, training tricks improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#31354;&#38388;&#21040;&#31616;&#21270;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#31354;&#38388;&#30340;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;AC-State&#26041;&#27861;&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11940</link><description>&lt;p&gt;
&#22810;&#27493;&#21453;&#21521;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Multistep Inverse Is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#31354;&#38388;&#21040;&#31616;&#21270;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#31354;&#38388;&#30340;&#32534;&#30721;&#22120;&#23398;&#20064;&#65292;AC-State&#26041;&#27861;&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25511;&#21046;&#35774;&#32622;&#20013;&#65292;&#35266;&#27979;&#31354;&#38388;&#36890;&#24120;&#26159;&#19981;&#24517;&#35201;&#30340;&#39640;&#32500;&#19988;&#21463;&#21040;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#30340;&#21487;&#25511;&#21160;&#24577;&#36890;&#24120;&#36828;&#27604;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#30340;&#21160;&#24577;&#31616;&#21333;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#23558;&#35266;&#27979;&#31354;&#38388;&#26144;&#23556;&#21040;&#19968;&#20010;&#21253;&#21547;&#25511;&#21046;&#30456;&#20851;&#21464;&#37327;&#30340;&#31616;&#21270;&#31354;&#38388;&#26159;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#30001;Efroni&#31561;&#20154;&#65288;2022&#24180;&#65289;&#39318;&#27425;&#25552;&#20986;&#30340;Ex-BMDP&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#35266;&#27979;&#21487;&#20197;&#20998;&#35299;&#20026;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#28508;&#22312;&#29366;&#24577;&#21644;&#29420;&#31435;&#20110;&#21160;&#20316;&#30340;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#65292;&#24182;&#24418;&#24335;&#21270;&#20102;&#33021;&#22815;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#22330;&#26223;&#12290;Lamb&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#20102;&#8220;AC-State&#8221;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#20174;&#36825;&#20123;&#38382;&#39064;&#20013;&#30340;&#35266;&#27979;&#20013;&#25552;&#21462;&#21253;&#21547;&#23436;&#25972;&#20381;&#36182;&#20110;&#21160;&#20316;&#30340;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#12290;AC-State&#26159;&#19968;&#20010;&#22810;&#27493;&#21453;&#21521;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#36335;&#24452;&#20013;&#31532;&#19968;&#20010;&#21644;&#26368;&#21518;&#19968;&#20010;&#29366;&#24577;&#30340;&#32534;&#30721;&#26469;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11940v1 Announce Type: new  Abstract: In real-world control settings, the observation space is often unnecessarily high-dimensional and subject to time-correlated noise. However, the controllable dynamics of the system are often far simpler than the dynamics of the raw observations. It is therefore desirable to learn an encoder to map the observation space to a simpler space of control-relevant variables. In this work, we consider the Ex-BMDP model, first proposed by Efroni et al. (2022), which formalizes control problems where observations can be factorized into an action-dependent latent state which evolves deterministically, and action-independent time-correlated noise. Lamb et al. (2022) proposes the "AC-State" method for learning an encoder to extract a complete action-dependent latent state representation from the observations in such problems. AC-State is a multistep-inverse method, in that it uses the encoding of the the first and last state in a path to predict the 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#29702;&#25968;&#25454;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;OmniJet-$\alpha$&#26159;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05618</link><description>&lt;p&gt;
OmniJet-$\alpha$: &#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OmniJet-$\alpha$: The first cross-task foundation model for particle physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05618
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#25968;&#25454;&#21644;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;OmniJet-$\alpha$&#26159;&#39318;&#20010;&#36328;&#20219;&#21153;&#22522;&#30784;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#24182;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#19978;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#22810;&#25968;&#25454;&#38598;&#21644;&#22810;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#19968;&#32463;&#39044;&#35757;&#32451;&#65292;&#20415;&#21487;&#34987;&#24494;&#35843;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#25104;&#21151;&#24320;&#21457;&#20986;&#36825;&#31181;&#36890;&#29992;&#29289;&#29702;&#25968;&#25454;&#27169;&#22411;&#23558;&#26159;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#21487;&#23454;&#29616;&#30340;&#29289;&#29702;&#24615;&#33021;&#65292;&#21516;&#26102;&#22823;&#24133;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#25361;&#25112;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#26469;&#35780;&#21028;&#20174;&#29289;&#29702;&#25968;&#25454;&#36716;&#25442;&#20026;&#36866;&#21512;&#21464;&#21387;&#22120;&#26550;&#26500;&#65288;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#39592;&#24178;&#65289;&#36827;&#34892;&#33258;&#22238;&#24402;&#29983;&#25104;&#31890;&#23376;&#21943;&#27969;&#30340;&#34920;&#31034;&#36136;&#37327;&#12290;&#36825;&#20123;&#25514;&#26045;&#25903;&#25345;&#20102;&#30456;&#36739;&#20110;&#20808;&#21069;&#24037;&#20316;&#30340;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#26631;&#35760;&#21270;&#30340;&#36873;&#25321;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#38382;&#39064;&#65288;&#21943;&#27969;&#29983;&#25104;&#65289;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05618v1 Announce Type: cross  Abstract: Foundation models are multi-dataset and multi-task machine learning methods that once pre-trained can be fine-tuned for a large variety of downstream applications. The successful development of such general-purpose models for physics data would be a major breakthrough as they could improve the achievable physics performance while at the same time drastically reduce the required amount of training time and data.   We report significant progress on this challenge on several fronts. First, a comprehensive set of evaluation methods is introduced to judge the quality of an encoding from physics data into a representation suitable for the autoregressive generation of particle jets with transformer architectures (the common backbone of foundation models). These measures motivate the choice of a higher-fidelity tokenization compared to previous works. Finally, we demonstrate transfer learning between an unsupervised problem (jet generation) an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;</title><link>https://arxiv.org/abs/2403.04650</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Context-Based Multimodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04650
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20294;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#23616;&#38480;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27169;&#22411;&#31216;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65288;CBMF&#65289;&#65292;&#32467;&#21512;&#20102;&#27169;&#24577;&#34701;&#21512;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#65292;&#36890;&#36807;&#29305;&#23450;&#19978;&#19979;&#25991;&#21521;&#37327;&#34920;&#31034;&#27599;&#20010;&#27169;&#24577;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#36827;&#34892;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04650v1 Announce Type: cross  Abstract: The fusion models, which effectively combine information from different sources, are widely used in solving multimodal tasks. However, they have significant limitations related to aligning data distributions across different modalities. This challenge can lead to inconsistencies and difficulties in learning robust representations. Alignment models, while specifically addressing this issue, often require training "from scratch" with large datasets to achieve optimal results, which can be costly in terms of resources and time. To overcome these limitations, we propose an innovative model called Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and data distribution alignment. In CBMF, each modality is represented by a specific context vector, fused with the embedding of each modality. This enables the use of large pre-trained models that can be frozen, reducing the computational and training data requirements. A
&lt;/p&gt;</description></item><item><title>DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;</title><link>https://arxiv.org/abs/2403.00321</link><description>&lt;p&gt;
DEEP-IoT: &#19979;&#34892;&#22686;&#24378;&#22411;&#39640;&#25928;&#33021;&#29289;&#32852;&#32593;
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT: Downlink-Enhanced Efficient-Power Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00321
&lt;/p&gt;
&lt;p&gt;
DEEP-IoT&#36890;&#36807;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#29289;&#32852;&#32593;&#36890;&#20449;&#27169;&#22411;&#65292;&#22823;&#24133;&#38477;&#20302;&#33021;&#32791;&#24182;&#25552;&#39640;&#35774;&#22791;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DEEP-IoT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#38761;&#21629;&#24847;&#20041;&#30340;&#36890;&#20449;&#33539;&#20363;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#29289;&#32852;&#32593;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#26041;&#24335;&#12290;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;&#8220;&#26356;&#22810;&#30417;&#21548;&#65292;&#26356;&#23569;&#20256;&#36755;&#8221;&#30340;&#31574;&#30053;&#65292;DEEP-IoT&#25361;&#25112;&#21644;&#36716;&#21464;&#20102;&#20256;&#32479;&#30340;&#21457;&#36865;&#26041;&#65288;&#29289;&#32852;&#32593;&#35774;&#22791;&#65289;&#20026;&#20013;&#24515;&#30340;&#36890;&#20449;&#27169;&#22411;&#65292;&#23558;&#25509;&#25910;&#26041;&#65288;&#25509;&#20837;&#28857;&#65289;&#20316;&#20026;&#20851;&#38190;&#35282;&#33394;&#65292;&#20174;&#32780;&#38477;&#20302;&#33021;&#32791;&#24182;&#24310;&#38271;&#35774;&#22791;&#23551;&#21629;&#12290;&#25105;&#20204;&#19981;&#20165;&#27010;&#24565;&#21270;&#20102;DEEP-IoT&#65292;&#36824;&#36890;&#36807;&#22312;&#31364;&#24102;&#31995;&#32479;&#20013;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#30340;&#21453;&#39304;&#20449;&#36947;&#32534;&#30721;&#26469;&#23454;&#29616;&#23427;&#12290;&#27169;&#25311;&#32467;&#26524;&#26174;&#31034;&#65292;IoT&#21333;&#20803;&#30340;&#36816;&#34892;&#23551;&#21629;&#26174;&#33879;&#25552;&#39640;&#65292;&#27604;&#20351;&#29992;Turbo&#21644;Polar&#32534;&#30721;&#30340;&#20256;&#32479;&#31995;&#32479;&#25552;&#39640;&#20102;&#26368;&#22810;52.71%&#12290;&#36825;&#19968;&#36827;&#23637;&#26631;&#24535;&#30528;&#19968;&#31181;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00321v1 Announce Type: cross  Abstract: At the heart of the Internet of Things (IoT) -- a domain witnessing explosive growth -- the imperative for energy efficiency and the extension of device lifespans has never been more pressing. This paper presents DEEP-IoT, a revolutionary communication paradigm poised to redefine how IoT devices communicate. Through a pioneering "listen more, transmit less" strategy, DEEP-IoT challenges and transforms the traditional transmitter (IoT devices)-centric communication model to one where the receiver (the access point) play a pivotal role, thereby cutting down energy use and boosting device longevity. We not only conceptualize DEEP-IoT but also actualize it by integrating deep learning-enhanced feedback channel codes within a narrow-band system. Simulation results show a significant enhancement in the operational lifespan of IoT cells -- surpassing traditional systems using Turbo and Polar codes by up to 52.71%. This leap signifies a paradi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.00033</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#27880;&#24847;&#21147;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#22823;&#40635;&#20351;&#29992;&#32773;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00033
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#21644;LSTM&#25216;&#26415;&#65292;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#25552;&#20986;&#20102;HOGAB&#27169;&#22411;&#65292;&#23545;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#38745;&#24687;&#24577;fMRI&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#22810;&#22270;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#30340;&#25345;&#32493;&#20351;&#29992;&#26126;&#26174;&#24433;&#21709;&#20154;&#20204;&#30340;&#29983;&#27963;&#21644;&#20581;&#24247;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;HOGAB&#65288;High-Order Attention Graph Attention&#31070;&#32463;&#32593;&#32476;&#65289;&#27169;&#22411;&#65292;&#20197;&#20998;&#26512;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#24930;&#24615;&#22823;&#40635;&#29992;&#25143;&#30340;&#23616;&#37096;&#24322;&#24120;&#33041;&#27963;&#21160;&#12290;HOGAB&#23558;&#21160;&#24577;&#20869;&#22312;&#21151;&#33021;&#32593;&#32476;&#19982;LSTM&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25429;&#25417;&#22823;&#40635;&#29992;&#25143;fMRI&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#38454;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#23545;&#37051;&#22495;&#33410;&#28857;&#36827;&#34892;&#20449;&#24687;&#34701;&#21512;&#21644;&#28040;&#24687;&#20256;&#36882;&#65292;&#22686;&#24378;&#38271;&#26399;&#22823;&#40635;&#29992;&#25143;&#30340;&#31038;&#21306;&#32858;&#31867;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;&#22810;&#22270;&#20998;&#31867;&#20013;&#23454;&#29616;&#20102;85.1%&#30340;AUC&#21644;80.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#32447;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;HODAB&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.15374</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Outlier detection by ensembling uncertainty with negative objectness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15374
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#36127;&#23545;&#35937;&#24615;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#24182;&#22312;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#23884;&#20837;&#65292;&#21487;&#29420;&#31435;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#30417;&#30563;&#24335;&#35270;&#35273;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#21151;&#33021;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#40723;&#21169;&#26631;&#20934;&#23553;&#38381;&#38598;&#27169;&#22411;&#22312;&#36127;&#35757;&#32451;&#25968;&#25454;&#20013;&#20135;&#29983;&#20302;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#28151;&#28102;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#36127;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#30452;&#25509;&#39044;&#27979;K+1&#20010;logits&#65292;&#36825;&#20123;logits&#23545;&#24212;&#20110;K&#20010;&#22522;&#26412;&#30495;&#23454;&#31867;&#21035;&#21644;&#19968;&#20010;&#24322;&#24120;&#31867;&#21035;&#12290;&#36825;&#31181;&#35774;&#32622;&#20801;&#35768;&#25105;&#20204;&#21046;&#23450;&#19968;&#31181;&#26032;&#22855;&#30340;&#24322;&#24120;&#24471;&#20998;&#65292;&#20316;&#20026;&#20998;&#24067;&#20869;&#19981;&#30830;&#23450;&#24615;&#21644;&#24322;&#24120;&#31867;&#21035;&#30340;&#21518;&#39564;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#36127;&#23545;&#35937;&#24615;&#12290;&#29616;&#22312;&#65292;&#24322;&#24120;&#20540;&#21487;&#20197;&#36890;&#36807;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#36127;&#25968;&#25454;&#30456;&#20284;&#20043;&#22788;&#29420;&#31435;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23884;&#20837;&#21040;&#19968;&#20010;&#23494;&#38598;&#39044;&#27979;&#32467;&#26500;&#20013;&#65292;&#35813;&#32467;&#26500;&#20855;&#26377;K+2&#20010;&#31867;&#21035;&#30340;&#25513;&#30721;&#32423;&#21035;&#35782;&#21035;&#12290;&#35757;&#32451;&#36807;&#31243;&#40723;&#21169;&#26032;&#39062;&#30340;K+2-th&#31867;&#21035;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15374v1 Announce Type: cross  Abstract: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn n
&lt;/p&gt;</description></item><item><title>MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10093</link><description>&lt;p&gt;
MIM-Refiner&#65306;&#19968;&#31181;&#20174;&#20013;&#38388;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#33719;&#24471;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10093
&lt;/p&gt;
&lt;p&gt;
MIM-Refiner&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;MIM&#27169;&#22411;&#20013;&#30340;&#20013;&#38388;&#23618;&#34920;&#31034;&#21644;&#22810;&#20010;&#23545;&#27604;&#22836;&#65292;&#33021;&#22815;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#65292;&#24182;&#22312;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MIM-Refiner&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;MIM&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#25552;&#21319;&#26041;&#27861;&#12290;MIM-Refiner&#30340;&#21160;&#26426;&#22312;&#20110;MIM&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#34920;&#31034;&#36890;&#24120;&#20301;&#20110;&#20013;&#38388;&#23618;&#12290;&#22240;&#27492;&#65292;MIM-Refiner&#21033;&#29992;&#36830;&#25509;&#21040;&#19981;&#21516;&#20013;&#38388;&#23618;&#30340;&#22810;&#20010;&#23545;&#27604;&#22836;&#12290;&#22312;&#27599;&#20010;&#22836;&#20013;&#65292;&#20462;&#25913;&#21518;&#30340;&#26368;&#36817;&#37051;&#30446;&#26631;&#24110;&#21161;&#26500;&#24314;&#30456;&#24212;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;&#27492;&#36807;&#31243;&#30701;&#32780;&#26377;&#25928;&#65292;&#22312;&#20960;&#20010;epochs&#20869;&#65292;&#25105;&#20204;&#23558;MIM&#27169;&#22411;&#30340;&#29305;&#24449;&#20174;&#27425;&#20248;&#30340;&#29366;&#24577;&#25552;&#21319;&#21040;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;&#20351;&#29992;data2vec 2.0&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;ViT-H&#32463;&#36807;&#25913;&#36827;&#21518;&#65292;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#20302;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65288;&#20998;&#21035;&#20026;84.7%&#21644;64.2%&#65289;&#65292;&#36229;&#36807;&#20102;&#22312;ImageNet-1K&#19978;&#39044;&#35757;&#32451;&#30340;&#20854;&#20182;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10093v1 Announce Type: cross  Abstract: We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. The motivation behind MIM-Refiner is rooted in the insight that optimal representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple contrastive heads that are connected to diverse intermediate layers. In each head, a modified nearest neighbor objective helps to construct respective semantic clusters.   The refinement process is short but effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, achieves new state-of-the-art results in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. In ImageNet-1K 1-shot classification, MIM-Refiner sets a new state-of-the-art of 64.2%, outperforming larger mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09470</link><description>&lt;p&gt;
&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Rolling Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28378;&#21160;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#38388;&#25968;&#25454;&#65292;&#36890;&#36807;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#24182;&#26681;&#25454;&#24103;&#22312;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20808;&#21518;&#20998;&#37197;&#19981;&#21516;&#30340;&#22122;&#22768;&#37327;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#35270;&#39057;&#39044;&#27979;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25193;&#25955;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26102;&#38388;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#12289;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#25110;&#27668;&#20505;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21518;&#32493;&#24103;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#37327;&#35270;&#20026;&#30456;&#31561;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28378;&#21160;&#25193;&#25955;&#65306;&#19968;&#31181;&#20351;&#29992;&#28369;&#21160;&#31383;&#21475;&#21435;&#22122;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#30830;&#20445;&#25193;&#25955;&#36807;&#31243;&#36880;&#28176;&#36890;&#36807;&#26102;&#38388;&#36827;&#34892;&#30772;&#22351;&#65292;&#36890;&#36807;&#23558;&#26356;&#22810;&#30340;&#22122;&#22768;&#20998;&#37197;&#32473;&#24207;&#21015;&#20013;&#20986;&#29616;&#36739;&#26202;&#30340;&#24103;&#65292;&#21453;&#26144;&#20986;&#38543;&#30528;&#29983;&#25104;&#36807;&#31243;&#30340;&#23637;&#24320;&#65292;&#23545;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#36234;&#26469;&#36234;&#22823;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#26102;&#38388;&#21160;&#24577;&#22797;&#26434;&#26102;&#65292;&#28378;&#21160;&#25193;&#25955;&#20248;&#20110;&#26631;&#20934;&#25193;&#25955;&#12290;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;Kinetics-600&#35270;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#35270;&#39057;&#39044;&#27979;&#20219;&#21153;&#21644;&#28151;&#27788;&#27969;&#20307;&#21160;&#21147;&#23398;&#39044;&#27979;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#36825;&#19968;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09470v1 Announce Type: new  Abstract: Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04856</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Explaining Learned Reward Functions with Counterfactual Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04856
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#30340;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#29983;&#25104;&#31526;&#21512;&#36136;&#37327;&#26631;&#20934;&#30340;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#33021;&#25552;&#39640;&#20854;&#39044;&#27979;&#19982;&#22870;&#21169;&#20989;&#25968;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#25110;&#21453;&#39304;&#20013;&#23398;&#20064;&#22870;&#21169;&#26159;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#22987;&#32456;&#25552;&#21462;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#21644;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#36712;&#36857;&#35299;&#37322;&#65288;CTEs&#65289;&#65292;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#36712;&#36857;&#21644;&#21453;&#20107;&#23454;&#37096;&#20998;&#36712;&#36857;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#25509;&#25910;&#30340;&#22870;&#21169;&#26469;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;CTEs&#21046;&#23450;&#20102;&#20845;&#20010;&#36136;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Monte-Carlo&#30340;&#26032;&#31639;&#27861;&#26469;&#29983;&#25104;&#20248;&#21270;&#36825;&#20123;&#36136;&#37327;&#26631;&#20934;&#30340;CTEs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20154;&#27169;&#22411;&#26469;&#34913;&#37327;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#20854;&#30340;&#20449;&#24687;&#24615;&#12290;CTEs&#23545;&#20110;&#20195;&#29702;&#20154;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#20449;&#24687;&#24615;&#65292;&#22686;&#21152;&#20102;&#20854;&#39044;&#27979;&#19982;&#26410;&#35265;&#36712;&#36857;&#19978;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23398;&#20250;&#20102;&#20934;&#30830;&#21028;&#26029;&#36712;&#36857;&#20043;&#38388;&#30340;&#22870;&#21169;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning rewards from human behaviour or feedback is a promising approach to aligning AI systems with human values but fails to consistently extract correct reward functions. Interpretability tools could enable users to understand and evaluate possible flaws in learned reward functions. We propose Counterfactual Trajectory Explanations (CTEs) to interpret reward functions in reinforcement learning by contrasting an original with a counterfactual partial trajectory and the rewards they each receive. We derive six quality criteria for CTEs and propose a novel Monte-Carlo-based algorithm for generating CTEs that optimises these quality criteria. Finally, we measure how informative the generated explanations are to a proxy-human model by training it on CTEs. CTEs are demonstrably informative for the proxy-human model, increasing the similarity between its predictions and the reward function on unseen trajectories. Further, it learns to accurately judge differences in rewards between trajec
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20316;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#65292;&#36827;&#34892;&#20102;&#21442;&#25968;&#25512;&#26029;&#30740;&#31350;&#65292;&#21487;&#20197;&#20943;&#23569;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2312.17336</link><description>&lt;p&gt;
Li-ion&#30005;&#27744;&#27169;&#22411;&#30340;PINN&#20195;&#29702;&#29992;&#20110;&#21442;&#25968;&#25512;&#26029;&#12290;&#31532;II&#37096;&#20998;&#65306;&#27491;&#21017;&#21270;&#21644;&#20266;&#20108;&#32500;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PINN surrogate of Li-ion battery models for parameter inference. Part II: Regularization and application of the pseudo-2D model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17336
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20316;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#65292;&#36827;&#34892;&#20102;&#21442;&#25968;&#25512;&#26029;&#30740;&#31350;&#65292;&#21487;&#20197;&#20943;&#23569;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bayesian&#21442;&#25968;&#25512;&#26029;&#23545;&#25913;&#36827;&#38146;&#31163;&#23376;&#30005;&#27744;&#35786;&#26029;&#26377;&#29992;&#65292;&#24182;&#26377;&#21161;&#20110;&#21046;&#23450;&#30005;&#27744;&#32769;&#21270;&#27169;&#22411;&#12290;&#20026;&#20102;&#38477;&#20302;Bayesian&#26657;&#20934;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#21487;&#20197;&#29992;&#26356;&#24555;&#30340;&#20195;&#29702;&#26367;&#25442;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#12290;&#23558;&#19968;&#20010;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24320;&#21457;&#20026;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#30005;&#27744;&#27169;&#22411;&#26657;&#20934;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17336v2 Announce Type: replace  Abstract: Bayesian parameter inference is useful to improve Li-ion battery diagnostics and can help formulate battery aging models. However, it is computationally intensive and cannot be easily repeated for multiple cycles, multiple operating conditions, or multiple replicate cells. To reduce the computational cost of Bayesian calibration, numerical solvers for physics-based models can be replaced with faster surrogates. A physics-informed neural network (PINN) is developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For the P2D surrogate, additional training regularization was needed as compared to the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and P2D surrogate models are exercised for parameter inference and compared to data obtained from a direct numerical solution of the governing equations. A parameter inference study highlights the ability to use these PINNs to calibrate scaling paramet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PINN&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#24110;&#21161;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2312.17329</link><description>&lt;p&gt;
&#22522;&#20110; PINN &#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#21442;&#25968;&#25512;&#26029;&#20195;&#29702;&#27169;&#22411;&#12290;&#31532;&#19968;&#37096;&#20998;&#65306;&#23454;&#29616;&#19982;&#22810;&#20445;&#30495;&#24230;&#31561;&#32423;&#32467;&#26500;&#29992;&#20110;&#21333;&#31890;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PINN surrogate of Li-ion battery models for parameter inference. Part I: Implementation and multi-fidelity hierarchies for the single-particle model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17329
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PINN&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65292;&#24110;&#21161;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#20110;&#24555;&#36895;&#20934;&#30830;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#21644;&#20248;&#21270;&#33021;&#37327;&#23384;&#20648;&#38656;&#27714;&#38656;&#35201;&#32771;&#34385;&#38146;&#31163;&#23376;&#30005;&#27744;&#32769;&#21270;&#21160;&#24577;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#25216;&#26415;&#20934;&#30830;&#24555;&#36895;&#22320;&#35786;&#26029;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#65288;&#22914;&#21333;&#31890;&#23376;&#27169;&#22411;&#65288;SPM&#65289;&#21644;&#20266;&#20108;&#32500;&#65288;P2D&#65289;&#27169;&#22411;&#65289;&#26367;&#20195;&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#20195;&#29702;&#26469;&#20943;&#23569;&#30830;&#23450;&#30005;&#27744;&#20869;&#37096;&#29366;&#24577;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19968;&#39033;&#20004;&#37096;&#20998;&#31995;&#21015;&#30340;&#31532;&#19968;&#37096;&#20998;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#21442;&#25968;&#25512;&#26029;&#65288;&#21363;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#65289;&#30340;&#38146;&#31163;&#23376;&#30005;&#27744;&#27169;&#22411;&#30340;PINN&#20195;&#29702;&#12290;&#22312;&#36825;&#31532;&#19968;&#37096;&#20998;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26500;&#24314;SPM&#30340;PINN&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22810;&#20445;&#30495;&#24230;&#20998;&#23618;&#35757;&#32451;&#65292;&#20854;&#20013;&#26377;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17329v2 Announce Type: replace  Abstract: To plan and optimize energy storage demands that account for Li-ion battery aging dynamics, techniques need to be developed to diagnose battery internal states accurately and rapidly. This study seeks to reduce the computational resources needed to determine a battery's internal states by replacing physics-based Li-ion battery models -- such as the single-particle model (SPM) and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN) surrogate. The surrogate model makes high-throughput techniques, such as Bayesian calibration, tractable to determine battery internal parameters from voltage responses. This manuscript is the first of a two-part series that introduces PINN surrogates of Li-ion battery models for parameter inference (i.e., state-of-health diagnostics). In this first part, a method is presented for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical training, where several neural ne
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.00817</link><description>&lt;p&gt;
&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#29992;&#20110;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00817
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TimelyGPT&#30340;&#21487;&#25512;&#24191;&#30340;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#21487;&#25512;&#24191;&#30340;&#20301;&#32622;&#23884;&#20837;&#21644;&#24490;&#29615;&#27880;&#24847;&#21147;&#20197;&#21450;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#26377;&#25928;&#22320;&#25429;&#25417;&#36229;&#38271;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#65292;&#22914;BERT&#21644;GPT&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;PTMs&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#21457;&#23637;&#28382;&#21518;&#12290;&#36825;&#20984;&#26174;&#20102;&#29616;&#26377;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21363;&#26102;&#29983;&#25104;&#39044;&#35757;&#32451;Transformer&#65288;TimelyGPT&#65289;&#12290;TimelyGPT&#37319;&#29992;&#21487;&#25512;&#24191;&#20301;&#32622;&#65288;xPos&#65289;&#23884;&#20837;&#23558;&#36235;&#21183;&#21644;&#21608;&#26399;&#27169;&#24335;&#32534;&#30721;&#21040;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20013;&#12290;&#23427;&#36824;&#38598;&#25104;&#20102;&#24490;&#29615;&#27880;&#24847;&#21147;&#21644;&#26102;&#38388;&#21367;&#31215;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TimelyGPT&#22312;&#24314;&#27169;&#36830;&#32493;&#30417;&#27979;&#30340;&#29983;&#29289;&#20449;&#21495;&#21644;&#32463;&#24120;&#20986;&#29616;&#22312;&#32437;&#21521;&#30005;&#30913;&#27874;&#39046;&#22495;&#20013;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00817v2 Announce Type: replace-cross  Abstract: Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electro
&lt;/p&gt;</description></item><item><title>LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2310.14894</link><description>&lt;p&gt;
&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;-- &#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#20855;&#26377;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Local Universal Explainer (LUX) -- a rule-based explainer with factual, counterfactual and visual explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14894
&lt;/p&gt;
&lt;p&gt;
LUX&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#30340;&#23616;&#37096;&#27010;&#24565;&#26469;&#24418;&#25104;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#36817;&#24180;&#26469;&#26368;&#34987;&#24191;&#27867;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20043;&#19968;&#12290;&#23427;&#20063;&#26159;&#26368;&#20998;&#25955;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#26377;&#22810;&#31181;&#26041;&#27861;&#19987;&#27880;&#20110;&#35299;&#37322;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#36825;&#20351;&#24471;&#19968;&#27425;&#24615;&#20197;&#32039;&#20945;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#33719;&#24471;&#23436;&#25972;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26412;&#22320;&#36890;&#29992;&#35299;&#37322;&#22120;&#65288;LUX&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#22120;&#65292;&#21487;&#20197;&#29983;&#25104;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#35270;&#35273;&#35299;&#37322;&#12290;&#23427;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#20801;&#35768;&#26012;&#20132;&#21644;&#38598;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;XAI&#26041;&#27861;&#65292;&#22914;SHAP&#25110;LIME&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#21453;&#65292;&#23427;&#19981;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36873;&#25321;&#20197;&#39640;&#23494;&#24230;&#31751;&#24418;&#24335;&#20986;&#29616;&#30340;&#30495;&#23454;&#25968;&#25454;&#30340;&#23616;&#37096;&#27010;&#24565;&#65292;&#36825;&#20123;&#23616;&#37096;&#27010;&#24565;&#23545;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#24418;&#25104;&#26368;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) is one of the most intensively developed area of AI in recent years. It is also one of the most fragmented with multiple methods that focus on different aspects of explanations. This makes difficult to obtain the full spectrum of explanation at once in a compact and consistent way. To address this issue, we present Local Universal Explainer (LUX), which is a rule-based explainer that can generate factual, counterfactual and visual explanations. It is based on a modified version of decision tree algorithms that allows for oblique splits and integration with feature importance XAI methods such as SHAP or LIME. It does not use data generation in opposite to other algorithms, but is focused on selecting local concepts in a form of high-density clusters of real data that have the highest impact on forming the decision boundary of the explained model. We tested our method on real and synthetic datasets and compared it with state-of-the-art rule-based
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2209.03275</link><description>&lt;p&gt;
&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Multimodal Speech Enhancement Using Burst Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.03275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#29190;&#21457;&#20256;&#25773;&#30340;&#22810;&#27169;&#24577;&#35821;&#38899;&#22686;&#24378;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#24182;&#25233;&#21046;&#22122;&#22768;&#65292;&#20174;&#32780;&#36171;&#20104;&#35821;&#38899;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MBURST&#30340;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#65292;&#24182;&#32771;&#34385;&#20102;&#26377;&#20851;&#21069;&#39069;&#21494;&#30382;&#23618;&#21644;&#20854;&#20182;&#33041;&#21306;&#37329;&#23383;&#22612;&#32454;&#32990;&#30340;&#26368;&#26032;&#31070;&#32463;&#23398;&#21457;&#29616;&#12290;&#25152;&#35859;&#30340;&#29190;&#21457;&#20256;&#25773;&#36890;&#36807;&#21453;&#39304;&#26041;&#24335;&#23454;&#29616;&#20102;&#20960;&#20010;&#20934;&#21017;&#65292;&#20197;&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#30340;&#26041;&#24335;&#35299;&#20915;&#20449;&#20219;&#20998;&#37197;&#38382;&#39064;&#65306;&#36890;&#36807;&#21453;&#39304;&#25511;&#21046;&#22609;&#24615;&#30340;&#31526;&#21495;&#21644;&#24133;&#24230;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26435;&#37325;&#36830;&#25509;&#22312;&#21508;&#23618;&#20043;&#38388;&#22810;&#36335;&#22797;&#29992;&#21453;&#39304;&#21644;&#21069;&#39304;&#20449;&#24687;&#65292;&#36817;&#20284;&#21453;&#39304;&#21644;&#21069;&#39304;&#36830;&#25509;&#65292;&#24182;&#32447;&#24615;&#21270;&#21453;&#39304;&#20449;&#21495;&#12290;MBURST&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#23398;&#20064;&#22122;&#22768;&#20449;&#21495;&#21644;&#35270;&#35273;&#21050;&#28608;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#36890;&#36807;&#25918;&#22823;&#30456;&#20851;&#20449;&#24687;&#21644;&#25233;&#21046;&#22122;&#22768;&#36171;&#20104;&#35821;&#38899;&#20197;&#21547;&#20041;&#12290;&#22312;Grid Corpus&#21644;&#22522;&#20110;CHiME3&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MBURST&#33021;&#22815;&#22797;&#29616;&#31867;&#20284;&#30340;&#25513;&#27169;&#37325;&#24314;&#65292;&#19982;&#22810;&#27169;&#24577;&#21453;&#21521;&#20256;&#25773;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the MBURST, a novel multimodal solution for audio-visual speech enhancements that consider the most recent neurological discoveries regarding pyramidal cells of the prefrontal cortex and other brain regions. The so-called burst propagation implements several criteria to address the credit assignment problem in a more biologically plausible manner: steering the sign and magnitude of plasticity through feedback, multiplexing the feedback and feedforward information across layers through different weight connections, approximating feedback and feedforward connections, and linearizing the feedback signals. MBURST benefits from such capabilities to learn correlations between the noisy signal and the visual stimuli, thus attributing meaning to the speech by amplifying relevant information and suppressing noise. Experiments conducted over a Grid Corpus and CHiME3-based dataset show that MBURST can reproduce similar mask reconstructions to the multimodal backpropagation-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2107.08020</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Graph Topology Learning from Matrix-valued Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2107.08020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;&#22270;&#25299;&#25169;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#20197;&#36866;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#38024;&#23545;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#36827;&#34892;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#36825;&#20123;&#25968;&#25454;&#26159;&#22312;&#19968;&#20010;&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#25910;&#38598;&#30340;&#65288;&#36890;&#24120;&#26159;&#19968;&#32452;&#31354;&#38388;&#20301;&#32622;&#65289;&#65292;&#35266;&#27979;&#21040;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#27599;&#20010;&#26102;&#38388;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#27599;&#20010;&#20256;&#24863;&#22120;&#30001;&#19968;&#20010;&#21521;&#37327;&#26102;&#24207;&#21015;&#26469;&#25551;&#36848;&#12290;&#25105;&#20204;&#24076;&#26395;&#35782;&#21035;&#36825;&#20123;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#24182;&#29992;&#22270;&#24418;&#26469;&#34920;&#31034;&#23427;&#12290;&#24403;&#27599;&#20010;&#20256;&#24863;&#22120;&#21482;&#26377;&#19968;&#20010;&#29305;&#24449;&#26102;&#65292;&#30690;&#37327;&#33258;&#22238;&#24402;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#26029;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#32467;&#26500;&#12290;&#25152;&#24471;&#21040;&#30340;&#22270;&#34987;&#31216;&#20026;&#22240;&#26524;&#22270;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23558;VAR&#27169;&#22411;&#25193;&#23637;&#20026;&#30697;&#38453;&#21464;&#37327;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#22270;&#24418;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36807;&#31243;&#65292;&#20998;&#21035;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#24773;&#20917;&#65292;&#22312;&#26032;&#26679;&#26412;&#21040;&#36798;&#26102;&#21487;&#20197;&#24555;&#36895;&#26356;&#26032;&#31995;&#25968;&#30340;&#20272;&#35745;&#12290;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Lasso-type&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25299;&#25169;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the statistical analysis of matrix-valued time series. These are data collected over a network of sensors (typically a set of spatial locations) along time, where a vector of features is observed per time instant per sensor. Thus each sensor is characterized by a vectorial time series. We would like to identify the dependency structure among these sensors and represent it by a graph. When there is only one feature per sensor, the vector auto-regressive models have been widely adapted to infer the structure of Granger causality. The resulting graph is referred to as causal graph. Our first contribution is then extending VAR models to matrix-variate models to serve the purpose of graph learning. Secondly, we propose two online procedures respectively in low and high dimensions, which can update quickly the estimates of coefficients when new samples arrive. In particular in high dimensional regime, a novel Lasso-type is introduced and we develop its homotopy a
&lt;/p&gt;</description></item><item><title>Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01479</link><description>&lt;p&gt;
Kernel-U-Net: &#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01479
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#26159;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#22522;&#20110;U-Net&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#30495;&#23454;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#22522;&#20110;&#34917;&#19969;&#27169;&#22411;&#25110;&#32447;&#24615;&#27169;&#22411;&#30340;&#27169;&#22411;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#23618;&#27425;&#21270;&#30340;&#26694;&#26550;&#65292;Kernel-U-Net&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#23558;&#36755;&#20837;&#24207;&#21015;&#20999;&#21106;&#25104;&#29255;&#27573;&#65292;&#28982;&#21518;&#20351;&#29992;&#21367;&#31215;&#26680;&#36827;&#34892;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#23427;&#25193;&#23637;&#20102;&#32463;&#20856;U-Net&#20013;&#30340;&#21367;&#31215;&#26680;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#25509;&#21463;&#31526;&#21512;&#30456;&#21516;&#35774;&#35745;&#27169;&#24335;&#30340;&#33258;&#23450;&#20041;&#21367;&#31215;&#26680;&#12290;&#19982;&#29616;&#26377;&#30340;&#32447;&#24615;&#25110;&#22522;&#20110;transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65306;1&#65289;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65306;&#21442;&#25968;&#22823;&#23567;&#20026;$O(log(L)^2)$&#65292;&#20854;&#20013;$L$&#20026;&#22238;&#28335;&#31383;&#21475;&#22823;&#23567;&#65307;2&#65289;&#28789;&#27963;&#24615;&#65306;&#20854;&#21367;&#31215;&#26680;&#21487;&#20197;&#23450;&#21046;&#21644;&#36866;&#24212;&#25968;&#25454;&#38598;&#65307;3&#65289;&#35745;&#31639;&#25928;&#29575;&#65306;&#22914;&#26524;&#20351;&#29992;&#27492;&#27169;&#22411;&#65292;transformer&#27169;&#22359;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23567;&#20026;$O(log(L)^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting task predicts future trends based on historical information. Recent U-Net-based methods have demonstrated superior performance in predicting real-world datasets. However, the performance of these models is lower than patch-based models or linear models. In this work, we propose a symmetric and hierarchical framework, Kernel-U-Net, which cuts the input sequence into slices at each layer of the network and then computes them using kernels. Furthermore, it generalizes the concept of convolutional kernels in classic U-Net to accept custom kernels that follow the same design pattern. Compared to the existing linear or transformer-based solution, our model contains 3 advantages: 1) A small number of parameters: the parameters size is $O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its kernels can be customized and fitted to the datasets, 3) Computation efficiency: the computation complexity of transformer modules is reduced to $O(log(L)^2)$ if th
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.15910</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15910
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#25351;&#30340;&#26159;&#26681;&#25454;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#35831;&#27714;&#65292;&#38477;&#20302;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#38500;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37027;&#23601;&#26159;&#24378;&#21270;&#23398;&#20064;&#12290;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#24448;&#24448;&#20250;&#35760;&#24518;&#29615;&#22659;&#30340;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26681;&#25454;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65292;&#29615;&#22659;&#30340;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#23637;&#19968;&#20010;&#26032;&#39062;&#19988;&#32039;&#36843;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#8220;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#8221;&#12290;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#20391;&#37325;&#20110;&#25764;&#38144;&#25972;&#20010;&#29615;&#22659;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#19968;&#29420;&#29305;&#29305;&#24449;&#24102;&#26469;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#25552;&#20986;&#28040;&#38500;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12403</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23567;&#25209;&#27425;
&lt;/p&gt;
&lt;p&gt;
Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26102;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#36807;&#31243;&#38750;&#24120;&#23494;&#38598;&#12290;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#19982;&#22270;&#37319;&#26679;&#30456;&#32467;&#21512;&#12290;GNN&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#23567;&#25209;&#37327;&#20013;&#30340;&#39033;&#20855;&#26377;&#37325;&#21472;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29420;&#31435;&#23567;&#25209;&#37327;&#26041;&#27861;&#23558;&#27599;&#20010;&#22788;&#29702;&#21333;&#20803;&#65288;PE&#65289;&#20998;&#37197;&#32473;&#33258;&#24049;&#30340;&#23567;&#25209;&#37327;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#37325;&#22797;&#35745;&#31639;&#21644;&#36328;PE&#30340;&#36755;&#20837;&#25968;&#25454;&#35775;&#38382;&#12290;&#36825;&#25918;&#22823;&#20102;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#36825;&#26159;&#38480;&#21046;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#22810;PE&#29615;&#22659;&#20013;NEP&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#26159;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20985;&#20989;&#25968;&#36825;&#19968;&#29305;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable 
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2310.07644</link><description>&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07644
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#32771;&#34385;&#20102;&#22522;&#20110;DNA&#24207;&#21015;&#30340;BERT-like&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#21644;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#37117;&#21462;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25104;&#21151;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;DNA&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#22240;&#20854;&#25429;&#25417;&#22522;&#22240;&#30340;&#36890;&#29992;&#20449;&#24687;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DNA&#24207;&#21015;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30452;&#25509;&#24341;&#20837;&#30340;BERT&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#21644;&#19987;&#38376;&#23450;&#21046;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#25506;&#32034;&#24615;&#23454;&#39564;&#65292;&#24182;&#33719;&#24471;&#20102;&#20960;&#20010;&#26377;&#21551;&#21457;&#24615;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;1&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#38454;&#27573;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#32780;&#19981;&#26159;K-mer&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#26102;&#65292;&#37325;&#21472;&#21644;&#38750;&#37325;&#21472;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#22343;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;2&#65289;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;K-mer&#37325;&#21472;&#26631;&#35760;&#21270;&#20250;&#36805;&#36895;&#20135;&#29983;&#28165;&#26224;&#30340;K-mer&#23884;&#20837;&#65292;&#24182;&#23558;&#25439;&#22833;&#38477;&#20302;&#21040;&#38750;&#24120;&#20302;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the success of large-scale pretraining in NLP, there is an increasing trend of applying it to the domain of life sciences. In particular, pretraining methods based on DNA sequences have garnered growing attention due to their potential to capture generic information about genes. However, existing pretraining methods for DNA sequences largely rely on direct adoptions of BERT pretraining from NLP, lacking a comprehensive understanding and a specifically tailored approach. To address this research gap, we first conducted a series of exploratory experiments and gained several insightful observations: 1) In the fine-tuning phase of downstream tasks, when using K-mer overlapping tokenization instead of K-mer non-overlapping tokenization, both overlapping and non-overlapping pretraining weights show consistent performance improvement.2) During the pre-training process, using K-mer overlapping tokenization quickly produces clear K-mer embeddings and reduces the loss to a very low level, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03342</link><description>&lt;p&gt;
LESSON: &#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework. (arXiv:2310.03342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36873;&#39033;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#25506;&#32034;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36873;&#39033;&#25209;&#21028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#32479;&#19968;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23398;&#20064;&#38598;&#25104;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#36866;&#24212;&#24615;&#22320;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#32473;&#23450;&#20219;&#21153;&#19979;&#30340;&#30456;&#20851;&#25506;&#32034;-&#24320;&#21457;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MiniGrid&#21644;Atari&#29615;&#22659;&#20013;&#36827;&#34892;&#21508;&#31181;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#25506;&#32034;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a unified framework for exploration in reinforcement learning (RL) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the MiniGrid and Atari environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03165</link><description>&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23618;&#21098;&#26525;&#21644;&#25439;&#22833;&#26354;&#38754;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;DNN&#26550;&#26500;&#30340;&#31616;&#21270;&#21644;&#20934;&#30830;&#24615;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#24182;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#21487;&#20943;&#23569;DNN&#23618;&#30340;&#21442;&#25968;&#65292;&#31616;&#21270;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#23618;&#21098;&#26525;&#31616;&#21270;DNN&#26550;&#26500;&#21644;&#25439;&#22833;&#26354;&#38754;&#12290;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#26368;&#36817;&#34987;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#33021;&#22815;&#26816;&#26597;DNN&#30340;&#26435;&#37325;&#23618;&#35889;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30830;&#23450;&#35201;&#20174;DNN&#30340;&#26435;&#37325;&#23618;&#20013;&#21435;&#38500;&#30340;&#22855;&#24322;&#20540;&#30340;&#25968;&#37327;&#65292;&#36825;&#26377;&#21161;&#20110;&#31616;&#21270;DNN&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#22312;MNIST&#21644;Fashion MNIST&#25968;&#25454;&#38598;&#19978;&#22521;&#35757;&#31616;&#21333;&#30340;DNN&#27169;&#22411;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;DNN&#30340;&#20219;&#20309;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#23618;&#65292;&#20943;&#23569;&#20102;&#23618;&#30340;&#21442;&#25968;&#24182;&#31616;&#21270;&#20102;DNN&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26681;&#25454;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#20002;&#24323;&#23567;&#30340;&#22855;&#24322;&#20540;&#65292;&#27979;&#35797;&#38598;&#30340;&#20934;&#30830;&#24615;&#20445;&#25345;&#19968;&#33268;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;DNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15669</link><description>&lt;p&gt;
&#20851;&#20110;&#35745;&#31639;&#32416;&#32544;&#21450;&#20854;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
On Computational Entanglement and Its Interpretation in Adversarial Machine Learning. (arXiv:2309.15669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#23558;&#20854;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20013;&#30340;&#32416;&#32544;&#27010;&#24565;&#32852;&#31995;&#36215;&#26469;&#65292;&#21457;&#29616;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#32416;&#32544;&#29616;&#35937;&#65292;&#25361;&#25112;&#20102;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27450;&#39575;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#22240;&#27492;&#24050;&#25104;&#20026;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#24615;&#19982;&#29233;&#22240;&#26031;&#22374;&#30340;&#29305;&#27530;&#30456;&#23545;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#65292;&#36890;&#36807;&#32416;&#32544;&#30340;&#27010;&#24565;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23545;&#35745;&#31639;&#32416;&#32544;&#36827;&#34892;&#20102;&#23450;&#20041;&#65292;&#24182;&#35777;&#26126;&#20102;&#36828;&#31243;&#29305;&#24449;&#26679;&#26412;&#21487;&#20197;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#65292;&#31867;&#20284;&#20110;&#37327;&#23376;&#39046;&#22495;&#20013;&#30340;&#32416;&#32544;&#12290;&#36825;&#19968;&#21457;&#29616;&#25361;&#25112;&#20102;&#23545;&#24403;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#23545;&#25239;&#21487;&#20256;&#36882;&#24615;&#29616;&#35937;&#30340;&#20256;&#32479;&#25551;&#36848;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples in machine learning has emerged as a focal point of research due to their remarkable ability to deceive models with seemingly inconspicuous input perturbations, potentially resulting in severe consequences. In this study, we embark on a comprehensive exploration of adversarial machine learning models, shedding light on their intrinsic complexity and interpretability. Our investigation reveals intriguing links between machine learning model complexity and Einstein's theory of special relativity, through the concept of entanglement. More specific, we define entanglement computationally and demonstrate that distant feature samples can exhibit strong correlations, akin to entanglement in quantum realm. This revelation challenges conventional perspectives in describing the phenomenon of adversarial transferability observed in contemporary machine learning models. By drawing parallels with the relativistic effects of time dilation and length contraction during computatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CA-PCA&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#26354;&#29575;&#26657;&#20934;&#30340;&#23616;&#37096;PCA&#29256;&#26412;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#32500;&#24230;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13478</link><description>&lt;p&gt;
CA-PCA: &#27979;&#37327;&#26354;&#29575;&#30340;&#27969;&#24418;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CA-PCA: Manifold Dimension Estimation, Adapted for Curvature. (arXiv:2309.13478v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CA-PCA&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#26354;&#29575;&#26657;&#20934;&#30340;&#23616;&#37096;PCA&#29256;&#26412;&#65292;&#36890;&#36807;&#32771;&#34385;&#24213;&#23618;&#27969;&#24418;&#30340;&#26354;&#29575;&#65292;&#25913;&#36827;&#20102;&#32500;&#24230;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#31639;&#27861;&#30340;&#25104;&#21151;&#24120;&#24402;&#22240;&#20110;&#27969;&#24418;&#20551;&#35774;&#65292;&#21363;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#22312;&#25110;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#22312;&#36827;&#34892;&#32500;&#24230;&#32422;&#31616;&#20043;&#21069;&#65292;&#30830;&#23450;&#25110;&#20272;&#35745;&#35813;&#27969;&#24418;&#30340;&#32500;&#24230;&#36890;&#24120;&#26159;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#32500;&#24230;&#20272;&#35745;&#26041;&#27861;&#20351;&#29992;&#24179;&#22374;&#21333;&#20301;&#29699;&#36827;&#34892;&#26657;&#20934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CA-PCA&#65292;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#23884;&#20837;&#26657;&#20934;&#30340;&#23616;&#37096;PCA&#29256;&#26412;&#65292;&#20197;&#32771;&#34385;&#24213;&#23618;&#27969;&#24418;&#30340;&#26354;&#29575;&#12290;&#22823;&#37327;&#30340;&#31934;&#24515;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#24615;&#25913;&#36827;&#20102;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of algorithms in the analysis of high-dimensional data is often attributed to the manifold hypothesis, which supposes that this data lie on or near a manifold of much lower dimension. It is often useful to determine or estimate the dimension of this manifold before performing dimension reduction, for instance. Existing methods for dimension estimation are calibrated using a flat unit ball. In this paper, we develop CA-PCA, a version of local PCA based instead on a calibration of a quadratic embedding, acknowledging the curvature of the underlying manifold. Numerous careful experiments show that this adaptation improves the estimator in a wide range of settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.10831</link><description>&lt;p&gt;
&#27963;&#21160;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#24212;&#23545;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#22240;&#20026;&#21463;&#25511;&#23454;&#39564;&#23460;/&#20223;&#30495;&#21644;&#23454;&#38469;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#23545;&#20110;&#20960;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#26159;&#23433;&#20840;&#30340;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#21516;&#65292;&#25506;&#27979;&#21644;&#23433;&#20840;&#24615;&#30001;&#25511;&#21046;&#22120;&#33258;&#36523;&#33258;&#21160;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#23398;&#20064;&#12290;&#19968;&#20010;&#20223;&#30495;&#31034;&#20363;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2309.03665</link><description>&lt;p&gt;
&#22914;&#20309;&#25915;&#20987;&#21487;&#20197;&#24178;&#25200;&#30475;&#20284;&#31283;&#23450;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#22914;&#20309;&#36890;&#36807;&#24494;&#23567;&#20462;&#25913;&#24178;&#25200;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#65292;&#21253;&#25324;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#21040;&#24433;&#21709;&#65292;&#21516;&#26102;&#23545;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#39564;&#35777;&#23454;&#39564;&#36824;&#34920;&#26126;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21407;&#26412;&#20934;&#30830;&#30340;&#23398;&#20064;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#20855;&#26377;&#35773;&#21050;&#24847;&#21619;&#30340;&#26159;&#65292;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21363;&#20351;&#31995;&#32479;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#22823;&#24133;&#24230;&#38543;&#26426;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#23567;&#20247;&#12289;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#21487;&#33021;&#26159;&#39640;&#32500;&#36755;&#20837;&#25968;&#25454;&#19979;&#20998;&#31867;&#22120;&#30340;&#19968;&#20010;&#22522;&#26412;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36890;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#26694;&#26550;&#65292;&#20854;&#20013;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#34892;&#20026;&#20855;&#26377;&#39640;&#27010;&#29575;&#20986;&#29616;&#65292;&#23588;&#20854;&#26159;&#65288;&#21407;&#26412;&#20934;&#30830;&#30340;&#65289;&#27169;&#22411;&#23545;&#26131;&#20110;&#26500;&#36896;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#21516;&#26102;&#23481;&#26131;&#21463;&#21040;&#36755;&#20837;&#25968;&#25454;&#30340;&#38543;&#26426;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#30456;&#21516;&#29616;&#35937;&#22312;&#23454;&#38469;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30452;&#25509;&#35266;&#23519;&#32467;&#26524;&#65292;&#21363;&#20351;&#26159;&#22823;&#24133;&#24230;&#30340;&#21152;&#24615;&#38543;&#26426;&#22122;&#22768;&#20063;&#26080;&#27861;&#24178;&#25200;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability -- notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.03301</link><description>&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#20013;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimating Conditional Mutual Information for Dynamic Feature Selection. (arXiv:2306.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#24182;&#35774;&#35745;&#20102;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#21028;&#21035;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#22810;&#39033;&#25913;&#36827;&#25514;&#26045;&#20197;&#24212;&#23545;&#26356;&#22810;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29305;&#24449;&#36873;&#25321;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#29305;&#24449;&#20197;&#22312;&#26368;&#23567;&#30340;&#39044;&#31639;&#20869;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#20197;&#20943;&#23569;&#29305;&#24449;&#33719;&#21462;&#25104;&#26412;&#65292;&#24182;&#20026;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#36879;&#26126;&#24230;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20010;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20351;&#29992;&#20219;&#24847;&#29305;&#24449;&#38598;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#23398;&#20064;&#31574;&#30053;&#20197;&#30830;&#23450;&#26368;&#26377;&#20215;&#20540;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20174;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26681;&#25454;&#29305;&#24449;&#19982;&#21709;&#24212;&#21464;&#37327;&#30340;&#20114;&#20449;&#24687;&#23545;&#29305;&#24449;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#12290;&#20854;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#23398;&#20064;&#27492;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#30452;&#25509;&#26032;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21028;&#21035;&#32780;&#38750;&#29983;&#25104;&#27169;&#24335;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;&#24314;&#31435;&#22312;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#65306;&#20801;&#35768;&#22312;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#21487;&#21464;&#30340;&#29305;&#24449;&#39044;&#31639;&#12289;&#25903;&#25345;&#19981;&#21516;&#29305;&#24449;&#20043;&#38388;&#30340;&#38750;&#22343;&#21248;&#25104;&#26412;&#12289;&#32467;&#21512;&#20808;&#21069;&#30340;&#20449;&#24687;&#21644;&#25506;&#31350;&#29616;&#20195;&#26550;&#26500;&#20197;&#22788;&#29702;&#37096;&#20998;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into the prediction process. The problem is challenging, however, as it requires both making predictions with arbitrary feature sets and learning a policy to identify the most valuable selections. Here, we take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is learning this selection policy, and we design a straightforward new modeling approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our learning approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform costs between features, incorporating prior information, and exploring modern architectures to handle partial input in
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09868</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Principle of Uncertain Maximum Entropy. (arXiv:2305.09868v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09868
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#35813;&#21407;&#29702;&#21487;&#20197;&#22788;&#29702;&#27169;&#22411;&#20803;&#32032;&#19981;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#65292;&#24182;&#20248;&#20110;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#12290;&#21516;&#26102;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#29109;&#21407;&#29702;&#22312;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#24341;&#20837;&#65292;&#20026;&#32479;&#35745;&#21147;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#29983;&#24577;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#20854;&#24471;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20652;&#21270;&#21058;&#65292;&#20419;&#36827;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#32463;&#39564;&#35266;&#23519;&#26144;&#23556;&#21040;&#33719;&#21462;&#26080;&#20559;&#27169;&#22411;&#65292;&#21516;&#26102;&#21152;&#28145;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#21644;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#20803;&#32032;&#19981;&#30452;&#25509;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#23384;&#22312;&#22122;&#22768;&#25110;&#30524;&#37096;&#36974;&#25377;&#30340;&#24773;&#20917;&#19979;&#65292;&#26631;&#20934;&#26368;&#22823;&#29109;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21305;&#37197;&#29305;&#24449;&#32422;&#26463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#30830;&#23450;&#26368;&#22823;&#29109;&#21407;&#29702;&#20316;&#20026;&#19968;&#31181;&#26041;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#20219;&#24847;&#22122;&#22768;&#35266;&#23519;&#65292;&#23427;&#21516;&#26102;&#23558;&#25152;&#26377;&#21487;&#29992;&#20449;&#24687;&#32534;&#30721;&#65292;&#32780;&#19988;&#20248;&#20110;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#26368;&#22823;&#29109;&#26041;&#27861;&#30340;&#20934;&#30830;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#29992;&#20316;&#19981;&#30830;&#23450;&#26426;&#22120;&#29109;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#22312;&#19982;&#26368;&#22823;&#20284;&#28982;&#31639;&#27861;&#30456;&#27604;&#26102;&#24314;&#31435;&#20102;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of maximum entropy, as introduced by Jaynes in information theory, has contributed to advancements in various domains such as Statistical Mechanics, Machine Learning, and Ecology. Its resultant solutions have served as a catalyst, facilitating researchers in mapping their empirical observations to the acquisition of unbiased models, whilst deepening the understanding of complex systems and phenomena. However, when we consider situations in which the model elements are not directly observable, such as when noise or ocular occlusion is present, possibilities arise for which standard maximum entropy approaches may fail, as they are unable to match feature constraints. Here we show the Principle of Uncertain Maximum Entropy as a method that both encodes all available information in spite of arbitrarily noisy observations while surpassing the accuracy of some ad-hoc methods. Additionally, we utilize the output of a black-box machine learning model as input into an uncertain ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07831</link><description>&lt;p&gt;
&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#22312;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network Sensitivity Under Probabilistic Error Model. (arXiv:2203.07831v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#35823;&#24046;&#27169;&#22411;&#19979;&#37051;&#25509;&#30697;&#38453;&#30340;&#21463;&#38480;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#20102;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21487;&#20197;&#36890;&#36807;&#22270;&#21367;&#31215;&#25104;&#21151;&#23398;&#20064;&#22270;&#20449;&#21495;&#34920;&#31034;&#12290;&#22270;&#21367;&#31215;&#20381;&#36182;&#20110;&#22270;&#28388;&#27874;&#22120;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#25454;&#30340;&#25299;&#25169;&#20381;&#36182;&#20851;&#31995;&#24182;&#20256;&#25773;&#25968;&#25454;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#20256;&#25773;&#30697;&#38453;&#65288;&#20363;&#22914;&#37051;&#25509;&#30697;&#38453;&#65289;&#20013;&#30340;&#20272;&#35745;&#35823;&#24046;&#21487;&#33021;&#23545;&#22270;&#28388;&#27874;&#22120;&#21644;GCNs&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#27010;&#29575;&#22270;&#35823;&#24046;&#27169;&#22411;&#23545;GCN&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#37051;&#25509;&#30697;&#38453;&#21463;&#21040;&#22270;&#22823;&#23567;&#21644;&#35823;&#24046;&#27010;&#29575;&#20989;&#25968;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#24102;&#26377;&#33258;&#24490;&#29615;&#30340;&#24402;&#19968;&#21270;&#37051;&#25509;&#30697;&#38453;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#30740;&#31350;&#31616;&#21333;GCN&#22312;&#36825;&#31181;&#27010;&#29575;&#35823;&#24046;&#27169;&#22411;&#19979;&#30340;&#20934;&#30830;&#24615;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) can successfully learn the graph signal representation by graph convolution. The graph convolution depends on the graph filter, which contains the topological dependency of data and propagates data features. However, the estimation errors in the propagation matrix (e.g., the adjacency matrix) can have a significant impact on graph filters and GCNs. In this paper, we study the effect of a probabilistic graph error model on the performance of the GCNs. We prove that the adjacency matrix under the error model is bounded by a function of graph size and error probability. We further analytically specify the upper bound of a normalized adjacency matrix with self-loop added. Finally, we illustrate the error bounds by running experiments on a synthetic dataset and study the sensitivity of a simple GCN under this probabilistic error model on accuracy.
&lt;/p&gt;</description></item></channel></rss>