<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.16137</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#35843;&#26597;&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Survey on Self-Supervised Pre-Training of Graph Foundation Models: A Knowledge-Based Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22522;&#20110;&#30693;&#35782;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28041;&#21450;&#24494;&#35266;&#21644;&#23439;&#35266;&#30693;&#35782;&#65292;&#21253;&#25324;9&#20010;&#30693;&#35782;&#31867;&#21035;&#12289;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#29616;&#22312;&#26159;&#39044;&#35757;&#32451;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39318;&#36873;&#26041;&#27861;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#21464;&#25442;&#22120;&#65292;&#20197;&#21450;&#26356;&#36817;&#26399;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#22270;&#27169;&#22411;&#12290;&#25991;&#31456;&#20840;&#38754;&#35843;&#26597;&#21644;&#20998;&#26512;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#35270;&#35282;&#19979;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21253;&#25324;&#24494;&#35266;&#65288;&#33410;&#28857;&#12289;&#38142;&#25509;&#31561;&#65289;&#21644;&#23439;&#35266;&#30693;&#35782;&#65288;&#31751;&#12289;&#20840;&#23616;&#32467;&#26500;&#31561;&#65289;&#12290;&#28085;&#30422;&#20102;&#20849;&#35745;9&#20010;&#30693;&#35782;&#31867;&#21035;&#21644;25&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16137v1 Announce Type: new  Abstract: Graph self-supervised learning is now a go-to method for pre-training graph foundation models, including graph neural networks, graph transformers, and more recent large language model (LLM)-based graph models. There is a wide variety of knowledge patterns embedded in the structure and properties of graphs which may be used for pre-training, but we lack a systematic overview of self-supervised pre-training tasks from the perspective of graph knowledge. In this paper, we comprehensively survey and analyze the pre-training tasks of graph foundation models from a knowledge-based perspective, consisting of microscopic (nodes, links, etc) and macroscopic knowledge (clusters, global structure, etc). It covers a total of 9 knowledge categories and 25 pre-training tasks, as well as various downstream task adaptation strategies. Furthermore, an extensive list of the related papers with detailed metadata is provided at https://github.com/Newiz430/
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12712</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#21464;&#24418;&#35299;&#20915;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Source Scale Bias via Image Warping for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#30001;&#20110;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#21644;&#22270;&#20687;&#22823;&#23567;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#65292;&#23610;&#24230;&#20559;&#24046;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#27880;&#20837;&#23610;&#24230;&#19981;&#21464;&#24615;&#20808;&#39564;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#25110;&#32773;&#22312;&#25512;&#26029;&#26102;&#35843;&#25972;&#23610;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#23610;&#24230;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20250;&#22686;&#21152;&#35757;&#32451;&#36807;&#31243;&#30340;&#35745;&#31639;&#36127;&#36733;&#21644;&#25512;&#26029;&#36807;&#31243;&#30340;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#21147;&#22788;&#29702;&#8212;&#8212;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23601;&#22320;&#25197;&#26354;&#22270;&#20687;&#26469;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#28304;&#23610;&#24230;&#20998;&#24067;&#21487;&#20197;&#25913;&#21892;&#20027;&#24178;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#20197;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#22320;&#29702;&#12289;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
&lt;/p&gt;</description></item><item><title>UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07187</link><description>&lt;p&gt;
UPS: &#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#23454;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UPS: Towards Foundation Models for PDE Solving via Cross-Modal Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07187
&lt;/p&gt;
&lt;p&gt;
UPS&#36890;&#36807;&#36328;&#27169;&#24577;&#36866;&#24212;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#19979;&#36798;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UPS&#65288;&#32479;&#19968;PDE&#27714;&#35299;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#22495;&#12289;&#32500;&#24230;&#21644;&#20998;&#36776;&#29575;&#19978;&#23450;&#20041;&#30340;&#21508;&#31181;&#26102;&#31354;PDE&#12290;UPS&#23558;&#19981;&#21516;&#30340;PDE&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#24182;&#20351;&#29992;&#23558;LLMs&#19982;&#29305;&#23450;&#22495;&#31070;&#32463;&#31639;&#23376;&#30456;&#32467;&#21512;&#30340;&#32479;&#19968;&#32593;&#32476;&#26550;&#26500;&#22788;&#29702;&#21508;&#31181;PDE&#25968;&#25454;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#36807;&#31243;&#35757;&#32451;&#32593;&#32476;&#65292;&#21033;&#29992;&#27169;&#24577;&#23545;&#40784;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#36827;&#34892;&#35843;&#25972;&#24182;&#21033;&#29992;&#25991;&#26412;&#24418;&#24335;&#30340;&#20803;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;UPS&#22312;PDEBench&#30340;&#24191;&#27867;1D&#21644;2D&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#23545;&#32771;&#34385;&#30340;10&#20010;&#20219;&#21153;&#20013;&#30340;8&#20010;&#20219;&#21153;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#33021;&#22815;&#23569;&#26679;&#26412;&#24555;&#36895;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07187v1 Announce Type: new  Abstract: We introduce UPS (Unified PDE Solver), an effective and data-efficient approach to solve diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. UPS unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators. We train the network via a two-stage cross-modal adaptation process, leveraging ideas of modality alignment and multi-task learning. By adapting from pretrained LLMs and exploiting text-form meta information, we are able to use considerably fewer training samples than previous methods while obtaining strong empirical results. UPS outperforms existing baselines, often by a large margin, on a wide range of 1D and 2D datasets in PDEBench, achieving state-of-the-art results on 8 of 10 tasks considered. Meanwhile, it is capable of few-shot transfer to different PDE f
&lt;/p&gt;</description></item><item><title>GTVMin&#22312;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;</title><link>https://arxiv.org/abs/2403.06298</link><description>&lt;p&gt;
&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Total Variation Minimization for Clustered Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06298
&lt;/p&gt;
&lt;p&gt;
GTVMin&#22312;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#33268;&#21516;&#36136;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#32676;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#31181;&#38598;&#32676;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#26159;&#24191;&#20041;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#65288;GTVMin&#65289;&#12290;&#35813;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#30456;&#20284;&#24615;&#22270;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#23398;&#20064;&#25216;&#26415;&#26469;&#33719;&#24471;&#12290;&#22312;&#19968;&#20010;&#24191;&#27867;&#36866;&#29992;&#30340;&#38598;&#32676;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;GTVMin&#35299;&#19982;&#20854;&#25353;&#31751;&#24179;&#22343;&#20540;&#20043;&#38388;&#30340;&#20559;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20010;&#30028;&#38480;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;GTVMin&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06298v1 Announce Type: new  Abstract: A key challenge in federated learning applications is the statistical heterogeneity of local datasets. Clustered federated learning addresses this challenge by identifying clusters of local datasets that are approximately homogeneous. One recent approach to clustered federated learning is generalized total variation minimization (GTVMin). This approach requires a similarity graph which can be obtained by domain expertise or in a data-driven fashion via graph learning techniques. Under a widely applicable clustering assumption, we derive an upper bound the deviation between GTVMin solutions and their cluster-wise averages. This bound provides valuable insights into the effectiveness and robustness of GTVMin in addressing statistical heterogeneity within federated learning environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.06082</link><description>&lt;p&gt;
FrameQuant: Transformer&#30340;&#28789;&#27963;&#20302;&#27604;&#29305;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FrameQuant: Flexible Low-Bit Quantization for Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06082
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#34701;&#21512;&#26694;&#26550;&#23558;Transformer&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65292;&#20165;&#26377;&#36731;&#24494;&#31934;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#35768;&#22810;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#24378;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#25903;&#26609;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;/&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#36739;&#22823;&#65292;&#22240;&#27492;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#24448;&#24448;&#38656;&#35201;&#26114;&#36149;&#30340;&#39640;&#31471;&#30828;&#20214;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#22256;&#38590;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#35797;&#22270;&#20462;&#25913;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#20854;&#37327;&#21270;&#20026;&#20843;&#20301;&#25110;&#26356;&#20302;&#30340;&#20301;&#25968;&#65292;&#26174;&#30528;&#25552;&#39640;&#35745;&#31639;/&#20869;&#23384;/&#24310;&#36831;&#25928;&#29575;&#12290;&#26082;&#21487;&#20197;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#37327;&#21270;&#20026;&#22235;&#20301;&#65292;&#20294;&#24615;&#33021;&#26377;&#25152;&#25439;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#26696;&#65292;&#23558;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#37327;&#21270;&#20026;&#20165;&#20004;&#20301;&#65288;&#21152;&#19968;&#20123;&#39069;&#22806;&#24320;&#38144;&#65289;&#65292;&#20165;&#20250;&#26377;&#36731;&#24494;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#21046;&#23450;&#20851;&#38190;&#22312;&#20110;&#20174;&#35856;&#27874;&#20998;&#26512;&#20013;&#20511;&#37492;&#20102;&#19968;&#31181;&#31216;&#20026;&#34701;&#21512;&#26694;&#26550;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#37327;&#21270;&#19981;&#24212;&#35813;&#22312;&#21407;&#22987;&#26435;&#37325;&#31354;&#38388;&#20013;&#36827;&#34892;&#65292;&#32780;&#26159;&#24212;&#35813;&#22312;&#34701;&#21512;&#26694;&#26550;&#34920;&#31034;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06082v1 Announce Type: cross  Abstract: Transformers are the backbone of powerful foundation models for many Vision and Natural Language Processing tasks. But their compute and memory/storage footprint is large, and so, serving such models is expensive often requiring high-end hardware. To mitigate this difficulty, Post-Training Quantization seeks to modify a pre-trained model and quantize it to eight bits or lower, significantly boosting compute/memory/latency efficiency. Such models have been successfully quantized to four bits with some performance loss. In this work, we outline a simple scheme to quantize Transformer-based models to just two bits (plus some overhead) with only a small drop in accuracy. Key to our formulation is a concept borrowed from Harmonic analysis called Fusion Frames. Our main finding is that the quantization must take place not in the original weight space, but instead in the Fusion Frame representations. If quantization is interpreted as the addi
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.05385</link><description>&lt;p&gt;
&#22312;&#25209;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20999;&#25442;&#25439;&#22833;&#20989;&#25968;&#26469;&#38477;&#20302;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Switching the Loss Reduces the Cost in Batch Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05385
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#30446;&#26631;&#26102;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#20013;&#65292;&#20854;&#26679;&#26412;&#25968;&#37327;&#38656;&#27714;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#33021;&#22815;&#25552;&#20379;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#20013;&#65292;FQI-LOG&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25968;&#25439;&#22833;&#65288;FQI-LOG&#65289;&#26469;&#35757;&#32451;&#36866;&#21512;&#30340;Q&#36845;&#20195;&#30340;&#25209;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;FQI-LOG&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#37327;&#19982;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#25104;&#27604;&#20363;&#65292;&#23545;&#20110;&#37027;&#20123;&#36890;&#36807;&#26368;&#20248;&#34892;&#20026;&#23454;&#29616;&#30446;&#26631;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#26368;&#20248;&#31574;&#30053;&#30340;&#32047;&#31215;&#25104;&#26412;&#20026;&#38646;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#25209;RL&#20013;&#35777;&#26126;&#20855;&#26377;&#19982;&#26368;&#20248;&#21487;&#36798;&#25104;&#26412;&#25104;&#27604;&#20363;&#30340;&#8220;&#23567;&#25104;&#26412;&#8221;&#30028;&#38480;&#30340;&#19968;&#33324;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32463;&#39564;&#19978;&#39564;&#35777;&#65292;FQI-LOG&#22312;&#37027;&#20123;&#26368;&#20248;&#31574;&#30053;&#21487;&#38752;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#38382;&#39064;&#19978;&#20351;&#29992;&#30340;&#26679;&#26412;&#27604;&#20351;&#29992;&#24179;&#26041;&#25439;&#22833;&#35757;&#32451;&#30340;FQI&#35201;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05385v1 Announce Type: new  Abstract: We propose training fitted Q-iteration with log-loss (FQI-LOG) for batch reinforcement learning (RL). We show that the number of samples needed to learn a near-optimal policy with FQI-LOG scales with the accumulated cost of the optimal policy, which is zero in problems where acting optimally achieves the goal and incurs no cost. In doing so, we provide a general framework for proving $\textit{small-cost}$ bounds, i.e. bounds that scale with the optimal achievable cost, in batch RL. Moreover, we empirically verify that FQI-LOG uses fewer samples than FQI trained with squared loss on problems where the optimal policy reliably achieves the goal.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05122</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#22810;&#22612;&#22810;&#20852;&#36259;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Tower Multi-Interest Recommendation with User Representation Repel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#36807;&#36733;&#30340;&#26102;&#20195;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#28145;&#21051;&#35748;&#35782;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#20215;&#20540;&#12290;&#29305;&#21035;&#26159;&#22810;&#20852;&#36259;&#24207;&#21015;&#25512;&#33616;&#26159;&#36817;&#24180;&#26469;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#12290;&#36890;&#36807;&#29983;&#25104;&#22810;&#29992;&#25143;&#34920;&#31034;&#65292;&#22810;&#20852;&#36259;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#37117;&#27604;&#21333;&#29992;&#25143;&#34920;&#31034;&#27169;&#22411;&#20855;&#26377;&#26356;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#22256;&#25200;&#30528;&#22810;&#20852;&#36259;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#21487;&#37319;&#29992;&#24615;&#65292;&#21363;&#35757;&#32451;&#21644;&#37096;&#32626;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#12289;&#26080;&#27861;&#35775;&#38382;&#21830;&#21697;&#20449;&#24687;&#20197;&#21450;&#30001;&#20110;&#20854;&#21333;&#22612;&#26550;&#26500;&#32780;&#38590;&#20197;&#24037;&#19994;&#37319;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#20855;&#26377;&#29992;&#25143;&#34920;&#31034;&#25490;&#26021;&#30340;&#26032;&#22411;&#22810;&#22612;&#22810;&#20852;&#36259;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#36328;&#22810;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05122v1 Announce Type: cross  Abstract: In the era of information overload, the value of recommender systems has been profoundly recognized in academia and industry alike. Multi-interest sequential recommendation, in particular, is a subfield that has been receiving increasing attention in recent years. By generating multiple-user representations, multi-interest learning models demonstrate superior expressiveness than single-user representation models, both theoretically and empirically. Despite major advancements in the field, three major issues continue to plague the performance and adoptability of multi-interest learning methods, the difference between training and deployment objectives, the inability to access item information, and the difficulty of industrial adoption due to its single-tower architecture. We address these challenges by proposing a novel multi-tower multi-interest framework with user representation repel. Experimental results across multiple large-scale 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03861</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36873;&#25321;&#35774;&#35745;&#20449;&#24687;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Designing Informative Metrics for Few-Shot Example Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25552;&#20379;&#36866;&#24403;&#26684;&#24335;&#30340;&#31034;&#20363;&#26102;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#8220;&#26368;&#20339;&#8221;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36873;&#25321;&#31034;&#20363;&#30340;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23545;&#40784;&#27979;&#35797;&#21477;&#23376;&#21644;&#31034;&#20363;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#31034;&#20363;&#30340;&#22797;&#26434;&#24230;&#19982;&#32771;&#34385;&#20013;&#30340;&#65288;&#27979;&#35797;&#65289;&#21477;&#23376;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;PLMs&#20013;&#25552;&#21462;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#22312;&#23569;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CoNLL2003&#25968;&#25454;&#38598;&#19978;&#23545;GPT-4&#30340;F1&#20998;&#25968;&#23454;&#29616;&#20102;5%&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#22312;&#20687;GPT-j-6B&#36825;&#26679;&#30340;&#36739;&#23567;&#27169;&#22411;&#20013;&#30475;&#21040;&#20102;&#39640;&#36798;28.85&#20010;&#28857;&#65288;F1/Acc.&#65289;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13989</link><description>&lt;p&gt;
FedADMM-InSa: &#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;ADMM
&lt;/p&gt;
&lt;p&gt;
FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#23398;&#20064;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#12290;&#26377;&#25928;&#30340;FL&#31639;&#27861;&#30340;&#21457;&#23637;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#24322;&#26500;&#25968;&#25454;&#21644;&#31995;&#32479;&#12289;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#20197;&#21450;&#21463;&#38480;&#30340;&#26412;&#22320;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;FedADMM&#26041;&#27861;&#23545;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#38887;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36229;&#21442;&#25968;&#27809;&#26377;&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#21517;&#20026;FedADMM-InSa&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20197;&#28040;&#38500;&#24517;&#39035;&#26681;&#25454;&#32463;&#39564;&#35774;&#32622;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#21487;&#20197;&#30001;&#27599;&#20010;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#26681;&#25454;&#20854;&#29420;&#29305;&#26465;&#20214;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#26412;&#22320;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#19981;&#33391;&#30340;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12539</link><description>&lt;p&gt;
&#25968;&#25454;&#20351;&#29992;&#23545;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12539
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20855;&#26377;&#26234;&#33021;&#20648;&#33021;&#30340;&#24314;&#31569;&#20013;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26356;&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#24320;&#21457;&#29992;&#20110;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#20013;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26041;&#26696;&#30340;&#39044;&#27979;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20351;&#29992;&#20250;&#20135;&#29983;&#25910;&#38598;&#21644;&#21033;&#29992;&#26041;&#38754;&#30340;&#25104;&#26412;&#12290;&#30830;&#23450;&#25104;&#26412;&#26368;&#20248;&#25968;&#25454;&#20351;&#29992;&#38656;&#35201;&#20102;&#35299;&#20854;&#24102;&#26469;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20197;&#21450;&#32467;&#26524; MPC &#36816;&#34892;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#21382;&#21490;&#24314;&#31569;&#33021;&#28304;&#25968;&#25454;&#22312;&#19968;&#20010;&#22810;&#24314;&#31569;&#33021;&#28304;&#31995;&#32479;&#27169;&#25311;&#20013;&#65292;&#30740;&#31350;&#20102;&#31616;&#21333;&#21644;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#22312; MPC &#20013;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20197;&#19979;&#25968;&#25454;&#25928;&#29575;&#25514;&#26045;&#65292;&#21363;&#37325;&#26032;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#12289;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#37327;&#12289;&#20943;&#23569;&#27169;&#22411;&#25968;&#25454;&#29305;&#24449;&#21644;&#22312;&#32447;&#27169;&#22411;&#35757;&#32451;&#65292;&#37327;&#21270;&#20102;&#25968;&#25454;&#20351;&#29992;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31616;&#21333;&#30340;&#32447;&#24615;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12539v1 Announce Type: cross  Abstract: Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;&#23454;&#29616;&#20102;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;&#65292;&#30830;&#20445;&#22312;&#38081;&#30913;&#24615;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#8220;&#31934;&#30830;&#8221;$\lambda_*$&#20351;&#24471;&#35745;&#31639;&#30340;&#37197;&#20998;&#20989;&#25968;$Z=Z^{(\lambda_*)}$&#12290;</title><link>https://arxiv.org/abs/2301.10369</link><description>&lt;p&gt;
&#31934;&#30830;&#20998;&#25968;&#25512;&#26029;&#65306;&#37325;&#26032;&#21442;&#25968;&#21270;&#21450;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Exact Fractional Inference via Re-Parametrization &amp; Interpolation between Tree-Re-Weighted- and Belief Propagation- Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10369
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;&#23454;&#29616;&#20102;&#26641;&#37325;&#26032;&#21152;&#26435;&#21644;&#20449;&#24565;&#20256;&#25773;&#31639;&#27861;&#20043;&#38388;&#30340;&#25554;&#20540;&#65292;&#30830;&#20445;&#22312;&#38081;&#30913;&#24615;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#8220;&#31934;&#30830;&#8221;$\lambda_*$&#20351;&#24471;&#35745;&#31639;&#30340;&#37197;&#20998;&#20989;&#25968;$Z=Z^{(\lambda_*)}$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#26029;&#24037;&#20316;--&#35745;&#31639;Ising&#27169;&#22411;&#22312;N&#20010;&#8220;&#33258;&#26059;&#8221;&#32452;&#25104;&#30340;&#22270;&#19978;&#30340;&#37197;&#20998;&#20989;&#25968;$Z$&#25152;&#38656;&#30340;&#24037;&#20316;--&#24456;&#21487;&#33021;&#38543;&#30528;N&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#39640;&#25928;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#22914;&#20449;&#24565;&#20256;&#25773;&#65288;BP&#65289;&#21644;&#26641;&#37325;&#26032;&#21152;&#26435;&#65288;TRW&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#26368;&#23567;&#21270;&#21508;&#33258;&#65288;BP&#25110;TRW&#65289;&#33258;&#30001;&#33021;&#30340;$Z$&#26469;&#36817;&#20284;&#35745;&#31639;$Z$&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;$\lambda$-&#20998;&#25968;&#20840;&#21516;&#32500;&#25968;&#65292;$Z^{(\lambda)}$&#65292;&#20854;&#20013;$\lambda=0$&#21644;$\lambda=1$&#20998;&#21035;&#23545;&#24212;&#20110;TRW&#21644;BP&#30340;&#36817;&#20284;&#65292;&#19988;$Z^{(\lambda)}$&#38543;$\lambda$&#21333;&#35843;&#20943;&#23569;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#20998;&#25968;&#26041;&#26696;&#20445;&#35777;&#22312;&#21560;&#24341;&#21147;&#65288;&#38081;&#30913;&#24615;&#65289;&#24773;&#20917;&#19979;$Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20010;&#21807;&#19968;&#30340;&#65288;&#8220;&#31934;&#30830;&#8221;&#65289;$\lambda_*$&#65292;&#20351;&#24471;$Z=Z^{(\lambda_*)}$&#12290;&#36890;&#36807;&#25512;&#24191;\citep {wainwright_tree-based_2002}&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21644;\citep {chertkov_loop_2006}&#30340;&#29615;&#32423;&#25968;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36827;&#34892;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10369v2 Announce Type: replace  Abstract: Inference efforts -- required to compute partition function, $Z$, of an Ising model over a graph of $N$ ``spins" -- are most likely exponential in $N$. Efficient variational methods, such as Belief Propagation (BP) and Tree Re-Weighted (TRW) algorithms, compute $Z$ approximately minimizing respective (BP- or TRW-) free energy. We generalize the variational scheme building a $\lambda$-fractional-homotopy, $Z^{(\lambda)}$, where $\lambda=0$ and $\lambda=1$ correspond to TRW- and BP-approximations, respectively, and $Z^{(\lambda)}$ decreases with $\lambda$ monotonically. Moreover, this fractional scheme guarantees that in the attractive (ferromagnetic) case $Z^{(TRW)}\geq Z^{(\lambda)}\geq Z^{(BP)}$, and there exists a unique (``exact") $\lambda_*$ such that, $Z=Z^{(\lambda_*)}$. Generalizing the re-parametrization approach of \citep{wainwright_tree-based_2002} and the loop series approach of \citep{chertkov_loop_2006}, we show how to e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.12231</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;
&lt;/p&gt;
&lt;p&gt;
Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#26041;&#27861;DisCo&#65292;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#22270;&#30340;&#39640;&#25928;&#32553;&#20957;&#65292;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#21387;&#32553;&#22270;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#35299;&#32544;&#32467;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#25216;&#26415;&#65292;&#20026;&#22823;&#35268;&#27169;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32039;&#20945;&#20294;&#20449;&#24687;&#20016;&#23500;&#30340;&#23567;&#22270;&#65292;&#20197;&#33410;&#30465;&#22823;&#35268;&#27169;&#22270;&#23398;&#20064;&#30340;&#26114;&#36149;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#20808;&#21069;&#30340;&#22270;&#35299;&#32544;&#32467;&#26041;&#27861;&#24120;&#24120;&#37319;&#29992;&#32416;&#32544;&#30340;&#32553;&#20957;&#31574;&#30053;&#65292;&#21516;&#26102;&#28041;&#21450;&#33410;&#28857;&#21644;&#36793;&#30340;&#32553;&#20957;&#65292;&#23548;&#33268;&#22823;&#37327;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#12290;&#36825;&#31181;&#32416;&#32544;&#30340;&#31574;&#30053;&#26497;&#22823;&#22320;&#38459;&#30861;&#20102;&#22270;&#35299;&#32544;&#32467;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#21066;&#24369;&#20102;&#23427;&#23545;&#26497;&#22823;&#35268;&#27169;&#22270;&#30340;&#32553;&#20957;&#21644;&#39640;&#20445;&#30495;&#24230;&#21387;&#32553;&#22270;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22823;&#35268;&#27169;&#22270;&#30340;&#35299;&#32544;&#32467;&#20957;&#32858;&#65292;&#31616;&#31216;&#20026;DisCo&#65292;&#20197;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#22270;&#35299;&#32544;&#32467;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#35268;&#27169;&#30340;&#22270;&#12290;DisCo&#30340;&#26680;&#24515;&#26159;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#65292;&#21363;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#27169;&#22359;&#65292;&#22312;&#35299;&#32544;&#30340;&#26041;&#24335;&#19979;&#23454;&#29616;&#33410;&#28857;&#21644;&#36793;&#30340;&#20957;&#32858;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph condensation has emerged as an intriguing technique to provide Graph Neural Networks for large-scale graphs with a more compact yet informative small graph to save the expensive costs of large-scale graph learning. Despite the promising results achieved, previous graph condensation methods often employ an entangled condensation strategy that involves condensing nodes and edges simultaneously, leading to substantial GPU memory demands. This entangled strategy has considerably impeded the scalability of graph condensation, impairing its capability to condense extremely large-scale graphs and produce condensed graphs with high fidelity. Therefore, this paper presents Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to provide scalable graph condensation for graphs of varying sizes. At the heart of DisCo are two complementary components, namely node and edge condensation modules, that realize the condensation of nodes and edges in a disentangled manner. In the 
&lt;/p&gt;</description></item><item><title>&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.11694</link><description>&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Parametric Matrix Models. (arXiv:2401.11694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11694
&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#35774;&#35745;&#65292;&#36890;&#36807;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#36827;&#34892;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312;&#35745;&#31639;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22522;&#20110;&#30697;&#38453;&#26041;&#31243;&#65292;&#24182;&#19988;&#20854;&#35774;&#35745;&#21463;&#21040;&#20102;&#29992;&#20110;&#36817;&#20284;&#35299;&#21442;&#25968;&#26041;&#31243;&#30340;&#31616;&#21270;&#22522;&#30784;&#26041;&#27861;&#30340;&#25928;&#29575;&#21551;&#21457;&#12290;&#20381;&#36182;&#21464;&#37327;&#21487;&#20197;&#38544;&#24335;&#25110;&#26174;&#24335;&#23450;&#20041;&#65292;&#24182;&#19988;&#26041;&#31243;&#21487;&#20197;&#20351;&#29992;&#20195;&#25968;&#12289;&#24494;&#20998;&#25110;&#31215;&#20998;&#20851;&#31995;&#12290;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#21487;&#20197;&#20165;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#35745;&#31639;&#12290;&#34429;&#28982;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#65292;&#20294;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#26159;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#22312;&#20171;&#32461;&#22522;&#30784;&#29702;&#35770;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25361;&#25112;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#25152;&#26377;&#22312;&#36825;&#37324;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#21442;&#25968;&#30697;&#38453;&#27169;&#22411;&#22312;&#20801;&#35768;&#35745;&#31639;&#30340;&#26694;&#26550;&#20869;&#20135;&#29983;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a general class of machine learning algorithms called parametric matrix models. Parametric matrix models are based on matrix equations, and the design is motivated by the efficiency of reduced basis methods for approximating solutions of parametric equations. The dependent variables can be defined implicitly or explicitly, and the equations may use algebraic, differential, or integral relations. Parametric matrix models can be trained with empirical data only, and no high-fidelity model calculations are needed. While originally designed for scientific computing, parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within a computational framework that allows 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2312.11514</link><description>&lt;p&gt;
&#38378;&#23384;LLM&#65306;&#22312;&#26377;&#38480;&#20869;&#23384;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM in a flash: Efficient Large Language Model Inference with Limited Memory. (arXiv:2312.11514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#20869;&#23384;&#26465;&#20214;&#19979;&#39640;&#25928;&#36816;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#24182;&#25353;&#38656;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26500;&#24314;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#24182;&#20248;&#21270;&#25968;&#25454;&#20256;&#36755;&#21644;&#35835;&#21462;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#31383;&#21475;&#21270;&#21644;&#34892;&#21015;&#32465;&#23450;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;DRAM&#23481;&#37327;&#30340;&#35774;&#22791;&#32780;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38378;&#23384;&#20013;&#65292;&#24182;&#25353;&#38656;&#23558;&#20854;&#20256;&#36755;&#21040;DRAM&#30340;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#36229;&#36807;&#21487;&#29992;DRAM&#23481;&#37327;&#30340;LLM&#39640;&#25928;&#36816;&#34892;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#38378;&#23384;&#29305;&#24615;&#30340;&#25512;&#29702;&#25104;&#26412;&#27169;&#22411;&#65292;&#24341;&#23548;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#65306;&#20943;&#23569;&#20174;&#38378;&#23384;&#20256;&#36755;&#30340;&#25968;&#25454;&#37327;&#65292;&#24182;&#20197;&#36739;&#22823;&#12289;&#26356;&#36830;&#32493;&#30340;&#22359;&#35835;&#21462;&#25968;&#25454;&#12290;&#22312;&#36825;&#20010;&#21463;&#30828;&#20214;&#21551;&#21457;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#20027;&#35201;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#8220;&#31383;&#21475;&#21270;&#8221;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#20043;&#21069;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#26469;&#31574;&#30053;&#24615;&#22320;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#65292;&#20854;&#27425;&#65292;&#8220;&#34892;&#21015;&#32465;&#23450;&#8221;&#36866;&#24212;&#20102;&#38378;&#23384;&#30340;&#39034;&#24207;&#25968;&#25454;&#35775;&#38382;&#29305;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First, "windowing" strategically reduces data transfer by reusing previously activated neurons, and second, "row-column bundling", tailored to the sequential data access strengths of flash memory, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.19967</link><description>&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#20197;&#25913;&#21892;&#36716;&#35786;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis to improve referrals using multimodal machine learning from blood testing, semi-structured and unstructured patient records. (arXiv:2310.19967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19967
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#20174;&#34880;&#28082;&#27979;&#35797;&#12289;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#30149;&#21382;&#20013;&#23398;&#20064;&#65292;&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#21487;&#20197;&#25913;&#21892;&#36716;&#35786;&#65292;&#20026;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;&#30149;&#24773;&#24694;&#21270;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#28814;&#30151;&#24615;&#20851;&#33410;&#28814;&#65288;IA&#65289;&#23545;&#20110;&#39640;&#25928;&#20934;&#30830;&#30340;&#21307;&#38498;&#36716;&#35786;&#20998;&#27969;&#20197;&#21450;&#21450;&#26102;&#27835;&#30103;&#21644;&#39044;&#38450;IA&#30142;&#30149;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#26377;&#38480;&#30340;&#21307;&#30103;&#36164;&#28304;&#19979;&#12290;&#25163;&#21160;&#35780;&#20272;&#27969;&#31243;&#26159;&#30446;&#21069;&#23454;&#36341;&#20013;&#26368;&#24120;&#35265;&#30340;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#20302;&#25928;&#29575;&#12290;&#27599;&#20010;&#20174;&#19968;&#33324;&#23454;&#36341;&#65288;GP&#65289;&#21040;&#21307;&#38498;&#30340;&#36716;&#35786;&#37117;&#38656;&#35201;&#35780;&#20272;&#22823;&#37327;&#30340;&#20020;&#24202;&#20449;&#24687;&#12290;&#26426;&#22120;&#23398;&#20064;&#22312;&#33258;&#21160;&#21270;&#37325;&#22797;&#35780;&#20272;&#20219;&#21153;&#21644;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#26089;&#26399;&#26816;&#27979;IA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IA&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#34880;&#28082;&#27979;&#35797;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#34880;&#28082;&#27979;&#35797;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#22312;&#36716;&#35786;&#26102;&#21487;&#29992;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#21033;&#29992;&#21322;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#26089;&#26399;&#26816;&#27979;IA&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;&#21644;
&lt;/p&gt;
&lt;p&gt;
Early detection of inflammatory arthritis (IA) is critical to efficient and accurate hospital referral triage for timely treatment and preventing the deterioration of the IA disease course, especially under limited healthcare resources. The manual assessment process is the most common approach in practice for the early detection of IA, but it is extremely labor-intensive and inefficient. A large amount of clinical information needs to be assessed for every referral from General Practice (GP) to the hospitals. Machine learning shows great potential in automating repetitive assessment tasks and providing decision support for the early detection of IA. However, most machine learning-based methods for IA detection rely on blood testing results. But in practice, blood testing data is not always available at the point of referrals, so we need methods to leverage multimodal data such as semi-structured and unstructured data for early detection of IA. In this research, we present fusion and en
&lt;/p&gt;</description></item><item><title>&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.15469</link><description>&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15469
&lt;/p&gt;
&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2018&#24180;&#21518;&#30340;&#26102;&#20195;&#26631;&#24535;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;OpenAI&#30340;ChatGPT&#31561;&#21019;&#26032;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#38543;&#30528;&#34892;&#19994;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#20154;&#31867;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#20063;&#20986;&#29616;&#20102;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#25454;&#33719;&#21462;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#24847;&#22806;&#31215;&#32047;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#22806;&#30340;PII&#27844;&#38706;&#39118;&#38505;&#12290;&#34429;&#28982;&#20687;RLHF&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#36825;&#26679;&#30340;&#31574;&#30053;&#24050;&#34987;&#29992;&#26469;&#25511;&#21046;&#38544;&#31169;&#20405;&#26435;&#30340;&#39118;&#38505;&#65292;&#20294;LLM&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#20197;OpenAI&#30340;GPT-3.5&#30340;&#24494;&#35843;&#30028;&#38754;&#20026;&#20195;&#34920;&#65289;&#37325;&#26032;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#26377;&#20154;&#21487;&#33021;&#20250;&#38382;&#65306;LLM&#30340;&#24494;&#35843;&#26159;&#21542;&#20250;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#30340;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#65311;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#23581;&#35797;&#23547;&#27714;&#31572;&#26696;&#30340;&#21162;&#21147;&#65292;&#37325;&#28857;&#26159;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#37117;&#21487;&#20197;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05954</link><description>&lt;p&gt;
Raman&#25918;&#22823;&#22120;&#30340;&#20248;&#21270;&#65306;&#40657;&#30418;&#12289;&#28784;&#30418;&#21644;&#30333;&#30418;&#27169;&#22411;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Optimization of Raman amplifiers: a comparison between black-, grey- and white-box modeling. (arXiv:2310.05954v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05954
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#37117;&#21487;&#20197;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20809;&#36890;&#20449;&#31995;&#32479;&#19981;&#26029;&#21162;&#21147;&#25552;&#39640;&#21534;&#21520;&#37327;&#30340;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#21644;&#20248;&#21270;&#20809;&#25918;&#22823;&#22120;&#20197;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#31163;&#32447;&#20248;&#21270;&#20809;&#25918;&#22823;&#22120;&#20381;&#36182;&#20110;&#20174;&#28145;&#20837;&#29289;&#29702;&#23398;&#30340;&#30333;&#30418;&#27169;&#22411;&#21040;&#25968;&#25454;&#39537;&#21160;&#30340;&#19982;&#29289;&#29702;&#26080;&#20851;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#30333;&#30418;&#12289;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#22312;&#21452;&#21521;Raman&#25918;&#22823;&#22120;&#20013;&#23454;&#29616;&#30446;&#26631;&#39057;&#36317;&#25918;&#22823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30740;&#31350;&#30340;&#20219;&#20309;&#19968;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#22312;100&#20844;&#37324;&#33539;&#22260;&#20869;&#23454;&#29616;C&#27874;&#27573;&#19979;&#36798;&#21040;1dB&#30340;&#39057;&#36317;&#24179;&#22374;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#30446;&#26631;&#24212;&#29992;&#22330;&#26223;&#35752;&#35770;&#20102;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12289;&#20248;&#21183;&#21644;&#32570;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#20248;&#21270;&#36895;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing and optimizing optical amplifiers to maximize system performance is becoming increasingly important as optical communication systems strive to increase throughput. Offline optimization of optical amplifiers relies on models ranging from white-box models deeply rooted in physics to black-box data-driven physics-agnostic models. Here, we compare the capabilities of white-, grey- and black-box models to achieve a target frequency-distance amplification in a bidirectional Raman amplifier. We show that any of the studied methods can achieve down to 1 dB of frequency-distance flatness over the C-band in a 100-km span. Then, we discuss the models' applicability, advantages, and drawbacks based on the target application scenario, in particular in terms of optimization speed and access to training data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16491</link><description>&lt;p&gt;
&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65306;&#25945;&#23398;&#29983;&#65292;&#21516;&#26102;&#27979;&#35797;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#27491;&#38754;&#20020;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#32435;&#20837;&#35838;&#22530;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#19968;&#26041;&#27861;&#26159;&#21542;&#21487;&#34892;&#65292;&#22914;&#26524;&#21487;&#34892;&#65292;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;-&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;-&#24212;&#35813;&#26399;&#26395;&#20160;&#20040;&#12290;&#23398;&#29983;&#33021;&#22815;&#22312;&#35838;&#22530;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#21527;&#65311;&#25945;&#32946;&#32773;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#22914;&#20309;&#65311;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#22914;&#20309;&#24110;&#21161;&#35780;&#20272;&#21644;&#25913;&#36827;&#31185;&#23398;&#30340;&#29616;&#29366;&#65311;&#26412;&#30740;&#31350;&#22312;EPFL&#25945;&#25480;&#30340;&#24212;&#29992;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#65288;CS-401&#65289;&#30340;&#39033;&#30446;&#37096;&#20998;&#20013;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65288;N=354&#21517;&#23398;&#29983;&#65289;&#12290;&#22312;&#27492;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#35838;&#31243;&#26399;&#38388;&#36827;&#34892;&#30340;&#35843;&#26597;&#25552;&#21069;&#36827;&#34892;&#27880;&#20876;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#29983;&#21487;&#20197;&#22797;&#21046;&#20808;&#21069;&#21457;&#34920;&#30340;&#31185;&#23398;&#35770;&#25991;&#65292;&#22823;&#37096;&#20998;&#26159;&#23450;&#24615;&#30340;&#65292;&#26377;&#20123;&#26159;&#23436;&#20840;&#19968;&#26679;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;</title><link>http://arxiv.org/abs/2308.09175</link><description>&lt;p&gt;
&#25193;&#23637;AI&#65306;&#21521;&#25317;&#26377;&#21019;&#36896;&#24615;&#30340;AlphaZero&#22269;&#38469;&#35937;&#26827;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Diversifying AI: Towards Creative Chess with AlphaZero. (arXiv:2308.09175v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;AI&#22312;&#35745;&#31639;&#20219;&#21153;&#20013;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;AI&#31995;&#32479;&#22242;&#38431;&#65292;&#22312;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#36229;&#36234;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#24819;&#27861;&#12290;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21508;&#31181;&#35745;&#31639;&#20219;&#21153;&#19978;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19968;&#26679;&#65292;AI&#31995;&#32479;&#20063;&#20250;&#29359;&#38169;&#35823;&#65292;&#26377;&#30450;&#28857;&#65292;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#26032;&#24773;&#20917;&#26102;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24403;AI&#31995;&#32479;&#30340;&#35745;&#31639;&#21512;&#29702;&#24615;&#25512;&#21040;&#26497;&#38480;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#20174;&#21019;&#36896;&#24615;&#30340;&#20915;&#31574;&#26426;&#21046;&#20013;&#21463;&#30410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#36890;&#36807;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#30340;&#22810;&#26679;&#21270;AI&#31995;&#32479;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#21487;&#20197;&#32988;&#36807;&#21333;&#20010;AI&#65292;&#36890;&#36807;&#29983;&#25104;&#26356;&#22810;&#30340;&#24819;&#27861;&#65292;&#28982;&#21518;&#36873;&#25321;&#26368;&#22909;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#20197;&#22269;&#38469;&#35937;&#26827;&#36825;&#20010;&#34987;&#31216;&#20026;AI&#26524;&#34631;&#30340;&#28216;&#25103;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;AlphaZero (AZ)&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#28508;&#21464;&#26465;&#20214;&#26550;&#26500;&#25193;&#23637;&#23427;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20195;&#29702;&#22242;&#38431;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;AZ_db&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#22810;&#26679;&#24615;&#25216;&#26415;&#23545;AZ_db&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#65292;&#24182;&#36890;&#36807;&#27425;&#21152;&#24615;&#35745;&#21010;&#36873;&#25321;&#26368;&#26377;&#24076;&#26395;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AZ_db&#20197;&#19981;&#21516;&#26041;&#24335;&#19979;&#22269;&#38469;&#35937;&#26827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) systems have surpassed human intelligence in a variety of computational tasks. However, AI systems, like humans, make mistakes, have blind spots, hallucinate, and struggle to generalize to new situations. This work explores whether AI can benefit from creative decision-making mechanisms when pushed to the limits of its computational rationality. In particular, we investigate whether a team of diverse AI systems can outperform a single AI in challenging tasks by generating more ideas as a group and then selecting the best ones. We study this question in the game of chess, the so-called drosophila of AI. We build on AlphaZero (AZ) and extend it to represent a league of agents via a latent-conditioned architecture, which we call AZ_db. We train AZ_db to generate a wider range of ideas using behavioral diversity techniques and select the most promising ones with sub-additive planning. Our experiments suggest that AZ_db plays chess in diverse wa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#20197;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.06709</link><description>&lt;p&gt;
&#30828;&#32422;&#26463;PINNs&#29992;&#20110;&#30028;&#38754;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Hard-Constraint PINNs for Interface Optimal Control Problems. (arXiv:2308.06709v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#20197;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#19982;&#26368;&#36817;&#24320;&#21457;&#30340;&#19981;&#36830;&#32493;&#24615;&#25429;&#33719;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#30028;&#38754;&#21644;&#19968;&#20123;&#25511;&#21046;&#32422;&#26463;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20248;&#21270;&#25511;&#21046;&#38382;&#39064;&#30340;&#27714;&#35299;&#12290;&#35813;&#31639;&#27861;&#26080;&#32593;&#26684;&#19988;&#21487;&#25193;&#23637;&#21040;&#19981;&#21516;&#30340;PDE&#65292;&#24182;&#30830;&#20445;&#20005;&#26684;&#28385;&#36275;&#25511;&#21046;&#32422;&#26463;&#12290;&#30001;&#20110;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20197;&#21450;PDE&#37117;&#34987;&#35270;&#20026;&#36719;&#32422;&#26463;&#65292;&#36890;&#36807;&#23558;&#23427;&#20204;&#27719;&#24635;&#21040;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#27492;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#23427;&#20204;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#33021;&#22815;&#23436;&#20840;&#28385;&#36275;&#12290;&#36825;&#31435;&#21363;&#24341;&#36215;&#20102;&#22312;&#30456;&#24212;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#35843;&#25972;&#26435;&#37325;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#24182;&#30830;&#20445;&#25968;&#20540;&#31934;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;PINNs&#20013;&#23558;&#36793;&#30028;&#21644;&#30028;&#38754;&#26465;&#20214;&#20316;&#20026;&#30828;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the physics-informed neural networks (PINNs), in combination with some recently developed discontinuity capturing neural networks, can be applied to solve optimal control problems subject to partial differential equations (PDEs) with interfaces and some control constraints. The resulting algorithm is mesh-free and scalable to different PDEs, and it ensures the control constraints rigorously. Since the boundary and interface conditions, as well as the PDEs, are all treated as soft constraints by lumping them into a weighted loss function, it is necessary to learn them simultaneously and there is no guarantee that the boundary and interface conditions can be satisfied exactly. This immediately causes difficulties in tuning the weights in the corresponding loss function and training the neural networks. To tackle these difficulties and guarantee the numerical accuracy, we propose to impose the boundary and interface conditions as hard constraints in PINNs by developing a nove
&lt;/p&gt;</description></item><item><title>SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04430</link><description>&lt;p&gt;
SILO&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#23384;&#20648;&#20013;&#38548;&#31163;&#27861;&#24459;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04430
&lt;/p&gt;
&lt;p&gt;
SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#22312;&#21463;&#29256;&#26435;&#25110;&#21463;&#20854;&#20182;&#38480;&#21046;&#30340;&#25968;&#25454;&#19978;&#30340;&#21512;&#27861;&#24615;&#36827;&#34892;&#28608;&#28872;&#36777;&#35770;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#22312;&#20302;&#39118;&#38505;&#25991;&#26412;&#65288;&#20363;&#22914;&#36807;&#26399;&#29256;&#26435;&#22270;&#20070;&#25110;&#25919;&#24220;&#25991;&#20214;&#65289;&#19978;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#35813;&#25991;&#26412;&#30340;&#35268;&#27169;&#21644;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SILO&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#31181;&#39118;&#38505;-&#24615;&#33021;&#26435;&#34913;&#12290;SILO&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#26500;&#24314;&#65306;&#65288;1&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#26032;&#35821;&#26009;&#24211;&#8220;&#24320;&#25918;&#35768;&#21487;&#35777;&#35821;&#26009;&#24211;&#8221;&#65288;OLC&#65289;&#19978;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;LM&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;228B&#20010;&#20844;&#20849;&#39046;&#22495;&#21644;&#35768;&#21487;&#25991;&#26412;&#12290;&#65288;2&#65289;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#65288;&#20363;&#22914;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20070;&#25110;&#26032;&#38395;&#30340;&#25968;&#25454;&#65289;&#23545;&#20854;&#36827;&#34892;&#25193;&#20805;&#65292;&#35813;&#25968;&#25454;&#23384;&#20648;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#34987;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#23384;&#20648;&#20801;&#35768;&#20351;&#29992;&#39640;&#39118;&#38505;&#25968;&#25454;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#25903;&#25345;&#21477;&#32423;&#25968;&#25454;&#24402;&#23646;&#65292;&#24182;&#20351;&#25968;&#25454;&#29983;&#20135;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#23384;&#20648;&#20013;&#21024;&#38500;&#20869;&#23481;&#26469;&#36873;&#25321;&#36864;&#20986;&#27169;&#22411;&#12290;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#20419;&#36827;&#23545;&#25968;&#25454;&#20351;&#29992;&#35268;&#33539;&#30340;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16189</link><description>&lt;p&gt;
&#28779;&#26143;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65306;&#19968;&#31181;&#22810;&#23610;&#24230;&#23884;&#22871;&#26041;&#27861;&#20013;&#30340;&#22240;&#23376;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28304;&#20998;&#31163;&#28041;&#21450;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#35760;&#24405;&#30340;&#26410;&#30693;&#28304;&#20449;&#21495;&#30340;&#20998;&#35299;&#65292;&#20854;&#20013;&#23545;&#28304;&#30340;&#20808;&#39564;&#30693;&#35782;&#26377;&#38480;&#65292;&#20165;&#21487;&#20197;&#35775;&#38382;&#20449;&#21495;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19981;&#36866;&#29992;&#30340;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#21463;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#28304;&#23637;&#29616;&#20986;&#30340;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;(1)&#27010;&#29575;&#22320;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#28304;&#36827;&#34892;&#32858;&#31867;&#21644;&#36880;&#23618;&#38750;&#30417;&#30563;&#28304;&#20998;&#31163;&#65292;(2)&#22312;&#27599;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#65292;(3)&#23398;&#20064;&#28304;&#20449;&#21495;&#30340;&#22240;&#23376;&#34920;&#31034;&#65292;(4)&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#29983;&#25104;&#26410;&#30693;&#28304;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;MRO&#19978;&#30340;&#19977;&#20010;&#39057;&#36947;&#30340;&#21487;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
&lt;/p&gt;</description></item></channel></rss>