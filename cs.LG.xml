<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16582</link><description>&lt;p&gt;
&#22312;&#21033;&#29992;&#20840;&#29699;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20998;&#31867;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16582
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20998;&#31867;&#22312;&#30740;&#31350;&#20316;&#29289;&#27169;&#24335;&#21464;&#21270;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#30899;&#22266;&#23384;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#21033;&#29992;&#21508;&#31181;&#26102;&#38388;&#25968;&#25454;&#28304;&#26159;&#24517;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32423;&#34920;&#31034;&#20197;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#29486;&#23545;&#22810;&#35270;&#22270;&#23398;&#20064;&#65288;MVL&#65289;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#20855;&#26377;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;&#23616;&#37096;&#22320;&#21306;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20892;&#30000;&#22303;&#22320;&#21644;&#20316;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06023</link><description>&lt;p&gt;
&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#20197;&#21450;&#31038;&#20132;&#23186;&#20307;&#19978;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Persian Slang Text Conversion to Formal and Deep Learning of Persian Short Texts on Social Media for Sentiment Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06023
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;PSC&#24037;&#20855;&#23558;&#27874;&#26031;&#35821;&#20442;&#35821;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#36866;&#21512;&#20998;&#26512;&#27874;&#26031;&#35821;&#20250;&#35805;&#25991;&#26412;&#30340;&#24037;&#20855;&#20351;&#24471;&#23545;&#36825;&#20123;&#25991;&#26412;&#65288;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#65289;&#30340;&#21508;&#31181;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#25552;&#20379;PSC&#65288;&#27874;&#26031;&#35821;&#20442;&#35821;&#36716;&#25442;&#22120;&#65289;&#65292;&#23558;&#20250;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;&#27491;&#24335;&#25991;&#26412;&#65292;&#24182;&#32467;&#21512;&#26368;&#26032;&#21644;&#26368;&#20339;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#26356;&#23481;&#26131;&#29702;&#35299;&#36825;&#20123;&#25991;&#26412;&#65292;&#26356;&#22909;&#22320;&#36827;&#34892;&#27874;&#26031;&#35821;&#30701;&#25991;&#26412;&#30340;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06023v1 Announce Type: new  Abstract: The lack of a suitable tool for the analysis of conversational texts in the Persian language has made various analyses of these texts, including Sentiment Analysis, difficult. In this research, we tried to make the understanding of these texts easier for the machine by providing PSC, Persian Slang Converter, a tool for converting conversational texts into formal ones, and by using the most up-to-date and best deep learning methods along with the PSC, the sentiment learning of short Persian language texts for the machine in a better way. be made More than 10 million unlabeled texts from various social networks and movie subtitles (as Conversational texts) and about 10 million news texts (as formal texts) have been used for training unsupervised models and formal implementation of the tool. 60,000 texts from the comments of Instagram social network users with positive, negative, and neutral labels are considered supervised data for trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#35780;&#20272;&#22810;&#20219;&#21153;PIML&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181;PDE&#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.11126</link><description>&lt;p&gt;
Kolmogorov n-&#23485;&#24230;&#29992;&#20110;&#22810;&#20219;&#21153;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#26041;&#27861;&#65306;&#26397;&#21521;&#31283;&#20581;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning (PIML) Methods: Towards Robust Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#35780;&#20272;&#22810;&#20219;&#21153;PIML&#26550;&#26500;&#30340;&#26377;&#25928;&#24615;&#30340;&#26041;&#27861;&#65292;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181;PDE&#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11126v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#20316;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#25163;&#27573;&#65292;&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#22312;&#35745;&#31639;&#31185;&#23398;&#19982;&#24037;&#31243;&#65288;CS&amp;E&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290; &#36825;&#20010;&#35805;&#39064;&#28085;&#30422;&#20102;&#26088;&#22312;&#35299;&#20915;&#21333;&#20010;&#25110;&#22810;&#20010;PDE&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#27169;&#22411;&#65292;&#31216;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290; PIML &#30340;&#29305;&#28857;&#26159;&#22312;&#35299;&#20915; PDE &#38382;&#39064;&#26102;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#32780;&#19981;&#26159;&#22823;&#25968;&#25454;&#12290; &#23613;&#31649;&#36825;&#19968;&#31995;&#21015;&#26041;&#27861;&#30340;&#25972;&#20307;&#25104;&#21151;&#65292;&#20294;&#20998;&#26512;&#12289;&#22522;&#20934;&#27979;&#35797;&#21644;&#36890;&#24120;&#27604;&#36739;&#19968;&#31181;&#26041;&#27861;&#19982;&#21478;&#19968;&#31181;&#26041;&#27861;&#20173;&#28982;&#38750;&#24120;&#22256;&#38590;&#12290; &#25105;&#20204;&#20351;&#29992; Kolmogorov n-&#23485;&#24230;&#20316;&#20026;&#36817;&#20284;&#20989;&#25968;&#26377;&#25928;&#24615;&#30340;&#34913;&#37327;&#26631;&#20934;&#65292;&#23457;&#24910;&#22320;&#23558;&#36825;&#19968;&#25351;&#26631;&#24212;&#29992;&#20110;&#27604;&#36739;&#21508;&#31181;&#22810;&#20219;&#21153; PIML &#32467;&#26500;&#12290; &#25105;&#20204;&#35745;&#31639;&#36739;&#20302;&#30340;&#20934;&#30830;&#24230;&#19979;&#30028;&#65292;&#24182;&#20998;&#26512;&#27169;&#22411;&#22312;&#21508;&#31181; PDE &#38382;&#39064;&#19978;&#23398;&#21040;&#30340;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11126v1 Announce Type: new  Abstract: Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDE) has garnered much attention in the Computational Science and Engineering (CS&amp;E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model's learned basis functions on various PDE problems. This is the fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#32477;&#23545;&#26143;&#31561;$M_B$&#36827;&#34892;&#32422;&#26463;&#65292;&#24182;&#21457;&#29616;&#22312;$z\approx 1$&#21306;&#22495;&#23384;&#22312;&#36716;&#25240;&#32418;&#31227;&#36857;&#35937;</title><link>https://arxiv.org/abs/2402.10502</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;$M_B$&#30340;&#26202;&#26399;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Late-time transition of $M_B$ inferred via neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#32477;&#23545;&#26143;&#31561;$M_B$&#36827;&#34892;&#32422;&#26463;&#65292;&#24182;&#21457;&#29616;&#22312;$z\approx 1$&#21306;&#22495;&#23384;&#22312;&#36716;&#25240;&#32418;&#31227;&#36857;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23431;&#23449;&#21442;&#25968;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#23548;&#33268;&#23545;&#26631;&#20934;&#23431;&#23449;&#23398;&#22522;&#26412;&#26041;&#38754;&#30340;&#37325;&#26032;&#32771;&#34385;&#12290;&#21704;&#21187;&#24120;&#25968;&#30340;&#32039;&#24352;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#23616;&#37096;&#21644;&#26089;&#26399;&#23431;&#23449;&#23545;Ia&#22411;&#36229;&#26032;&#26143;&#32477;&#23545;&#26143;&#31561;$ M_B $&#30340;&#32422;&#26463;&#20043;&#38388;&#30340;&#32039;&#24352;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#37325;&#26032;&#32771;&#34385;&#20102;&#35813;&#21442;&#25968;&#30340;&#21464;&#21270;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26080;&#20559;&#22320;&#38480;&#21046;&#32477;&#23545;&#26143;&#31561;&#20540;&#65292;&#24182;&#35780;&#20272;&#19982;Pantheon+&#27719;&#32534;&#20013;$ M_B $&#38543;&#32418;&#31227;&#21464;&#21270;&#30340;&#24433;&#21709;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#20197;&#21450;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;$ z\approx 1 $&#21306;&#22495;&#30340;&#36716;&#25240;&#32418;&#31227;&#30340;&#36857;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10502v1 Announce Type: cross  Abstract: The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology. The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In this work, we reconsider the possibility of a variation of this parameter in a model-independent way. We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture. We find an indication for a transition redshift at the $z\approx 1$ region.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#36890;&#20449;&#23454;&#29616;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#25191;&#34892;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10064</link><description>&lt;p&gt;
&#23548;&#33322;&#29577;&#31859;&#65306;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;
&lt;/p&gt;
&lt;p&gt;
Navigating the Maize: Cyclic and conditional computational graphs for molecular simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#27169;&#25311;&#30340;&#24490;&#29615;&#21644;&#26465;&#20214;&#35745;&#31639;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#21270;&#21644;&#36890;&#20449;&#23454;&#29616;&#20219;&#24847;&#22270;&#32467;&#26500;&#30340;&#25191;&#34892;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#21270;&#23398;&#21644;&#20998;&#23376;&#27169;&#25311;&#24037;&#20316;&#27969;&#31243;&#21487;&#20197;&#34920;&#31034;&#20026;&#35745;&#31639;&#22270;&#12290;&#36825;&#31181;&#25277;&#35937;&#26377;&#21161;&#20110;&#27169;&#22359;&#21270;&#21644;&#28508;&#22312;&#22320;&#37325;&#29992;&#29616;&#26377;&#32452;&#20214;&#65292;&#24182;&#25552;&#20379;&#24182;&#34892;&#21270;&#21644;&#26131;&#20110;&#22797;&#21046;&#12290;&#29616;&#26377;&#24037;&#20855;&#23558;&#35745;&#31639;&#34920;&#31034;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#20174;&#32780;&#36890;&#36807;&#24182;&#34892;&#21270;&#24182;&#21457;&#20998;&#25903;&#26469;&#23454;&#29616;&#39640;&#25928;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#34920;&#31034;&#24490;&#29615;&#21644;&#26465;&#20214;&#24037;&#20316;&#27969;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Maize&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#31243;&#32534;&#31243;&#21407;&#29702;&#30340;&#12289;&#29992;&#20110;&#24490;&#29615;&#21644;&#26465;&#20214;&#22270;&#30340;&#24037;&#20316;&#27969;&#31649;&#29702;&#22120;&#12290;&#36890;&#36807;&#22312;&#21333;&#29420;&#30340;&#36827;&#31243;&#20013;&#21516;&#26102;&#36816;&#34892;&#22270;&#20013;&#30340;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20219;&#20309;&#26102;&#38388;&#36890;&#36807;&#19987;&#29992;&#30340;&#33410;&#28857;&#38388;&#36890;&#36947;&#36827;&#34892;&#36890;&#20449;&#65292;&#21487;&#20197;&#25191;&#34892;&#20219;&#24847;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#35745;&#31639;&#33647;&#29289;&#35774;&#35745;&#20013;&#36827;&#34892;&#21160;&#24577;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#26469;&#23637;&#31034;&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#28041;&#21450;&#20351;&#29992;&#23567;&#20998;&#23376; gen
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10064v1 Announce Type: cross  Abstract: Many computational chemistry and molecular simulation workflows can be expressed as graphs. This abstraction is useful to modularize and potentially reuse existing components, as well as provide parallelization and ease reproducibility. Existing tools represent the computation as a directed acyclic graph (DAG), thus allowing efficient execution by parallelization of concurrent branches. These systems can, however, generally not express cyclic and conditional workflows. We therefore developed Maize, a workflow manager for cyclic and conditional graphs based on the principles of flow-based programming. By running each node of the graph concurrently in separate processes and allowing communication at any time through dedicated inter-node channels, arbitrary graph structures can be executed. We demonstrate the effectiveness of the tool on a dynamic active learning task in computational drug design, involving the use of a small molecule gen
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.02438</link><description>&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and interpretable Support Vector Classification based on the truncated ANOVA decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02438
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25130;&#26029;ANOVA&#20998;&#35299;&#30340;&#24555;&#36895;&#21487;&#35299;&#37322;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#27861;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#21644;&#23569;&#37327;&#32500;&#24230;&#30340;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#26469;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#36827;&#34892;&#39640;&#32500;&#25955;&#20081;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#22312;&#25955;&#20081;&#25968;&#25454;&#19978;&#36827;&#34892;&#20998;&#31867;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36890;&#24120;&#38656;&#35201;&#22788;&#29702;&#35768;&#22810;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#19977;&#35282;&#20989;&#25968;&#25110;&#23567;&#27874;&#30340;&#29305;&#24449;&#26144;&#23556;&#26469;&#35299;&#20915;SVM&#30340;&#21407;&#22987;&#24418;&#24335;&#12290;&#22312;&#23567;&#32500;&#24230;&#35774;&#32622;&#20013;&#65292;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#21644;&#30456;&#20851;&#26041;&#27861;&#26159;&#22788;&#29702;&#25152;&#32771;&#34385;&#22522;&#20989;&#25968;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#38271;&#65292;&#30001;&#20110;&#32500;&#25968;&#28798;&#38590;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;FFT&#30340;&#26041;&#27861;&#21464;&#24471;&#20302;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38480;&#21046;&#33258;&#24049;&#20351;&#29992;&#22810;&#21464;&#37327;&#22522;&#20989;&#25968;&#65292;&#27599;&#20010;&#22522;&#20989;&#25968;&#21482;&#20381;&#36182;&#20110;&#23569;&#25968;&#20960;&#20010;&#32500;&#24230;&#12290;&#36825;&#26159;&#30001;&#20110;&#25928;&#24212;&#30340;&#31232;&#30095;&#24615;&#21644;&#26368;&#36817;&#20851;&#20110;&#20989;&#25968;&#20174;&#25955;&#20081;&#25968;&#25454;&#20013;&#30340;&#25130;&#26029;&#26041;&#24046;&#20998;&#35299;&#30340;&#37325;&#24314;&#30340;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#21160;&#26426;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#32806;&#21512;&#26041;&#38754;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machines (SVMs) are an important tool for performing classification on scattered data, where one usually has to deal with many data points in high-dimensional spaces. We propose solving SVMs in primal form using feature maps based on trigonometric functions or wavelets. In small dimensional settings the Fast Fourier Transform (FFT) and related methods are a powerful tool in order to deal with the considered basis functions. For growing dimensions the classical FFT-based methods become inefficient due to the curse of dimensionality. Therefore, we restrict ourselves to multivariate basis functions, each one of them depends only on a small number of dimensions. This is motivated by the well-known sparsity of effects and recent results regarding the reconstruction of functions from scattered data in terms of truncated analysis of variance (ANOVA) decomposition, which makes the resulting model even interpretable in terms of importance of the features as well as their coupling
&lt;/p&gt;</description></item><item><title>$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.17293</link><description>&lt;p&gt;
$\mu$GUIDE:&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#20351;&#29992;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#25512;&#26029;&#26469;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE: a framework for microstructure imaging via generalized uncertainty-driven inference using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17293
&lt;/p&gt;
&lt;p&gt;
$\mu$GUIDE&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24494;&#32467;&#26500;&#25104;&#20687;&#65292;&#33021;&#22815;&#26377;&#25928;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#37327;&#21270;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mu$GUIDE:&#19968;&#31181;&#36890;&#29992;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20219;&#20309;&#32473;&#23450;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#25110;MRI&#20449;&#21495;&#34920;&#31034;&#20013;&#20272;&#35745;&#32452;&#32455;&#24494;&#32467;&#26500;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36890;&#36807;&#25193;&#25955;&#21152;&#26435;MRI&#30340;&#31034;&#20363;&#28436;&#31034;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#20449;&#21495;&#29305;&#24449;&#36873;&#25321;&#65292;&#32467;&#21512;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#21644;&#21518;&#39564;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;$\mu$GUIDE&#32469;&#36807;&#20102;&#20256;&#32479;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#37319;&#38598;&#32422;&#26463;&#26469;&#23450;&#20041;&#27169;&#22411;&#29305;&#23450;&#30340;&#25688;&#35201;&#32479;&#35745;&#37327;&#12290;&#33719;&#24471;&#30340;&#21518;&#39564;&#20998;&#24067;&#21487;&#31361;&#20986;&#26174;&#31034;&#27169;&#22411;&#23450;&#20041;&#20013;&#23384;&#22312;&#30340;&#36864;&#21270;&#24615;&#65292;&#24182;&#37327;&#21270;&#25152;&#20272;&#35745;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08627</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21644;&#35299;&#37322;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;
&lt;/p&gt;
&lt;p&gt;
Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks. (arXiv:2401.08627v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23646;&#29627;&#29827;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#24207;&#26448;&#26009;&#12290;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#26448;&#26009;&#31185;&#23398;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#23398;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#65292;&#24182;&#30740;&#31350;&#32467;&#26500;&#19982;&#30456;&#24212;&#30340;&#23616;&#37096;&#33021;&#22418;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25454;&#20449;&#33021;&#22815;&#25511;&#21046;&#37329;&#23646;&#29627;&#29827;&#30340;&#35768;&#22810;&#20851;&#38190;&#29289;&#29702;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#33021;&#22418;&#30340;&#26032;&#22411;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SymGNN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#21453;&#23556;&#12290;&#36825;&#31181;&#19981;&#21464;&#24615;&#26159;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#26080;&#27861;&#25429;&#25417;&#30340;&#24615;&#36136;&#12290;SymGNN&#36890;&#36807;&#32858;&#21512;&#22270;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#26469;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#25152;&#26377;&#19977;&#32500;&#27491;&#20132;&#21464;&#25442;&#36827;&#34892;&#26368;&#20248;&#20998;&#24067;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metallic Glasses (MGs) are widely used disordered materials. Understanding the relationship between the local structure and physical properties of MGs is one of the greatest challenges for both material science and condensed matter physics. In this work, we utilize Graph Neural Networks (GNNs) to model the atomic graph structure and study the connection between the structure and the corresponding local energy barrier, which is believed to govern many critical physical properties in MGs. One of our key contributions is to propose a novel Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is invariant under orthogonal transformations of the structure, e.g., rotations and reflections. Such invariance is a desired property that standard GNNs like Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by aggregating over orthogonal transformations of the graph structure for representation learning, and an optimal distribution over all 3D orthogonal 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.00208</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#22120;&#65306;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Transformers as Recognizers of Formal Languages: A Survey on Expressivity. (arXiv:2311.00208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;transformers&#22312;&#24418;&#24335;&#35821;&#35328;&#35782;&#21035;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#20026;&#29702;&#35299;&#20854;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#20174;&#29702;&#35770;&#19978;&#25506;&#35752;&#23427;&#20204;&#33021;&#21542;&#35299;&#20915;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#35270;&#20026;&#24418;&#24335;&#35821;&#35328;&#12290;&#25506;&#32034;&#36825;&#31867;&#38382;&#39064;&#23558;&#26377;&#21161;&#20110;&#27604;&#36739;transformers&#19982;&#20854;&#20182;&#27169;&#22411;&#20197;&#21450;&#19981;&#21516;&#21464;&#31181;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#36825;&#20010;&#23376;&#39046;&#22495;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23545;&#36825;&#26041;&#38754;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35760;&#24405;&#20102;&#19981;&#21516;&#32467;&#26524;&#32972;&#21518;&#30340;&#21508;&#31181;&#20551;&#35774;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#35843;&#30475;&#20284;&#30456;&#20114;&#30683;&#30462;&#30340;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring questions such as this will help to compare transformers with other models, and transformer variants with one another, for various tasks. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.08705</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#19982;&#65288;&#20934;&#65289;&#25928;&#29575;&#65306;&#20449;&#24687;&#20849;&#20139;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Multi-agent RL with (Quasi-)Efficiency: The Blessing of Information Sharing. (arXiv:2308.08705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#20449;&#24687;&#20849;&#20139;&#21644;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#65292;&#25552;&#20986;&#20102;&#26500;&#24314;&#36817;&#20284;&#27169;&#22411;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#38543;&#26426;&#21338;&#24328;&#65288;POSGs&#65289;&#30340;&#21487;&#35777;&#26126;&#22810;Agent&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#20026;&#20102;&#35268;&#36991;&#24050;&#30693;&#30340;&#38590;&#24230;&#38382;&#39064;&#21644;&#20351;&#29992;&#35745;&#31639;&#19981;&#21487;&#34892;&#30340;&#39044;&#35328;&#26426;&#65292;&#25105;&#20204;&#20513;&#23548;&#21033;&#29992;Agent&#20043;&#38388;&#30340;&#28508;&#22312;&#8220;&#20449;&#24687;&#20849;&#20139;&#8221;&#65292;&#36825;&#26159;&#23454;&#35777;MARL&#20013;&#30340;&#24120;&#35265;&#20570;&#27861;&#65292;&#20063;&#26159;&#20855;&#22791;&#36890;&#20449;&#21151;&#33021;&#30340;&#22810;Agent&#25511;&#21046;&#31995;&#32479;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33509;&#24178;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#65292;&#26469;&#35777;&#26126;&#20449;&#24687;&#20849;&#20139;&#30340;&#24517;&#35201;&#24615;&#65292;&#20197;&#21450;&#35266;&#27979;&#21487;&#33021;&#24615;&#20551;&#35774;&#20026;&#20102;&#27714;&#35299;POSGs&#20013;&#30340;&#35745;&#31639;&#25928;&#29575;&#24050;&#32463;&#20351;&#24471;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#21333;Agent&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#20934;&#25928;&#29575;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#36827;&#19968;&#27493;&#8220;&#36817;&#20284;&#8221;&#20849;&#20139;&#30340;&#20844;&#20849;&#20449;&#24687;&#26500;&#24314;POSG&#30340;&#8220;&#36817;&#20284;&#27169;&#22411;&#8221;&#65292;&#22312;&#35813;&#27169;&#22411;&#20013;&#35745;&#21010;&#19968;&#20010;&#36817;&#20284;&#22343;&#34913;&#65288;&#20174;&#35299;&#20915;&#21407;&#22987;POSG&#30340;&#35282;&#24230;&#65289;&#21487;&#20197;&#23454;&#29616;&#20934;&#25928;&#29575;&#65292;&#21363;&#20934;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#21069;&#25552;&#26159;&#19978;&#36848;&#20551;&#35774;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study provable multi-agent reinforcement learning (MARL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \emph{information-sharing} among agents, a common practice in empirical MARL, and a standard model for multi-agent control systems with communications. We first establish several computation complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-efficient single-agent RL with partial observations, for computational efficiency in solving POSGs. We then propose to further \emph{approximate} the shared common information to construct an {approximate model} of the POSG, in which planning an approximate equilibrium (in terms of solving the original POSG) can be quasi-efficient, i.e., of quasi-polynomial-time, under the aforementioned assumptions. Furthermo
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20026;&#21160;&#24577;&#20915;&#31574;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#40065;&#26834;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23558;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.18420</link><description>&lt;p&gt;
&#26041;&#24046;&#20943;&#23569;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity of Variance-reduced Distributionally Robust Q-learning. (arXiv:2305.18420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#20026;&#21160;&#24577;&#20915;&#31574;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#40065;&#26834;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#23558;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#21644;&#24212;&#29992;&#20013;&#65292;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#21160;&#24577;&#20915;&#31574;&#26159;&#22522;&#26412;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#25910;&#38598;&#25152;&#22522;&#20110;&#30340;&#29615;&#22659;&#20998;&#24067;&#21487;&#33021;&#20250;&#19981;&#21516;&#20110;&#27169;&#22411;&#37096;&#32626;&#25152;&#22522;&#20110;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21363;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#21644;&#23427;&#30340;&#26041;&#24046;&#20943;&#23569;&#23545;&#24212;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#23613;&#31649;&#20250;&#38754;&#23545;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#20123;&#31639;&#27861;&#26088;&#22312;&#23558;&#24102;&#26377;Kullback-Leibler&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#26080;&#38480;&#26102;&#22495;$\gamma$-&#25240;&#25187;&#40065;&#26834;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;$q$-&#20989;&#25968;&#20197;&#20803;&#32032;$\epsilon$-&#31934;&#24230;&#26377;&#25928;&#36924;&#36817;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#26041;&#24046;&#20943;&#23569;&#30340;&#20998;&#24067;&#24335;&#40065;&#26834;Q-learning&#23558;&#21516;&#27493;Q-learning&#19982;&#26041;&#24046;&#20943;&#23569;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#20854;&#24615;&#33021;&#65292;&#24182;&#19988;&#25105;&#20204;&#24314;&#31435;&#20102;&#23427;&#36798;&#21040;$ \tilde O(|S||A|(1-\gamma)^{-4}\epsilon^{-4}$&#30340;&#26368;&#23567;&#26368;&#22823;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic decision making under distributional shifts is of fundamental interest in theory and applications of reinforcement learning: The distribution of the environment on which the data is collected can differ from that of the environment on which the model is deployed. This paper presents two novel model-free algorithms, namely the distributionally robust Q-learning and its variance-reduced counterpart, that can effectively learn a robust policy despite distributional shifts. These algorithms are designed to efficiently approximate the $q$-function of an infinite-horizon $\gamma$-discounted robust Markov decision process with Kullback-Leibler uncertainty set to an entry-wise $\epsilon$-degree of precision. Further, the variance-reduced distributionally robust Q-learning combines the synchronous Q-learning with variance-reduction techniques to enhance its performance. Consequently, we establish that it attains a minmax sample complexity upper bound of $\tilde O(|S||A|(1-\gamma)^{-4}\e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11055</link><description>&lt;p&gt;
Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#30340;&#23567;&#22122;&#22768;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Small noise analysis for Tikhonov and RKHS regularizations. (arXiv:2305.11055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21453;&#38382;&#39064;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#22522;&#26412;&#27604;&#36739;&#20998;&#26512;&#20173;&#28982;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#33539;&#25968;&#22312;&#39640;&#26031;&#22122;&#22768;&#30340;&#19981;&#36866;&#23450;&#32447;&#24615;&#21453;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#35813;&#26694;&#26550;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#22312;&#23567;&#22122;&#22768;&#26497;&#38480;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#35843;&#25972;&#20998;&#25968;&#20809;&#28369;&#24230;&#21442;&#25968;&#65292;&#35813;&#31867;&#35206;&#30422;&#20102;L2 Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#22120;&#12290;&#19968;&#20010;&#20196;&#20154;&#24778;&#22855;&#30340;&#35266;&#28857;&#26159;&#65292;&#36890;&#36807;&#36825;&#20123;&#20998;&#25968;&#38454;RKHS&#36827;&#34892;&#36807;&#24230;&#24179;&#28369;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#26368;&#20339;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#34928;&#20943;&#24471;&#22826;&#24555;&#32780;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization plays a pivotal role in ill-posed machine learning and inverse problems. However, the fundamental comparative analysis of various regularization norms remains open. We establish a small noise analysis framework to assess the effects of norms in Tikhonov and RKHS regularizations, in the context of ill-posed linear inverse problems with Gaussian noise. This framework studies the convergence rates of regularized estimators in the small noise limit and reveals the potential instability of the conventional L2-regularizer. We solve such instability by proposing an innovative class of adaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHS regularizations by adjusting the fractional smoothness parameter. A surprising insight is that over-smoothing via these fractional RKHSs consistently yields optimal convergence rates, but the optimal hyper-parameter may decay too fast to be selected in practice.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.11783</link><description>&lt;p&gt;
&#36731;&#37327;&#32423;&#23545;&#27604;&#34507;&#30333;&#36136;&#32467;&#26500;-&#24207;&#21015;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
Lightweight Contrastive Protein Structure-Sequence Transformation. (arXiv:2303.11783v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#36991;&#20813;&#20102;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#34507;&#30333;&#36136;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#26080;&#26631;&#31614;&#30340;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#32467;&#26500;&#27169;&#22411;&#26159;&#20851;&#38190;&#22522;&#30784;&#12290;&#20256;&#32479;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#26041;&#27861;&#36981;&#24490;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20363;&#22914;&#21435;&#22122;&#37325;&#26500;&#21644;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20294;&#36890;&#24120;&#20250;&#30772;&#22351;&#30495;&#23454;&#30340;&#31354;&#38388;&#32467;&#26500;&#34920;&#31034;&#12290;&#20854;&#20182;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#21487;&#33021;&#20250;&#39044;&#27979;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#23450;&#23545;&#35937;&#31867;&#21035;&#65292;&#20854;&#20013;&#21463;&#38480;&#30340;&#30417;&#30563;&#26041;&#24335;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#25351;&#23450;&#20219;&#20309;&#20854;&#20182;&#30340;&#34507;&#30333;&#36136;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#35758;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23545;&#40784;&#26469;&#25351;&#23548;&#32467;&#26500;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#32467;&#26500;&#32422;&#26463;&#65292;&#20197;&#36827;&#19968;&#27493;&#23398;&#20064;&#20869;&#22312;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained protein structure models without labels are crucial foundations for the majority of protein downstream applications. The conventional structure pretraining methods follow the mature natural language pretraining methods such as denoised reconstruction and masked language modeling but usually destroy the real representation of spatial structures. The other common pretraining methods might predict a fixed set of predetermined object categories, where a restricted supervised manner limits their generality and usability as additional labeled data is required to specify any other protein concepts. In this work, we introduce a novel unsupervised protein structure representation pretraining with a robust protein language model. In particular, we first propose to leverage an existing pretrained language model to guide structure model learning through an unsupervised contrastive alignment. In addition, a self-supervised structure constraint is proposed to further learn the intrinsic i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.00180</link><description>&lt;p&gt;
FaceRNET: &#19968;&#31181;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FaceRNET: a Facial Expression Intensity Estimation Network. (arXiv:2303.00180v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FaceRNET&#30340;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#37319;&#29992;&#20102;&#34920;&#31034;&#25552;&#21462;&#22120;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#32452;&#21512;&#65292;&#33021;&#22815;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#35270;&#39057;&#12290;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#35270;&#39057;&#20013;&#36827;&#34892;&#38754;&#37096;&#34920;&#24773;&#24378;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65306;i) &#19968;&#20010;&#34920;&#31034;&#25552;&#21462;&#22120;&#32593;&#32476;&#65292;&#20174;&#27599;&#20010;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#21508;&#31181;&#24773;&#24863;&#25551;&#36848;&#31526;&#65288;&#20215;&#20540;-&#21796;&#37266;&#12289;&#21160;&#20316;&#21333;&#20803;&#21644;&#22522;&#26412;&#34920;&#24773;&#65289;&#65307;ii) &#19968;&#20010;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#25513;&#30721;&#23618;&#65292;&#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#23454;&#29616;&#23545;&#19981;&#21516;&#36755;&#20837;&#35270;&#39057;&#38271;&#24230;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#22312;Hume-Reaction&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our approach for Facial Expression Intensity Estimation from videos. It includes two components: i) a representation extractor network that extracts various emotion descriptors (valence-arousal, action units and basic expressions) from each videoframe; ii) a RNN that captures temporal information in the data, followed by a mask layer which enables handling varying input video lengths through dynamic routing. This approach has been tested on the Hume-Reaction dataset yielding excellent results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#21487;&#34892;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#20351;&#24471;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2208.10733</link><description>&lt;p&gt;
&#36882;&#24402;&#21487;&#34892;&#30340;&#24102;&#26377;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Recursively Feasible Probabilistic Safe Online Learning with Control Barrier Functions. (arXiv:2208.10733v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36882;&#24402;&#21487;&#34892;&#30340;&#27010;&#29575;&#23433;&#20840;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#21644;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#20351;&#24471;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#26041;&#26696;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#26696;&#65292;&#20445;&#35777;&#31995;&#32479;&#22312;&#22312;&#32447;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20445;&#25345;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24403;&#21069;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#20316;&#20026;&#25968;&#23398;&#24037;&#20855;&#65292;&#20026;&#20855;&#26377;&#24050;&#30693;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#20445;&#25345;&#23433;&#20840;&#30340;&#25511;&#21046;&#21512;&#25104;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;CBF&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#22120;&#30340;&#37325;&#26500;&#65292;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#22238;&#24402;&#26469;&#24314;&#31435;&#36817;&#20284;&#25968;&#23398;&#27169;&#22411;&#19982;&#30495;&#23454;&#31995;&#32479;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24471;&#21040;&#30340;&#40065;&#26834;&#23433;&#20840;&#20851;&#38190;&#25511;&#21046;&#22120;&#30340;&#21487;&#34892;&#24615;&#12290;&#35813;&#21487;&#34892;&#24615;&#20998;&#26512;&#32467;&#26524;&#23548;&#33268;&#20102;&#20851;&#20110;&#31995;&#32479;&#21487;&#29992;&#20449;&#24687;&#24212;&#28385;&#36275;&#30340;&#19968;&#31995;&#21015;&#20016;&#23500;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based control schemes have recently shown great efficacy performing complex tasks for a wide variety of applications. However, in order to deploy them in real systems, it is of vital importance to guarantee that the system will remain safe during online training and execution. Among the currently most popular methods to tackle this challenge, Control Barrier Functions (CBFs) serve as mathematical tools that provide a formal safety-preserving control synthesis procedure for systems with known dynamics. In this paper, we first introduce a model-uncertainty-aware reformulation of CBF-based safety-critical controllers using Gaussian Process (GP) regression to bridge the gap between an approximate mathematical model and the real system. Compared to previous approaches, we study the feasibility of the resulting robust safety-critical controller. This feasibility analysis results in a set of richness conditions that the available information about the system should satisfy to guarant
&lt;/p&gt;</description></item></channel></rss>