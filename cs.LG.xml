<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#26681;&#25454;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#12289;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#26356;&#20855;&#30408;&#21033;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#30456;&#20851;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20132;&#26131;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01441</link><description>&lt;p&gt;
&#23398;&#20064;&#24066;&#22330;&#65306;&#22522;&#20110;&#24773;&#24863;&#30340;&#38598;&#25104;&#20132;&#26131;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Learning the Market: Sentiment-Based Ensemble Trading Agents
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#21487;&#20197;&#26681;&#25454;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#12289;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#26356;&#20855;&#30408;&#21033;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#30456;&#20851;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#22266;&#23450;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20132;&#26131;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24773;&#24863;&#20998;&#26512;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#31639;&#27861;&#24212;&#29992;&#20110;&#32929;&#31080;&#20132;&#26131;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#32490;&#21160;&#24577;&#35843;&#25972;&#25152;&#20351;&#29992;&#30340;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#21462;&#26032;&#38395;&#24773;&#24863;&#65292;&#24182;&#23558;&#20854;&#19982;&#23545;&#29616;&#26377;&#20316;&#21697;&#30340;&#19968;&#33324;&#25913;&#36827;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#24471;&#21040;&#26377;&#25928;&#32771;&#34385;&#23450;&#24615;&#24066;&#22330;&#22240;&#32032;&#21644;&#23450;&#37327;&#32929;&#31080;&#25968;&#25454;&#30340;&#33258;&#21160;&#20132;&#26131;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#31181;&#30408;&#21033;&#12289;&#31283;&#20581;&#19988;&#39118;&#38505;&#26368;&#23567;&#30340;&#31574;&#30053;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#38598;&#25104;&#31574;&#30053;&#20197;&#21450;&#21333;&#19968;&#26234;&#33021;&#20307;&#31639;&#27861;&#21644;&#24066;&#22330;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#27599;&#38548;&#22266;&#23450;&#26376;&#20221;&#26356;&#25442;&#38598;&#25104;&#26234;&#33021;&#20307;&#30340;&#20570;&#27861;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#22522;&#20110;&#24773;&#24863;&#30340;&#21160;&#24577;&#26694;&#26550;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#36825;&#20123;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#35774;&#35745;&#31616;&#21333;&#19988;...
&lt;/p&gt;
&lt;p&gt;
We propose the integration of sentiment analysis and deep-reinforcement learning ensemble algorithms for stock trading, and design a strategy capable of dynamically altering its employed agent given concurrent market sentiment. In particular, we create a simple-yet-effective method for extracting news sentiment and combine this with general improvements upon existing works, resulting in automated trading agents that effectively consider both qualitative market factors and quantitative stock data. We show that our approach results in a strategy that is profitable, robust, and risk-minimal -- outperforming the traditional ensemble strategy as well as single agent algorithms and market metrics. Our findings determine that the conventional practice of switching ensemble agents every fixed-number of months is sub-optimal, and that a dynamic sentiment-based framework greatly unlocks additional performance within these agents. Furthermore, as we have designed our algorithm with simplicity and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;</title><link>https://rss.arxiv.org/abs/2312.05356</link><description>&lt;p&gt;
Neuron Patching: &#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#19982;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neuron Patching: Neuron-level Model Editing on Code Generation and LLMs
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.05356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#31070;&#32463;&#20803;&#23618;&#38754;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;API&#24207;&#21015;&#25512;&#33616;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#31561;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#25104;&#21151;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#12290;&#26356;&#26032;&#36825;&#20123;&#27169;&#22411;&#30340;&#26032;&#30693;&#35782;&#38750;&#24120;&#26114;&#36149;&#65292;&#36890;&#24120;&#38656;&#35201;&#20840;&#38754;&#23454;&#29616;&#20854;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;MENT&#65292;&#29992;&#20110;&#22312;&#32534;&#30721;&#20219;&#21153;&#20013;&#20462;&#34917;LLM&#27169;&#22411;&#12290;&#22522;&#20110;&#29983;&#25104;&#24335;LLM&#30340;&#26426;&#21046;&#65292;MENT&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#20196;&#29260;&#26102;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;&#24120;&#35265;&#30340;&#32534;&#30721;&#20219;&#21153;&#12290;MENT&#20855;&#26377;&#39640;&#25928;&#12289;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;&#29305;&#28857;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#20462;&#34917;1&#25110;2&#20010;&#31070;&#32463;&#20803;&#26469;&#32416;&#27491;&#31070;&#32463;&#27169;&#22411;&#12290;&#20316;&#20026;&#31070;&#32463;&#20803;&#23618;&#38754;&#19978;&#29983;&#25104;&#27169;&#22411;&#32534;&#36753;&#30340;&#20808;&#39537;&#24037;&#20316;&#65292;&#25105;&#20204;&#35268;&#33539;&#20102;&#32534;&#36753;&#36807;&#31243;&#24182;&#20171;&#32461;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34913;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#32534;&#30721;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;API&#24207;&#21015;&#25512;&#33616;&#12289;&#34892;&#32423;&#20195;&#30721;&#29983;&#25104;&#21644;&#20266;&#20195;&#30721;&#21040;&#20195;&#30721;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are successfully adopted in software engineering, especially in code generation. Updating these models with new knowledge is very expensive, and is often required to fully realize their value. In this paper, we propose a novel and effective model editing approach, \textsc{MENT}, to patch LLMs in coding tasks. Based on the mechanism of generative LLMs, \textsc{MENT} enables model editing in next-token predictions, and further supports common coding tasks. \textsc{MENT} is effective, efficient, and reliable. It can correct a neural model by patching 1 or 2 neurons. As the pioneer work on neuron-level model editing of generative models, we formalize the editing process and introduce the involved concepts. Besides, we also introduce new measures to evaluate its generalization ability, and build a benchmark for further study. Our approach is evaluated on three coding tasks, including API-seq recommendation, line-level code generation, and pseudocode-to-code transaction
&lt;/p&gt;</description></item><item><title>PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15182</link><description>&lt;p&gt;
PDE-CNNs&#65306;&#20844;&#29702;&#25512;&#23548;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs: Axiomatic Derivations and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15182
&lt;/p&gt;
&lt;p&gt;
PDE-CNNs&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;PDE&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;&#20256;&#32479;&#30340;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#30340;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#32452;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PDE-G-CNNs&#65289;&#21033;&#29992;&#20855;&#26377;&#20960;&#20309;&#24847;&#20041;&#30340;&#28436;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#27714;&#35299;&#22120;&#26367;&#20195;G-CNNs&#20013;&#24120;&#35268;&#32452;&#20214;&#12290;PDE-G-CNNs&#21516;&#26102;&#25552;&#20379;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#22266;&#26377;&#31561;&#21464;&#24615;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#20960;&#20309;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#29305;&#24449;&#22270;&#22312;&#25972;&#20010;&#32593;&#32476;&#20013;&#20026;&#20108;&#32500;&#30340;&#27431;&#20960;&#37324;&#24503;&#31561;&#21464;PDE-G-CNNs&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#30340;&#21464;&#20307;&#31216;&#20026;PDE-CNN&#12290;&#25105;&#20204;&#21015;&#20986;&#20102;&#20960;&#20010;&#22312;&#23454;&#36341;&#20013;&#20196;&#20154;&#28385;&#24847;&#30340;&#20844;&#29702;&#65292;&#24182;&#20174;&#20013;&#25512;&#23548;&#20986;&#24212;&#22312;PDE-CNN&#20013;&#20351;&#29992;&#21738;&#20123;PDE&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#20856;&#32447;&#24615;&#21644;&#24418;&#24577;&#23610;&#24230;&#31354;&#38388;&#29702;&#35770;&#30340;&#20844;&#29702;&#21463;&#21551;&#21457;&#65292;&#36890;&#36807;&#24341;&#20837;&#21322;&#22495;&#20540;&#20449;&#21495;&#23545;&#20854;&#36827;&#34892;&#25512;&#24191;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#30456;&#23545;&#20110;&#23567;&#22411;&#32593;&#32476;&#65292;PDE-CNN&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#21442;&#25968;&#12289;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15182v1 Announce Type: new  Abstract: PDE-based Group Convolutional Neural Networks (PDE-G-CNNs) utilize solvers of geometrically meaningful evolution PDEs as substitutes for the conventional components in G-CNNs. PDE-G-CNNs offer several key benefits all at once: fewer parameters, inherent equivariance, better performance, data efficiency, and geometric interpretability. In this article we focus on Euclidean equivariant PDE-G-CNNs where the feature maps are two dimensional throughout. We call this variant of the framework a PDE-CNN. We list several practically desirable axioms and derive from these which PDEs should be used in a PDE-CNN. Here our approach to geometric learning via PDEs is inspired by the axioms of classical linear and morphological scale-space theory, which we generalize by introducing semifield-valued signals. Furthermore, we experimentally confirm for small networks that PDE-CNNs offer fewer parameters, better performance, and data efficiency in compariso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#23454;&#29616;&#21160;&#24577;&#38477;&#20302;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#26410;&#26469;&#35774;&#35745;&#36731;&#37327;&#32423;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#38138;&#24179;&#20102;&#36947;&#36335;</title><link>https://arxiv.org/abs/2403.05601</link><description>&lt;p&gt;
&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#65306;&#20998;&#23618;&#20998;&#31867;&#32593;&#32476;&#20013;&#30340;&#39640;&#25928;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Select High-Level Features: Efficient Experts from a Hierarchical Classification Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39640;&#32423;&#29305;&#24449;&#23454;&#29616;&#21160;&#24577;&#38477;&#20302;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#20026;&#26410;&#26469;&#35774;&#35745;&#36731;&#37327;&#32423;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#38138;&#24179;&#20102;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21160;&#24577;&#20943;&#23569;&#20219;&#21153;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#39044;&#27979;&#24615;&#33021;&#12290;&#23427;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20998;&#31867;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#23558;&#36890;&#29992;&#20302;&#32423;&#29305;&#24449;&#30340;&#39034;&#24207;&#22788;&#29702;&#19982;&#39640;&#32423;&#29305;&#24449;&#30340;&#24182;&#34892;&#22788;&#29702;&#21644;&#23884;&#22871;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32467;&#26500;&#20801;&#35768;&#21019;&#26032;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65306;&#33021;&#22815;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#31867;&#21035;&#30340;&#39640;&#32423;&#29305;&#24449;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#21487;&#20197;&#36339;&#36807;&#25152;&#26377;&#19981;&#24517;&#35201;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26465;&#20214;&#19979;&#38750;&#24120;&#26377;&#30410;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#20026;&#26410;&#26469;&#36731;&#37327;&#32423;&#21644;&#21487;&#36866;&#24212;&#30340;&#32593;&#32476;&#35774;&#35745;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20174;&#32039;&#20945;&#36793;&#32536;&#35774;&#22791;&#21040;&#22823;&#22411;&#20113;&#31471;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#21160;&#24577;&#25512;&#29702;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#25490;&#38500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05601v1 Announce Type: new  Abstract: This study introduces a novel expert generation method that dynamically reduces task and computational complexity without compromising predictive performance. It is based on a new hierarchical classification network topology that combines sequential processing of generic low-level features with parallelism and nesting of high-level features. This structure allows for the innovative extraction technique: the ability to select only high-level features of task-relevant categories. In certain cases, it is possible to skip almost all unneeded high-level features, which can significantly reduce the inference cost and is highly beneficial in resource-constrained conditions. We believe this method paves the way for future network designs that are lightweight and adaptable, making them suitable for a wide range of applications, from compact edge devices to large-scale clouds. In terms of dynamic inference our methodology can achieve an exclusion 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10115</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#20174;EEG&#35760;&#24405;&#20013;&#29983;&#25104;&#35270;&#35273;&#21050;&#28608;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Stimuli from EEG Recordings using Transformer-encoder based EEG encoder and GAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#21644;GAN&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#25104;&#22270;&#20687;&#20174;EEG&#20449;&#21495;&#20013;&#24674;&#22797;&#20986;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#32467;&#21512;&#23545;&#25239;&#25439;&#22833;&#21644;&#24863;&#30693;&#25439;&#22833;&#65292;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#24863;&#30693;&#24615;&#33041;&#35299;&#30721;&#39046;&#22495;&#30340;&#19968;&#20010;&#29616;&#20195;&#30740;&#31350;&#25361;&#25112;&#65292;&#21363;&#20351;&#29992;&#23545;&#25239;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20174;EEG&#20449;&#21495;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#20855;&#20307;&#30446;&#26631;&#26159;&#21033;&#29992;&#20027;&#20307;&#35266;&#30475;&#22270;&#20687;&#26102;&#33719;&#24471;&#30340;EEG&#35760;&#24405;&#37325;&#26032;&#21019;&#24314;&#23646;&#20110;&#21508;&#31181;&#23545;&#35937;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;EEG&#32534;&#30721;&#22120;&#29983;&#25104;EEG&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#20316;&#20026;GAN&#32593;&#32476;&#30340;&#29983;&#25104;&#22120;&#32452;&#20214;&#30340;&#36755;&#20837;&#12290;&#38500;&#20102;&#23545;&#25239;&#25439;&#22833;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#24863;&#30693;&#25439;&#22833;&#26469;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10115v1 Announce Type: new  Abstract: In this study, we tackle a modern research challenge within the field of perceptual brain decoding, which revolves around synthesizing images from EEG signals using an adversarial deep learning framework. The specific objective is to recreate images belonging to various object categories by leveraging EEG recordings obtained while subjects view those images. To achieve this, we employ a Transformer-encoder based EEG encoder to produce EEG encodings, which serve as inputs to the generator component of the GAN network. Alongside the adversarial loss, we also incorporate perceptual loss to enhance the quality of the generated images.
&lt;/p&gt;</description></item><item><title>RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.08823</link><description>&lt;p&gt;
RanDumb: &#19968;&#31181;&#36136;&#30097;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#25928;&#26524;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RanDumb: A Simple Approach that Questions the Efficacy of Continual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08823
&lt;/p&gt;
&lt;p&gt;
RanDumb&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#21407;&#22987;&#20687;&#32032;&#24182;&#23398;&#20064;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#36136;&#30097;&#20102;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290; &#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;RanDumb&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RanDumb&#26469;&#26816;&#39564;&#25345;&#32493;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;RanDumb&#23558;&#21407;&#22987;&#20687;&#32032;&#20351;&#29992;&#19968;&#20010;&#22266;&#23450;&#30340;&#38543;&#26426;&#21464;&#25442;&#23884;&#20837;&#65292;&#36825;&#20010;&#21464;&#25442;&#36817;&#20284;&#20102;RBF-Kernel&#65292;&#22312;&#30475;&#21040;&#20219;&#20309;&#25968;&#25454;&#20043;&#21069;&#21021;&#22987;&#21270;&#65292;&#24182;&#23398;&#20064;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#19988;&#19968;&#33268;&#30340;&#21457;&#29616;&#65306;&#22312;&#20247;&#22810;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RanDumb&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#34920;&#26126;&#22312;&#36825;&#20123;&#24773;&#26223;&#19979;&#34920;&#31034;&#23398;&#20064;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;RanDumb&#19981;&#23384;&#20648;&#26679;&#26412;&#65292;&#24182;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21333;&#27425;&#36941;&#21382;&#65292;&#19968;&#27425;&#22788;&#29702;&#19968;&#20010;&#26679;&#26412;&#12290;&#23427;&#19982;GDumb&#30456;&#36741;&#30456;&#25104;&#65292;&#22312;GDumb&#24615;&#33021;&#29305;&#21035;&#24046;&#30340;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#24403;&#23558;RanDumb&#25193;&#23637;&#21040;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26367;&#25442;&#38543;&#26426;&#21464;&#25442;&#30340;&#24773;&#26223;&#26102;&#65292;&#25105;&#20204;&#24471;&#20986;&#30456;&#21516;&#19968;&#33268;&#30340;&#32467;&#35770;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#32467;&#26524;&#26082;&#20196;&#20154;&#24778;&#35766;&#21448;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#34920;&#31034;&#23398;&#20064;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#34920;&#29616;&#31967;&#31957;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08823v1 Announce Type: cross Abstract: We propose RanDumb to examine the efficacy of continual representation learning. RanDumb embeds raw pixels using a fixed random transform which approximates an RBF-Kernel, initialized before seeing any data, and learns a simple linear classifier on top. We present a surprising and consistent finding: RanDumb significantly outperforms the continually learned representations using deep networks across numerous continual learning benchmarks, demonstrating the poor performance of representation learning in these scenarios. RanDumb stores no exemplars and performs a single pass over the data, processing one sample at a time. It complements GDumb, operating in a low-exemplar regime where GDumb has especially poor performance. We reach the same consistent conclusions when RanDumb is extended to scenarios with pretrained models replacing the random transform with pretrained feature extractor. Our investigation is both surprising and alarming as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2401.17739</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Operator learning without the adjoint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#65292;&#24182;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#31639;&#23376;&#23398;&#20064;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#23376;&#23398;&#20064;&#20013;&#23384;&#22312;&#19968;&#20010;&#35868;&#22242;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#25506;&#27979;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#38750;&#33258;&#20276;&#38543;&#31639;&#23376;&#65311;&#30446;&#21069;&#30340;&#23454;&#38469;&#26041;&#27861;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;&#30001;&#31639;&#23376;&#30340;&#27491;&#21521;&#20316;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#31639;&#23376;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20276;&#38543;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#20197;&#30452;&#35266;&#30340;&#26041;&#24335;&#30475;&#65292;&#20284;&#20046;&#26377;&#24517;&#35201;&#37319;&#26679;&#20276;&#38543;&#31639;&#23376;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37096;&#20998;&#35299;&#37322;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#36890;&#36807;&#35777;&#26126;&#22312;&#19981;&#26597;&#35810;&#20276;&#38543;&#31639;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;Fourier&#22522;&#19978;&#36827;&#34892;&#25237;&#24433;&#26469;&#36924;&#36817;&#19968;&#31867;&#38750;&#33258;&#20276;&#38543;&#30340;&#26080;&#38480;&#32500;&#32039;&#31639;&#23376;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#32467;&#26524;&#24212;&#29992;&#20110;&#24674;&#22797;&#26925;&#22278;&#22411;&#20559;&#24494;&#20998;&#31639;&#23376;&#30340;&#26684;&#26519;&#20989;&#25968;&#65292;&#24182;&#23548;&#20986;&#19968;&#20010;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;&#31639;&#23376;&#23398;&#20064;&#30340;&#20302;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20294;&#25105;&#20204;&#30340;&#26159;&#31532;&#19968;&#20010;&#35797;&#22270;&#22635;&#34917;&#29702;&#35770;&#19982;&#23454;&#36341;&#24046;&#36317;&#30340;&#26080;&#38656;&#20276;&#38543;&#31639;&#23376;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a mystery at the heart of operator learning: how can one recover a non-self-adjoint operator from data without probing the adjoint? Current practical approaches suggest that one can accurately recover an operator while only using data generated by the forward action of the operator without access to the adjoint. However, naively, it seems essential to sample the action of the adjoint. In this paper, we partially explain this mystery by proving that without querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional compact operators via projection onto a Fourier basis. We then apply the result to recovering Green's functions of elliptic partial differential operators and derive an adjoint-free sample complexity bound. While existing theory justifies low sample complexity in operator learning, ours is the first adjoint-free analysis that attempts to close the gap between theory and practice.
&lt;/p&gt;</description></item><item><title>GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.08677</link><description>&lt;p&gt;
GDL-DS: &#20998;&#24067;&#36716;&#25442;&#19979;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts. (arXiv:2310.08677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08677
&lt;/p&gt;
&lt;p&gt;
GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;(GDL)&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#25797;&#38271;&#23545;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20854;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#30456;&#20851;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GDL-DS&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;GDL&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#31890;&#23376;&#29289;&#29702;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#21040;&#29983;&#29289;&#21270;&#23398;&#30340;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#20998;&#24067;&#36716;&#25442;&#65292;&#21253;&#25324;&#26465;&#20214;&#12289;&#21327;&#21464;&#21644;&#27010;&#24565;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#20449;&#24687;&#35775;&#38382;&#30340;&#19977;&#20010;&#32423;&#21035;&#65292;&#21253;&#25324;&#27809;&#26377;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#12289;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#21644;&#24102;&#26377;&#23569;&#25968;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#28041;&#21450;30&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#35780;&#20272;3&#31181;&#20449;&#24687;&#35775;&#38382;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30740;&#31350;&#20102;&#22996;&#25176;&#30340;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#22865;&#32422;&#35299;&#20915;&#20102;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.01837</link><description>&lt;p&gt;
&#22996;&#25176;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
Delegating Data Collection in Decentralized Machine Learning. (arXiv:2309.01837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01837
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#22312;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20013;&#30740;&#31350;&#20102;&#22996;&#25176;&#30340;&#25968;&#25454;&#25910;&#38598;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#35745;&#26368;&#20248;&#22865;&#32422;&#35299;&#20915;&#20102;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20998;&#25955;&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20986;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#22996;&#25176;&#38382;&#39064;&#12290;&#20197;&#22865;&#32422;&#29702;&#35770;&#20026;&#20986;&#21457;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#35299;&#20915;&#20004;&#20010;&#22522;&#26412;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#30340;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#22865;&#32422;&#65306;&#27169;&#22411;&#36136;&#37327;&#35780;&#20272;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#23545;&#20219;&#20309;&#27169;&#22411;&#26368;&#20248;&#24615;&#33021;&#30340;&#32570;&#20047;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#32447;&#24615;&#22865;&#32422;&#21487;&#20197;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#21363;&#20351;&#22996;&#25176;&#20154;&#21482;&#26377;&#19968;&#20010;&#23567;&#30340;&#27979;&#35797;&#38598;&#65292;&#20063;&#33021;&#23454;&#29616;1-1/e&#30340;&#19968;&#31561;&#25928;&#29992;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22996;&#25176;&#20154;&#27979;&#35797;&#38598;&#22823;&#23567;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#21487;&#20197;&#36798;&#21040;&#23545;&#26368;&#20248;&#25928;&#29992;&#30340;&#36924;&#36817;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;&#26368;&#20248;&#24615;&#33021;&#32570;&#20047;&#39044;&#20808;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20984;&#38382;&#39064;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#22865;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the emergence of decentralized machine learning ecosystems, we study the delegation of data collection. Taking the field of contract theory as our starting point, we design optimal and near-optimal contracts that deal with two fundamental machine learning challenges: lack of certainty in the assessment of model quality and lack of knowledge regarding the optimal performance of any model. We show that lack of certainty can be dealt with via simple linear contracts that achieve 1-1/e fraction of the first-best utility, even if the principal has a small test set. Furthermore, we give sufficient conditions on the size of the principal's test set that achieves a vanishing additive approximation to the optimal utility. To address the lack of a priori knowledge regarding the optimal performance, we give a convex program that can adaptively and efficiently compute the optimal contract.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11367</link><description>&lt;p&gt;
&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#29992;&#20110;&#20154;&#31867;&#30561;&#30496;&#23039;&#21183;&#21644;&#21160;&#24577;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Smart Pressure e-Mat for Human Sleeping Posture and Dynamic Activity Recognition. (arXiv:2305.11367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#35782;&#21035;&#20154;&#20307;&#23039;&#21183;&#21644;&#36816;&#21160;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#35843;&#21307;&#30103;&#20445;&#20581;&#12289;&#26089;&#26399;&#25945;&#32946;&#21644;&#20581;&#36523;&#26041;&#38754;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38750;&#20405;&#20837;&#24335;&#27979;&#37327;&#21644;&#35782;&#21035;&#26041;&#27861;&#21463;&#21040;&#20851;&#27880;&#12290;&#21387;&#21147;&#24863;&#24212;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32467;&#26500;&#12289;&#26131;&#20110;&#35775;&#38382;&#12289;&#21487;&#35270;&#21270;&#24212;&#29992;&#21644;&#26080;&#23475;&#24615;&#32780;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#25935;&#26448;&#26009;Velostat&#30340;&#26234;&#33021;&#21387;&#21147;&#30005;&#23376;&#22443;(SP e-Mat)&#31995;&#32479;&#65292;&#29992;&#20110;&#20154;&#20307;&#30417;&#27979;&#24212;&#29992;&#65292;&#21253;&#25324;&#30561;&#30496;&#23039;&#21183;&#12289;&#36816;&#21160;&#21644;&#29788;&#20285;&#35782;&#21035;&#12290;&#22312;&#23376;&#31995;&#32479;&#25195;&#25551;&#30005;&#23376;&#22443;&#35835;&#25968;&#24182;&#22788;&#29702;&#20449;&#21495;&#21518;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21387;&#21147;&#22270;&#20687;&#27969;&#12290;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26469;&#25311;&#21512;&#21644;&#35757;&#32451;&#21387;&#21147;&#22270;&#20687;&#27969;&#65292;&#24182;&#35782;&#21035;&#30456;&#24212;&#30340;&#20154;&#31867;&#34892;&#20026;&#12290;&#22235;&#31181;&#30561;&#30496;&#23039;&#21183;&#21644;&#21463;Nintendo Switch Ring Fit Adventure(RFA)&#21551;&#21457;&#30340;&#20116;&#31181;&#21160;&#24577;&#27963;&#21160;&#34987;&#29992;&#20316;&#25311;&#35758;&#30340;SPeM&#31995;&#32479;&#30340;&#21021;&#27493;&#39564;&#35777;&#12290;SPeM&#31995;&#32479;&#22312;&#20004;&#31181;&#24212;&#29992;&#20013;&#22343;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#35777;&#26126;&#20102;&#20854;&#39640;&#31934;&#24230;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emphasis on healthcare, early childhood education, and fitness, non-invasive measurement and recognition methods have received more attention. Pressure sensing has been extensively studied due to its advantages of simple structure, easy access, visualization application, and harmlessness. This paper introduces a smart pressure e-mat (SPeM) system based on a piezoresistive material Velostat for human monitoring applications, including sleeping postures, sports, and yoga recognition. After a subsystem scans e-mat readings and processes the signal, it generates a pressure image stream. Deep neural networks (DNNs) are used to fit and train the pressure image stream and recognize the corresponding human behavior. Four sleeping postures and five dynamic activities inspired by Nintendo Switch Ring Fit Adventure (RFA) are used as a preliminary validation of the proposed SPeM system. The SPeM system achieves high accuracies on both applications, which demonstrates the high accuracy and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.09957</link><description>&lt;p&gt;
&#28145;&#24230;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#23545;&#24212;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20174;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20808;&#39564;&#26465;&#20214;&#24320;&#22987;&#21021;&#22987;&#21270;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#38544;&#34255;&#23618;&#31070;&#32463;&#20803;&#25968;&#30446;&#36275;&#22815;&#22823;&#30340;&#26497;&#38480;&#19979;&#25910;&#25947;&#21040;&#39640;&#26031;&#36807;&#31243;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#20063;&#23384;&#22312;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;Haar&#38543;&#26426;&#37193;&#25110;&#27491;&#20132;&#28145;QNNs&#30340;&#26576;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#32500;&#24230;$d$&#36275;&#22815;&#22823;&#26102;&#20250;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#20110;&#36755;&#20837;&#29366;&#24577;&#12289;&#27979;&#37327;&#30340;&#21487;&#35266;&#27979;&#37327;&#20197;&#21450;&#37193;&#30697;&#38453;&#30340;&#20803;&#32032;&#19981;&#29420;&#31435;&#31561;&#22240;&#32032;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#23545;&#36825;&#19968;&#32467;&#26524;&#30340;&#25512;&#23548;&#27604;&#32463;&#20856;&#24773;&#24418;&#26356;&#21152;&#24494;&#22937;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#65292;&#36825;&#20010;&#32467;&#26524;&#24471;&#21040;&#30340;&#39640;&#26031;&#36807;&#31243;&#19981;&#33021;&#36890;&#36807;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26469;&#26377;&#25928;&#22320;&#39044;&#27979;QNN&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;Haar&#38543;&#26426;QNNs&#20013;&#30340;&#27979;&#37327;&#29616;&#35937;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#35201;&#26356;&#20005;&#37325;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28436;&#21592;&#30340;&#38598;&#20013;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.09267</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#27861;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Approximation Approaches to Group Distributionally Robust Optimization. (arXiv:2302.09267v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#36924;&#36817;&#27861;&#65292;&#29992;&#20110;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#30446;&#30340;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#22312;$m$&#20010;&#19981;&#21516;&#20998;&#24067;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;GDRO&#24314;&#27169;&#20026;&#38543;&#26426;&#20984;&#20985;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;$m$&#20010;&#26679;&#26412;&#30340;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#27861;(SMD)&#65292;&#33021;&#22815;&#23454;&#29616;$O(m(\log m)/\epsilon ^2)$&#20010;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;&#65292;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#35299;&#65292;&#36825;&#19982;$\Omega(m/\epsilon ^2)$&#30340;&#19979;&#30028;&#24819;&#21305;&#37197;&#65292;&#38500;&#20102;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#27599;&#36718;&#25152;&#38656;&#30340;&#26679;&#26412;&#25968;&#20174;$m$&#20010;&#38477;&#33267;$1$&#20010;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;GDRO&#26500;&#36896;&#20026;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#19968;&#20010;&#29609;&#23478;&#31616;&#21333;&#22320;&#25191;&#34892;SMD&#65292;&#21478;&#19968;&#20010;&#25191;&#34892;&#19968;&#31181;&#29992;&#20110;&#38750;&#26126;&#26174;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#22312;&#32447;&#31639;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#21487;&#20197;&#20174;&#27599;&#20010;&#20998;&#24067;&#20013;&#32472;&#21046;&#30340;&#26679;&#26412;&#25968;&#37327;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates group distributionally robust optimization (GDRO), with the purpose to learn a model that performs well over $m$ different distributions. First, we formulate GDRO as a stochastic convex-concave saddle-point problem, and demonstrate that stochastic mirror descent (SMD), using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$ sample complexity for finding an $\epsilon$-optimal solution, which matches the $\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make use of techniques from online learning to reduce the number of samples required in each round from $m$ to $1$, keeping the same sample complexity. Specifically, we cast GDRO as a two-players game where one player simply performs SMD and the other executes an online algorithm for non-oblivious multi-armed bandits. Next, we consider a more practical scenario where the number of samples that can be drawn from each distribution is different, and propose a novel formulation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#26550;&#26500;&#22312;&#35813;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#35777;&#25454;&#65292;&#21457;&#29616;&#20102;&#19968;&#31867;&#32593;&#32476;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#65292;&#24182;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#29616;&#35937;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2301.13105</link><description>&lt;p&gt;
&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#12289;&#36923;&#36753;&#25512;&#29702;&#21644;&#23398;&#20301;&#35838;&#31243;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generalization on the Unseen, Logic Reasoning and Degree Curriculum. (arXiv:2301.13105v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#26550;&#26500;&#22312;&#35813;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#35777;&#25454;&#65292;&#21457;&#29616;&#20102;&#19968;&#31867;&#32593;&#32476;&#27169;&#22411;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#65292;&#24182;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#29616;&#35937;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#36923;&#36753;&#65288;&#24067;&#23572;&#65289;&#20989;&#25968;&#30340;&#23398;&#20064;&#65292;&#37325;&#28857;&#22312;&#20110;&#23545;&#26410;&#30693;&#25968;&#25454;&#30340;&#27867;&#21270;&#65288;GOTU&#65289;&#35774;&#23450;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#30340;&#26696;&#20363;&#12290;&#36825;&#26159;&#30001;&#20110;&#26576;&#20123;&#25512;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#31639;&#26415;/&#36923;&#36753;&#65289;&#20013;&#25968;&#25454;&#30340;&#20016;&#23500;&#32452;&#21512;&#24615;&#36136;&#20351;&#24471;&#20195;&#34920;&#24615;&#25968;&#25454;&#37319;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#22312;GOTU&#19979;&#25104;&#21151;&#23398;&#20064;&#20026;&#31532;&#19968;&#20010;&#8220;&#25512;&#29702;&#8221;&#23398;&#20064;&#32773;&#23637;&#31034;&#20102;&#19968;&#20010;&#23567;&#25554;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;(S)GD&#35757;&#32451;&#30340;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;GOTU&#19979;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#31867;&#21035;&#30340;&#32593;&#32476;&#27169;&#22411;&#65288;&#21253;&#25324;Transformer&#30340;&#23454;&#20363;&#12289;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#23545;&#35282;&#32447;&#32447;&#24615;&#32593;&#32476;&#65289;&#22312;&#26410;&#30693;&#25968;&#25454;&#19978;&#23398;&#20064;&#20102;&#26368;&#23567;&#24230;&#25554;&#20540;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#20854;&#20182;&#20855;&#26377;&#26356;&#22823;&#23398;&#20064;&#36895;&#29575;&#25110;&#22343;&#22330;&#32593;&#32476;&#30340;&#23454;&#20363;&#36798;&#21040;&#20102;&#28183;&#28431;&#26368;&#23567;&#24230;&#35299;&#12290;&#36825;&#20123;&#21457;&#29616;&#24102;&#26469;&#20102;&#20004;&#20010;&#24433;&#21709;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#38271;&#24230;&#26222;&#36890;&#21270;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
This paper considers the learning of logical (Boolean) functions with focus on the generalization on the unseen (GOTU) setting, a strong case of out-of-distribution generalization. This is motivated by the fact that the rich combinatorial nature of data in certain reasoning tasks (e.g., arithmetic/logic) makes representative data sampling challenging, and learning successfully under GOTU gives a first vignette of an 'extrapolating' or 'reasoning' learner. We then study how different network architectures trained by (S)GD perform under GOTU and provide both theoretical and experimental evidence that for a class of network models including instances of Transformers, random features models, and diagonal linear networks, a min-degree-interpolator is learned on the unseen. We also provide evidence that other instances with larger learning rates or mean-field networks reach leaky min-degree solutions. These findings lead to two implications: (1) we provide an explanation to the length genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10081</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#31163;&#25955;&#22411;Soft Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Discrete Soft Actor-Critic. (arXiv:2209.10081v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#20854;&#22312;Atari&#28216;&#25103;&#21644;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;Soft Actor-Critic&#26041;&#27861;&#65288;SAC&#65289;&#35843;&#25972;&#20026;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#30340;SAC&#26041;&#27861;&#65292;&#24182;&#28145;&#20837;&#29702;&#35299;&#20102;&#22312;&#31163;&#25955;&#35774;&#32622;&#19979;&#20854;Q&#20540;&#20302;&#20272;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29109;&#24809;&#32602;&#21644;&#20855;&#26377;Q-clip&#30340;&#21452;&#24179;&#22343;Q-learning&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;Atari&#28216;&#25103;&#21644;&#19968;&#20010;&#22823;&#35268;&#27169;MOBA&#28216;&#25103;&#22312;&#20869;&#30340;&#20856;&#22411;&#22522;&#20934;&#38382;&#39064;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;: https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;
&lt;p&gt;
We study the adaption of soft actor-critic (SAC) from continuous action space to discrete action space. We revisit vanilla SAC and provide an in-depth understanding of its Q value underestimation and performance instability issues when applied to discrete settings. We thereby propose entropy-penalty and double average Q-learning with Q-clip to address these issues. Extensive experiments on typical benchmarks with discrete action space, including Atari games and a large-scale MOBA game, show the efficacy of our proposed method. Our code is at:https://github.com/coldsummerday/Revisiting-Discrete-SAC.
&lt;/p&gt;</description></item></channel></rss>