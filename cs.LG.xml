<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;</title><link>https://rss.arxiv.org/abs/2402.01413</link><description>&lt;p&gt;
&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#20013;UDASE&#20219;&#21153;&#20013;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Objective and subjective evaluation of speech enhancement methods in the UDASE task of the 7th CHiME challenge
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#20013;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#24037;&#21512;&#25104;&#30340;&#24178;&#20928;&#35821;&#38899;&#21644;&#22122;&#22768;&#20449;&#21495;&#28151;&#21512;&#26469;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#35757;&#32451;&#26465;&#20214;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#33021;&#23548;&#33268;&#22312;&#27979;&#35797;&#22495;&#19982;&#21512;&#25104;&#35757;&#32451;&#22495;&#26174;&#33879;&#19981;&#21516;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19971;&#23626;CHiME&#25361;&#25112;&#36187;&#30340;UDASE&#20219;&#21153;&#26088;&#22312;&#21033;&#29992;&#27979;&#35797;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#22122;&#22768;&#35821;&#38899;&#24405;&#38899;&#26469;&#23545;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20010;&#27979;&#35797;&#22495;&#23545;&#24212;&#20110;CHiME-5&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#22312;&#22024;&#26434;&#21644;&#28151;&#21709;&#30340;&#23478;&#24237;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#30495;&#23454;&#22810;&#35828;&#35805;&#20154;&#23545;&#35805;&#24405;&#38899;&#32452;&#25104;&#65292;&#26080;&#27861;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#24178;&#20928;&#35821;&#38899;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25552;&#20132;&#21040;CHiME-7 UDASE&#20219;&#21153;&#30340;&#31995;&#32479;&#30340;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supervised models for speech enhancement are trained using artificially generated mixtures of clean speech and noise signals. However, the synthetic training conditions may not accurately reflect real-world conditions encountered during testing. This discrepancy can result in poor performance when the test domain significantly differs from the synthetic training domain. To tackle this issue, the UDASE task of the 7th CHiME challenge aimed to leverage real-world noisy speech recordings from the test domain for unsupervised domain adaptation of speech enhancement models. Specifically, this test domain corresponds to the CHiME-5 dataset, characterized by real multi-speaker and conversational speech recordings made in noisy and reverberant domestic environments, for which ground-truth clean speech signals are not available. In this paper, we present the objective and subjective evaluations of the systems that were submitted to the CHiME-7 UDASE task, and we provide an analysis of the resul
&lt;/p&gt;</description></item><item><title>Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;</title><link>https://arxiv.org/abs/2404.00411</link><description>&lt;p&gt;
Aardvark Weather:&#31471;&#23545;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#22825;&#27668;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather: end-to-end data-driven weather forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00411
&lt;/p&gt;
&lt;p&gt;
Aardvark Weather&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#33021;&#22815;&#21462;&#20195;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#31934;&#20934;&#39044;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#24443;&#24213;&#25913;&#21464;&#20013;&#31243;&#22825;&#27668;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#34987;&#24212;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#31649;&#36947;&#30340;&#29305;&#23450;&#21644;&#21333;&#20010;&#32452;&#20214;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26080;&#27861;&#22312;&#27809;&#26377;&#26469;&#33258;&#20256;&#32479;&#25805;&#20316;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#30340;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#37096;&#32626;&#65292;&#36825;&#26159;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#25903;&#25345;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26681;&#26412;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#20195;&#25972;&#20010;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#31649;&#36947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Aardvark Weather&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#25253;&#31995;&#32479;&#65292;&#23427;&#25509;&#21463;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#25552;&#20379;&#20840;&#29699;&#21644;&#26412;&#22320;&#39044;&#25253;&#12290;&#36825;&#20123;&#20840;&#29699;&#39044;&#25253;&#20197;&#19968;&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#65292;24&#23567;&#26102;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;&#22810;&#20010;&#21387;&#21147;&#27700;&#24179;24&#20010;&#21464;&#37327;&#20135;&#29983;&#65292;&#24182;&#22312;&#20116;&#21040;&#19971;&#22825;&#30340;&#21069;&#26399;&#39046;&#20808;&#26102;&#27573;&#23545;&#27599;&#23567;&#26102;&#27668;&#20505;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#22320;&#39044;&#25253;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00411v1 Announce Type: cross  Abstract: Machine learning is revolutionising medium-range weather prediction. However it has only been applied to specific and individual components of the weather prediction pipeline. Consequently these data-driven approaches are unable to be deployed without input from conventional operational numerical weather prediction (NWP) systems, which is computationally costly and does not support end-to-end optimisation. In this work, we take a radically different approach and replace the entire NWP pipeline with a machine learning model. We present Aardvark Weather, the first end-to-end data-driven forecasting system which takes raw observations as input and provides both global and local forecasts. These global forecasts are produced for 24 variables at multiple pressure levels at one-degree spatial resolution and 24 hour temporal resolution, and are skillful with respect to hourly climatology at five to seven day lead times. Local forecasts are pr
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17886</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#20219;&#21153;&#23884;&#20837;&#29992;&#20110;&#22320;&#29699;&#35266;&#27979;&#20013;&#25968;&#25454;&#39640;&#25928;&#19979;&#28216;&#35757;&#32451;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17886
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#30340;&#22810;&#20219;&#21153;&#23884;&#20837;&#26041;&#27861;&#22312;&#22320;&#29699;&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#65292;&#36890;&#36807;&#21387;&#32553;&#29575;&#19982;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21462;&#24471;&#20102;&#25968;&#25454;&#37327;&#26174;&#33879;&#20943;&#23569;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#20013;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#23384;&#20648;&#24211;&#22686;&#38271;&#65292;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#36716;&#31227;&#21644;&#23384;&#20648;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#65292;&#28040;&#32791;&#20102;&#22823;&#37327;&#36164;&#28304;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#31070;&#32463;&#23884;&#20837;&#21387;&#32553;&#65288;NEC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#23545;&#25968;&#25454;&#20351;&#29992;&#32773;&#20256;&#36755;&#21387;&#32553;&#30340;&#23884;&#20837;&#32780;&#19981;&#26159;&#21407;&#22987;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#31070;&#32463;&#21387;&#32553;&#26469;&#35843;&#25972;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#65292;&#29983;&#25104;&#22810;&#20219;&#21153;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#21387;&#32553;&#29575;&#21644;&#23884;&#20837;&#25928;&#29992;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#20165;&#38024;&#23545;FM&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65288;10%&#65289;&#36827;&#34892;&#26356;&#26032;&#65292;&#36827;&#34892;&#30701;&#26102;&#38388;&#35757;&#32451;&#65288;&#39044;&#35757;&#32451;&#36845;&#20195;&#30340;1%&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;EO&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;NEC&#65306;&#22330;&#26223;&#20998;&#31867;&#21644;&#35821;&#20041;&#20998;&#21106;&#12290;&#19982;&#23558;&#20256;&#32479;&#21387;&#32553;&#24212;&#29992;&#20110;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#65292;NEC&#22312;&#20943;&#23569;&#25968;&#25454;&#37327;&#26041;&#38754;&#21487;&#23454;&#29616;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#38477;&#20302;&#20102;75%&#21040;90%&#30340;&#25968;&#25454;&#37327;&#12290;&#21363;&#20351;&#22312;99.7%&#30340;&#21387;&#32553;&#19979;&#65292;&#22312;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#19978;&#24615;&#33021;&#20165;&#19979;&#38477;&#20102;5%&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;NEC&#26159;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
&lt;/p&gt;</description></item><item><title>eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;</title><link>https://arxiv.org/abs/2403.10153</link><description>&lt;p&gt;
&#25913;&#36827;&#21307;&#23398;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#19982;&#19987;&#23478;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Improving Medical Multi-modal Contrastive Learning with Expert Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10153
&lt;/p&gt;
&lt;p&gt;
eCLIP&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;CLIP&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#19987;&#23478;&#27880;&#37322;&#21644;&#28151;&#21512;&#22686;&#24378;&#26469;&#24212;&#23545;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#21644;&#27169;&#24577;&#24046;&#36317;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;CLIP&#27169;&#22411;&#8212;&#8212;eCLIP&#65292;&#23427;&#38598;&#25104;&#20102;&#25918;&#23556;&#31185;&#21307;&#29983;&#30524;&#29699;&#27880;&#35270;&#28909;&#22270;&#24418;&#24335;&#30340;&#19987;&#23478;&#27880;&#37322;&#12290;&#23427;&#35299;&#20915;&#20102;&#23545;&#27604;&#22810;&#27169;&#24577;&#21307;&#23398;&#24433;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#31232;&#32570;&#21644;&#8220;&#27169;&#24577;&#24046;&#36317;&#8221;&#8212;&#8212;&#22270;&#20687;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#23384;&#22312;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#38477;&#20302;&#20102;&#34920;&#31034;&#30340;&#36136;&#37327;&#24182;&#38459;&#30861;&#20102;&#36328;&#27169;&#24577;&#20114;&#25805;&#20316;&#24615;&#12290;eCLIP&#38598;&#25104;&#20102;&#19968;&#20010;&#28909;&#22270;&#22788;&#29702;&#22120;&#65292;&#24182;&#21033;&#29992;&#28151;&#21512;&#22686;&#24378;&#26469;&#26377;&#25928;&#21033;&#29992;&#31232;&#32570;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;eCLIP&#35774;&#35745;&#20026;&#36890;&#29992;&#30340;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#24418;&#24335;&#30340;CLIP&#21464;&#20307;&#65292;&#26080;&#38656;&#20462;&#25913;&#26680;&#24515;&#26550;&#26500;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#35814;&#32454;&#35780;&#20272;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#25512;&#26029;&#12289;&#32447;&#24615;&#25506;&#38024;&#12289;&#36328;&#27169;&#24577;&#26816;&#32034;&#20197;&#21450;&#20351;&#29992;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#65292;eCLIP&#23637;&#31034;&#20102;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10153v1 Announce Type: cross  Abstract: We introduce eCLIP, an enhanced version of the CLIP model that integrates expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key challenges in contrastive multi-modal medical imaging analysis, notably data scarcity and the "modality gap" -- a significant disparity between image and text embeddings that diminishes the quality of representations and hampers cross-modal interoperability. eCLIP integrates a heatmap processor and leverages mixup augmentation to efficiently utilize the scarce expert annotations, thus boosting the model's learning effectiveness. eCLIP is designed to be generally applicable to any variant of CLIP without requiring any modifications of the core architecture. Through detailed evaluations across several tasks, including zero-shot inference, linear probing, cross-modal retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using a frozen Large Language Model, eCLIP showca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07559</link><description>&lt;p&gt;
&#20026;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#38598;&#25104;&#20248;&#20808;&#32423;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Ensembling Prioritized Hybrid Policies for Multi-agent Pathfinding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Ensembling Prioritized Hybrid Policies (EPH)&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#21644;&#19977;&#31181;&#39640;&#32423;&#25512;&#29702;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#36890;&#20449;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#36817;&#26469;&#22240;&#20854;&#39640;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Ensembling Prioritized Hybrid Policies (EPH)&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22522;&#20110;&#36890;&#20449;&#30340;MARL-MAPF&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36873;&#25321;&#24615;&#36890;&#20449;&#27169;&#22359;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;Q-learning&#30340;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07559v1 Announce Type: cross  Abstract: Multi-Agent Reinforcement Learning (MARL) based Multi-Agent Path Finding (MAPF) has recently gained attention due to its efficiency and scalability. Several MARL-MAPF methods choose to use communication to enrich the information one agent can perceive. However, existing works still struggle in structured environments with high obstacle density and a high number of agents. To further improve the performance of the communication-based MARL-MAPF solvers, we propose a new method, Ensembling Prioritized Hybrid Policies (EPH). We first propose a selective communication block to gather richer information for better agent coordination within multi-agent environments and train the model with a Q-learning-based algorithm. We further introduce three advanced inference strategies aimed at bolstering performance during the execution phase. First, we hybridize the neural policy with single-agent expert guidance for navigating conflict-free zones. Se
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.06903</link><description>&lt;p&gt;
&#20855;&#26377;&#36866;&#24230;&#36755;&#20837;&#32500;&#24230;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#20013;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting in leaky ReLU networks with moderate input dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06903
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#23545;&#20110;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#39640;SNR&#20540;&#20250;&#23548;&#33268;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#20302;SNR&#20540;&#21017;&#20250;&#23548;&#33268;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#25506;&#35752;&#20102;&#19968;&#20010;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#32654;&#22320;&#25311;&#21512;&#22024;&#26434;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21516;&#26102;&#21448;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20108;&#23618;&#27844;&#28431;ReLU&#32593;&#32476;&#19978;&#20351;&#29992;&#38128;&#38142;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#38024;&#23545;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#65292;&#21487;&#20197;&#20998;&#35299;&#20026;&#19968;&#20010;&#20849;&#21516;&#20449;&#21495;&#21644;&#19968;&#20010;&#38543;&#26426;&#22122;&#22768;&#25104;&#20998;&#30340;&#24635;&#21644;&#65292;&#36825;&#20004;&#32773;&#30456;&#20114;&#27491;&#20132;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26465;&#20214;&#65292;&#23548;&#33268;&#20102;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#65288;&#26377;&#23475;&#65289;&#36807;&#25311;&#21512;&#65306;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;SNR&#24456;&#39640;&#65292;&#21017;&#21457;&#29983;&#33391;&#24615;&#36807;&#25311;&#21512;&#65292;&#30456;&#21453;&#65292;&#22914;&#26524;SNR&#24456;&#20302;&#65292;&#21017;&#21457;&#29983;&#26377;&#23475;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#23558;&#33391;&#24615;&#21644;&#38750;&#33391;&#24615;&#36807;&#25311;&#21512;&#24402;&#22240;&#20110;&#19968;&#20010;&#36817;&#20284;&#36793;&#30028;&#26368;&#22823;&#21270;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38128;&#38142;&#25439;&#22833;&#19979;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#35757;&#32451;&#30340;&#27844;&#28431;ReLU&#32593;&#32476;&#28385;&#36275;&#36825;&#19968;&#24615;&#36136;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;nea
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06903v1 Announce Type: new  Abstract: The problem of benign overfitting asks whether it is possible for a model to perfectly fit noisy training data and still generalize well. We study benign overfitting in two-layer leaky ReLU networks trained with the hinge loss on a binary classification task. We consider input data which can be decomposed into the sum of a common signal and a random noise component, which lie on subspaces orthogonal to one another. We characterize conditions on the signal to noise ratio (SNR) of the model parameters giving rise to benign versus non-benign, or harmful, overfitting: in particular, if the SNR is high then benign overfitting occurs, conversely if the SNR is low then harmful overfitting occurs. We attribute both benign and non-benign overfitting to an approximate margin maximization property and show that leaky ReLU networks trained on hinge loss with Gradient Descent (GD) satisfy this property. In contrast to prior work we do not require nea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04586</link><description>&lt;p&gt;
&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#23398;&#20064;&#39134;&#34892;&#25935;&#25463;&#24615;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Learning Agility Adaptation for Flight in Clutter
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20351;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#20855;&#26377;&#25935;&#25463;&#24615;&#35843;&#25972;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#22312;&#20223;&#30495;&#20013;&#26174;&#31034;&#20986;&#27604;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26356;&#20248;&#36234;&#30340;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#23398;&#20064;&#36866;&#24212;&#20854;&#36816;&#21160;&#33021;&#21147;&#21644;&#25805;&#20316;&#29615;&#22659;&#30340;&#25935;&#25463;&#24615;&#12290;&#31227;&#21160;&#26426;&#22120;&#20154;&#20063;&#24212;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#23558;&#25935;&#25463;&#24615;&#21644;&#23433;&#20840;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#26088;&#22312;&#36171;&#20104;&#39134;&#34892;&#22120;&#22312;&#26410;&#30693;&#19988;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#26434;&#20081;&#29615;&#22659;&#20013;&#36866;&#24212;&#25935;&#25463;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23398;&#20064;&#21644;&#35268;&#21010;&#26694;&#26550;&#65292;&#32467;&#21512;&#35797;&#38169;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#36712;&#36857;&#29983;&#25104;&#26041;&#27861;&#26469;&#20840;&#38754;&#23398;&#20064;&#25935;&#25463;&#24615;&#31574;&#30053;&#12290;&#25105;&#20204;&#20351;&#29992;&#22312;&#32447;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#24494;&#35843;&#22870;&#21169;&#26041;&#26696;&#26469;&#33719;&#24471;&#21487;&#37096;&#32626;&#30340;&#31574;&#30053;&#12290;&#22312;&#20223;&#30495;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#24658;&#23450;&#25935;&#25463;&#24615;&#22522;&#32447;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39134;&#34892;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#35813;&#31574;&#30053;&#23548;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04586v1 Announce Type: cross  Abstract: Animals learn to adapt agility of their movements to their capabilities and the environment they operate in. Mobile robots should also demonstrate this ability to combine agility and safety. The aim of this work is to endow flight vehicles with the ability of agility adaptation in prior unknown and partially observable cluttered environments. We propose a hierarchical learning and planning framework where we utilize both trial and error to comprehensively learn an agility policy with the vehicle's observation as the input, and well-established methods of model-based trajectory generation. Technically, we use online model-free reinforcement learning and a pre-training-fine-tuning reward scheme to obtain the deployable policy. The statistical results in simulation demonstrate the advantages of our method over the constant agility baselines and an alternative method in terms of flight efficiency and safety. In particular, the policy leads
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02502</link><description>&lt;p&gt;
&#35797;&#38169;&#27861;&#65306;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ETO&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;LLM&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#35757;&#32451;&#25104;&#21151;&#19987;&#23478;&#36712;&#36857;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20174;&#20854;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#36825;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#23436;&#25104;&#25351;&#23450;&#20219;&#21153;&#65292;&#25910;&#38598;&#22833;&#36133;&#36712;&#36857;&#20197;&#21019;&#24314;&#23545;&#27604;&#36712;&#36857;&#23545;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#29702;&#21033;&#29992;&#36825;&#20123;&#36712;&#36857;&#20559;&#22909;&#23545;&#26356;&#26032;&#20854;&#31574;&#30053;&#65292;&#20351;&#29992;&#31867;&#20284;DPO&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#25506;&#32034;&#21644;&#35757;&#32451;&#30340;&#36845;&#20195;&#24490;&#29615;&#20419;&#36827;&#20102;&#20195;&#29702;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;</title><link>https://arxiv.org/abs/2402.12231</link><description>&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#25913;&#21892;&#27010;&#29575;&#31215;&#20998;&#22120;&#23545;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#20272;&#35745;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12231
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#22238;&#28779;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#21487;&#25913;&#21892;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#22312;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;&#25910;&#25947;&#24615;&#65292;&#23454;&#29616;&#23545;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#21442;&#25968;&#30340;&#21487;&#38752;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25551;&#36848;&#31185;&#23398;&#20013;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20294;&#30830;&#23450;&#35299;&#37322;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#30340;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#25955;&#22238;&#28779;&#36825;&#19968;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#23427;&#38024;&#23545;ODEs&#20013;&#30340;&#27010;&#29575;&#25968;&#20540;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;&#26799;&#24230;&#20248;&#21270;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#12290;&#36890;&#36807;&#36845;&#20195;&#20943;&#23569;&#27010;&#29575;&#31215;&#20998;&#22120;&#30340;&#19968;&#20010;&#22122;&#22768;&#21442;&#25968;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#20110;&#20855;&#26377;&#23454;&#38469;&#30456;&#20851;&#21442;&#25968;&#25968;&#37327;&#30340;Hodgkin-Huxley&#27169;&#22411;&#33719;&#24471;&#21487;&#38752;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11354</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#36335;&#30001;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;PEOs&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#22270;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11354v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;ANNS&#30340;&#20248;&#36234;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#21508;&#31181;&#22522;&#20110;&#22270;&#30340;ANNS&#20248;&#21270;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#32570;&#20047;&#27491;&#24335;&#29702;&#35770;&#25903;&#25345;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#22270;&#30340;ANNS&#20013;&#30340;&#36335;&#30001;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#22270;&#20013;&#33410;&#28857;&#30340;&#37051;&#23621;&#26102;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#27010;&#29575;&#36335;&#30001;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#25935;&#24863;&#25216;&#26415;&#24320;&#21457;&#20102;&#20004;&#31181;&#22522;&#20934;&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PEOs&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#35782;&#21035;&#22270;&#20013;&#24212;&#32771;&#34385;&#36827;&#34892;&#31934;&#30830;&#36317;&#31163;&#35745;&#31639;&#30340;&#37051;&#23621;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#23454;&#36341;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
&lt;/p&gt;</description></item><item><title>Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.05657</link><description>&lt;p&gt;
Agent Lumos: &#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#35757;&#32451;&#24320;&#28304;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Lumos: Unified and Modular Training for Open-Source Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05657
&lt;/p&gt;
&lt;p&gt;
Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#36127;&#25285;&#24471;&#36215;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#12290;&#36825;&#20419;&#20351;&#20102;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; LUMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#35757;&#32451;&#24320;&#28304; LLM-based &#20195;&#29702;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;LUMOS&#20855;&#26377;&#21487;&#23398;&#20064;&#12289;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#25509;&#22320;&#27169;&#22359;&#65292;&#29992;&#20110;&#20351;&#29992;&#25191;&#34892;&#27169;&#22359;&#20013;&#30340;&#21508;&#31181;&#24037;&#20855;&#23558;&#36825;&#20123;&#36716;&#21270;&#20026;&#21160;&#20316;&#12290;&#36825;&#31181;&#35774;&#35745;&#20801;&#35768;&#27169;&#22359;&#21270;&#21319;&#32423;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20114;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#28304;&#33258;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#19981;&#21516;&#22320;&#38754;&#30495;&#23454;&#25512;&#29702;&#21407;&#29702;&#30340;&#22823;&#35268;&#27169;&#12289;&#32479;&#19968;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#27880;&#37322;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;LUMOS&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;LUMOS&#22312;&#22810;&#20010;&#36739;&#22823;&#30340;&#24320;&#28304;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
&lt;/p&gt;</description></item><item><title>DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.17000</link><description>&lt;p&gt;
DistriBlock: &#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.17000
&lt;/p&gt;
&lt;p&gt;
DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#35823;&#23548;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20351;&#20854;&#39044;&#27979;&#20219;&#24847;&#30446;&#26631;&#25991;&#26412;&#65292;&#20174;&#32780;&#26500;&#25104;&#26126;&#26174;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistriBlock&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;ASR&#31995;&#32479;&#30340;&#39640;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#39044;&#27979;&#36755;&#20986;&#26631;&#35760;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#35813;&#20998;&#24067;&#30340;&#19968;&#32452;&#29305;&#24449;&#36827;&#34892;&#27979;&#37327;&#65306;&#36755;&#20986;&#27010;&#29575;&#30340;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#65292;&#20998;&#24067;&#30340;&#29109;&#65292;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#21644;Jensen-Shannon&#25955;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#20998;&#31867;&#12289;&#36825;&#31181;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2211.10777</link><description>&lt;p&gt;
&#26080;&#30456;&#24178;&#31354;&#20013;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Non-Coherent Over-the-Air Decentralized Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26080;&#32447;&#31995;&#32479;&#30340;DGD&#31639;&#27861;&#65292;&#36890;&#36807;&#26080;&#30456;&#24178;&#31354;&#20013;&#20849;&#35782;&#26041;&#26696;&#23454;&#29616;&#26080;&#38656;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#35832;&#22914;&#36828;&#31243;&#24863;&#30693;&#12289;&#20998;&#24067;&#24335;&#25512;&#26029;&#12289;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#21508;&#31181;&#39046;&#22495;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#21463;&#21040;&#22122;&#22768;&#12289;&#34928;&#33853;&#21644;&#24102;&#23485;&#21463;&#38480;&#30340;&#26080;&#32447;&#31995;&#32479;&#19978;&#25191;&#34892;DGD&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#24230;&#20256;&#36755;&#20197;&#20943;&#36731;&#24178;&#25200;&#65292;&#24182;&#33719;&#21462;&#25299;&#25169;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36825;&#22312;&#26080;&#32447;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#26159;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#26080;&#32447;&#31995;&#32479;&#23450;&#21046;&#30340;DGD&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#22312;&#26080;&#38656;&#36827;&#34892;&#26234;&#33021;&#20307;&#21327;&#35843;&#12289;&#25299;&#25169;&#20449;&#24687;&#25110;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#12290;&#20854;&#26680;&#24515;&#26159;&#19968;&#31181;&#26080;&#30456;&#24178;&#31354;&#20013;&#65288;NCOTA&#65289;&#20849;&#35782;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#26080;&#32447;&#20449;&#36947;&#30340;&#22122;&#22768;&#33021;&#37327;&#21472;&#21152;&#29305;&#24615;&#12290;&#36890;&#36807;&#38543;&#26426;&#21270;&#20256;&#36755;&#31574;&#30053;&#26469;&#36866;&#24212;&#21322;&#21452;&#24037;&#25805;&#20316;&#65292;&#21457;&#23556;&#26426;&#23558;&#20301;&#32622;&#26144;&#23556;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10777v2 Announce Type: replace-cross  Abstract: Decentralized Gradient Descent (DGD) is a popular algorithm used to solve decentralized optimization problems in diverse domains such as remote sensing, distributed inference, multi-agent coordination, and federated learning. Yet, executing DGD over wireless systems affected by noise, fading and limited bandwidth presents challenges, requiring scheduling of transmissions to mitigate interference and the acquisition of topology and channel state information -- complex tasks in wireless decentralized systems. This paper proposes a DGD algorithm tailored to wireless systems. Unlike existing approaches, it operates without inter-agent coordination, topology information, or channel state information. Its core is a Non-Coherent Over-The-Air (NCOTA) consensus scheme, exploiting a noisy energy superposition property of wireless channels. With a randomized transmission strategy to accommodate half-duplex operation, transmitters map loca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.14057</link><description>&lt;p&gt;
&#24038;/&#21491;&#33041;&#12289;&#20154;&#31867;&#36816;&#21160;&#25511;&#21046;&#21450;&#23545;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#24038;&#21491;&#21322;&#29699;&#19987;&#38376;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#22312;&#19981;&#21516;&#30340;&#36816;&#21160;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21327;&#35843;&#24615;&#12289;&#36816;&#21160;&#25928;&#29575;&#21644;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36816;&#21160;&#25511;&#21046;&#22120;&#30456;&#23545;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#21508;&#31181;&#20248;&#28857;&#65292;&#28982;&#32780;&#30001;&#20110;&#20854;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#31934;&#30830;&#36816;&#21160;&#65292;&#22240;&#27492;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#37319;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21452;&#20391;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#36816;&#21160;&#20219;&#21153;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#21322;&#29699;&#19987;&#38376;&#21270;&#65306;&#20248;&#21183;&#31995;&#32479;&#65288;&#36890;&#24120;&#26159;&#21491;&#25163;&#12289;&#24038;&#21322;&#29699;&#65289;&#25797;&#38271;&#21327;&#35843;&#21644;&#36816;&#21160;&#25928;&#29575;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#20248;&#21183;&#31995;&#32479;&#22312;&#38656;&#35201;&#20301;&#32622;&#31283;&#23450;&#24615;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#23545;&#21322;&#29699;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#19987;&#38376;&#21270;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#19987;&#38376;&#21270;&#21322;&#29699;&#21644;&#26080;&#19987;&#38376;&#21270;&#21322;&#29699;&#12289;&#20855;&#26377;&#21322;&#29699;&#38388;&#36830;&#25509;&#65288;&#20195;&#34920;&#29983;&#29289;&#23398;&#33041;&#26725;&#65289;&#21644;&#26080;&#21322;&#29699;&#38388;&#36830;&#25509;&#12289;&#20855;&#26377;&#19987;&#38376;&#21270;&#21644;&#26080;&#19987;&#38376;&#21270;&#30340;&#21333;&#20391;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Network movement controllers promise a variety of advantages over conventional control methods however they are not widely adopted due to their inability to produce reliably precise movements. This research explores a bilateral neural network architecture as a control system for motor tasks. We aimed to achieve hemispheric specialisation similar to what is observed in humans across different tasks; the dominant system (usually the right hand, left hemisphere) excels at tasks involving coordination and efficiency of movement, and the non-dominant system performs better at tasks requiring positional stability. Specialisation was achieved by training the hemispheres with different loss functions tailored toward the expected behaviour of the respective hemispheres. We compared bilateral models with and without specialised hemispheres, with and without inter-hemispheric connectivity (representing the biological Corpus Callosum), and unilateral models with and without specialisation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#30340;&#39640;&#25928;&#19968;&#38454;&#22810;&#26799;&#24230;&#31639;&#27861;FORUM&#65292;&#36890;&#36807;&#20540;&#20989;&#25968;&#26041;&#27861;&#23558;MOBLO&#38382;&#39064;&#37325;&#26032;&#24418;&#24335;&#21270;&#20026;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09257</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#30340;&#19968;&#38454;&#22810;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization. (arXiv:2401.09257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#30340;&#39640;&#25928;&#19968;&#38454;&#22810;&#26799;&#24230;&#31639;&#27861;FORUM&#65292;&#36890;&#36807;&#20540;&#20989;&#25968;&#26041;&#27861;&#23558;MOBLO&#38382;&#39064;&#37325;&#26032;&#24418;&#24335;&#21270;&#20026;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#21452;&#23618;&#20248;&#21270;&#65288;MOBLO&#65289;&#38382;&#39064;&#65292;&#20854;&#20013;&#19978;&#23618;&#23376;&#38382;&#39064;&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#19979;&#23618;&#23376;&#38382;&#39064;&#26159;&#19968;&#20010;&#26631;&#37327;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#22522;&#20110;&#26799;&#24230;&#30340;MOBLO&#31639;&#27861;&#38656;&#35201;&#35745;&#31639;Hessian&#30697;&#38453;&#65292;&#20174;&#32780;&#23548;&#33268;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19968;&#38454;&#22810;&#26799;&#24230;&#31639;&#27861;FORUM&#29992;&#20110;MOBLO&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#20540;&#20989;&#25968;&#26041;&#27861;&#23558;MOBLO&#38382;&#39064;&#37325;&#26032;&#24418;&#24335;&#21270;&#20026;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32422;&#26463;MOO&#38382;&#39064;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#21644;&#38750;&#28176;&#36827;&#25910;&#25947;&#32467;&#26524;&#12290;&#20174;&#23454;&#35777;&#19978;&#35762;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;FORUM&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#23427;&#36798;&#21040;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-gradient method for MOBLO, called FORUM. Specifically, we reformulate MOBLO problems as a constrained multi-objective optimization (MOO) problem via the value-function approach. Then we propose a novel multi-gradient aggregation method to solve the challenging constrained MOO problem. Theoretically, we provide the complexity analysis to show the efficiency of the proposed method and a non-asymptotic convergence result. Empirically, extensive experiments demonstrate the effectiveness and efficiency of the proposed FORUM method in different learning problems. In particular, it achieves state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Lie&#32676;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#32039;&#33268;&#38750;&#38463;&#36125;&#23572;&#30340;Lie&#32676;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;$\text{GL}^{+}(n, \mathbb{R})$&#21644;$\text{SL}(n, \mathbb{R})$&#36825;&#20004;&#20010;Lie&#32676;&#12290;</title><link>http://arxiv.org/abs/2310.11366</link><description>&lt;p&gt;
Lie Group Decompositions for Equivariant Neural Networks. (arXiv:2310.11366v1 [cs.LG]) (&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;Lie&#32676;&#20998;&#35299;)
&lt;/p&gt;
&lt;p&gt;
Lie Group Decompositions for Equivariant Neural Networks. (arXiv:2310.11366v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Lie&#32676;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#38750;&#32039;&#33268;&#38750;&#38463;&#36125;&#23572;&#30340;Lie&#32676;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;$\text{GL}^{+}(n, \mathbb{R})$&#21644;$\text{SL}(n, \mathbb{R})$&#36825;&#20004;&#20010;Lie&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#65288;&#21367;&#31215;&#65289;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#65292;&#23545;&#20960;&#20309;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#30340;&#23545;&#31216;&#32676;&#20026;&#32039;&#33268;&#25110;&#38463;&#36125;&#23572;&#32676;&#65292;&#25110;&#32773;&#20004;&#32773;&#37117;&#26159;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25299;&#23637;&#20102;&#20351;&#29992;&#30340;&#21464;&#25442;&#31867;&#21035;&#21040;Lie&#32676;&#30340;&#24773;&#20917;&#65292;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#20854;Lie&#20195;&#25968;&#20197;&#21450;&#32676;&#30340;&#25351;&#25968;&#21644;&#23545;&#25968;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#22312;&#36866;&#29992;&#20110;&#26356;&#22823;&#30340;&#21464;&#25442;&#32676;&#26102;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#26681;&#25454;&#25152;&#20851;&#24515;&#30340;&#32676;$G$&#30340;&#19981;&#21516;&#65292;&#25351;&#25968;&#26144;&#23556;&#21487;&#33021;&#19981;&#28385;&#23556;&#12290;&#24403;$G$&#26082;&#19981;&#26159;&#32039;&#33268;&#32676;&#20063;&#19981;&#26159;&#38463;&#36125;&#23572;&#32676;&#26102;&#65292;&#36824;&#20250;&#36935;&#21040;&#36827;&#19968;&#27493;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21033;&#29992;Lie&#32676;&#21450;&#20854;&#40784;&#27425;&#31354;&#38388;&#30340;&#32467;&#26500;&#21644;&#20960;&#20309;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22788;&#29702;&#36825;&#31867;&#32676;&#30340;&#26694;&#26550;&#65292;&#20027;&#35201;&#20851;&#27880;Lie&#32676;$G = \text{GL}^{+}(n, \mathbb{R})$&#21644;$G = \text{SL}(n, \mathbb{R}$&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest $G$, the exponential map may not be surjective. Further limitations are encountered when $G$ is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups $G = \text{GL}^{+}(n, \mathbb{R})$ and $G = \text{SL}(n, \mathbb{R}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.07402</link><description>&lt;p&gt;
NuTime: &#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;Transformer&#26550;&#26500;&#21644;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#20351;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#30740;&#31350;&#26174;&#31034;&#20986;&#23398;&#20064;&#35821;&#20041;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#38480;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#25968;&#21315;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#25991;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#37319;&#29992;Transformer&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#31383;&#21475;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31383;&#21475;&#30340;&#26631;&#20934;&#21270;&#24418;&#29366;&#21644;&#20004;&#20010;&#26631;&#37327;&#20540;&#34920;&#31034;&#27599;&#20010;&#31383;&#21475;&#20869;&#30340;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#12290;&#20026;&#20102;&#23558;&#21487;&#33021;&#20855;&#26377;&#20219;&#24847;&#25968;&#20540;&#23610;&#24230;&#30340;&#26631;&#37327;&#20540;&#23884;&#20837;&#21040;&#39640;&#32500;&#21521;&#37327;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#27169;&#22359;&#65292;&#26522;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#26631;&#37327;&#20540;&#23610;&#24230;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#20540;&#22810;&#23610;&#24230;&#23884;&#20837;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31616;&#21333;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on time-series self-supervised models shows great promise in learning semantic representations. However, it has been limited to small-scale datasets, e.g., thousands of temporal sequences. In this work, we make key technical contributions that are tailored to the numerical properties of time-series data and allow the model to scale to large datasets, e.g., millions of temporal sequences. We adopt the Transformer architecture by first partitioning the input into non-overlapping windows. Each window is then characterized by its normalized shape and two scalar values denoting the mean and standard deviation within each window. To embed scalar values that may possess arbitrary numerical scales to high-dimensional vectors, we propose a numerically multi-scaled embedding module enumerating all possible scales for the scalar values. The model undergoes pretraining using the proposed numerically multi-scaled embedding with a simple contrastive objective on a large-scale dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.07338</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Foundation Models for Learning on Tabular Data. (arXiv:2310.07338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#25903;&#25745;&#30528;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#24320;&#21457;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26377;&#25928;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#30446;&#21069;&#21487;&#36716;&#31227;&#30340;&#34920;&#26684;&#27169;&#22411;&#20173;&#28982;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#35201;&#20040;&#32570;&#20047;&#30452;&#25509;&#25351;&#20196;&#36319;&#38543;&#26032;&#20219;&#21153;&#30340;&#25903;&#25345;&#65292;&#35201;&#20040;&#24573;&#35270;&#20174;&#19981;&#21516;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#22522;&#30784;&#30693;&#35782;&#21644;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tabular Foundation Models (TabFMs)&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;TabFMs&#21033;&#29992;&#29983;&#25104;&#22411;&#34920;&#26684;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#37319;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32463;&#36807;&#19987;&#38376;&#35774;&#35745;&#30340;&#30446;&#26631;&#22312;&#22823;&#33539;&#22260;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#36171;&#20104;TabFMs&#28145;&#21051;&#30340;&#29702;&#35299;&#21644;&#26222;&#36941;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#24378;&#35843;&#20102;TabFM&#30340;&#26377;&#25928;&#24615;&#65306;&#23427;&#19981;&#20165;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#25512;&#29702;&#31561;&#36981;&#24490;&#25351;&#20196;&#30340;&#20219;&#21153;&#20013;&#26126;&#26174;&#20986;&#33394;&#65292;
&lt;/p&gt;
&lt;p&gt;
Learning on tabular data underpins numerous real-world applications. Despite considerable efforts in developing effective learning models for tabular data, current transferable tabular models remain in their infancy, limited by either the lack of support for direct instruction following in new tasks or the neglect of acquiring foundational knowledge and capabilities from diverse tabular datasets. In this paper, we propose Tabular Foundation Models (TabFMs) to overcome these limitations. TabFMs harness the potential of generative tabular learning, employing a pre-trained large language model (LLM) as the base model and fine-tuning it using purpose-designed objectives on an extensive range of tabular datasets. This approach endows TabFMs with a profound understanding and universal capabilities essential for learning on tabular data. Our evaluations underscore TabFM's effectiveness: not only does it significantly excel in instruction-following tasks like zero-shot and in-context inference
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.05833</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance-Covariance Decomposition of Kernel Scores for Generative Models. (arXiv:2310.05833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#24341;&#20837;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#26680;&#35780;&#20998;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#24212;&#29992;&#22312;&#25193;&#25955;&#27169;&#22411;&#19978;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23578;&#19981;&#23384;&#22312;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#27867;&#21270;&#34892;&#20026;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#20197;&#19968;&#31181;&#29305;&#23450;&#20219;&#21153;&#30340;&#20020;&#26102;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#33258;&#28982;&#35821;&#35328;&#26041;&#27861;&#19981;&#33021;&#24212;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#29992;&#20110;&#26680;&#35780;&#20998;&#21450;&#20854;&#30456;&#20851;&#29109;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#37327;&#30340;&#26080;&#20559;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#65292;&#21482;&#38656;&#35201;&#29983;&#25104;&#26679;&#26412;&#32780;&#19981;&#38656;&#35201;&#24213;&#23618;&#27169;&#22411;&#26412;&#36523;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#27867;&#21270;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#23569;&#25968;&#32676;&#20307;&#30340;&#27169;&#24335;&#22349;&#32553;&#26159;&#19968;&#31181;&#19982;&#36807;&#25311;&#21512;&#30456;&#21453;&#30340;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26041;&#24046;&#21644;&#39044;&#27979;&#26680;&#29109;&#26159;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#34892;&#24230;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#26679;&#26412;&#29983;&#25104;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#19988;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models, like large language models, are becoming increasingly relevant in our daily lives, yet a theoretical framework to assess their generalization behavior and uncertainty does not exist. Particularly, the problem of uncertainty estimation is commonly solved in an ad-hoc manner and task dependent. For example, natural language approaches cannot be transferred to image generation. In this paper we introduce the first bias-variance-covariance decomposition for kernel scores and their associated entropy. We propose unbiased and consistent estimators for each quantity which only require generated samples but not the underlying model itself. As an application, we offer a generalization evaluation of diffusion models and discover how mode collapse of minority groups is a contrary phenomenon to overfitting. Further, we demonstrate that variance and predictive kernel entropy are viable measures of uncertainty for image, audio, and language generation. Specifically, our approach f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13815</link><description>&lt;p&gt;
SyMOT-Flow: &#23398;&#20064;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21160;&#21450;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#25442;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#21644;&#25191;&#34892;&#23494;&#24230;&#20272;&#35745;&#12289;&#26679;&#26412;&#29983;&#25104;&#21644;&#32479;&#35745;&#25512;&#26029;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#19968;&#20010;&#30701;&#36317;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#36716;&#25442;&#12290;&#24471;&#21040;&#30340;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20302;&#32500;&#31034;&#20363;&#21644;&#39640;&#32500;&#29983;&#25104;&#26679;&#26412;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2307.14012</link><description>&lt;p&gt;
MCMC-&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#27169;&#22411;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
MCMC-Correction of Score-Based Diffusion Models for Model Composition. (arXiv:2307.14012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#27491;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;&#21508;&#31181;MCMC&#26041;&#27861;&#32467;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#32452;&#21512;&#21644;&#36827;&#34892;&#26356;&#22909;&#30340;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29992;&#24471;&#20998;&#25110;&#33021;&#37327;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#12290;&#33021;&#37327;&#21442;&#25968;&#21270;&#20855;&#26377;&#26356;&#22909;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#20027;&#35201;&#26159;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#25552;&#35758;&#26679;&#26412;&#20013;&#24635;&#33021;&#37327;&#30340;&#21464;&#21270;&#22522;&#20110;Metropolis-Hastings&#20462;&#27491;&#27493;&#39588;&#26469;&#36827;&#34892;&#25193;&#23637;&#37319;&#26679;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#23427;&#20284;&#20046;&#20135;&#29983;&#20102;&#31245;&#24494;&#36739;&#24046;&#30340;&#24615;&#33021;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30001;&#20110;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26222;&#36941;&#27969;&#34892;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#33021;&#37327;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21066;&#24369;&#20102;&#27169;&#22411;&#32452;&#21512;&#30340;&#30446;&#30340;&#65292;&#21363;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#21512;&#36215;&#26469;&#20174;&#26032;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#24314;&#35758;&#20445;&#30041;&#24471;&#20998;&#21442;&#25968;&#21270;&#65292;&#32780;&#26159;&#36890;&#36807;&#23545;&#24471;&#20998;&#20989;&#25968;&#36827;&#34892;&#32447;&#31215;&#20998;&#26469;&#35745;&#31639;&#22522;&#20110;&#33021;&#37327;&#30340;&#25509;&#21463;&#27010;&#29575;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#29992;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#21508;&#31181;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#26041;&#27861;&#32452;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#26799;&#24230;&#27969;&#20013;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#19978;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2307.00144</link><description>&lt;p&gt;
&#36981;&#23432;&#27861;&#24459;&#24182;&#36981;&#24490;&#27969;&#31243;&#65306;&#26799;&#24230;&#27969;&#30340;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows. (arXiv:2307.00144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#21644;&#30740;&#31350;&#26799;&#24230;&#27969;&#20013;&#30340;&#23432;&#24658;&#23450;&#24459;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#19978;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25581;&#31034;&#20102;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#30340;&#20960;&#20309;&#29305;&#24615;&#26159;&#35299;&#23494;&#38750;&#24120;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26368;&#36817;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35266;&#23519;&#26159;&#65292;&#36229;&#21442;&#25968;&#35843;&#33410;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#19968;&#20123;&#20248;&#21270;&#21021;&#22987;&#21270;&#30340;&#29305;&#24615;&#12290;&#36825;&#31181;&#8220;&#38544;&#24335;&#20559;&#24046;&#8221;&#34987;&#35748;&#20026;&#26159;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26377;&#21033;&#29305;&#24615;&#24182;&#33021;&#35299;&#37322;&#20854;&#33391;&#22909;&#27867;&#21270;&#29305;&#24615;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26377;&#19977;&#20010;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20005;&#26684;&#20171;&#32461;&#20102;&#8220;&#23432;&#24658;&#23450;&#24459;&#8221;&#30340;&#23450;&#20041;&#21644;&#22522;&#26412;&#24615;&#36136;&#65292;&#36825;&#20123;&#23432;&#24658;&#23450;&#24459;&#26159;&#22312;&#32473;&#23450;&#27169;&#22411;&#65288;&#20363;&#22914;&#20855;&#26377;&#32473;&#23450;&#26550;&#26500;&#30340;ReLU&#32593;&#32476;&#65289;&#30340;&#26799;&#24230;&#27969;&#20013;&#29420;&#31435;&#20445;&#25345;&#30340;&#26368;&#22823;&#37327;&#12290;&#19981;&#35770;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#25968;&#25454;&#21644;&#20219;&#20309;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#22914;&#20309;&#36890;&#36807;&#23545;&#27169;&#22411;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#29983;&#25104;&#30340;&#26446;&#20195;&#25968;&#36827;&#34892;&#26377;&#38480;&#32500;&#20195;&#25968;&#36816;&#31639;&#65292;&#25214;&#21040;&#36825;&#20123;&#25968;&#37327;&#30340;&#30830;&#20999;&#25968;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;SageMath&#20013;&#23454;&#29616;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms (implemented in SageMath) to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#35299;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20026;&#21333;&#21464;&#37327;&#22238;&#24402;&#65292;&#24182;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#22238;&#24402;&#23384;&#22312;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04718</link><description>&lt;p&gt;
&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Neural Symbolic Regression using Control Variables. (arXiv:2306.04718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04718
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20998;&#35299;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20026;&#21333;&#21464;&#37327;&#22238;&#24402;&#65292;&#24182;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#65292;&#35299;&#20915;&#20102;&#31526;&#21495;&#22238;&#24402;&#23384;&#22312;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#20998;&#26512;&#25968;&#23398;&#34920;&#36798;&#24335;&#30340;&#24378;&#26377;&#21147;&#25216;&#26415;&#65292;&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#65292;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#21464;&#37327;&#30340;&#22797;&#26434;&#26041;&#31243;&#26102;&#38754;&#20020;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SRCV&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#22686;&#24378;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#22810;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#21464;&#37327; SR &#38382;&#39064;&#65292;&#28982;&#21518;&#20174;&#24213;&#37096;&#21521;&#19978;&#32452;&#21512;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22235;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20174;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#25968;&#25454;&#29983;&#25104;&#22120;&#36890;&#36807;&#25511;&#21046;&#36755;&#20837;&#21464;&#37327;&#26469;&#29983;&#25104;&#29305;&#23450;&#21464;&#37327;&#30340;&#26679;&#26412;&#12290;&#31532;&#19977;&#65292;&#24212;&#29992;&#21333;&#21464;&#37327;&#31526;&#21495;&#22238;&#24402;&#26469;&#20272;&#35745;&#30456;&#24212;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a powerful technique for discovering the analytical mathematical expression from data, finding various applications in natural sciences due to its good interpretability of results. However, existing methods face scalability issues when dealing with complex equations involving multiple variables. To address this challenge, we propose SRCV, a novel neural symbolic regression method that leverages control variables to enhance both accuracy and scalability. The core idea is to decompose multi-variable symbolic regression into a set of single-variable SR problems, which are then combined in a bottom-up manner. The proposed method involves a four-step process. First, we learn a data generator from observed data using deep neural networks (DNNs). Second, the data generator is used to generate samples for a certain variable by controlling the input variables. Thirdly, single-variable symbolic regression is applied to estimate the corresponding mathematical expressio
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04489</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fair Column Subset Selection. (arXiv:2306.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04489
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20102;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#24050;&#30693;&#26041;&#27861;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;1.5&#20493;&#30340;&#22823;&#23567;&#19979;&#23454;&#29616;&#19982;&#20004;&#20493;&#30456;&#21516;&#30340;&#36817;&#20284;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20844;&#24179;&#30340;&#21015;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20013;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#65292;&#24182;&#19988;&#25152;&#36873;&#21015;&#23376;&#38598;&#24517;&#39035;&#30456;&#23545;&#20110;&#23427;&#20204;&#21508;&#33258;&#30340;&#26368;&#20339;&#31209;-k&#36924;&#36817;&#25552;&#20379;&#33391;&#22909;&#30340;&#36817;&#20284;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20844;&#24179;&#35774;&#32622;&#24341;&#20837;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#20026;&#20102;&#25193;&#23637;&#24050;&#30693;&#32467;&#26524;&#65292;&#20154;&#20204;&#19981;&#33021;&#20570;&#24471;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#21407;&#22987;&#26041;&#27861;&#30340;&#20004;&#20493;&#21015;&#26356;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#26464;&#26438;&#20998;&#25968;&#37319;&#26679;&#30340;&#24050;&#30693;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#20004;&#20010;&#32676;&#20307;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20165;&#37319;&#26679;&#36866;&#24403;&#22823;&#23567;&#30340;&#23376;&#38598;&#23601;&#21464;&#24471;NP&#38590;&#12290;&#32780;&#25214;&#21040;&#20004;&#20493;&#20110;&#25152;&#38656;&#22823;&#23567;&#30340;&#23376;&#38598;&#21017;&#38750;&#24120;&#31616;&#21333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22522;&#26412;&#19978;1.5&#20493;&#30340;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30456;&#21516;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of fair column subset selection. In particular, we assume that two groups are present in the data, and the chosen column subset must provide a good approximation for both, relative to their respective best rank-k approximations. We show that this fair setting introduces significant challenges: in order to extend known results, one cannot do better than the trivial solution of simply picking twice as many columns as the original methods. We adopt a known approach based on deterministic leverage-score sampling, and show that merely sampling a subset of appropriate size becomes NP-hard in the presence of two groups. Whereas finding a subset of two times the desired size is trivial, we provide an efficient algorithm that achieves the same guarantees with essentially 1.5 times that size. We validate our methods through an extensive set of experiments on real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13498</link><description>&lt;p&gt;
&#29992;&#20110;&#24102;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise. (arXiv:2305.13498v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27979;&#37327;&#22122;&#22768;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#21644;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#25913;&#21892;&#25968;&#25454;&#20998;&#26512;&#30340;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#23545;Ornstein-Uhlenbeck&#36807;&#31243;&#21442;&#25968;&#25311;&#21512;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20056;&#24615;&#22122;&#22768;&#21644;&#28909;&#22122;&#22768;&#23545;&#20449;&#21495;&#20998;&#31163;&#31934;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#25928;&#21306;&#20998;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12289;&#25913;&#21892;&#21442;&#25968;&#20272;&#35745;&#31934;&#24230;&#30340;&#31639;&#27861;&#21644;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20056;&#24615;&#21644;&#28909;&#22122;&#22768;&#23545;&#23454;&#38469;&#20449;&#21495;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#26377;&#25928;&#20998;&#31163;&#28909;&#22122;&#22768;&#30340;&#31639;&#27861;&#65292;&#20854;&#24615;&#33021;&#21487;&#19982;Hamilton Monte Carlo (HMC)&#30456;&#23218;&#32654;&#65292;&#20294;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20056;&#24615;&#22122;&#22768;&#65292;&#24182;&#35777;&#26126;&#20102;HMC&#26080;&#27861;&#38548;&#31163;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#39069;&#22806;&#20102;&#35299;&#28909;&#22122;&#22768;&#21644;&#20056;&#24615;&#22122;&#22768;&#20043;&#38388;&#27604;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#22320;&#20272;&#35745;&#21442;&#25968;&#21644;&#20998;&#31163;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article aims to investigate the impact of noise on parameter fitting for an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and thermal noise on the accuracy of signal separation. To address these issues, we propose algorithms and methods that can effectively distinguish between thermal and multiplicative noise and improve the precision of parameter estimation for optimal data analysis. Specifically, we explore the impact of both multiplicative and thermal noise on the obfuscation of the actual signal and propose methods to resolve them. Firstly, we present an algorithm that can effectively separate thermal noise with comparable performance to Hamilton Monte Carlo (HMC) but with significantly improved speed. Subsequently, we analyze multiplicative noise and demonstrate that HMC is insufficient for isolating thermal and multiplicative noise. However, we show that, with additional knowledge of the ratio between thermal and multiplicative noise, we can accuratel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05506</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#38544;&#31169;&#19982;&#23433;&#20840;&#65306;FedGT&#30340;&#32676;&#20307;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework. (arXiv:2305.05506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;FedGT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#24182;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#12290;&#21463;&#21040;&#32676;&#20307;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#37325;&#21472;&#30340;&#23458;&#25143;&#32452;&#26469;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#30340;&#23384;&#22312;&#65292;&#24182;&#36890;&#36807;&#35793;&#30721;&#25805;&#20316;&#35782;&#21035;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#34987;&#35782;&#21035;&#30340;&#23458;&#25143;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#21024;&#38500;&#65292;&#24182;&#22312;&#20854;&#20313;&#23458;&#25143;&#20043;&#38388;&#25191;&#34892;&#35757;&#32451;&#12290;FedGT&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20801;&#35768;&#25913;&#36827;&#35782;&#21035;&#33021;&#21147;&#21516;&#26102;&#20173;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26381;&#21153;&#22120;&#23398;&#20064;&#27599;&#20010;&#32452;&#20013;&#23458;&#25143;&#30340;&#32858;&#21512;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedGT&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#65292;&#20855;&#26377;&#20302;&#35823;&#26816;&#21644;&#34394;&#35686;&#27010;&#29575;&#65292;&#20135;&#29983;&#39640;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FedGT, a novel framework for identifying malicious clients in federated learning with secure aggregation. Inspired by group testing, the framework leverages overlapping groups of clients to detect the presence of malicious clients in the groups and to identify them via a decoding operation. The identified clients are then removed from the training of the model, which is performed over the remaining clients. FedGT strikes a balance between privacy and security, allowing for improved identification capabilities while still preserving data privacy. Specifically, the server learns the aggregated model of the clients in each group. The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets, showing its ability to identify malicious clients with low misdetection and false alarm probabilities, resulting in high model utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14824</link><description>&lt;p&gt;
&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20083;&#21046;&#21697;&#24066;&#22330;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#20892;&#27665;&#24517;&#39035;&#19981;&#26029;&#25913;&#36827;&#20182;&#20204;&#30340;&#30044;&#29287;&#29983;&#20135;&#31995;&#32479;&#12290;&#31934;&#30830;&#30044;&#29287;&#19994;&#25216;&#26415;&#25552;&#20379;&#20102;&#21830;&#19994;&#20892;&#22330;&#21160;&#29289;&#20010;&#20307;&#21270;&#30417;&#27979;&#65292;&#20248;&#21270;&#30044;&#29287;&#29983;&#20135;&#12290;&#36830;&#32493;&#30340;&#22768;&#23398;&#30417;&#27979;&#26159;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#24863;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#33258;&#30001;&#25918;&#29287;&#29275;&#30340;&#26085;&#21453;&#21005;&#21644;&#21507;&#33609;&#26102;&#38388;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#29287;&#22330;&#19978;&#30340;&#20856;&#22411;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26126;&#26174;&#24433;&#21709;&#24403;&#21069;&#22768;&#23398;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#26041;&#27861;&#65292;&#31216;&#20026;&#25239;&#22122;&#22768;&#35269;&#39135;&#27963;&#21160;&#35782;&#21035;&#22120; (NRFAR)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#30830;&#23450;&#35269;&#39135;&#27963;&#21160;&#30340;&#31361;&#21457;&#12290;NRFAR &#30340;&#21152;&#24615;&#22122;&#22768;&#40065;&#26834;&#24615;&#20351;&#29992;&#38745;&#24577;&#39640;&#26031;&#30333;&#22122;&#22768;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#38750;&#38745;&#24577;&#33258;&#28982;&#22122;&#22768;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
&lt;/p&gt;</description></item><item><title>CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;</title><link>http://arxiv.org/abs/2303.17568</link><description>&lt;p&gt;
CodeGeeX&#65306;&#22810;&#35821;&#35328;&#35780;&#20272;&#19979;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. (arXiv:2303.17568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17568
&lt;/p&gt;
&lt;p&gt;
CodeGeeX&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;130&#20159;&#21442;&#25968;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#27492;&#22806;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;OpenAI Codex&#65289;&#21487;&#20197;&#29983;&#25104;&#27491;&#30830;&#35821;&#27861;&#21644;&#21151;&#33021;&#30340;&#20195;&#30721;&#65292;&#20351;&#31243;&#24207;&#21592;&#30340;&#32534;&#30721;&#26356;&#21152;&#39640;&#25928;&#65292;&#20351;&#25105;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36861;&#27714;&#26356;&#21152;&#36148;&#36817;&#29616;&#23454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeGeeX&#65292;&#19968;&#20010;&#20855;&#26377;130&#20159;&#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;CodeGeeX&#22312;2022&#24180;6&#26376;&#26102;&#22522;&#20110;23&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;8500&#20159;&#20196;&#29260;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CodeGeeX&#22312;HumanEval-X&#19978;&#30340;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#35268;&#27169;&#30456;&#20284;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#12290;&#22312;HumanEval&#65288;&#20165;&#38480;Python&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;HumanEval-X&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#25163;&#20889;C ++&#12289;Java&#12289;JavaScript&#21644;Go&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#35780;&#20272;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;Visual Studio Code&#12289;JetBrains&#21644;Cloud Studio&#19978;&#26500;&#24314;&#20102;&#22522;&#20110;CodeGeeX&#30340;&#25193;&#23637;&#65292;&#27599;&#21608;&#20026;&#25968;&#20197;&#19975;&#35745;&#30340;&#27963;&#36291;&#29992;&#25143;&#29983;&#25104;47&#20159;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;CodeGeeX&#21487;&#20197;&#23558;&#31243;&#24207;&#21592;&#30340;&#29983;&#20135;&#21147;&#25552;&#39640;22%&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2203.07139</link><description>&lt;p&gt;
&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#65306;&#22810;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#20844;&#24179;&#24615;&#19982;&#20262;&#29702;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-model Fairness: Empirical Study of Fairness and Ethics Under Model Multiplicity. (arXiv:2203.07139v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#26032;&#23450;&#20041;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#36873;&#25321;&#19981;&#21516;&#39044;&#27979;&#22120;&#36827;&#34892;&#27169;&#22411;&#22810;&#26679;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#20010;&#20154;&#21463;&#20260;&#23475;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39044;&#27979;&#27169;&#22411;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#25216;&#26415;&#26500;&#36896;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#22312;&#31038;&#20250;&#32972;&#26223;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#21892;&#24847;&#30340;&#24037;&#31243;&#36873;&#25321;&#21487;&#33021;&#24102;&#26469;&#38544;&#21547;&#30340;&#12289;&#38388;&#25509;&#30340;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#29616;&#23454;&#21518;&#26524;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#65292;&#28041;&#21450;&#21040;&#20010;&#20154;&#21644;&#32676;&#20307;&#65292;&#26159;&#19968;&#20010;&#30456;&#20851;&#30340;&#32771;&#34385;&#22240;&#32032;&#65307;&#23427;&#22312;&#25968;&#25454;&#25429;&#25417;&#21487;&#23548;&#33268;&#20154;&#20204;&#21463;&#21040;&#27495;&#35270;&#30340;&#21463;&#20445;&#25252;&#29305;&#24449;&#26102;&#20986;&#29616;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20010;&#27010;&#24565;&#20027;&#35201;&#38024;&#23545;&#22266;&#23450;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#20998;&#31867;&#38408;&#20540;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#21147;&#22270;&#35782;&#21035;&#21644;&#28040;&#38500;&#20854;&#36816;&#20316;&#20013;&#19981;&#24076;&#26395;&#30340;&#12289;&#20855;&#26377;&#27495;&#35270;&#24615;&#21644;&#21487;&#33021;&#36829;&#27861;&#30340;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#28335;&#20102;&#36825;&#20010;&#22266;&#23450;&#27169;&#22411;&#30340;&#20551;&#35774;&#65292;&#25552;&#20986;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#22411;&#20844;&#24179;&#24615;&#23450;&#20041;&#65292;&#21363;&#22312;&#20174;&#19968;&#32452;&#34920;&#29616;&#21516;&#26679;&#20986;&#33394;&#30340;&#27169;&#22411;&#20013;&#29305;&#23450;&#36873;&#25321;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20010;&#20154;&#21487;&#33021;&#21463;&#21040;&#20260;&#23475;&#65292;&#21363;&#22312;&#22522;&#20110;&#25928;&#29992;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#30340;&#35270;&#22270;&#19979;&#12290;&#30001;&#20110;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#19979;&#21487;&#33021;&#34987;&#20998;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data-driven predictive models are a strictly technological construct, they may operate within a social context in which benign engineering choices entail implicit, indirect and unexpected real-life consequences. Fairness of such systems -- pertaining both to individuals and groups -- is one relevant consideration in this space; it arises when data capture protected characteristics upon which people may be discriminated. To date, this notion has predominantly been studied for a fixed model, often under different classification thresholds, striving to identify and eradicate undesirable, discriminative and possibly unlawful aspects of its operation. Here, we backtrack on this fixed model assumption to propose and explore a novel definition of cross-model fairness where individuals can be harmed when one predictor is chosen ad hoc from a group of equally-well performing models, i.e., in view of utility-based model multiplicity. Since a person may be classified differently across mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2107.11972</link><description>&lt;p&gt;
&#24403;&#26426;&#20250;&#26469;&#20020;&#26102;&#36827;&#34892;&#20132;&#26131;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#30340;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling. (arXiv:2107.11972v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#24403;&#21069;&#24066;&#22330;&#24773;&#20917;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#39044;&#27979;&#37329;&#34701;&#36164;&#20135;&#30340;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#20302;&#20449;&#22122;&#27604;&#21644;&#38543;&#26426;&#24615;&#26497;&#24378;&#65292;&#22909;&#30340;&#20132;&#26131;&#26426;&#20250;&#26497;&#20026;&#31232;&#23569;&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#19981;&#20180;&#32454;&#36873;&#25321;&#28508;&#22312;&#30340;&#30408;&#21033;&#26679;&#26412;&#65292;&#36825;&#20123;ML&#26041;&#27861;&#23481;&#26131;&#25429;&#25417;&#21040;&#22122;&#22768;&#32780;&#19981;&#26159;&#30495;&#23454;&#20449;&#21495;&#30340;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LARA&#30340;&#26032;&#22411;&#20215;&#26684;&#21464;&#21160;&#39044;&#27979;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#23616;&#37096;&#24863;&#30693;&#27880;&#24847;&#21147;&#65288;LA-Attention&#65289;&#21644;&#36845;&#20195;&#32454;&#21270;&#26631;&#27880;&#65288;IRL&#65289;&#12290;LA-Attention&#26088;&#22312;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#37329;&#34701;&#25968;&#25454;&#20013;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#32780;IRL&#21017;&#26088;&#22312;&#36845;&#20195;&#22320;&#32454;&#21270;&#26631;&#27880;&#36807;&#31243;&#65292;&#36807;&#28388;&#25481;&#22122;&#22768;&#21644;&#26080;&#20851;&#26679;&#26412;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LARA&#22312;&#20934;&#30830;&#24615;&#21644;&#30408;&#21033;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative R
&lt;/p&gt;</description></item></channel></rss>