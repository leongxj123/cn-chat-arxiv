<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;</title><link>https://arxiv.org/abs/2404.01332</link><description>&lt;p&gt;
&#31561;&#31561;&#65292;&#36825;&#37117;&#26159;&#20196;&#29260;&#22122;&#38899;&#65311;&#19968;&#30452;&#23601;&#26159;&#21527;&#65306;&#21033;&#29992; Shapley &#20540;&#35299;&#37322; LLM &#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior Using Shapley Value
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01332
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Shapley&#20540;&#26041;&#27861;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#25581;&#31034;&#20102;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#25581;&#31034;&#20102;LLMs&#30340;&#20915;&#31574;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#32452;&#20214;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#35748;&#30693;&#36807;&#31243;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#24066;&#22330;&#30740;&#31350;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#20998;&#26512;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#30340;&#26174;&#33879;&#24046;&#24322;&#26263;&#31034;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#36807;&#31243;&#22312;&#36215;&#20316;&#29992;&#65292;&#20197;&#21450;LLMs&#23545;&#25552;&#31034;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20154;&#31867;&#20027;&#20307;&#30340;&#26367;&#20195;&#20173;&#28982;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#20013;Shapley&#20540;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;LLM&#34892;&#20026;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;&#25552;&#31034;&#32452;&#20214;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#36890;&#36807;&#20004;&#20010;&#24212;&#29992;--&#19968;&#20010;&#31163;&#25955;&#36873;&#25321;&#23454;&#39564;&#21644;&#19968;&#20010;&#35748;&#30693;&#20559;&#35265;&#35843;&#26597;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Shapley&#20540;&#26041;&#27861;&#22914;&#20309;&#25581;&#31034;&#25105;&#20204;&#25152;&#35859;&#30340;&#8220;&#20196;&#29260;&#22122;&#38899;&#8221;&#25928;&#24212;&#65292;&#21363;LLM&#20915;&#31574;&#21463;&#21040;&#30340;&#24433;&#21709;&#20005;&#37325;&#20559;&#21521;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01332v1 Announce Type: cross  Abstract: The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications-a discrete choice experiment and an investigation of cognitive biases-we demonstrate how the Shapley value method can uncover what we term "token noise" effects, a phenomenon where LLM decisions are disproportionately influenced by 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.18136</link><description>&lt;p&gt;
&#20445;&#25252;GNN&#65306;&#22522;&#20110;&#35299;&#37322;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Securing GNNs: Explanation-Based Identification of Backdoored Training Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;GNN&#20013;&#30340;&#21518;&#38376;&#35757;&#32451;&#22270;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#20197;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#25915;&#20987;&#36827;&#34892;&#20102;&#26041;&#27861;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs)&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#27969;&#34892;&#36215;&#26469;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#36947;&#24503;&#24212;&#29992;&#12290;&#26816;&#27979;&#36825;&#20123;&#25915;&#20987;&#23545;&#20110;&#20445;&#25345;GNN&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26377;&#25928;&#30340;&#26816;&#27979;&#25216;&#26415;&#24182;&#19981;&#22810;&#35265;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#22270;&#32423;&#35299;&#37322;&#33021;&#22815;&#25552;&#20379;&#19968;&#20123;&#26377;&#38480;&#30340;&#35265;&#35299;&#65292;&#20294;&#23427;&#20204;&#22312;&#26816;&#27979;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#26159;&#19981;&#19968;&#33268;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#21462;&#24182;&#36716;&#25442;GNN&#35299;&#37322;&#26426;&#21046;&#30340;&#27425;&#35201;&#36755;&#20986;&#65292;&#35774;&#35745;&#20102;&#19971;&#31181;&#26356;&#26377;&#25928;&#22320;&#26816;&#27979;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25915;&#20987;&#26469;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#26816;&#26597;&#20854;&#23545;&#21508;&#31181;&#25915;&#20987;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#36739;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18136v1 Announce Type: cross  Abstract: Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet they are vulnerable to backdoor attacks that can compromise their performance and ethical application. The detection of these attacks is crucial for maintaining the reliability and security of GNN classification tasks, but effective detection techniques are lacking. Following an initial investigation, we observed that while graph-level explanations can offer limited insights, their effectiveness in detecting backdoor triggers is inconsistent and incomplete. To bridge this gap, we extract and transform secondary outputs of GNN explanation mechanisms, designing seven novel metrics that more effectively detect backdoor attacks. Additionally, we develop an adaptive attack to rigorously evaluate our approach. We test our method on multiple benchmark datasets and examine its efficacy against various attack models. Our results show that our method can achieve high de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#26597;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#21033;&#29992;&#24322;&#36136;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.17351</link><description>&lt;p&gt;
&#20174;&#24322;&#36136;&#24615;&#23398;&#20064;&#65306;&#24322;&#36136;&#20449;&#24687;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#26597;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#21033;&#29992;&#24322;&#36136;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#29615;&#22659;&#20013;&#65292;Graph Neural Networks (GNNs)&#36890;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#19981;&#21516;&#26631;&#31614;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#22522;&#20110;&#35821;&#20041;&#21547;&#20041;&#30456;&#36830;&#12290;&#30446;&#21069;&#20851;&#20110;&#22270;&#24322;&#36136;&#24615;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32858;&#21512;&#26657;&#20934;&#25110;&#37051;&#23621;&#25193;&#23637;&#19978;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#25110;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;GNN&#34920;&#31034;&#20197;&#35299;&#20915;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#24322;&#36136;&#24615;&#20013;&#20869;&#22312;&#30340;&#23453;&#36149;&#35821;&#20041;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;&#35843;&#26597;&#22270;&#20013;&#27599;&#20010;&#21333;&#29420;&#33410;&#28857;&#30340;&#37051;&#23621;&#20998;&#24067;&#26469;&#26377;&#25928;&#22320;&#21033;&#29992;&#22312;&#22270;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#35770;&#35777;&#20102;&#36825;&#19968;&#29702;&#24565;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;HiGNN&#65292;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#26032;&#22270;&#32467;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20998;&#24067;&#25972;&#21512;&#24322;&#36136;&#20449;&#24687;&#26469;&#22686;&#24378;&#22270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17351v1 Announce Type: new  Abstract: Under circumstances of heterophily, where nodes with different labels tend to be connected based on semantic meanings, Graph Neural Networks (GNNs) often exhibit suboptimal performance. Current studies on graph heterophily mainly focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing node features or structural information to improve GNN representations. In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in graph learning by investigating the distribution of neighbors for each individual node within the graph. The theoretical analysis is carried out to demonstrate the efficacy of the idea in enhancing graph learning. Based on this analysis, we propose HiGNN, an innovative approach that constructs an additional new graph structure, that integrates heterophilous information by leveraging node distribution to enha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13848</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#24046;&#24322;&#20445;&#25252;&#20294;&#20934;&#30830;&#35268;&#21017;&#21015;&#34920;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13848
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#20445;&#25252;&#65288;DP&#65289;&#26426;&#21046;&#21487;&#20197;&#23884;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#20445;&#25252;&#25152;&#24471;&#27169;&#22411;&#20813;&#21463;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#23613;&#31649;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#26469;&#25552;&#20986;&#19968;&#20010;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#24179;&#28369;&#25935;&#24863;&#24230;&#30340;DP&#35268;&#21017;&#21015;&#34920;&#27169;&#22411;&#20855;&#26377;&#27604;&#20351;&#29992;&#20840;&#23616;&#25935;&#24863;&#24230;&#30340;&#20854;&#20182;DP&#26694;&#26550;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13848v1 Announce Type: cross  Abstract: Differentially-private (DP) mechanisms can be embedded into the design of a machine learningalgorithm to protect the resulting model against privacy leakage, although this often comes with asignificant loss of accuracy. In this paper, we aim at improving this trade-off for rule lists modelsby establishing the smooth sensitivity of the Gini impurity and leveraging it to propose a DP greedyrule list algorithm. In particular, our theoretical analysis and experimental results demonstrate thatthe DP rule lists models integrating smooth sensitivity have higher accuracy that those using otherDP frameworks based on global sensitivity.
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;</title><link>https://arxiv.org/abs/2403.12203</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#20223;&#30340;&#22686;&#24378;&#23398;&#20064;&#20026;&#22522;&#20110;&#35270;&#35273;&#30340;&#25935;&#25463;&#39134;&#34892;&#24341;&#23548;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Reinforcement Learning with Imitation for Vision-Based Agile Flight
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12203
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#20013;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#36827;&#34892;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#26080;&#20154;&#26426;&#31454;&#36895;&#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26377;&#25928;&#24615;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#30340;&#25928;&#29575;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35797;&#38169;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#22797;&#26434;&#25511;&#21046;&#22120;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#38754;&#20020;&#30528;&#26679;&#26412;&#25928;&#29575;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#32500;&#24230;&#36739;&#39640;&#12290;&#30456;&#21453;&#65292;IL&#22312;&#20174;&#35270;&#35273;&#28436;&#31034;&#20013;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#25928;&#29575;&#65292;&#20294;&#21463;&#21040;&#28436;&#31034;&#36136;&#37327;&#30340;&#38480;&#21046;&#65292;&#24182;&#38754;&#20020;&#35832;&#22914;&#21327;&#21464;&#37327;&#28418;&#31227;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;RL&#21644;IL&#20248;&#21183;&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#29305;&#26435;&#29366;&#24577;&#20449;&#24687;&#30340;&#24072;&#20613;&#31574;&#30053;&#30340;&#21021;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;IL&#23558;&#27492;&#31574;&#30053;&#33976;&#39311;&#20026;&#23398;&#29983;&#31574;&#30053;&#65292;&#20197;&#21450;&#24615;&#33021;&#21463;&#38480;&#30340;&#33258;&#36866;&#24212;RL&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12203v1 Announce Type: cross  Abstract: We combine the effectiveness of Reinforcement Learning (RL) and the efficiency of Imitation Learning (IL) in the context of vision-based, autonomous drone racing. We focus on directly processing visual input without explicit state estimation. While RL offers a general framework for learning complex controllers through trial and error, it faces challenges regarding sample efficiency and computational demands due to the high dimensionality of visual inputs. Conversely, IL demonstrates efficiency in learning from visual demonstrations but is limited by the quality of those demonstrations and faces issues like covariate shift. To overcome these limitations, we propose a novel training framework combining RL and IL's advantages. Our framework involves three stages: initial training of a teacher policy using privileged state information, distilling this policy into a student policy using IL, and performance-constrained adaptive RL fine-tunin
&lt;/p&gt;</description></item><item><title>RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.00043</link><description>&lt;p&gt;
RiNALMo: &#36890;&#29992;RNA&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00043
&lt;/p&gt;
&lt;p&gt;
RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00043v1 &#36890;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#22312;&#22522;&#30784;&#29983;&#29289;&#36807;&#31243;&#20013;&#25198;&#28436;&#30528;&#21508;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;RNA&#24050;&#25104;&#20026;&#19968;&#20010;&#26377;&#36259;&#30340;&#33647;&#29289;&#38774;&#28857;&#65292;&#24378;&#35843;&#20102;&#25552;&#39640;&#25105;&#20204;&#23545;&#20854;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#29702;&#35299;&#30340;&#24517;&#35201;&#24615;&#12290;&#22810;&#24180;&#26469;&#65292;&#27979;&#24207;&#25216;&#26415;&#24050;&#20135;&#29983;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;RNA&#25968;&#25454;&#65292;&#20854;&#20013;&#38544;&#34255;&#30528;&#37325;&#35201;&#30340;&#30693;&#35782;&#21644;&#28508;&#21147;&#12290;&#21463;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#31958;&#26680;&#37240;&#35821;&#35328;&#27169;&#22411;&#65288;RiNALMo&#65289;&#20197;&#24110;&#21161;&#25581;&#31034;RNA&#30340;&#38544;&#34255;&#23494;&#30721;&#12290;RiNALMo&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;RNA&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;650&#20159;&#20010;&#21442;&#25968;&#65292;&#39044;&#20808;&#35757;&#32451;&#20102;&#26469;&#33258;&#20960;&#20010;&#21487;&#29992;&#25968;&#25454;&#24211;&#30340;3600&#19975;&#20010;&#38750;&#32534;&#30721;RNA&#24207;&#21015;&#12290;RiNALMo&#33021;&#22815;&#25552;&#21462;&#38544;&#34255;&#30693;&#35782;&#24182;&#38544;&#21547;&#22320;&#25429;&#25417;RNA&#24207;&#21015;&#20013;&#20869;&#23884;&#30340;&#22522;&#26412;&#32467;&#26500;&#20449;&#24687;&#12290;RiNALMo&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00043v1 Announce Type: cross  Abstract: Ribonucleic acid (RNA) plays a variety of crucial roles in fundamental biological processes. Recently, RNA has become an interesting drug target, emphasizing the need to improve our understanding of its structures and functions. Over the years, sequencing technologies have produced an enormous amount of unlabeled RNA data, which hides important knowledge and potential. Motivated by the successes of protein language models, we introduce RiboNucleic Acid Language Model (RiNALMo) to help unveil the hidden code of RNA. RiNALMo is the largest RNA language model to date with $650$ million parameters pre-trained on $36$ million non-coding RNA sequences from several available databases. RiNALMo is able to extract hidden knowledge and capture the underlying structure information implicitly embedded within the RNA sequences. RiNALMo achieves state-of-the-art results on several downstream tasks. Notably, we show that its generalization capabiliti
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#30340;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#25104;&#20026;&#39318;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#65292;&#24182;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14585</link><description>&lt;p&gt;
&#20855;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#19987;&#23478;&#24314;&#35758;&#19979;&#30340;&#36172;&#24466;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bandits with Abstention under Expert Advice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14585
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#20854;&#21033;&#29992;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#30340;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#25104;&#20026;&#39318;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#65292;&#24182;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36172;&#24466;&#21453;&#39304;&#19979;&#21033;&#29992;&#19987;&#23478;&#24314;&#35758;&#36827;&#34892;&#39044;&#27979;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20551;&#35774;&#19968;&#31181;&#34892;&#21160;&#65292;&#21363;&#23398;&#20064;&#32773;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#65292;&#22312;&#27599;&#27425;&#35797;&#39564;&#20013;&#37117;&#27809;&#26377;&#22870;&#21169;&#25110;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CBA&#31639;&#27861;&#65292;&#21033;&#29992;&#36825;&#19968;&#20551;&#35774;&#33719;&#24471;&#20102;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#32463;&#20856;Exp4&#31639;&#27861;&#30340;&#22870;&#21169;&#30028;&#38480;&#12290;&#25105;&#20204;&#21487;&#20197;&#23558;&#25105;&#20204;&#30340;&#38382;&#39064;&#35270;&#20026;&#22312;&#23398;&#20064;&#32773;&#26377;&#25918;&#24323;&#21442;&#19982;&#28216;&#25103;&#36873;&#39033;&#26102;&#23545;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#36827;&#34892;&#32858;&#21512;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23545;&#19968;&#33324;&#32622;&#20449;&#35780;&#32423;&#39044;&#27979;&#22120;&#30340;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#23454;&#29616;&#30028;&#38480;&#30340;&#30740;&#31350;&#32773;&#12290;&#22312;&#19987;&#23478;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#30028;&#38480;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#20043;&#21069;&#22312;&#19987;&#23478;Exp&#65288;&#23558;&#24323;&#26435;&#35270;&#20026;&#21478;&#19968;&#31181;&#34892;&#21160;&#65289;&#30340;&#36793;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#24212;&#29992;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#26377;&#38480;&#24230;&#37327;&#31354;&#38388;&#20013;&#23398;&#20064;&#29699;&#30340;&#24182;&#38598;&#12290;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;CBA&#30340;&#26377;&#25928;&#23454;&#29616;&#65292;re
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14585v1 Announce Type: new  Abstract: We study the classic problem of prediction with expert advice under bandit feedback. Our model assumes that one action, corresponding to the learner's abstention from play, has no reward or loss on every trial. We propose the CBA algorithm, which exploits this assumption to obtain reward bounds that can significantly improve those of the classical Exp4 algorithm. We can view our problem as the aggregation of confidence-rated predictors when the learner has the option of abstention from play. Importantly, we are the first to achieve bounds on the expected cumulative reward for general confidence-rated predictors. In the special case of specialists we achieve a novel reward bound, significantly improving previous bounds of SpecialistExp (treating abstention as another action). As an example application, we discuss learning unions of balls in a finite metric space. In this contextual setting, we devise an efficient implementation of CBA, re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;</title><link>https://arxiv.org/abs/2402.11658</link><description>&lt;p&gt;
&#20998;&#23618;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dynamic planning in hierarchical active inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11658
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#21160;&#24577;&#35268;&#21010;&#39046;&#22495;&#20013;&#27169;&#25311;&#24037;&#20855;&#20351;&#29992;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#65292;&#35813;&#39046;&#22495;&#32771;&#34385;&#21040;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#20154;&#31867;&#22823;&#33041;&#25512;&#26029;&#21644;&#26045;&#21152;&#19982;&#35748;&#30693;&#20915;&#31574;&#30456;&#20851;&#30340;&#36816;&#21160;&#36712;&#36857;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#33539;&#24335;&#65292;&#20027;&#21160;&#25512;&#26029;&#65292;&#20026;&#29983;&#29289;&#26377;&#26426;&#20307;&#36866;&#24212;&#24102;&#26469;&#20102;&#22522;&#26412;&#35265;&#35299;&#65292;&#19981;&#26029;&#21162;&#21147;&#26368;&#23567;&#21270;&#39044;&#27979;&#35823;&#24046;&#20197;&#23558;&#33258;&#24049;&#38480;&#21046;&#22312;&#19982;&#29983;&#21629;&#20860;&#23481;&#30340;&#29366;&#24577;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#20154;&#31867;&#21644;&#21160;&#29289;&#34892;&#20026;&#21487;&#20197;&#35299;&#37322;&#20026;&#20027;&#21160;&#25512;&#26029;&#36807;&#31243;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#31163;&#25955;&#20915;&#31574;&#36824;&#26159;&#36830;&#32493;&#36816;&#21160;&#25511;&#21046;&#65292;&#37117;&#28608;&#21457;&#20102;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#22914;&#20309;&#26377;&#25928;&#22320;&#22312;&#21464;&#21270;&#29615;&#22659;&#20013;&#35268;&#21010;&#34892;&#21160;&#30340;&#20840;&#38754;&#23637;&#26395;&#12290;&#25105;&#20204;&#35774;&#23450;&#20102;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#24314;&#27169;&#30340;&#30446;&#26631;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#20027;&#21160;&#25512;&#26029;&#20013;&#30340;&#21160;&#24577;&#35268;&#21010;&#20027;&#39064;&#65292;&#29282;&#35760;&#20004;&#20010;&#29983;&#29289;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#29702;&#35299;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.07314</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;KL&#27491;&#21017;&#21270;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20174;&#19968;&#20010;&#27010;&#29575;&#20559;&#22909;&#27169;&#22411;&#25552;&#20379;&#30340;&#20559;&#22909;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20197;&#19968;&#20010;&#25552;&#31034;&#21644;&#20004;&#20010;&#21709;&#24212;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#20998;&#25968;&#65292;&#34920;&#31034;&#23545;&#19968;&#20010;&#21709;&#24212;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#21709;&#24212;&#30340;&#20559;&#22909;&#31243;&#24230;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26368;&#27969;&#34892;&#30340;RLHF&#33539;&#24335;&#26159;&#22522;&#20110;&#22870;&#21169;&#30340;&#65292;&#23427;&#20174;&#22870;&#21169;&#24314;&#27169;&#30340;&#21021;&#22987;&#27493;&#39588;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#26500;&#24314;&#30340;&#22870;&#21169;&#20026;&#21518;&#32493;&#30340;&#22870;&#21169;&#20248;&#21270;&#38454;&#27573;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#20989;&#25968;&#30340;&#23384;&#22312;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#65292;&#22522;&#20110;&#22870;&#21169;&#30340;RLHF&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#26377;&#23616;&#38480;&#24615;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#23398;&#20064;&#33539;&#24335;Nash&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;NLHF&#65289;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#35813;&#23398;&#20064;&#33539;&#24335;&#32771;&#34385;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#23558;&#23545;&#40784;&#36807;&#31243;&#23450;&#20041;&#20026;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#23398;&#20064;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses
&lt;/p&gt;</description></item><item><title>sGMC&#27169;&#22411;&#20316;&#20026;LASSO&#30340;&#38750;&#20984;&#27491;&#21017;&#21270;&#26367;&#20195;&#21697;&#65292;&#22312;&#20445;&#30041;LASSO&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#20854;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#19982;LASSO&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#19988;&#20248;&#38597;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.18438</link><description>&lt;p&gt;
&#19968;&#20010;&#38750;&#20984;&#27491;&#21017;&#21270;&#20984;&#31232;&#30095;&#27169;&#22411;&#30340;&#35299;&#38598;&#20960;&#20309;&#19982;&#27491;&#21017;&#21270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Solution-Set Geometry and Regularization Path of a Nonconvexly Regularized Convex Sparse Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18438
&lt;/p&gt;
&lt;p&gt;
sGMC&#27169;&#22411;&#20316;&#20026;LASSO&#30340;&#38750;&#20984;&#27491;&#21017;&#21270;&#26367;&#20195;&#21697;&#65292;&#22312;&#20445;&#30041;LASSO&#27169;&#22411;&#20248;&#21183;&#30340;&#21516;&#26102;&#65292;&#20854;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#19982;LASSO&#27169;&#22411;&#20855;&#26377;&#30456;&#20284;&#19988;&#20248;&#38597;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#26497;&#23567;&#26497;&#22823;&#20985;&#65288;GMC&#65289;&#24809;&#32602;&#26159;&#19968;&#31181;&#38750;&#20984;&#31232;&#30095;&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#20197;&#20445;&#25345;&#27491;&#21017;&#21270;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#30340;&#25972;&#20307;&#20984;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;GMC&#27169;&#22411;&#30340;&#19968;&#20010;&#37325;&#35201;&#23454;&#20363;&#65292;&#31216;&#20026;&#32553;&#25918;GMC&#65288;sGMC&#65289;&#65292;&#24182;&#23601;&#20854;&#35299;&#38598;&#20960;&#20309;&#21644;&#27491;&#21017;&#21270;&#36335;&#24452;&#25552;&#20986;&#21508;&#31181;&#26174;&#33879;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#34429;&#28982;sGMC&#24809;&#32602;&#26159;LASSO&#24809;&#32602;&#30340;&#38750;&#20984;&#25193;&#23637;&#65288;&#21363;$\ell_1$&#33539;&#25968;&#65289;&#65292;&#20294;sGMC&#27169;&#22411;&#20445;&#30041;&#20102;LASSO&#27169;&#22411;&#30340;&#35768;&#22810;&#33879;&#21517;&#29305;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#20316;&#20026;LASSO&#30340;&#19968;&#20010;&#20559;&#24046;&#36739;&#23567;&#30340;&#26367;&#20195;&#21697;&#32780;&#19981;&#20250;&#22833;&#21435;&#20854;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;$\lambda$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;sGMC&#27169;&#22411;&#30340;&#35299;&#38598;&#20960;&#20309;&#12289;&#35299;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#21487;&#20197;&#20197;&#19968;&#31181;&#31867;&#20284;&#20248;&#38597;&#30340;&#26041;&#24335;&#21051;&#30011;&#20026;LASSO&#27169;&#22411;&#65288;&#21442;&#35265;&#65292;&#20363;&#22914;&#65292;Osborne&#31561;&#20154;2000&#24180;&#65292;R. J. Tibshirani 2013&#24180;&#65289;&#12290;&#23545;&#20110;&#21464;&#21270;&#30340;$\lambda$&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18438v2 Announce Type: cross  Abstract: The generalized minimax concave (GMC) penalty is a nonconvex sparse regularizer which can preserve the overall-convexity of the regularized least-squares problem. In this paper, we focus on a significant instance of the GMC model termed scaled GMC (sGMC), and present various notable findings on its solution-set geometry and regularization path. Our investigation indicates that while the sGMC penalty is a nonconvex extension of the LASSO penalty (i.e., the $\ell_1$-norm), the sGMC model preserves many celebrated properties of the LASSO model, hence can serve as a less biased surrogate of LASSO without losing its advantages. Specifically, for a fixed regularization parameter $\lambda$, we show that the solution-set geometry, solution uniqueness and sparseness of the sGMC model can be characterized in a similar elegant way to the LASSO model (see, e.g., Osborne et al. 2000, R. J. Tibshirani 2013). For a varying $\lambda$, we prove that th
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.04916</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Explainable Identification of Hate Speech towards Islam using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04916
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#21644;&#35782;&#21035;&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#65292;&#27169;&#22411;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#33021;&#22815;&#35299;&#37322;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#26031;&#20848;&#25945;&#20167;&#24680;&#35328;&#35770;&#22312;&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#35782;&#21035;&#21644;&#28040;&#38500;&#36825;&#31181;&#20167;&#24680;&#26159;&#36808;&#21521;&#21644;&#35856;&#19982;&#21644;&#24179;&#26410;&#26469;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38024;&#23545;&#20234;&#26031;&#20848;&#25945;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#12289;&#25552;&#21462;&#24182;&#21033;&#29992;&#19981;&#21516;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22987;&#32456;&#33021;&#22815;&#22312;&#20445;&#25345;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#20379;&#23545;&#28508;&#22312;&#30456;&#20851;&#24615;&#21644;&#22240;&#26524;&#20851;&#31995;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04916v2 Announce Type: cross  Abstract: Islamophobic language is a prevalent challenge on online social interaction platforms. Identifying and eliminating such hatred is a crucial step towards a future of harmony and peace. This study presents a novel paradigm for identifying and explaining hate speech towards Islam using graph neural networks. Utilizing the intrinsic ability of graph neural networks to find, extract, and use relationships across disparate data points, our model consistently achieves outstanding performance while offering explanations for the underlying correlations and causation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.12275</link><description>&lt;p&gt;
&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;Agent&#21160;&#24577;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#20851;&#31995;&#32467;&#26500;&#30340;&#28436;&#21270;&#65292;&#26469;&#23454;&#29616;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#26041;&#27861;&#21253;&#25324;&#25512;&#26029;&#36229;&#36793;&#32536;&#20197;&#23454;&#29616;&#32676;&#20307;&#25512;&#29702;&#21644;&#36712;&#36857;&#39044;&#27979;&#22120;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#31181;&#24773;&#26223;&#19979;&#21487;&#20197;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#38656;&#35201;&#23433;&#20840;&#30340;&#20154;&#26426;&#20132;&#20114;&#21644;&#39640;&#25928;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#22312;&#22810;Agent&#20132;&#20114;&#31995;&#32479;&#20013;&#65292;&#24314;&#27169;&#25104;&#23545;&#30340;&#20851;&#31995;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#25429;&#25417;&#26356;&#22823;&#35268;&#27169;&#30340;&#32676;&#20307;&#27963;&#21160;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#20851;&#31995;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#25512;&#26029;&#27491;&#22312;&#28436;&#21464;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22810;Agent&#36712;&#36857;&#39044;&#27979;&#21644;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#36793;&#32536;&#65288;&#21363;Agent&#65289;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#25512;&#26029;&#36229;&#36793;&#32536;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36830;&#25509;&#22810;&#20010;&#33410;&#28857;&#65292;&#20197;&#20415;&#36827;&#34892;&#32676;&#20307;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#21160;&#24577;&#28436;&#21270;&#30340;&#20851;&#31995;&#22270;&#21644;&#36229;&#22270;&#65292;&#20197;&#25429;&#25417;&#20851;&#31995;&#30340;&#28436;&#21270;&#65292;&#36712;&#36857;&#39044;&#27979;&#22120;&#21033;&#29992;&#36825;&#20123;&#22270;&#26469;&#29983;&#25104;&#26410;&#26469;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#21644;&#36923;&#36753;&#31232;&#30095;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social robot navigation can be helpful in various contexts of daily life but requires safe human-robot interactions and efficient trajectory planning. While modeling pairwise relations has been widely studied in multi-agent interacting systems, the ability to capture larger-scale group-wise activities is limited. In this paper, we propose a systematic relational reasoning approach with explicit inference of the underlying dynamically evolving relational structures, and we demonstrate its effectiveness for multi-agent trajectory prediction and social robot navigation. In addition to the edges between pairs of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect multiple nodes to enable group-wise reasoning in an unsupervised manner. Our approach infers dynamically evolving relation graphs and hypergraphs to capture the evolution of relations, which the trajectory predictor employs to generate future states. Meanwhile, we propose to regularize the sharpness and sp
&lt;/p&gt;</description></item><item><title>CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;</title><link>http://arxiv.org/abs/2401.08897</link><description>&lt;p&gt;
CFASL&#65306;&#29992;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#30340;&#22797;&#21512;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in Variational AutoEncoder. (arXiv:2401.08897v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08897
&lt;/p&gt;
&lt;p&gt;
CFASL&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#32544;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#23545;&#31216;&#24615;&#23398;&#20064;&#19982;VAE&#38598;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20855;&#26377;&#19977;&#20010;&#26032;&#29305;&#24449;&#65306;&#23545;&#40784;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#21040;&#21487;&#23398;&#20064;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#23398;&#20064;&#22797;&#21512;&#23545;&#31216;&#24615;&#26469;&#34920;&#36798;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#24341;&#20837;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#26469;&#35757;&#32451;VAE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#21644;&#28508;&#22312;&#21521;&#37327;&#30340;&#23545;&#31216;&#24615;&#20026;VAE&#20013;&#30340;&#35299;&#32544;&#23398;&#20064;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29978;&#33267;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20063;&#38656;&#35201;&#24050;&#30693;&#30340;&#22240;&#23376;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Composite Factor-Aligned Symmetry Learning (CFASL)&#65292;&#23558;&#20854;&#38598;&#25104;&#21040;VAE&#20013;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#19981;&#38656;&#35201;&#20219;&#20309;&#25968;&#25454;&#38598;&#22240;&#23376;&#20449;&#24687;&#30340;&#30693;&#35782;&#12290;CFASL&#21253;&#25324;&#19977;&#20010;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#35299;&#32544;&#30340;&#26032;&#29305;&#24449;&#65306;1)&#27880;&#20837;&#24402;&#32435;&#20559;&#32622;&#65292;&#23558;&#28508;&#22312;&#21521;&#37327;&#32500;&#24230;&#23545;&#40784;&#21040;&#26126;&#30830;&#21487;&#23398;&#20064;&#30340;&#23545;&#31216;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65307;2)&#23398;&#20064;&#19968;&#20010;&#22797;&#21512;&#23545;&#31216;&#24615;&#65292;&#36890;&#36807;&#23398;&#20064;&#20195;&#30721;&#31807;&#20013;&#30340;&#22240;&#23376;&#23545;&#40784;&#23545;&#31216;&#24615;&#65292;&#26469;&#34920;&#36798;&#20004;&#20010;&#38543;&#26426;&#26679;&#26412;&#20043;&#38388;&#30340;&#26410;&#30693;&#22240;&#32032;&#30340;&#21464;&#21270;&#65307;3)&#22312;&#35757;&#32451;VAE&#26102;&#65292;&#24341;&#20837;&#20855;&#26377;&#32676;&#31561;&#21464;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#20004;&#20010;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetries of input and latent vectors have provided valuable insights for disentanglement learning in VAEs.However, only a few works were proposed as an unsupervised method, and even these works require known factor information in training data. We propose a novel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integrated into VAEs for learning symmetry-based disentanglement in unsupervised learning without any knowledge of the dataset factor information.CFASL incorporates three novel features for learning symmetry-based disentanglement: 1) Injecting inductive bias to align latent vector dimensions to factor-aligned symmetries within an explicit learnable symmetry codebook 2) Learning a composite symmetry to express unknown factors change between two random samples by learning factor-aligned symmetries within the codebook 3) Inducing group equivariant encoder and decoder in training VAEs with the two conditions. In addition, we propose an extended evaluation metri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#19988;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2310.06929</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#22312;&#23431;&#23449;&#23398;&#27169;&#25311;&#20013;&#23454;&#29616;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Stochastic Super-resolution of Cosmological Simulations with Denoising Diffusion Models. (arXiv:2310.06929v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#19988;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22686;&#24378;&#20302;&#20998;&#36776;&#29575;&#30340;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#23567;&#23610;&#24230;&#20449;&#24687;&#65292;&#36825;&#34987;&#31216;&#20026;&#8220;&#36229;&#20998;&#36776;&#29575;&#8221;&#20219;&#21153;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#23431;&#23449;&#23398;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20381;&#36182;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#39640;&#24230;&#36924;&#30495;&#30340;&#32467;&#26524;&#65292;&#20294;&#23384;&#22312;&#21508;&#31181;&#32570;&#28857;&#65288;&#20363;&#22914;&#20302;&#26679;&#26412;&#22810;&#26679;&#24615;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26469;&#23454;&#29616;&#36229;&#20998;&#36776;&#29575;&#30340;&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#39044;&#27979;&#65288;&#20316;&#20026;&#20108;&#32500;&#30340;&#39318;&#20010;&#27010;&#24565;&#35777;&#26126;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#23567;&#23610;&#24230;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#28388;&#27874;&#22686;&#24378;&#8221;&#35757;&#32451;&#26041;&#27861;&#65292;&#37325;&#26032;&#20998;&#37197;&#20102;&#20687;&#32032;&#32423;&#35757;&#32451;&#30446;&#26631;&#20013;&#19981;&#21516;&#23610;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#30334;&#20998;&#20043;&#19968;&#27700;&#24179;&#19968;&#33268;&#30340;&#21151;&#29575;&#35889;&#65292;&#24182;&#19988;&#33021;&#22815;&#37325;&#29616;&#19982;&#23567;&#23610;&#24230;&#29305;&#24449;&#22810;&#26679;&#24615;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning models have been successfully employed for augmenting low-resolution cosmological simulations with small-scale information, a task known as "super-resolution". So far, these cosmological super-resolution models have relied on generative adversarial networks (GANs), which can achieve highly realistic results, but suffer from various shortcomings (e.g. low sample diversity). We introduce denoising diffusion models as a powerful generative model for super-resolving cosmic large-scale structure predictions (as a first proof-of-concept in two dimensions). To obtain accurate results down to small scales, we develop a new "filter-boosted" training approach that redistributes the importance of different scales in the pixel-wise training objective. We demonstrate that our model not only produces convincing super-resolution images and power spectra consistent at the percent level, but is also able to reproduce the diversity of small-scale features consistent with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;</title><link>http://arxiv.org/abs/2310.04361</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#25512;&#29702;&#26469;&#21033;&#29992;Transformer&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploiting Transformer Activation Sparsity with Dynamic Inference. (arXiv:2310.04361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSTI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;Transformer&#27169;&#22411;&#36716;&#25442;&#20026;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#29256;&#26412;&#26469;&#26497;&#22823;&#22320;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;Transformer&#27169;&#22411;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#23613;&#31649;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#24120;&#38754;&#20020;&#23454;&#38469;&#38480;&#21046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#26174;&#33879;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#23384;&#22312;&#20887;&#20313;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21160;&#24577;&#31232;&#30095;&#21270;Transformer&#25512;&#29702;&#65288;DSTI&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#28608;&#27963;&#31232;&#30095;&#24615;&#24182;&#23558;&#23494;&#38598;&#27169;&#22411;&#36716;&#25442;&#20026;&#20854;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#29256;&#26412;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#38477;&#20302;Transformer&#27169;&#22411;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#35757;&#32451;&#20986;&#25104;&#21151;&#39044;&#27979;&#27599;&#20010;&#19987;&#23478;&#30456;&#23545;&#36129;&#29486;&#30340;&#23567;&#22411;&#38376;&#25511;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#30830;&#23450;&#27599;&#20010;&#20196;&#29260;&#25191;&#34892;&#30340;&#19987;&#23478;&#25968;&#37327;&#30340;&#26426;&#21046;&#12290;DSTI&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#38024;&#23545;BERT-base&#20998;&#31867;&#27169;&#22411;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#25512;&#29702;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models, despite their impressive performance, often face practical limitations due to their high computational requirements. At the same time, previous studies have revealed significant activation sparsity in these models, indicating the presence of redundant computations. In this paper, we propose Dynamic Sparsified Transformer Inference (DSTI), a method that radically reduces the inference cost of Transformer models by enforcing activation sparsity and subsequently transforming a dense model into its sparse Mixture of Experts (MoE) version. We demonstrate that it is possible to train small gating networks that successfully predict the relative contribution of each expert during inference. Furthermore, we introduce a mechanism that dynamically determines the number of executed experts individually for each token. DSTI can be applied to any Transformer-based architecture and has negligible impact on the accuracy. For the BERT-base classification model, we reduce inference c
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#21644;&#29983;&#25104;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.06797</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#24555;&#36895;&#19988;&#21151;&#33021;&#24615;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fast and Functional Structured Data Generators Rooted in Out-of-Equilibrium Physics. (arXiv:2307.06797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21892;&#27169;&#22411;&#30340;&#20998;&#31867;&#33021;&#21147;&#21644;&#29983;&#25104;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#22312;&#22797;&#26434;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65288;&#22914;&#20154;&#21475;&#22522;&#22240;&#32452;&#23398;&#12289;RNA&#25110;&#34507;&#30333;&#36136;&#24207;&#21015;&#25968;&#25454;&#65289;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#26631;&#31614;&#29305;&#23450;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#30001;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#28151;&#21512;&#25928;&#29575;&#20302;&#19979;&#32780;&#36935;&#21040;&#22256;&#38590;&#65292;&#36825;&#24433;&#21709;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#24182;&#22686;&#21152;&#20102;&#29983;&#25104;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21033;&#29992;&#38750;&#24179;&#34913;&#25928;&#24212;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26679;&#26412;&#30340;&#27491;&#30830;&#20998;&#31867;&#33021;&#21147;&#65292;&#24182;&#21482;&#38656;&#23569;&#25968;&#20960;&#20010;&#37319;&#26679;&#27493;&#39588;&#21363;&#21487;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#20854;&#25104;&#21151;&#24212;&#29992;&#20110;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#24471;&#21040;&#35777;&#26126;&#65306;&#25163;&#20889;&#25968;&#23383;&#65292;&#25353;&#22823;&#38470;&#36215;&#28304;&#20998;&#31867;&#30340;&#20154;&#31867;&#22522;&#22240;&#32452;&#31361;&#21464;&#65292;&#37238;&#34507;&#30333;&#23478;&#26063;&#30340;&#21151;&#33021;&#24207;&#21015;&#65292;&#20197;&#21450;&#29305;&#23450;&#20998;&#31867;&#27861;&#30340;&#21516;&#28304;RNA&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the challenge of using energy-based models to produce high-quality, label-specific data in complex structured datasets, such as population genetics, RNA or protein sequences data. Traditional training methods encounter difficulties due to inefficient Markov chain Monte Carlo mixing, which affects the diversity of synthetic data and increases generation times. To address these issues, we use a novel training algorithm that exploits non-equilibrium effects. This approach, applied on the Restricted Boltzmann Machine, improves the model's ability to correctly classify samples and generate high-quality synthetic data in only a few sampling steps. The effectiveness of this method is demonstrated by its successful application to four different types of data: handwritten digits, mutations of human genomes classified by continental origin, functionally characterized sequences of an enzyme protein family, and homologous RNA sequences from specific taxonomies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16945</link><description>&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search with Context Models. (arXiv:2305.16945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levin Tree Search&#65288;LTS&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#31574;&#30053;&#65288;&#21160;&#20316;&#30340;&#27010;&#29575;&#20998;&#24067;&#65289;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20851;&#20110;&#36798;&#21040;&#30446;&#26631;&#33410;&#28857;&#20043;&#21069;&#25193;&#23637;&#27425;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#21462;&#20915;&#20110;&#31574;&#30053;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20445;&#35777;&#31216;&#20026;LTS&#25439;&#22833;&#65292;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#34920;&#31034;&#31574;&#30053;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LTS+NN&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29992;&#22312;&#32447;&#21387;&#32553;&#25991;&#29486;&#20013;&#30340;&#21442;&#25968;&#21270;&#19978;&#19979;&#25991;&#27169;&#22411;&#26469;&#26367;&#20195;&#65288;LTS+CM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#26032;&#27169;&#22411;&#19979;LTS&#25439;&#22833;&#26159;&#20984;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#20984;&#20248;&#21270;&#24037;&#20855;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#35299;&#36712;&#36857;&#38598;&#21512;&#65292;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#21487;&#20197;&#33719;&#24471;&#21040;&#26368;&#20248;&#21442;&#25968;&#30340;&#25910;&#25947;&#20445;&#35777;&#8212;&#8212;&#32780;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25552;&#20379;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#26032;&#30340;LTS+CM&#31639;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;LTS+NN&#30456;&#27604;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65306;Sokoban&#65288;Boxoban&#65289;&#12289;The Witness&#21644;24-Sliding Tile Puzzle&#65288;STP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -- guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.03641</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints. (arXiv:2304.03641v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#22312;&#32479;&#35745;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#36136;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#27714;&#35299;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#20197;&#19979;&#19968;&#20010;&#25110;&#22810;&#20010;&#38480;&#21046;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#23427;&#20204;&#26159;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#20840;&#26799;&#24230;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#19968;&#33324;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#20204;&#26159;&#19981;&#21487;&#34892;&#26041;&#27861;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#26497;&#38480;&#28857;&#22788;&#23454;&#29616;&#35299;&#30340;&#21487;&#34892;&#24615;&#65307;&#65288;iv&#65289;&#23427;&#20204;&#32570;&#20047;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#65307;&#65288;v&#65289;&#23427;&#20204;&#21482;&#33021;&#33719;&#24471;&#20851;&#38190;&#28857;&#30340;&#24369;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290;OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20250;&#26356;&#26032;...
&lt;/p&gt;
&lt;p&gt;
Nonsmooth composite optimization with orthogonality constraints has a broad spectrum of applications in statistical learning and data science. However, this problem is generally challenging to solve due to its non-convex and non-smooth nature. Existing solutions are limited by one or more of the following restrictions: (i) they are full gradient methods that require high computational costs in each iteration; (ii) they are not capable of solving general nonsmooth composite problems; (iii) they are infeasible methods and can only achieve the feasibility of the solution at the limit point; (iv) they lack rigorous convergence guarantees; (v) they only obtain weak optimality of critical points. In this paper, we propose \textit{\textbf{OBCD}}, a new Block Coordinate Descent method for solving general nonsmooth composite problems under Orthogonality constraints. \textit{\textbf{OBCD}} is a feasible method with low computation complexity footprints. In each iteration, our algorithm updates $
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#38024;&#23545;&#38169;&#20301;&#22788;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03247</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#19981;&#21487;&#36991;&#20813;&#39118;&#38505;&#23384;&#22312;&#19979;&#36827;&#34892;&#22797;&#21457;&#20107;&#20214;&#22240;&#26524;&#20998;&#26512;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Framework for Causal Analysis of Recurrent Events in Presence of Immortal Risk. (arXiv:2304.03247v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03247
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#38024;&#23545;&#38169;&#20301;&#22788;&#29702;&#38382;&#39064;&#65292;&#23558;&#20854;&#35270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#32479;&#35745;&#23398;&#20013;&#23545;&#22797;&#21457;&#20107;&#20214;&#29575;&#30340;&#35266;&#27979;&#30740;&#31350;&#24456;&#24120;&#35265;&#12290;&#36890;&#24120;&#30340;&#30446;&#26631;&#26159;&#22312;&#35268;&#23450;&#30340;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#65292;&#20272;&#35745;&#22312;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#20154;&#32676;&#20013;&#20004;&#31181;&#27835;&#30103;&#26041;&#27861;&#30340;&#20107;&#20214;&#29575;&#24046;&#24322;&#12290;&#20351;&#29992;&#35266;&#27979;&#24615;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#30446;&#26631;&#20154;&#32676;&#30340;&#25104;&#21592;&#36164;&#26684;&#26041;&#38754;&#23450;&#20041;&#26102;&#65292;&#24456;&#23569;&#22312;&#36164;&#26684;&#30830;&#35748;&#26102;&#20934;&#30830;&#20998;&#37197;&#27835;&#30103;&#26041;&#24335;&#12290;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#38169;&#20301;&#22788;&#29702;&#65292;&#27604;&#22914;&#22522;&#20110;&#21518;&#32493;&#20998;&#37197;&#65292;&#22312;&#36164;&#26684;&#30830;&#35748;&#26102;&#20998;&#37197;&#27835;&#30103;&#26041;&#24335;&#65292;&#36825;&#20250;&#23558;&#20808;&#21069;&#30340;&#20107;&#20214;&#29575;&#38169;&#35823;&#22320;&#24402;&#22240;&#20110;&#27835;&#30103;-&#20174;&#32780;&#20135;&#29983;&#19981;&#21487;&#36991;&#20813;&#30340;&#39118;&#38505;&#20559;&#24046;&#12290;&#21363;&#20351;&#36164;&#26684;&#21644;&#27835;&#30103;&#24050;&#32463;&#23545;&#40784;&#65292;&#32456;&#27490;&#20107;&#20214;&#36807;&#31243;&#65288;&#20363;&#22914;&#27515;&#20129;&#65289;&#20063;&#32463;&#24120;&#20572;&#27490;&#24863;&#20852;&#36259;&#30340;&#22797;&#21457;&#20107;&#20214;&#36807;&#31243;&#12290;&#21516;&#26679;&#65292;&#36825;&#20004;&#20010;&#36807;&#31243;&#20063;&#21463;&#21040;&#23457;&#26597;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#25972;&#20010;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#19981;&#33021;&#35266;&#23519;&#21040;&#20107;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#38169;&#20301;&#22788;&#29702;&#36716;&#21270;&#20026;&#27835;&#30103;&#20999;&#25442;&#38382;&#39064;&#65306;&#19968;&#20123;&#24739;&#32773;&#22312;&#25972;&#20010;&#38543;&#35775;&#26102;&#38388;&#31383;&#21475;&#20869;&#22362;&#25345;&#19968;&#20010;&#29305;&#23450;&#30340;&#27835;&#30103;&#31574;&#30053;&#65292;&#21478;&#19968;&#20123;&#24739;&#32773;&#22312;&#36825;&#20010;&#26102;&#38388;&#31383;&#21475;&#20869;&#32463;&#21382;&#27835;&#30103;&#31574;&#30053;&#30340;&#20999;&#25442;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#65306;&#36890;&#36807;&#19968;&#20010;&#21512;&#29702;&#30340;&#26102;&#21051;&#20999;&#25442;&#27169;&#22411;&#65292;&#27491;&#30830;&#22320;&#24314;&#27169;&#27835;&#30103;&#20043;&#38388;&#30340;&#20999;&#25442;&#21644;&#19981;&#21487;&#36991;&#20813;&#39118;&#38505;&#65292;&#36890;&#36807;&#23558;&#38750;&#35266;&#23519;&#20107;&#20214;&#27169;&#22411;&#21270;&#20026;&#22797;&#21457;&#20107;&#20214;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#22797;&#22686;&#21644;&#26411;&#20107;&#20214;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Observational studies of recurrent event rates are common in biomedical statistics. Broadly, the goal is to estimate differences in event rates under two treatments within a defined target population over a specified followup window. Estimation with observational claims data is challenging because while membership in the target population is defined in terms of eligibility criteria, treatment is rarely assigned exactly at the time of eligibility. Ad-hoc solutions to this timing misalignment, such as assigning treatment at eligibility based on subsequent assignment, incorrectly attribute prior event rates to treatment - resulting in immortal risk bias. Even if eligibility and treatment are aligned, a terminal event process (e.g. death) often stops the recurrent event process of interest. Both processes are also censored so that events are not observed over the entire followup window. Our approach addresses misalignment by casting it as a treatment switching problem: some patients are on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20551;&#35774;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;SimTS&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18205</link><description>&lt;p&gt;
SimTS:&#37325;&#26032;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimTS: Rethinking Contrastive Representation Learning for Time Series Forecasting. (arXiv:2303.18205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20551;&#35774;&#30340;&#31616;&#21333;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;SimTS&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#22270;&#20687;&#25110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#23637;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#23398;&#20064;&#24847;&#20041;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#19981;&#22815;&#26126;&#26174;&#65292;&#22240;&#20026;&#23545;&#23454;&#20363;&#37492;&#21035;&#30340;&#20248;&#21270;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#20174;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#25216;&#26415;&#20013;&#27491;&#36127;&#31034;&#20363;&#23545;&#30340;&#26500;&#24314;&#24378;&#28872;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26222;&#36866;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SimTS&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#20174;&#36807;&#21435;&#39044;&#27979;&#26410;&#26469;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;SimTS&#19981;&#20381;&#36182;&#20110;&#36127;&#23545;&#25110;&#29305;&#23450;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SimTS&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning methods have shown an impressive ability to learn meaningful representations for image or time series classification. However, these methods are less effective for time series forecasting, as optimization of instance discrimination is not directly applicable to predicting the future state from the history context. Moreover, the construction of positive and negative pairs in current technologies strongly relies on specific time series characteristics, restricting their generalization across diverse types of time series data. To address these limitations, we propose SimTS, a simple representation learning approach for improving time series forecasting by learning to predict the future from the past in the latent space. SimTS does not rely on negative pairs or specific assumptions about the characteristics of the particular time series. Our extensive experiments on several benchmark time series forecasting datasets show that SimTS achieves competitive performance comp
&lt;/p&gt;</description></item></channel></rss>