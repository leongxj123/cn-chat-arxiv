<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01400</link><description>&lt;p&gt;
&#20302;&#26597;&#35810;&#25104;&#26412;&#24102;&#22122;&#22768;or&#21516;&#26102;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Query-Efficient Correlation Clustering with Noisy Oracle
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#26597;&#35810;&#25104;&#26412;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;&#23454;&#29616;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#35774;&#35745;&#20102;&#33021;&#22312;NP-hard&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#32858;&#31867;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#38656;&#35201;&#23545;n&#20010;&#20803;&#32032;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#23569;&#22320;&#21521;&#36820;&#22238;&#20004;&#20010;&#20803;&#32032;&#30456;&#20284;&#24615;&#30340;&#26377;&#22122;&#22768;&#30340;oracle&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#28085;&#30422;&#20102;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#30456;&#20284;&#24615;&#20989;&#25968;&#35745;&#31639;&#36215;&#26469;&#25104;&#26412;&#39640;&#24182;&#19988; inherently noisy&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32431;&#22312;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#25506;&#32034;&#33539;&#24335;(PE-CMAB)&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#39062;&#34920;&#36798;&#26041;&#27861;&#22266;&#23450;&#32622;&#20449;&#24230;&#21644;&#22266;&#23450;&#39044;&#31639;&#35774;&#32622;&#12290;&#23545;&#20110;&#36825;&#20004;&#31181;&#35774;&#32622;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23558;&#25277;&#26679;&#31574;&#30053;&#19982;&#32463;&#20856;&#30340;&#30456;&#20851;&#32858;&#31867;&#36817;&#20284;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36825;&#26679;&#30340;&#65306;&#36825;&#20123;&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#22312;&#24213;&#23618;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#20026;NP-hard&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a general clustering setting in which we have $n$ elements to be clustered, and we aim to perform as few queries as possible to an oracle that returns a noisy sample of the similarity between two elements. Our setting encompasses many application domains in which the similarity function is costly to compute and inherently noisy. We propose two novel formulations of online learning problems rooted in the paradigm of Pure Exploration in Combinatorial Multi-Armed Bandits (PE-CMAB): fixed confidence and fixed budget settings. For both settings, we design algorithms that combine a sampling strategy with a classic approximation algorithm for correlation clustering and study their theoretical guarantees. Our results are the first examples of polynomial-time algorithms that work for the case of PE-CMAB in which the underlying offline optimization problem is NP-hard.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#27969;&#24418;&#30340;&#32500;&#24230;&#21644;&#23398;&#20064;&#24230;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#22797;&#26434;&#24230;&#30340;&#24230;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00949</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Geometry of Polynomial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#23545;&#31070;&#32463;&#27969;&#24418;&#30340;&#32500;&#24230;&#21644;&#23398;&#20064;&#24230;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#21644;&#35757;&#32451;&#22797;&#26434;&#24230;&#30340;&#24230;&#37327;&#65292;&#24182;&#32473;&#20986;&#20102;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21333;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;PNN&#65289;&#30340;&#34920;&#36798;&#24615;&#21644;&#23398;&#20064;&#36807;&#31243;&#12290;&#32593;&#32476;&#30340;&#26435;&#37325;&#21442;&#25968;&#21270;&#20102;&#31070;&#32463;&#27969;&#24418;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#25968;&#20960;&#20309;&#24037;&#20855;&#30740;&#31350;&#20102;&#26576;&#20123;&#31070;&#32463;&#27969;&#24418;&#65306;&#25105;&#20204;&#32473;&#20986;&#20102;&#21322;&#20195;&#25968;&#38598;&#30340;&#26126;&#30830;&#25551;&#36848;&#24182;&#29305;&#24449;&#21270;&#20102;&#23427;&#20204;&#30340;Zariski&#38381;&#21253;&#65292;&#31216;&#20026;&#31070;&#32463;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#32500;&#24230;&#24182;&#23558;&#19968;&#20010;&#20195;&#25968;&#24230;&#37327;&#65292;&#23398;&#20064;&#24230;&#65292;&#19982;&#31070;&#32463;&#22810;&#26679;&#24615;&#30456;&#20851;&#32852;&#12290;&#32500;&#24230;&#20316;&#20026;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#23398;&#20064;&#24230;&#26159;&#35757;&#32451;&#32593;&#32476;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#25552;&#20379;&#21487;&#23398;&#20989;&#25968;&#25968;&#37327;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#36824;&#20276;&#38543;&#30528;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the expressivity and learning process for polynomial neural networks (PNNs) with monomial activation functions. The weights of the network parametrize the neuromanifold. In this paper, we study certain neuromanifolds using tools from algebraic geometry: we give explicit descriptions as semialgebraic sets and characterize their Zariski closures, called neurovarieties. We study their dimension and associate an algebraic degree, the learning degree, to the neurovariety. The dimension serves as a geometric measure for the expressivity of the network, the learning degree is a measure for the complexity of training the network and provides upper bounds on the number of learnable functions. These theoretical results are accompanied with experiments.
&lt;/p&gt;</description></item><item><title>GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02115</link><description>&lt;p&gt;
GINopic&#65306;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GINopic: Topic Modeling with Graph Isomorphism Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02115
&lt;/p&gt;
&lt;p&gt;
GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#20998;&#26512;&#21644;&#25506;&#32034;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#24191;&#27867;&#20351;&#29992;&#26041;&#27861;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#23884;&#20837;&#65292;&#32435;&#20837;&#20027;&#39064;&#24314;&#27169;&#20013;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21333;&#35789;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#20256;&#36798;&#30340;&#22266;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290; &#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GINopic&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; &#36890;&#36807;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20869;&#22312;&#30340;&#65288;&#23450;&#37327;&#21644;&#23450;&#24615;&#65289;&#21644;&#22806;&#37096;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#30456;&#27604;&#65292;GINopic&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.00521</link><description>&lt;p&gt;
CHAIN&#65306;&#36890;&#36807;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#22686;&#24378;&#25968;&#25454;&#39640;&#25928;GANs&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;CHAIN&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;GANs&#20013;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26174;&#30528;&#25512;&#21160;&#20102;&#22270;&#20687;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;GANs&#32463;&#24120;&#38754;&#20020;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#35782;&#21035;Batch Normalization&#65288;BN&#65289;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65306;&#22312;&#20013;&#24515;&#21270;&#21644;&#32553;&#25918;&#27493;&#39588;&#20013;&#26799;&#24230;&#29190;&#28856;&#30340;&#20542;&#21521;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CHAIN&#65288;&#21463;&#38480;&#21807;&#19968;&#24615;&#36830;&#32493;&#24615;&#35268;&#33539;&#21270;&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#20013;&#24515;&#21270;&#27493;&#39588;&#26367;&#25442;&#20026;&#38646;&#22343;&#20540;&#27491;&#21017;&#21270;&#65292;&#24182;&#22312;&#32553;&#25918;&#27493;&#39588;&#20013;&#38598;&#25104;&#20102;Lipschitz&#36830;&#32493;&#24615;&#32422;&#26463;&#12290;CHAIN&#36890;&#36807;&#33258;&#36866;&#24212;&#25554;&#20540;&#24402;&#19968;&#21270;&#21644;&#38750;&#24402;&#19968;&#21270;&#29305;&#24449;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;GANs&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#37492;&#21035;&#22120;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
&lt;/p&gt;</description></item><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#22312;&#36870;&#38382;&#39064;&#19978;&#24037;&#20316;&#65292;&#24182;&#36991;&#20813;&#25197;&#26354;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.19470</link><description>&lt;p&gt;
&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep decomposition method for the limited aperture inverse obstacle scattering problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19470
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#22312;&#36870;&#38382;&#39064;&#19978;&#24037;&#20316;&#65292;&#24182;&#36991;&#20813;&#25197;&#26354;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#38024;&#23545;&#26377;&#38480;&#23380;&#24452;&#36870;&#38556;&#30861;&#25955;&#23556;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#36182;&#25968;&#25454;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#24403;&#21482;&#26377;&#38388;&#25509;&#35266;&#27979;&#25968;&#25454;&#21644;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#21487;&#29992;&#26102;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#36870;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#38754;&#23545;&#36825;&#20123;&#23616;&#38480;&#24615;&#26102;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#26159;&#21542;&#21487;&#33021;&#20351;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#36870;&#38382;&#39064;&#65292;&#24182;&#19988;&#20102;&#35299;&#23427;&#27491;&#22312;&#23398;&#20064;&#30340;&#20869;&#23481;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36825;&#20123;&#30446;&#30340;&#30340;&#28145;&#24230;&#20998;&#35299;&#26041;&#27861;&#65288;DDM&#65289;&#65292;&#23427;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#23427;&#36890;&#36807;&#21521;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#19982;&#25955;&#23556;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#29289;&#29702;&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;DDM&#20013;&#36824;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#23436;&#25972;&#24615;&#26041;&#26696;&#65292;&#20197;&#38450;&#27490;&#25197;&#26354;&#26377;&#38480;&#23380;&#24452;&#25968;&#25454;&#30340;&#36870;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#35299;&#20915;i
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19470v1 Announce Type: cross  Abstract: In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16317</link><description>&lt;p&gt;
&#22312;&#26356;&#32454;&#31890;&#24230;&#19978;&#30340;&#20248;&#21270;&#65306;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#33021;&#24110;&#21161;&#26356;&#22909;&#29702;&#35299;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#38477;&#20302;oracle&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26377;&#30028;&#23616;&#37096;&#27425;&#26799;&#24230;&#21464;&#21270;&#30340;&#26465;&#20214;&#19979;&#24320;&#22987;&#30740;&#31350;&#38750;&#20809;&#28369;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#20551;&#35774;&#22312;&#28857;&#38468;&#36817;&#30340;&#23567;&#21306;&#22495;&#20869;&#65292;(&#27425;)&#26799;&#24230;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#29992;&#24179;&#22343;&#25110;&#26368;&#22823;&#26041;&#24335;&#27714;&#20540;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#21253;&#25324;&#20256;&#32479;&#20248;&#21270;&#20013;&#20256;&#32479;&#30740;&#31350;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#31867;&#26681;&#25454;&#30446;&#26631;&#20989;&#25968;&#30340;Lipschitz&#36830;&#32493;&#24615;&#25110;&#20854;&#26799;&#24230;&#30340;H\"{o}lder/Lipschitz&#36830;&#32493;&#24615;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#20041;&#31867;&#21253;&#21547;&#37027;&#20123;&#26082;&#19981;&#26159;Lipschitz&#36830;&#32493;&#30340;&#20063;&#27809;&#26377;H\"{o}lder&#36830;&#32493;&#26799;&#24230;&#30340;&#20989;&#25968;&#12290;&#24403;&#38480;&#21046;&#22312;&#20256;&#32479;&#20248;&#21270;&#38382;&#39064;&#31867;&#26102;&#65292;&#23450;&#20041;&#30740;&#31350;&#31867;&#30340;&#21442;&#25968;&#23548;&#33268;&#26356;&#21152;&#31934;&#32454;&#30340;&#22797;&#26434;&#24615;&#30028;&#38480;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#24674;&#22797;&#20256;&#32479;&#30340;oracle&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20294;&#19968;&#33324;&#24773;&#20917;&#19979;&#20250;&#23548;&#33268;&#37027;&#20123;&#19981;&#26159;&#8220;&#26368;&#22351;&#24773;&#20917;&#8221;&#30340;&#20989;&#25968;&#20855;&#26377;&#36739;&#20302;&#30340;oracle &#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16317v1 Announce Type: cross  Abstract: We initiate the study of nonsmooth optimization problems under bounded local subgradient variation, which postulates bounded difference between (sub)gradients in small local regions around points, in either average or maximum sense. The resulting class of objective functions encapsulates the classes of objective functions traditionally studied in optimization, which are defined based on either Lipschitz continuity of the objective or H\"{o}lder/Lipschitz continuity of its gradient. Further, the defined class contains functions that are neither Lipschitz continuous nor have a H\"{o}lder continuous gradient. When restricted to the traditional classes of optimization problems, the parameters defining the studied classes lead to more fine-grained complexity bounds, recovering traditional oracle complexity bounds in the worst case but generally leading to lower oracle complexity for functions that are not ``worst case.'' Some highlights of 
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15706</link><description>&lt;p&gt;
G-ACIL&#65306;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-ACIL: Analytic Learning for Exemplar-Free Generalized Class Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15706
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;&#24182;&#25552;&#20379;&#20102;&#23545;GCIL&#24773;&#26223;&#30340;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#27169;&#22411;&#24555;&#36895;&#36951;&#24536;&#21644;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;(CIL)&#22312;&#39034;&#24207;&#20219;&#21153;&#19978;&#35757;&#32451;&#32593;&#32476;&#65292;&#27599;&#20010;&#20219;&#21153;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20294;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#24555;&#36895;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#24191;&#20041;CIL(GCIL)&#26088;&#22312;&#35299;&#20915;&#26356;&#25509;&#36817;&#29616;&#23454;&#24773;&#26223;&#19979;&#30340;CIL&#38382;&#39064;&#65292;&#21363;&#26032;&#25968;&#25454;&#20855;&#26377;&#28151;&#21512;&#25968;&#25454;&#31867;&#21035;&#21644;&#26410;&#30693;&#26679;&#26412;&#20998;&#24067;&#22823;&#23567;&#65292;&#23548;&#33268;&#36951;&#24536;&#21152;&#21095;&#12290;&#29616;&#26377;&#30340;&#38024;&#23545;GCIL&#30340;&#23581;&#35797;&#35201;&#20040;&#24615;&#33021;&#19981;&#20339;&#65292;&#35201;&#20040;&#36890;&#36807;&#20445;&#23384;&#21382;&#21490;&#33539;&#20363;&#20405;&#29359;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#38750;&#33539;&#20363;&#21270;&#30340;&#24191;&#20041;&#20998;&#26512;&#31867;&#22686;&#37327;&#23398;&#20064;(G-ACIL)&#12290;G-ACIL&#37319;&#29992;&#20998;&#26512;&#23398;&#20064;(&#19968;&#31181;&#26080;&#26799;&#24230;&#35757;&#32451;&#25216;&#26415;)&#65292;&#24182;&#20026;GCIL&#24773;&#26223;&#25552;&#20379;&#20998;&#26512;&#35299;(&#21363;&#38381;&#21512;&#24418;&#24335;)&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20256;&#20837;&#25968;&#25454;&#20998;&#35299;&#20026;&#26292;&#38706;&#31867;&#21644;&#26410;&#26292;&#38706;&#31867;&#65292;&#23454;&#29616;&#20102;&#22686;&#38271;&#31867;&#20043;&#38388;&#30340;&#31561;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15706v1 Announce Type: new  Abstract: Class incremental learning (CIL) trains a network on sequential tasks with separated categories but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution, leading to intensified forgetting. Existing attempts for the GCIL either have poor performance, or invade data privacy by saving historical exemplars. To address this, in this paper, we propose an exemplar-free generalized analytic class incremental learning (G-ACIL). The G-ACIL adopts analytic learning (a gradient-free training technique), and delivers an analytical solution (i.e., closed-form) to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, allowing an equivalence between the incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13797</link><description>&lt;p&gt;
&#24357;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#37197;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;VLM&#36873;&#25321;&#26356;&#26377;&#21487;&#33021;&#26631;&#35782;&#20986;&#36866;&#21512;&#30340;VLM&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31574;&#30053;&#26159;&#20174;VLM&#21160;&#29289;&#22253;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;VLM&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25968;&#25454;&#32780;&#26080;&#38656;&#35775;&#38382;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#31181;&#20165;&#35821;&#35328;VLM&#36873;&#25321;&#20013;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#22312;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#24471;&#25991;&#26412;&#25104;&#20026;&#22270;&#20687;&#30340;&#19968;&#20010;&#19981;&#22826;&#21487;&#38752;&#30340;&#26367;&#20195;&#21697;&#65307;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#30340;&#25972;&#20307;&#25490;&#21517;&#19982;&#20854;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25490;&#21517;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#30452;&#25509;&#20174;&#27169;&#22411;&#30340;&#25972;&#20307;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#25968;&#25454;&#38598;&#29305;&#23450;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VLM&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
&lt;/p&gt;</description></item><item><title>CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11497</link><description>&lt;p&gt;
CLIP&#24635;&#26159;&#27604;ImageNet&#27169;&#22411;&#27867;&#21270;&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do CLIPs Always Generalize Better than ImageNet Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11497
&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#22312;&#38754;&#23545;&#20998;&#24067;&#36716;&#31227;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;CounterAnimal&#25968;&#25454;&#38598;&#26469;&#25506;&#31350;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;CLIP&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#12290;CLIP&#23637;&#31034;&#20102;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#25991;&#29486;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;CLIP&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#20026;ImageNet&#22522;&#20934;&#32780;&#35774;&#35745;&#30340;&#21464;&#31181;&#65292;&#21487;&#33021;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;CLIP&#22312;LAION&#31561;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;CounterAnimal&#65292;&#20854;&#20013;&#21253;&#21547;&#21160;&#29289;&#29031;&#29255;&#20013;&#21457;&#29616;&#30340;&#29616;&#23454;&#34394;&#20551;&#29305;&#24449;&#12290;CounterAnimal&#21253;&#25324;a&#65289;&#24120;&#35265;&#32452;&#65306;&#21253;&#25324;&#24120;&#35265;&#32972;&#26223;&#30340;&#21160;&#29289;&#65292;&#24182;&#19988; b) &#23545;&#29031;&#32452;&#65306;&#21253;&#25324;&#22312;&#19981;&#23547;&#24120;&#32972;&#26223;&#19979;&#30340;&#21160;&#29289;&#12290;&#20174;&#24120;&#35265;&#32452;&#21040;&#23545;&#29031;&#32452;&#30340;&#24615;&#33021;&#19979;&#38477;&#37327;&#21270;&#20102;&#27169;&#22411;&#23545;&#34394;&#20551;&#29305;&#24449;&#65288;&#21363;&#32972;&#26223;&#65289;&#39044;&#27979;&#21160;&#29289;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;LAION&#25110;OpenAI&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;CLIP&#21363;&#27809;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11497v1 Announce Type: cross  Abstract: Large vision language models, such as CLIPs, have revolutionized modern machine learning. CLIPs have demonstrated great generalizability under distribution shifts, supported by an increasing body of literature. However, the evaluation datasets for CLIPs are variations primarily designed for ImageNet benchmarks, which may not fully reflect the extent to which CLIPs, e.g., pre-trained on LAION, robust to spurious correlations. To bridge the gap, we collect a real-world dataset called CounterAnimal that contains realistic spurious features found in animal photos. CounterAnimal consists of a) the common group: comprising animals on common backgrounds, and b) the counter group: including animals on unusual backgrounds. The performance drops from the common to counter groups quantify the reliance of models on spurious features (i.e., backgrounds) to predict the animals. We find that CLIPs trained on either LAION or the OpenAI data exhibit no
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11425</link><description>&lt;p&gt;
&#21465;&#20107;&#29305;&#24449;&#36824;&#26159;&#32467;&#26500;&#29305;&#24449;&#65311;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Narrative Feature or Structured Feature? A Study of Large Language Models to Identify Cancer Patients at Risk of Heart Failure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11425
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#26032;&#39062;&#30340;&#21465;&#36848;&#29305;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#30284;&#30151;&#24739;&#32773;&#24739;&#24515;&#21147;&#34928;&#31469;&#30340;&#39118;&#38505;&#65292;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#27835;&#30103;&#24050;&#30693;&#20250;&#24341;&#20837;&#24515;&#27602;&#24615;&#65292;&#23545;&#39044;&#21518;&#21644;&#29983;&#23384;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#35782;&#21035;&#24739;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#23545;&#20110;&#25913;&#21892;&#30284;&#30151;&#27835;&#30103;&#32467;&#26524;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;ML&#12289;&#26102;&#38388;&#24863;&#30693;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;T-LSTM&#65289;&#21644;&#20351;&#29992;&#20174;&#32467;&#26500;&#21270;&#21307;&#23398;&#20195;&#30721;&#34893;&#29983;&#30340;&#26032;&#39062;&#21465;&#36848;&#29305;&#24449;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35782;&#21035;&#24739;HF&#39118;&#38505;&#30340;&#30284;&#30151;&#24739;&#32773;&#12290;&#25105;&#20204;&#20174;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#20102;&#19968;&#32452;&#21253;&#25324;12,806&#21517;&#32954;&#30284;&#12289;&#20083;&#33146;&#30284;&#21644;&#32467;&#30452;&#32928;&#30284;&#24739;&#32773;&#30340;&#30284;&#30151;&#38431;&#21015;&#65292;&#20854;&#20013;1,602&#20154;&#22312;&#30284;&#30151;&#21518;&#21457;&#23637;&#20026;HF&#12290;LLM GatorTron-3.9B&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;F1&#20998;&#25968;&#65292;&#27604;&#20256;&#32479;&#25903;&#25345;&#21521;&#37327;&#26426;&#39640;&#20986;39%&#65292;&#27604;T-LSTM&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39640;&#20986;7%&#65292;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;Transformer&#27169;&#22411;BERT&#39640;&#20986;5.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11425v1 Announce Type: cross  Abstract: Cancer treatments are known to introduce cardiotoxicity, negatively impacting outcomes and survivorship. Identifying cancer patients at risk of heart failure (HF) is critical to improving cancer treatment outcomes and safety. This study examined machine learning (ML) models to identify cancer patients at risk of HF using electronic health records (EHRs), including traditional ML, Time-Aware long short-term memory (T-LSTM), and large language models (LLMs) using novel narrative features derived from the structured medical codes. We identified a cancer cohort of 12,806 patients from the University of Florida Health, diagnosed with lung, breast, and colorectal cancers, among which 1,602 individuals developed HF after cancer. The LLM, GatorTron-3.9B, achieved the best F1 scores, outperforming the traditional support vector machines by 39%, the T-LSTM deep learning model by 7%, and a widely used transformer model, BERT, by 5.6%. The analysi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11351</link><description>&lt;p&gt;
&#22522;&#20110;SDP&#30340;&#20108;&#20998;&#22270;&#32858;&#31867;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An SDP-based Branch-and-Cut Algorithm for Biclustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#32858;&#31867;&#65292;&#20063;&#31216;&#20026;&#20849;&#32858;&#31867;&#12289;&#22359;&#32858;&#31867;&#25110;&#21452;&#21521;&#32858;&#31867;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#21516;&#26102;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#32452;&#65292;&#20351;&#24471;&#21516;&#19968;&#32452;&#20869;&#30340;&#34892;&#21644;&#21015;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#20316;&#20026;&#20108;&#20998;&#22270;&#32858;&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21152;&#26435;&#23436;&#20840;&#20108;&#20998;&#22270;&#20013;&#35782;&#21035; $k$ &#20010;&#19981;&#30456;&#20132;&#30340;&#23436;&#20840;&#20108;&#37096;&#23376;&#22270;&#65288;&#31216;&#20026;&#21452;&#22242;&#65289;&#65292;&#20351;&#23427;&#20204;&#30340;&#23494;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#12290;&#23545;&#20110;&#19978;&#30028;&#20363;&#31243;&#65292;&#25105;&#20204;&#32771;&#34385;&#21322;&#23450;&#35268;&#21010;&#25918;&#26494;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21152;&#24378;&#30028;&#38480;&#30340;&#26377;&#25928;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#38454;&#26041;&#27861;&#20197;&#20999;&#24179;&#38754;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#25918;&#26494;&#38382;&#39064;&#12290;&#23545;&#20110;&#19979;&#30028;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#22823;&#26435;&#21305;&#37197;&#33293;&#20837;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
&lt;/p&gt;</description></item><item><title>Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07815</link><description>&lt;p&gt;
Chronos: &#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Chronos: Learning the Language of Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07815
&lt;/p&gt;
&lt;p&gt;
Chronos&#26694;&#26550;&#36890;&#36807;&#22312;&#22266;&#23450;&#35789;&#27719;&#19978;&#35757;&#32451;&#39044;&#35757;&#32451;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#34920;&#29616;&#20986;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#34920;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Chronos&#65292;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26694;&#26550;&#12290;Chronos&#20351;&#29992;&#32553;&#25918;&#21644;&#37327;&#21270;&#23558;&#26102;&#38388;&#24207;&#21015;&#20540;&#26631;&#35760;&#21270;&#20026;&#22266;&#23450;&#35789;&#27719;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#36825;&#20123;&#26631;&#35760;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#19978;&#35757;&#32451;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;T5&#31995;&#21015;&#65288;&#21442;&#25968;&#33539;&#22260;&#20174;20M&#21040;710M&#65289;&#23545;Chronos&#27169;&#22411;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#21516;&#26102;&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#29983;&#25104;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#21547;42&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#28085;&#30422;&#20102;&#20256;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Chronos&#27169;&#22411;&#65306;&#65288;a&#65289;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;b&#65289;&#30456;&#23545;&#20110;&#19987;&#38376;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#21487;&#27604;&#29978;&#33267;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07815v1 Announce Type: cross  Abstract: We introduce Chronos, a simple yet effective framework for pretrained probabilistic time series models. Chronos tokenizes time series values using scaling and quantization into a fixed vocabulary and trains existing transformer-based language model architectures on these tokenized time series via the cross-entropy loss. We pretrained Chronos models based on the T5 family (ranging from 20M to 710M parameters) on a large collection of publicly available datasets, complemented by a synthetic dataset that we generated via Gaussian processes to improve generalization. In a comprehensive benchmark consisting of 42 datasets, and comprising both classical local models and deep learning methods, we show that Chronos models: (a) significantly outperform other methods on datasets that were part of the training corpus; and (b) have comparable and occasionally superior zero-shot performance on new datasets, relative to methods that were trained spe
&lt;/p&gt;</description></item><item><title>LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07536</link><description>&lt;p&gt;
LaB-GATr&#65306;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
LaB-GATr: geometric algebra transformers for large biomedical surface and volume meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07536
&lt;/p&gt;
&lt;p&gt;
LaB-GATr &#26159;&#19968;&#31181;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340; GATr &#26041;&#27861;&#24182;&#23562;&#37325;&#20102;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35299;&#21078;&#32467;&#26500;&#21487;&#20197;&#29992;&#34920;&#38754;&#25110;&#20307;&#31215;&#32593;&#26684;&#26469;&#25551;&#36848;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#36825;&#20123;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#32593;&#26684;&#36890;&#24120;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#39030;&#28857;&#65292;&#36825;&#22312;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#24739;&#32773;&#29305;&#24322;&#24615;&#32593;&#26684;&#21487;&#33021;&#27809;&#26377;&#32463;&#20856;&#23545;&#40784;&#65292;&#36825;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LaB-GATr&#65292;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#26631;&#35760;&#21270;&#30340;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24207;&#21015;&#21387;&#32553;&#21644;&#25554;&#20540;&#26377;&#25928;&#22320;&#23398;&#20064;&#22823;&#35268;&#27169;&#65288;&#29983;&#29289;&#65289;&#21307;&#23398;&#34920;&#38754;&#21644;&#20307;&#31215;&#32593;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20960;&#20309;&#20195;&#25968;&#21464;&#25442;&#22120;&#65288;GATr&#65289;&#65292;&#22240;&#27492;&#23562;&#37325;&#25152;&#26377;&#27431;&#20960;&#37324;&#24471;&#23545;&#31216;&#24615;&#65292;&#21363;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#65292;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#24739;&#32773;&#20043;&#38388;&#32463;&#20856;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07536v1 Announce Type: cross  Abstract: Many anatomical structures can be described by surface or volume meshes. Machine learning is a promising tool to extract information from these 3D models. However, high-fidelity meshes often contain hundreds of thousands of vertices, which creates unique challenges in building deep neural network architectures. Furthermore, patient-specific meshes may not be canonically aligned which limits the generalisation of machine learning algorithms. We propose LaB-GATr, a transfomer neural network with geometric tokenisation that can effectively learn with large-scale (bio-)medical surface and volume meshes through sequence compression and interpolation. Our method extends the recently proposed geometric algebra transformer (GATr) and thus respects all Euclidean symmetries, i.e. rotation, translation and reflection, effectively mitigating the problem of canonical alignment between patients. LaB-GATr achieves state-of-the-art results on three ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.07241</link><description>&lt;p&gt;
&#26657;&#20934;&#22810;&#27169;&#24577;&#34920;&#31034;&#65306;&#22312;&#19981;&#20351;&#29992;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#36861;&#27714;&#32676;&#20307;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#20174;&#32780;&#25552;&#39640;&#32676;&#20307;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#27880;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#22810;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#23384;&#22312;&#19968;&#20123;&#30171;&#28857;&#65306;(i) &#30452;&#25509;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26082;&#26102;&#38388;&#23494;&#38598;&#21448;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#27169;&#22411;&#24448;&#24448;&#21464;&#24471;&#39640;&#24230;&#19987;&#19994;&#21270;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#23454;&#29992;&#24615;&#65307;(ii) &#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20998;&#31867;&#22120;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#20266;&#29305;&#24449;-&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#27169;&#24335;&#65292;&#20294;&#19982;&#30495;&#23454;&#26631;&#31614;&#20989;&#25968;&#26080;&#20851;&#65307;(iii) &#29616;&#26377;&#20851;&#20110;&#20943;&#23569;&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#30740;&#31350;&#65292;&#20027;&#35201;&#22522;&#20110;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#36825;&#20123;&#29305;&#24449;&#30340;&#20551;&#35774;&#65292;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#24182;&#27809;&#26377;&#25552;&#20379;&#30830;&#20999;&#30340;&#20445;&#35777;&#12290;&#20316;&#20026;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#26412;&#24037;&#20316;&#20391;&#37325;&#20110;&#25506;&#32034;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;CLIP&#23545;&#20266;&#29305;&#24449;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07241v1 Announce Type: cross  Abstract: Fine-tuning pre-trained vision-language models, like CLIP, has yielded success on diverse downstream tasks. However, several pain points persist for this paradigm: (i) directly tuning entire pre-trained models becomes both time-intensive and computationally costly. Additionally, these tuned models tend to become highly specialized, limiting their practicality for real-world deployment; (ii) recent studies indicate that pre-trained vision-language classifiers may overly depend on spurious features -- patterns that correlate with the target in training data, but are not related to the true labeling function; and (iii) existing studies on mitigating the reliance on spurious features, largely based on the assumption that we can identify such features, does not provide definitive assurance for real-world applications. As a piloting study, this work focuses on exploring mitigating the reliance on spurious features for CLIP without using any 
&lt;/p&gt;</description></item><item><title>ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;</title><link>https://arxiv.org/abs/2403.05266</link><description>&lt;p&gt;
ERBench&#65306;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05266
&lt;/p&gt;
&lt;p&gt;
ERBench&#26159;&#19968;&#20010;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#22522;&#20934;&#65292;&#36890;&#36807;&#33258;&#21160;&#36716;&#25442;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#24182;&#26500;&#24314;&#21487;&#33258;&#21160;&#39564;&#35777;&#30340;&#38382;&#39064;&#65292;&#20197;&#25903;&#25345;&#22797;&#26434;&#24615;&#35780;&#20272;&#21644;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#24187;&#35273;&#22522;&#20934;&#35201;&#20040;&#26159;&#38745;&#24577;&#30340;&#65292;&#35201;&#20040;&#32570;&#20047;&#21487;&#35843;&#25972;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#29616;&#26377;&#30340;&#20851;&#31995;&#25968;&#25454;&#24211;&#26159;&#26500;&#24314;&#22522;&#20934;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#21487;&#20197;&#20934;&#30830;&#25551;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ERBench&#65292;&#21487;&#20197;&#33258;&#21160;&#23558;&#20219;&#20309;&#20851;&#31995;&#25968;&#25454;&#24211;&#36716;&#25442;&#20026;&#22522;&#20110;&#23454;&#20307;&#20851;&#31995;&#65288;ER&#65289;&#27169;&#22411;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#24211;&#27169;&#24335;&#12289;&#35760;&#24405;&#21644;&#21151;&#33021;&#20381;&#36182;&#26469;&#26500;&#24314;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22806;&#38190;&#32422;&#26463;&#26469;&#36830;&#25509;&#20851;&#31995;&#21644;&#26500;&#24314;&#22810;&#36339;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20219;&#24847;&#22797;&#26434;&#65292;&#29992;&#20110;&#35843;&#35797;LLMs&#30340;&#20013;&#38388;&#31572;&#26696;&#12290;&#26368;&#21518;&#65292;ERBench&#25903;&#25345;&#25345;&#32493;&#35780;&#20272;&#65292;&#22810;&#27169;&#24577;qu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05266v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved unprecedented performance in various applications, yet their evaluation remains a critical issue. Existing hallucination benchmarks are either static or lack adjustable complexity for thorough analysis. We contend that utilizing existing relational databases is a promising approach for constructing benchmarks due to their accurate knowledge description via functional dependencies. We propose ERBench to automatically convert any relational database into a benchmark based on the entity-relationship (ER) model. Our key idea is to construct questions using the database schema, records, and functional dependencies such that they can be automatically verified. In addition, we use foreign key constraints to join relations and construct multihop questions, which can be arbitrarily complex and used to debug the intermediate answers of LLMs. Finally, ERBench supports continuous evaluation, multimodal qu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.04317</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22312;&#32447;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Online Adaptation of Language Models with a Memory of Amortized Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20998;&#25674;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#22320;&#25552;&#21462;&#12289;&#21387;&#32553;&#24182;&#23384;&#20648;&#20449;&#24687;&#20197;&#20445;&#25345;&#24378;&#22823;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20449;&#24687;&#30340;&#24555;&#36895;&#29983;&#25104;&#21644;&#20256;&#25773;&#65292;&#21363;&#20351;&#24320;&#21457;&#25104;&#26412;&#24040;&#22823;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20063;&#24456;&#24555;&#36807;&#26102;&#12290;&#37492;&#20110;&#20445;&#25345;&#27169;&#22411;&#26356;&#26032;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;LLMs&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#19981;&#26029;&#25193;&#22823;&#30340;&#26410;&#35265;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#29616;&#20195;LLMs&#30340;&#22823;&#21442;&#25968;&#31354;&#38388;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Memory of Amortized Contexts&#65288;MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;LLMs&#30340;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#22312;&#32447;&#36866;&#24212;&#26694;&#26550;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30693;&#35782;&#20445;&#30041;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25674;&#38144;&#29305;&#24449;&#25552;&#21462;&#21644;&#35760;&#24518;&#22686;&#24378;&#26041;&#27861;&#65292;&#23558;&#26032;&#25991;&#26723;&#20013;&#30340;&#20449;&#24687;&#21387;&#32553;&#24182;&#25552;&#21462;&#20026;&#23384;&#20648;&#22312;&#35760;&#24518;&#24211;&#20013;&#30340;&#32039;&#20945;&#35843;&#21046;&#12290;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20851;&#27880;&#24182;&#20174;&#35813;&#35760;&#24518;&#24211;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#35843;&#21046;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04317v1 Announce Type: cross  Abstract: Due to the rapid generation and dissemination of information, large language models (LLMs) quickly run out of date despite enormous development costs. Due to this crucial need to keep models updated, online learning has emerged as a critical necessity when utilizing LLMs for real-world applications. However, given the ever-expanding corpus of unseen documents and the large parameter space of modern LLMs, efficient adaptation is essential. To address these challenges, we propose Memory of Amortized Contexts (MAC), an efficient and effective online adaptation framework for LLMs with strong knowledge retention. We propose an amortized feature extraction and memory-augmentation approach to compress and extract information from new documents into compact modulations stored in a memory bank. When answering questions, our model attends to and extracts relevant knowledge from this memory bank. To learn informative modulations in an efficient m
&lt;/p&gt;</description></item><item><title>&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.03322</link><description>&lt;p&gt;
&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#65306;&#19968;&#39033;&#31995;&#32479;&#24615;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Configuration Performance Learning: A Systematic Survey and Taxonomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03322
&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#26159;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;&#21487;&#20197;&#35828;&#26159;&#21453;&#26144;&#21487;&#37197;&#32622;&#36719;&#20214;&#31995;&#32479;&#34892;&#20026;&#30340;&#26368;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#20195;&#36719;&#20214;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;&#21508;&#31181;&#37197;&#32622;&#22914;&#20309;&#24433;&#21709;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#25104;&#20026;&#36719;&#20214;&#32500;&#25252;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#24615;&#33021;&#36890;&#24120;&#26159;&#22312;&#27809;&#26377;&#23545;&#36719;&#20214;&#31995;&#32479;&#26377;&#36879;&#24443;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#24314;&#27169;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#25968;&#25454;&#65292;&#36825;&#27491;&#22909;&#31526;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#21487;&#37197;&#32622;&#36719;&#20214;&#24615;&#33021;&#23398;&#20064;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#28085;&#30422;&#20102;948&#31687;&#26469;&#33258;&#20845;&#20010;&#32034;&#24341;&#26381;&#21153;&#30340;&#35770;&#25991;&#65292;&#22522;&#20110;&#27492;&#25552;&#21462;&#24182;&#20998;&#26512;&#20102;85&#31687;&#20027;&#35201;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24635;&#32467;&#20102;&#37197;&#32622;&#25968;&#25454;&#22914;&#20309;&#20934;&#22791;&#65292;&#28145;&#24230;&#37197;&#32622;&#24615;&#33021;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#26500;&#24314;&#65292;&#20197;&#21450;&#35813;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#35780;&#20272;&#31561;&#20851;&#38190;&#20027;&#39064;&#21644;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03322v1 Announce Type: cross  Abstract: Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning.   In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evalu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01371</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#21464;&#20998;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large-scale variational Gaussian state-space models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21464;&#20998;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#36924;&#36817;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35780;&#20272;ELBO&#21644;&#33719;&#21462;&#20302;&#26041;&#24046;&#30340;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#21644;&#25512;&#26029;&#32593;&#32476;&#30340;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#65292;&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#21270;&#20026;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23884;&#22871;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#21644;&#32467;&#26500;&#21270;&#21464;&#20998;&#36924;&#36817;&#26041;&#27861;&#65292;&#20854;&#20013;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30001;&#39640;&#26031;&#22122;&#22768;&#39537;&#21160;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#27809;&#26377;&#37319;&#29992;&#23545;&#35282;&#39640;&#26031;&#36924;&#36817;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#35780;&#20272;ELBO&#21644;&#20302;&#26041;&#24046;&#38543;&#26426;&#26799;&#24230;&#20272;&#35745;&#65292;&#36890;&#36807;&#21033;&#29992;&#65288;i&#65289;&#36890;&#36807;&#21160;&#21147;&#23398;&#23545;&#38544;&#29366;&#24577;&#36827;&#34892;&#36793;&#32536;&#21270;&#30340;&#33945;&#29305;&#21345;&#32599;&#36924;&#36817;&#30340;&#20302;&#31209;&#32467;&#26500;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#25512;&#26029;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#20302;&#31209;&#31934;&#24230;&#30697;&#38453;&#26356;&#26032;&#26469;&#36817;&#20284;&#26356;&#26032;&#27493;&#39588;&#65292;&#65288;iii&#65289;&#23558;&#24403;&#21069;&#21644;&#26410;&#26469;&#35266;&#27979;&#32534;&#30721;&#20026;&#20266;&#35266;&#27979;--&#23558;&#36817;&#20284;&#24179;&#28369;&#38382;&#39064;&#36716;&#25442;&#20026;&#65288;&#26356;&#31616;&#21333;&#30340;&#65289;&#36817;&#20284;&#28388;&#27874;&#38382;&#39064;&#12290;&#25972;&#20307;&#32780;&#35328;&#65292;&#24517;&#35201;&#30340;&#32479;&#35745;&#20449;&#24687;&#21644;ELBO&#21487;&#20197;&#22312;$O&#65288;TL&#65288;Sr+S^2+r^2&#65289;&#65289;$&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#20854;&#20013;$T$&#26159;&#31995;&#21015;&#38271;&#24230;&#65292;$L$&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#25968;&#65292;$S$&#26159;&#29992;&#20110;&#36924;&#36817;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.19072</link><description>&lt;p&gt;
TimeXer&#65306;&#21033;&#29992;&#22806;&#29983;&#21464;&#37327;&#22686;&#24378;&#21464;&#21387;&#22120;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#21464;&#21387;&#22120;&#23545;&#20869;&#29983;&#21464;&#37327;&#36827;&#34892;&#39044;&#27979;&#65292;&#24357;&#34917;&#20102;&#20197;&#24448;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#20013;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#37096;&#20998;&#35266;&#27979;&#24615;&#36136;&#65292;&#20165;&#19987;&#27880;&#20110;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#65292;&#20063;&#23601;&#26159;&#25152;&#35859;&#30340;&#20869;&#29983;&#21464;&#37327;&#65292;&#36890;&#24120;&#26159;&#19981;&#36275;&#20197;&#20445;&#35777;&#20934;&#30830;&#39044;&#27979;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#31995;&#32479;&#36890;&#24120;&#35760;&#24405;&#20026;&#22810;&#20010;&#21464;&#37327;&#65292;&#20854;&#20013;&#22806;&#29983;&#24207;&#21015;&#21487;&#20197;&#20026;&#20869;&#29983;&#21464;&#37327;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#22806;&#37096;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#19982;&#20808;&#21069;&#30830;&#31435;&#30340;&#22810;&#21464;&#37327;&#25110;&#21333;&#21464;&#37327;&#39044;&#27979;&#19981;&#21516;&#65292;&#23427;&#20204;&#35201;&#20040;&#23558;&#25152;&#26377;&#21464;&#37327;&#31561;&#21516;&#23545;&#24453;&#65292;&#35201;&#20040;&#24573;&#35270;&#22806;&#29983;&#20449;&#24687;&#65292;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#31181;&#23454;&#38469;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#22806;&#29983;&#21464;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TimeXer&#65292;&#21033;&#29992;&#22806;&#37096;&#20449;&#24687;&#22686;&#24378;&#20869;&#29983;&#21464;&#37327;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#24039;&#22937;&#35774;&#35745;&#30340;&#23884;&#20837;&#23618;&#65292;TimeXer&#20351;&#20256;&#32479;&#30340;Transformer&#26550;&#26500;&#20855;&#26377;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19072v1 Announce Type: cross  Abstract: Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.18888</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#25955;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;(FL)&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#19981;&#21516;&#23396;&#23707;&#38388;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;FL&#23548;&#20986;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#38476;&#29983;&#20998;&#24067;&#30340;&#25968;&#25454;&#23396;&#23707;&#26102;&#20250;&#34920;&#29616;&#20986;&#26126;&#26174;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#31616;&#21333;&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#32852;&#37030;&#23398;&#20064;(UEFL)&#12290;&#35813;&#26694;&#26550;&#21160;&#24577;&#22320;&#23558;&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#21487;&#35757;&#32451;&#30340;&#31163;&#25955;&#21521;&#37327;&#65292;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38024;&#23545;&#34920;&#29616;&#20986;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#23396;&#23707;&#29305;&#21035;&#22320;&#25193;&#23637;&#31163;&#25955;&#21270;&#35789;&#20856;&#25110;&#32534;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.15733</link><description>&lt;p&gt;
ArEEG_Chars: &#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#35774;&#24819;&#35821;&#38899;&#35782;&#21035;&#30340;&#38463;&#25289;&#20271;&#23383;&#31526;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for Arabic Characters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15733
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;ArEEG_Chars&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;97%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#33041;&#26426;&#25509;&#21475;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#26159;&#36817;&#24180;&#26469;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21487;&#20197;&#24110;&#21161;&#30251;&#30186;&#24739;&#32773;&#25913;&#21892;&#29983;&#27963;&#12290;&#26377;&#20960;&#39033;&#30740;&#31350;&#33258;&#21160;&#23558;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#20998;&#31867;&#20026;&#33521;&#25991;&#23383;&#31526;&#21644;&#21333;&#35789;&#12290;&#38463;&#25289;&#20271;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#35821;&#35328;&#20043;&#19968;&#12290;&#28982;&#32780;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;&#33041;&#30005;&#22270;&#20449;&#21495;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#23383;&#31526;&#30340;EEG&#25968;&#25454;&#38598;&#65292;&#24182;&#21629;&#21517;&#20026;ArEEG_Chars&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;ArEEG_Chars&#36827;&#34892;&#20102;&#22810;&#39033;&#23454;&#39564;&#12290;&#22312;&#20351;&#29992;LSTM&#26102;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;ArEEG_Chars&#25968;&#25454;&#38598;&#23558;&#23545;&#30740;&#31350;&#20154;&#21592;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15733v1 Announce Type: cross  Abstract: Brain-Computer-Interface (BCI) has been a hot research topic in the last few years that could help paralyzed people in their lives. Several researches were done to classify electroencephalography (EEG) signals automatically into English characters and words. Arabic language is one of the most used languages around the world. However, to the best of our knowledge, there is no dataset for Arabic characters EEG signals. In this paper, we have created an EEG dataset for Arabic characters and named it ArEEG_Chars. Moreover, several experiments were done on ArEEG_Chars using deep learning. Best results were achieved using LSTM and reached an accuracy of 97%. ArEEG_Chars dataset will be public for researchers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.10376</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#35299;&#37322;CLIP
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#23884;&#20837;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#39640;&#32500;&#31264;&#23494;&#21521;&#37327;&#34920;&#31034;&#24182;&#19981;&#23481;&#26131;&#35299;&#37322;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#36879;&#26126;&#24230;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;CLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;CLIP&#34920;&#31034;&#20998;&#35299;&#20026;&#20854;&#28508;&#22312;&#35821;&#20041;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#65292;&#29992;&#20110;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SpLiCE&#19981;&#38656;&#35201;&#27010;&#24565;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#21518;&#26399;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;SpLiCE&#36755;&#20986;&#30340;&#34920;&#31034;&#21487;&#20197;&#35299;&#37322;&#29978;&#33267;&#21462;&#20195;&#20256;&#32479;&#30340;&#23494;&#38598;CLIP&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati
&lt;/p&gt;</description></item><item><title>OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10176</link><description>&lt;p&gt;
OpenMathInstruct-1: &#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10176
&lt;/p&gt;
&lt;p&gt;
OpenMathInstruct-1&#26159;&#19968;&#20010;&#21253;&#21547;180&#19975;&#20010;&#25968;&#23398;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21512;&#25104;&#24320;&#28304;LLM&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#26469;&#26500;&#24314;&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#24320;&#28304;LLM&#22312;&#25968;&#23398;&#25216;&#33021;&#26041;&#38754;&#19982;&#38381;&#28304;LLM&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#20026;&#20102;&#33719;&#24471;&#29305;&#23450;&#30340;&#25216;&#33021;&#12290;&#30446;&#21069;&#30340;&#22823;&#35268;&#27169;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#22914;MetaMathQA&#21644;MAmmoTH&#65292;&#26159;&#20351;&#29992;&#26469;&#33258;&#21830;&#19994;&#38480;&#21046;&#35768;&#21487;&#30340;&#38381;&#28304;LLM&#30340;&#36755;&#20986;&#26500;&#24314;&#30340;&#12290;&#38480;&#21046;&#22312;&#36825;&#20123;&#25968;&#25454;&#29983;&#25104;&#27969;&#31243;&#20013;&#20351;&#29992;&#24320;&#28304;LLM&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#30446;&#21069;&#26368;&#22909;&#30340;&#38381;&#28304;LLM&#65288;&#22914;GPT-4&#65289;&#21644;&#26368;&#22909;&#30340;&#24320;&#28304;LLM&#20043;&#38388;&#22312;&#25968;&#23398;&#25216;&#33021;&#19978;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#22522;&#20110;&#24320;&#28304;LLM&#30340;&#26368;&#36817;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25552;&#31034;&#26041;&#24335;&#21644;&#19968;&#20123;&#24378;&#21147;&#32553;&#25918;&#65292;&#26500;&#24314;&#20102;OpenMathInstruct-1&#65292;&#19968;&#20010;&#25317;&#26377;180&#19975;&#20010;&#38382;&#39064;-&#35299;&#20915;&#26041;&#27861;&#23545;&#30340;&#25968;&#23398;&#25945;&#23398;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20351;&#29992;GSM8K&#21644;MATH&#36825;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#30340;&#20195;&#30721;&#35299;&#37322;&#22120;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#21512;&#25104;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10176v1 Announce Type: cross  Abstract: Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using t
&lt;/p&gt;</description></item><item><title>HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07785</link><description>&lt;p&gt;
HYPO&#65306;&#36229;&#29699;&#38754;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HYPO: Hyperspherical Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07785
&lt;/p&gt;
&lt;p&gt;
HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#65288;OOD&#65289;&#27867;&#21270;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#28857;&#21487;&#33021;&#20174;&#26681;&#26412;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23398;&#20064;&#36328;&#19981;&#21516;&#39046;&#22495;&#25110;&#29615;&#22659;&#30340;&#19981;&#21464;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;HYPO&#65288;&#36229;&#29699;&#38754;OOD&#27867;&#21270;&#65289;&#65292;&#23427;&#33021;&#22815;&#35777;&#26126;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#36229;&#29699;&#38754;&#23398;&#20064;&#31639;&#27861;&#26159;&#26681;&#25454;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#36827;&#34892;&#24341;&#23548;&#30340;&#65292;&#30830;&#20445;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#29305;&#24449;&#65288;&#36328;&#19981;&#21516;&#35757;&#32451;&#39046;&#22495;&#65289;&#19982;&#20854;&#31867;&#21035;&#21407;&#22411;&#32039;&#23494;&#23545;&#40784;&#65292;&#32780;&#19981;&#21516;&#31867;&#21035;&#30340;&#21407;&#22411;&#20043;&#38388;&#21017;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#30340;&#21407;&#22411;&#23398;&#20064;&#30446;&#26631;&#22914;&#20309;&#25913;&#21892;OOD&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;OOD&#22522;&#20934;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#26497;&#22823;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#30740;&#31350;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07598</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#26497;&#22823;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#65292;&#23427;&#26159;&#36817;&#20284;&#26368;&#23567;&#26497;&#22823;&#30340;&#65288;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#65289;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Zhang&#31561;&#20154;&#65288;2023&#65289;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;Bellman&#26041;&#31243;&#65292;&#21363;&#38543;&#26426;&#20998;&#31867;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;Bellman&#26041;&#31243;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#26041;&#31243;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20960;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24471;&#20986;&#20102;&#23545;&#23454;&#36341;&#32773;&#26377;&#24847;&#20041;&#30340;&#20960;&#20010;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARINA-P&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#65292;&#20248;&#21270;&#20102;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;MARINA-P&#22312;&#31639;&#27861;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#36215;&#28857;&#12290;&#36890;&#36807;&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#30340;&#32467;&#21512;&#65292;M3&#26041;&#27861;&#23454;&#29616;&#20102;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.06412</link><description>&lt;p&gt;
&#25552;&#39640;&#38750;&#20984;&#20998;&#24067;&#24335;&#20248;&#21270;&#22312;&#20989;&#25968;&#30456;&#20284;&#24615;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#21452;&#21521;&#36890;&#20449;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Worst-Case Bidirectional Communication Complexity for Nonconvex Distributed Optimization under Function Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARINA-P&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#65292;&#20248;&#21270;&#20102;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;MARINA-P&#22312;&#31639;&#27861;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#36215;&#28857;&#12290;&#36890;&#36807;&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#30340;&#32467;&#21512;&#65292;M3&#26041;&#27861;&#23454;&#29616;&#20102;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#22120;&#21644;&#24037;&#20316;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#25928;&#36890;&#20449;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#27969;&#34892;&#30340;&#19979;&#34892;&#21387;&#32553;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12290;&#39318;&#20808;&#32771;&#34385;&#19978;&#34892;&#36890;&#20449;&#25104;&#26412;&#21487;&#24573;&#30053;&#30340;&#32431;&#31929;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;MARINA-P&#65292;&#19968;&#31181;&#20351;&#29992;&#19968;&#31995;&#21015;&#30456;&#20851;&#21387;&#32553;&#22120;&#30340;&#26032;&#22411;&#19979;&#34892;&#21387;&#32553;&#26041;&#27861;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#20351;&#29992;&#25490;&#21015;&#21387;&#32553;&#22120;&#30340;MARINA-P&#21487;&#20197;&#23454;&#29616;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#33410;&#28857;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#38543;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#25552;&#39640;&#65292;&#22240;&#27492;&#22312;&#31639;&#27861;&#19978;&#21487;&#35777;&#26126;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;MARINA-P&#21487;&#20197;&#20316;&#20026;&#25903;&#25345;&#21452;&#21521;&#21387;&#32553;&#30340;&#26041;&#27861;&#30340;&#36215;&#28857;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;M3&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;MARINA-P&#19982;&#19978;&#34892;&#21387;&#32553;&#21644;&#21160;&#37327;&#27493;&#39588;&#32452;&#21512;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#29616;&#21452;&#21521;&#21387;&#32553;&#65292;&#24182;&#22312;&#24635;&#36890;&#20449;&#22797;&#26434;&#24230;&#19978;&#35777;&#26126;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective communication between the server and workers plays a key role in distributed optimization. In this paper, we focus on optimizing the server-to-worker communication, uncovering inefficiencies in prevalent downlink compression approaches. Considering first the pure setup where the uplink communication costs are negligible, we introduce MARINA-P, a novel method for downlink compression, employing a collection of correlated compressors. Theoretical analyses demonstrates that MARINA-P with permutation compressors can achieve a server-to-worker communication complexity improving with the number of workers, thus being provably superior to existing algorithms. We further show that MARINA-P can serve as a starting point for extensions such as methods supporting bidirectional compression. We introduce M3, a method combining MARINA-P with uplink compression and a momentum step, achieving bidirectional compression with provable improvements in total communication complexity as the number
&lt;/p&gt;</description></item><item><title>Shadowheart SGD&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#20248;&#21270;&#20102;&#20808;&#21069;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#23545;&#24212;&#30340;&#21452;&#21521;&#35774;&#32622;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04785</link><description>&lt;p&gt;
Shadowheart SGD: &#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD
&lt;/p&gt;
&lt;p&gt;
Shadowheart SGD: Distributed Asynchronous SGD with Optimal Time Complexity Under Arbitrary Computation and Communication Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04785
&lt;/p&gt;
&lt;p&gt;
Shadowheart SGD&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#24322;&#27493;SGD&#26041;&#27861;&#65292;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#22312;&#20219;&#24847;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24322;&#26500;&#24615;&#19979;&#20855;&#26377;&#26368;&#20248;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#26174;&#33879;&#20248;&#21270;&#20102;&#20808;&#21069;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#23545;&#24212;&#30340;&#21452;&#21521;&#35774;&#32622;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#27493;&#38598;&#20013;&#24335;&#20998;&#24067;&#24335;&#35774;&#32622;&#20013;&#32771;&#34385;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#24037;&#20316;&#32773;&#21040;&#26381;&#21153;&#22120;&#30340;&#36890;&#20449;&#26102;&#38388;&#19981;&#33021;&#24573;&#30053;&#65292;&#32780;&#25152;&#26377;&#24037;&#20316;&#32773;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26102;&#38388;&#21487;&#33021;&#19981;&#21516;&#12290;&#21033;&#29992;&#26080;&#20559;&#21387;&#32553;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;-Shadowheart SGD&#65292;&#23427;&#21487;&#35777;&#26126;&#20248;&#21270;&#20102;&#25152;&#26377;&#20808;&#21069;&#38598;&#20013;&#24335;&#26041;&#27861;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;Shadowheart SGD&#22312;&#21387;&#32553;&#36890;&#20449;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#26063;&#20013;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#26368;&#20248;&#30340;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#21452;&#21521;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20174;&#26381;&#21153;&#22120;&#21040;&#24037;&#20316;&#32773;&#30340;&#24191;&#25773;&#19981;&#21487;&#24573;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider nonconvex stochastic optimization problems in the asynchronous centralized distributed setup where the communication times from workers to a server can not be ignored, and the computation and communication times are potentially different for all workers. Using an unbiassed compression technique, we develop a new method-Shadowheart SGD-that provably improves the time complexities of all previous centralized methods. Moreover, we show that the time complexity of Shadowheart SGD is optimal in the family of centralized methods with compressed communication. We also consider the bidirectional setup, where broadcasting from the server to the workers is non-negligible, and develop a corresponding method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#35299;&#20915;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36824;&#36866;&#29992;&#20110;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#35813;&#26694;&#26550;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03883</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#21452;&#23618;&#20248;&#21270;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Bilevel Optimization on Riemannian Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#35299;&#20915;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36824;&#36866;&#29992;&#20110;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#35813;&#26694;&#26550;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#21464;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#22312;&#27969;&#24418;&#19978;&#30340;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#23545;&#27969;&#24418;&#19978;&#30340;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#30740;&#31350;&#25193;&#23637;&#21040;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#21644;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has seen an increasing presence in various domains of applications. In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds. We provide several hypergradient estimation strategies on manifolds and study their estimation error. We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. We also extend the developments to stochastic bilevel optimization and to the use of general retraction. We showcase the utility of the proposed framework on various applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02554</link><description>&lt;p&gt;
DeSparsify&#65306;&#23545;&#35270;&#35273;Transformer&#20013;&#30340;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#36827;&#34892;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms in Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#65292;&#20197;&#27492;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20570;&#20986;&#20102;&#24040;&#22823;&#36129;&#29486;&#65292;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#65289;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#35745;&#31639;&#35201;&#27714;&#38543;&#20351;&#29992;&#30340;Token&#25968;&#37327;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Token&#31232;&#30095;&#21270;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#20381;&#36182;&#36755;&#20837;&#30340;&#31574;&#30053;&#65292;&#23558;&#26080;&#20851;&#30340;Token&#20174;&#35745;&#31639;&#27969;&#31243;&#20013;&#20002;&#24323;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21160;&#24577;&#24615;&#21644;&#24179;&#22343;&#24773;&#20917;&#20551;&#35774;&#20351;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961; - &#32463;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#33021;&#22815;&#27450;&#39575;&#31232;&#30095;&#21270;&#26426;&#21046;&#65292;&#23548;&#33268;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#26041;&#27861;DeSparsify&#65292;&#38024;&#23545;&#20351;&#29992;Token&#31232;&#30095;&#21270;&#26426;&#21046;&#30340;&#35270;&#35273;Transformer&#30340;&#21487;&#29992;&#24615;&#12290;&#35813;&#25915;&#20987;&#26088;&#22312;&#32791;&#23613;&#25805;&#20316;&#31995;&#32479;&#30340;&#36164;&#28304;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have contributed greatly to advancements in the computer vision domain, demonstrating state-of-the-art performance in diverse tasks (e.g., image classification, object detection). However, their high computational requirements grow quadratically with the number of tokens used. Token sparsification techniques have been proposed to address this issue. These techniques employ an input-dependent strategy, in which uninformative tokens are discarded from the computation pipeline, improving the model's efficiency. However, their dynamism and average-case assumption makes them vulnerable to a new threat vector - carefully crafted adversarial examples capable of fooling the sparsification mechanism, resulting in worst-case performance. In this paper, we present DeSparsify, an attack targeting the availability of vision transformers that use token sparsification mechanisms. The attack aims to exhaust the operating system's resources, while maintaining its stealthiness. Our e
&lt;/p&gt;</description></item><item><title>EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02425</link><description>&lt;p&gt;
EuLagNet: &#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#30340;&#27431;&#25289;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02425
&lt;/p&gt;
&lt;p&gt;
EuLagNet&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#36319;&#36394;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#30001;&#20110;&#27431;&#25289;&#35266;&#23519;&#32780;&#23548;&#33268;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#22256;&#38590;&#65292;&#20026;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#30340;&#27969;&#20307;&#23545;&#27668;&#35937;&#23398;&#12289;&#28023;&#27915;&#23398;&#21644;&#31354;&#27668;&#21160;&#21147;&#23398;&#31561;&#24191;&#27867;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27969;&#20307;&#36890;&#24120;&#20174;&#27431;&#25289;&#35282;&#24230;&#35266;&#23519;&#65292;&#20854;&#27963;&#36291;&#21644;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#22312;&#38745;&#27490;&#30340;&#32593;&#26684;&#20013;&#20005;&#37325;&#34987;&#25513;&#30422;&#21644;&#28151;&#28102;&#65292;&#32473;&#39044;&#27979;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25289;&#26684;&#26391;&#26085;&#24341;&#23548;&#33539;&#24335;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#20026;&#23548;&#21521;&#30340;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#21452;&#37325;&#36882;&#24402;&#32593;&#32476;&#65288;EuLagNet&#65289;&#65292;&#36890;&#36807;&#36319;&#36394;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#20851;&#38190;&#31890;&#23376;&#30340;&#36816;&#21160;&#24182;&#38543;&#26102;&#38388;&#31215;&#32047;&#21160;&#21147;&#23398;&#20449;&#24687;&#26469;&#25429;&#25417;&#22810;&#23610;&#24230;&#27969;&#20307;&#21160;&#21147;&#23398;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EuLag&#22359;&#65292;&#29992;&#20110;&#22312;&#27599;&#20010;&#26102;&#21051;&#21644;&#23610;&#24230;&#19978;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#27431;&#25289;&#21644;&#25289;&#26684;&#26391;&#26085;&#29305;&#24449;&#65292;&#20854;&#20013;&#36319;&#36394;&#31890;&#23376;&#30340;&#36816;&#21160;&#26159;&#20174;&#27431;&#25289;&#35266;&#23519;&#20013;&#25512;&#26029;&#20986;&#26469;&#30340;&#65292;&#23427;&#20204;&#31215;&#32047;&#30340;&#21160;&#21147;&#23398;&#20449;&#24687;&#34987;&#32435;&#20837;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately predicting the future fluid is important to extensive areas, such as meteorology, oceanology and aerodynamics. However, since the fluid is usually observed from an Eulerian perspective, its active and intricate dynamics are seriously obscured and confounded in static grids, bringing horny challenges to the prediction. This paper introduces a new Lagrangian-guided paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting the future based on Eulerian observations, we propose the Eulerian-Lagrangian Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by tracking movements of adaptively sampled key particles on multiple scales and integrating dynamics information over time. Concretely, a EuLag Block is presented to communicate the learned Eulerian and Lagrangian features at each moment and scale, where the motion of tracked particles is inferred from Eulerian observations and their accumulated dynamics information is incorporated into
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.08724</link><description>&lt;p&gt;
&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Personalized Path Recourse for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32534;&#36753;&#21160;&#20316;&#36335;&#24452;&#26469;&#23454;&#29616;&#26399;&#26395;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#20284;&#24230;&#39640;&#65292;&#24182;&#19988;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#23454;&#29616;&#39044;&#23450;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20010;&#24615;&#21270;&#36335;&#24452;&#34917;&#25937;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22686;&#24378;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#34917;&#25937;&#36335;&#24452;&#12290;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#32534;&#36753;&#32473;&#23450;&#30340;&#21160;&#20316;&#36335;&#24452;&#20197;&#36798;&#21040;&#26399;&#26395;&#30340;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#30456;&#27604;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65289;&#65292;&#21516;&#26102;&#30830;&#20445;&#19982;&#20195;&#29702;&#30340;&#21407;&#22987;&#36335;&#24452;&#39640;&#24230;&#30456;&#20284;&#24182;&#20010;&#24615;&#21270;&#36866;&#24212;&#20195;&#29702;&#12290;&#20010;&#24615;&#21270;&#26159;&#25351;&#26032;&#36335;&#24452;&#22312;&#20174;&#31574;&#30053;&#20989;&#25968;&#20013;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#34892;&#20026;&#27169;&#24335;&#26041;&#38754;&#30340;&#23450;&#21046;&#31243;&#24230;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#34917;&#25937;&#20195;&#29702;&#26469;&#29983;&#25104;&#36825;&#26679;&#30340;&#20010;&#24615;&#21270;&#36335;&#24452;&#65292;&#36825;&#20123;&#36335;&#24452;&#26159;&#20351;&#29992;&#32771;&#34385;&#30446;&#26631;&#12289;&#30456;&#20284;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#33719;&#24471;&#30340;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#32416;&#27491;&#25110;&#25913;&#36827;&#21160;&#20316;&#24207;&#21015;&#25110;&#25968;&#25454;&#24207;&#21015;&#20197;&#36798;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08724v2 Announce Type: replace-cross  Abstract: This paper introduces Personalized Path Recourse, a novel method that generates recourse paths for a reinforcement learning agent. The goal is to edit a given path of actions to achieve desired goals (e.g., better outcomes compared to the agent's original path) while ensuring a high similarity to the agent's original paths and being personalized to the agent. Personalization refers to the extent to which the new path is tailored to the agent's observed behavior patterns from their policy function. We train a personalized recourse agent to generate such personalized paths, which are obtained using reward functions that consider the goal, similarity, and personalization. The proposed method is applicable to both reinforcement learning and supervised learning settings for correcting or improving sequences of actions or sequences of data to achieve a pre-determined goal. The method is evaluated in various settings. Experiments show
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2312.05440</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#21644;&#24555;&#36895;&#27169;&#25311;&#25512;&#26029;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Consistency Models for Scalable and Fast Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#65292;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25512;&#26029;&#65288;SBI&#65289;&#19981;&#26029;&#23547;&#25214;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#31639;&#27861;&#65292;&#20197;&#20934;&#30830;&#25512;&#26029;&#22797;&#26434;&#27169;&#22411;&#30340;&#21442;&#25968;&#20174;&#22024;&#26434;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#30340;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#21487;&#25193;&#23637;&#12289;&#24555;&#36895;&#21644;&#25674;&#38144;&#25512;&#26029;&#30340;&#26032;&#33258;&#30001;&#24418;&#24335;&#26465;&#20214;&#37319;&#26679;&#22120;&#65292;&#21033;&#29992;&#29983;&#25104;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;CMPE&#23558;&#26631;&#20934;&#21270;&#27969;&#21644;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#21040;&#21333;&#20010;&#29983;&#25104;&#26550;&#26500;&#20013;&#65306;&#23427;&#26412;&#36136;&#19978;&#25552;&#28860;&#20102;&#36830;&#32493;&#27010;&#29575;&#27969;&#65292;&#24182;&#33021;&#22815;&#21033;&#29992;&#26080;&#32422;&#26463;&#30340;&#32467;&#26500;&#24555;&#36895;&#36827;&#34892;&#23569;&#23556;&#25512;&#26029;&#65292;&#35813;&#32467;&#26500;&#21487;&#20197;&#23450;&#21046;&#21040;&#20272;&#35745;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;CMPE&#19981;&#20165;&#22312;&#19977;&#20010;&#22256;&#38590;&#30340;&#20302;&#32500;&#38382;&#39064;&#19978;&#20248;&#20110;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#65292;&#32780;&#19988;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#21435;&#22122;&#23454;&#39564;&#21644;&#20272;&#35745;&#35745;&#31639;&#23494;&#38598;&#22411;&#22810;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05440v2 Announce Type: replace-cross  Abstract: Simulation-based inference (SBI) is constantly in search of more expressive algorithms for accurately inferring the parameters of complex models from noisy data. We present consistency models for neural posterior estimation (CMPE), a new free-form conditional sampler for scalable, fast, and amortized SBI with generative neural networks. CMPE combines the advantages of normalizing flows and flow matching methods into a single generative architecture: It essentially distills a continuous probability flow and enables rapid few-shot inference with an unconstrained architecture that can be tailored to the structure of the estimation problem. Our empirical evaluation demonstrates that CMPE not only outperforms current state-of-the-art algorithms on three hard low-dimensional problems but also achieves competitive performance in a high-dimensional Bayesian denoising experiment and in estimating a computationally demanding multi-scale 
&lt;/p&gt;</description></item><item><title>LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17943</link><description>&lt;p&gt;
LayerCollapse: &#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LayerCollapse: Adaptive compression of neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17943
&lt;/p&gt;
&lt;p&gt;
LayerCollapse&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21098;&#26525;&#26469;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#23545;&#24615;&#33021;&#24433;&#21709;&#26377;&#38480;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#27491;&#21017;&#21270;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#19981;&#26029;&#22686;&#38271;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36229;&#21442;&#25968;&#21270;&#30340;Transformer&#32593;&#32476;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#38754;&#30340;&#19994;&#32489;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#21547;&#26377;&#25968;&#20159;&#20010;&#21442;&#25968;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayerCollapse&#65292;&#19968;&#31181;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#24418;&#24335;&#65292;&#29992;&#20110;&#20943;&#23569;&#20840;&#36830;&#25509;&#23618;&#30340;&#28145;&#24230;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#20801;&#35768;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#21518;&#21387;&#32553;&#65292;&#24182;&#23545;&#24615;&#33021;&#20135;&#29983;&#26377;&#38480;&#30340;&#24433;&#21709;&#12290;LayerCollapse&#36890;&#36807;&#23545;&#20840;&#36830;&#25509;&#23618;&#20043;&#38388;&#30340;&#28608;&#27963;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#35843;&#33410;&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#38477;&#20302;&#21040;&#30456;&#24212;&#32447;&#24615;&#36716;&#25442;&#30340;&#31209;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;LayerCollapse&#30340;&#21387;&#32553;&#33021;&#21147;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling the ever-increasing scale of contemporary deep learning and transformer-based models poses a significant challenge. Overparameterized Transformer networks outperform prior art in Natural Language processing and Computer Vision. These models contain hundreds of millions of parameters, demanding significant computational resources and making them prone to overfitting. In this work we present LayerCollapse, a form of structured pruning to reduce the depth of fully connected layers. We develop a novel regularizer allowing for post-training compression without finetuning, while having limited impact on performance. LayerCollapse controls model expressiveness with regularization on the activations between fully connected layers, modulating the linearity of activation functions. A linear activation function reduces the rank of the transformation to the rank of the corresponding linear transformation. We demonstrate the effectiveness of LayerCollapse by showing its compression capabil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.10671</link><description>&lt;p&gt;
&#22810;&#27169;&#25311;&#25512;&#29702;&#30340;&#28145;&#24230;&#34701;&#21512;&#65306;&#28145;&#24230;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10671
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745; (MultiNPE) &#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#25972;&#21512;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#65292;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#20934;&#30830;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22810;&#27169;&#24577;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;(MultiNPE)&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#25311;&#25512;&#29702;&#20013;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#21463;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#23427;&#36171;&#20104;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#24182;&#25512;&#26029;&#22797;&#26434;&#25968;&#23398;&#27169;&#22411;&#21442;&#25968;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#38024;&#23545;MultiNPE&#21046;&#23450;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#26041;&#27861;&#65288;&#26089;&#26399;&#12289;&#21518;&#26399;&#12289;&#28151;&#21512;&#65289;&#65292;&#24182;&#22312;&#19977;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#39564;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;MultiNPE&#19981;&#20165;&#22312;&#21442;&#32771;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#19968;&#25968;&#25454;&#28304;&#22522;&#32447;&#65292;&#36824;&#22312;&#31070;&#32463;&#31185;&#23398;&#21644;&#24515;&#33039;&#30149;&#23398;&#30340;&#31185;&#23398;&#27169;&#22411;&#25512;&#29702;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#25104;&#32489;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#37096;&#20998;&#32570;&#22833;&#25968;&#25454;&#23545;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#21518;&#26399;&#21644;&#28151;&#21512;&#34701;&#21512;&#25216;&#26415;&#25104;&#20026;&#22810;&#27169;&#24577;&#27169;&#25311;&#25512;&#29702;&#23454;&#38469;&#24212;&#29992;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10671v2 Announce Type: replace-cross  Abstract: We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate multimodal fusion approaches for \hbox{MultiNPE} (early, late, hybrid) and evaluate their performance in three challenging experiments. MultiNPE not only outperforms single-source baselines on a reference task, but also achieves superior inference on scientific models from neuroscience and cardiology. We systematically investigate the impact of partially missing data on the different fusion strategies. Across our experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based infere
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.10158</link><description>&lt;p&gt;
DISTINQT: &#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#38544;&#31169;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10158
&lt;/p&gt;
&lt;p&gt;
DISTINQT&#26159;&#19968;&#31181;&#38754;&#21521;&#26410;&#26469;&#31227;&#21160;&#21644;&#26080;&#32447;&#32593;&#32476;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;QoS&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#21644;6G&#20197;&#21518;&#30340;&#32593;&#32476;&#23558;&#25903;&#25345;&#20381;&#36182;&#19968;&#23450;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#30340;&#26032;&#30340;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29992;&#20363;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#21450;&#26102;&#39044;&#27979;QoS&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65288;&#22914;&#36710;&#36742;&#36890;&#20449;&#65289;&#23588;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#30452;&#21040;&#26368;&#36817;&#65292;QoS&#39044;&#27979;&#19968;&#30452;&#30001;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35299;&#20915;&#26041;&#26696;&#23436;&#25104;&#65292;&#20294;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#38544;&#31169;&#12289;&#35745;&#31639;&#21644;&#36816;&#33829;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#26367;&#20195;&#26041;&#26696;&#24050;&#32463;&#20986;&#29616;&#65288;&#22914;&#20998;&#21106;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#23558;&#22797;&#26434;&#24230;&#36739;&#20302;&#30340;AI&#20219;&#21153;&#20998;&#24067;&#22312;&#33410;&#28857;&#20043;&#38388;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#30340;&#24322;&#26500;&#24615;&#65292;&#24403;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26102;&#65292;&#20250;&#20986;&#29616;&#26032;&#30340;&#25361;&#25112;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISTINQT&#30340;&#38754;&#21521;QoS&#39044;&#27979;&#30340;&#38544;&#31169;&#24863;&#30693;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond 5G and 6G networks are expected to support new and challenging use cases and applications that depend on a certain level of Quality of Service (QoS) to operate smoothly. Predicting the QoS in a timely manner is of high importance, especially for safety-critical applications as in the case of vehicular communications. Although until recent years the QoS prediction has been carried out by centralized Artificial Intelligence (AI) solutions, a number of privacy, computational, and operational concerns have emerged. Alternative solutions have been surfaced (e.g. Split Learning, Federated Learning), distributing AI tasks of reduced complexity across nodes, while preserving the privacy of the data. However, new challenges rise when it comes to scalable distributed learning approaches, taking into account the heterogeneous nature of future wireless networks. The current work proposes DISTINQT, a privacy-aware distributed learning framework for QoS prediction. Our framework supports mult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23548;&#20989;&#25968;&#26469;&#36817;&#20284;&#23433;&#20840;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23547;&#25214;&#26368;&#20248;CBF&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05629</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23398;&#20064;&#20197;&#24615;&#33021;&#20026;&#23548;&#21521;&#30340;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Performance-Oriented Control Barrier Functions Under Complex Safety Constraints and Limited Actuation. (arXiv:2401.05629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23548;&#20989;&#25968;&#26469;&#36817;&#20284;&#23433;&#20840;&#38598;&#21512;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#21644;&#26377;&#38480;&#25191;&#34892;&#33021;&#21147;&#19979;&#23547;&#25214;&#26368;&#20248;CBF&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65288;CBFs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20248;&#38597;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#30340;&#36712;&#36857;&#32422;&#26463;&#22312;&#39044;&#23450;&#20041;&#23433;&#20840;&#38598;&#21512;&#30340;&#19981;&#21464;&#23376;&#38598;&#19978;&#65292;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#19968;&#20010;&#21516;&#26102;&#22312;&#26368;&#22823;&#21270;&#25511;&#21046;&#19981;&#21464;&#38598;&#20307;&#31215;&#21644;&#36866;&#24212;&#22797;&#26434;&#23433;&#20840;&#32422;&#26463;&#26041;&#38754;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CBF&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#25191;&#34892;&#32422;&#26463;&#30340;&#39640;&#30456;&#23545;&#24230;&#30340;&#31995;&#32479;&#20013;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#36825;&#20123;&#38556;&#30861;&#12290;&#32473;&#23450;&#23450;&#20041;&#23433;&#20840;&#38598;&#21512;&#30340;&#22810;&#20010;&#29366;&#24577;&#32422;&#26463;&#30340;&#24067;&#23572;&#32452;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#21487;&#23548;&#20989;&#25968;&#24320;&#22987;&#65292;&#20854;0&#36229;&#32423;&#32423;&#21035;&#38598;&#21512;&#25552;&#20379;&#20102;&#23433;&#20840;&#38598;&#21512;&#30340;&#20869;&#37096;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20989;&#25968;&#20197;&#21450;&#19968;&#20010;&#24179;&#28369;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;CBF&#20505;&#36873;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;-&#38597;&#21487;&#27604;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control Barrier Functions (CBFs) provide an elegant framework for designing safety filters for nonlinear control systems by constraining their trajectories to an invariant subset of a prespecified safe set. However, the task of finding a CBF that concurrently maximizes the volume of the resulting control invariant set while accommodating complex safety constraints, particularly in high relative degree systems with actuation constraints, continues to pose a substantial challenge. In this work, we propose a novel self-supervised learning framework that holistically addresses these hurdles. Given a Boolean composition of multiple state constraints that define the safe set, our approach starts with building a single continuously differentiable function whose 0-superlevel set provides an inner approximation of the safe set. We then use this function together with a smooth neural network to parameterize the CBF candidate. Finally, we design a training loss function based on a Hamilton-Jacobi
&lt;/p&gt;</description></item><item><title>H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.02905</link><description>&lt;p&gt;
H2G2-Net:&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#21457;&#29616;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network Framework for Discovery of Multi-Modal Physiological Responses. (arXiv:2401.02905v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02905
&lt;/p&gt;
&lt;p&gt;
H2G2-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#22810;&#27169;&#24577;&#29983;&#29702;&#21453;&#24212;&#30340;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#30740;&#31350;&#24212;&#29992;&#20013;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29983;&#29702;&#20449;&#21495;&#26469;&#21457;&#29616;&#20154;&#31867;&#35748;&#30693;&#21644;&#24773;&#24863;&#29366;&#24577;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20154;&#20307;&#30340;&#29983;&#29702;&#21453;&#24212;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#30340;&#24433;&#21709;&#65292;&#24120;&#29992;&#20110;&#20998;&#26512;&#35748;&#30693;&#29366;&#24577;&#12290;&#20174;&#32593;&#32476;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#24322;&#26500;&#29983;&#29702;&#27169;&#24335;&#22312;&#22270;&#32467;&#26500;&#20013;&#30340;&#20114;&#21160;&#21487;&#33021;&#25552;&#20379;&#26377;&#30410;&#30340;&#20449;&#24687;&#26469;&#25903;&#25345;&#35748;&#30693;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21150;&#27861;&#24471;&#21040;&#24322;&#26500;&#27169;&#24577;&#20043;&#38388;&#30340;&#31934;&#30830;&#36830;&#25509;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#30340;&#23376;&#27169;&#24577;&#12290;&#29616;&#26377;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#29992;&#20110;&#22312;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#19978;&#23398;&#20064;&#38750;&#23618;&#27425;&#21270;&#30340;&#21516;&#36136;&#22270;&#65292;&#26080;&#27861;&#20174;&#23618;&#27425;&#21270;&#30340;&#22810;&#27169;&#24577;&#29983;&#29702;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#22270;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#24322;&#26500;&#22270;&#29983;&#25104;&#32593;&#32476;&#65288;H2G2-Net&#65289;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#22270;&#32467;&#26500;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering human cognitive and emotional states using multi-modal physiological signals draws attention across various research applications. Physiological responses of the human body are influenced by human cognition and commonly used to analyze cognitive states. From a network science perspective, the interactions of these heterogeneous physiological modalities in a graph structure may provide insightful information to support prediction of cognitive states. However, there is no clue to derive exact connectivity between heterogeneous modalities and there exists a hierarchical structure of sub-modalities. Existing graph neural networks are designed to learn on non-hierarchical homogeneous graphs with pre-defined graph structures; they failed to learn from hierarchical, multi-modal physiological data without a pre-defined graph structure. To this end, we propose a hierarchical heterogeneous graph generative network (H2G2-Net) that automatically learns a graph structure without domain 
&lt;/p&gt;</description></item><item><title>SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01187</link><description>&lt;p&gt;
SASSL:&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01187
&lt;/p&gt;
&lt;p&gt;
SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22686;&#24378;&#27969;&#27700;&#32447;&#21253;&#25324;&#20102;&#21508;&#31181;&#21407;&#22987;&#30340;&#36716;&#25442;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;&#33258;&#28982;&#22270;&#20687;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#26679;&#26412;&#21487;&#33021;&#26174;&#31034;&#20986;&#36864;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20302;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#33258;&#30417;&#30563;&#34920;&#24449;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASSL&#30340;&#26032;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#23427;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#23558;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#35299;&#32806;&#65292;&#24182;&#20165;&#23545;&#39118;&#26684;&#24212;&#29992;&#36716;&#25442;&#65292;&#20445;&#25345;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#23427;&#20204;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24191;&#20026;&#25509;&#21463;&#30340;MoCo v2&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;ImageNet&#19978;&#30340;top-1&#20998;&#31867;&#24615;&#33021;&#25552;&#21319;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.11590</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#23548;&#33322;&#22330;&#26223;&#19979;&#25512;&#26029;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;&#36890;&#24120;&#36890;&#36807;&#35843;&#26597;&#38382;&#21367;&#26469;&#34913;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;SEAN TOGETHER&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#20013;&#20154;&#19982;&#31227;&#21160;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;5&#28857;&#37327;&#34920;&#35780;&#20215;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#31867;&#22411;&#65288;&#20363;&#22914;&#38754;&#37096;&#12289;&#31354;&#38388;&#21644;&#22320;&#22270;&#29305;&#24449;&#65289;&#26469;&#39044;&#27979;&#24863;&#30693;&#21040;&#30340;&#26426;&#22120;&#20154;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20165;&#38754;&#37096;&#34920;&#24773;&#23601;&#33021;&#25552;&#20379;&#20851;&#20110;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#21360;&#35937;&#30340;&#26377;&#29992;&#20449;&#24687;&#65307;&#20294;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#36825;&#31181;&#25512;&#26029;&#20219;&#21153;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
&lt;/p&gt;</description></item><item><title>Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04345</link><description>&lt;p&gt;
Neur2RO: &#31070;&#32463;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neur2RO: Neural Two-Stage Robust Optimization. (arXiv:2310.04345v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04345
&lt;/p&gt;
&lt;p&gt;
Neur2RO&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#24182;&#23884;&#20837;&#21040;&#32463;&#20856;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#31639;&#27861;&#20013;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#27714;&#35299;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#20248;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#24314;&#27169;&#21644;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#24037;&#20316;&#35299;&#20915;&#20102;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#65288;&#20063;&#31216;&#20026;&#21487;&#35843;&#25972;&#40065;&#26834;&#20248;&#21270;&#65289;&#38382;&#39064;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#23454;&#29616;&#20043;&#21069;&#21644;&#20043;&#21518;&#36827;&#34892;&#31532;&#19968;&#38454;&#27573;&#21644;&#31532;&#20108;&#38454;&#27573;&#30340;&#20915;&#31574;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#23884;&#22871;&#30340;&#26368;&#23567;-&#26368;&#22823;-&#26368;&#23567;&#20248;&#21270;&#38382;&#39064;&#65292;&#20174;&#35745;&#31639;&#19978;&#26469;&#35828;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#20915;&#31574;&#26159;&#31163;&#25955;&#30340;&#26102;&#20505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Neur2RO&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21015;-&#32422;&#26463;&#29983;&#25104;&#65288;CCG&#65289;&#30340;&#23454;&#20363;&#31639;&#27861;&#65292;CCG&#26159;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#30340;&#32463;&#20856;&#36845;&#20195;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#20272;&#35745;&#31532;&#20108;&#38454;&#27573;&#38382;&#39064;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#31181;&#26550;&#26500;&#26131;&#20110;&#20248;&#21270;&#12290;&#23558;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;CCG&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#24555;&#36895;&#24471;&#21040;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;&#36825;&#22312;&#20004;&#20010;&#20108;&#38454;&#27573;&#40065;&#26834;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#65288;&#32972;&#21253;&#38382;&#39064;&#21644;&#36164;&#26412;&#39044;&#31639;&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design. Embedding our neural network into CCG yields high-quality solutions quickly as evidenced by experiments on two 2RO benchmarks, knapsack and capital budgeting. For knapsack, Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.15710</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Object Detection in the Open World. (arXiv:2307.15710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#24182;&#20174;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#26377;&#19968;&#32452;&#22266;&#23450;&#30340;&#31867;&#21035;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#26041;&#27861;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#24212;&#29992;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#26410;&#26631;&#35760;&#21644;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#65292;&#21363;&#23646;&#20110;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25506;&#35752;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;OOD&#26679;&#26412;&#65292;&#22914;&#26524;&#21487;&#20197;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#20174;&#20013;&#23398;&#20064;&#65311;&#32771;&#34385;&#21040;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24320;&#25918;&#19990;&#30028;&#21322;&#30417;&#30563;&#26816;&#27979;&#26694;&#26550;&#65288;OWSSD&#65289;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;OOD&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#20174;ID&#21644;OOD&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#30001;&#20165;&#22312;ID&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#33258;&#32534;&#30721;&#22120;&#32593;&#32476;&#32452;&#25104;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#30446;&#26631;&#26816;&#27979;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches for semi-supervised object detection assume a fixed set of classes present in training and unlabeled datasets, i.e., in-distribution (ID) data. The performance of these techniques significantly degrades when these techniques are deployed in the open-world, due to the fact that the unlabeled and test data may contain objects that were not seen during training, i.e., out-of-distribution (OOD) data. The two key questions that we explore in this paper are: can we detect these OOD samples and if so, can we learn from them? With these considerations in mind, we propose the Open World Semi-supervised Detection framework (OWSSD) that effectively detects OOD data along with a semi-supervised learning pipeline that learns from both ID and OOD data. We introduce an ensemble based OOD detector consisting of lightweight auto-encoder networks trained only on ID data. Through extensive evalulation, we demonstrate that our method performs competitively against state-of-the-art OOD 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06766</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning. (arXiv:2306.06766v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23460;&#20869;&#26426;&#22120;&#20154;&#23548;&#33322;&#21033;&#29992;&#26080;&#32447;&#20449;&#21495;&#30340;&#19981;&#26029;&#20851;&#27880;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#65288;PIRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#22522;&#20110;&#23556;&#39057;&#20256;&#25773;&#30340;&#26041;&#27861;&#30452;&#35266;&#19988;&#33021;&#22815;&#36866;&#24212;&#31616;&#21333;&#30340;&#22330;&#26223;&#65292;&#20294;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#38590;&#20197;&#23548;&#33322;&#12290;&#32780;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#25216;&#26415;&#26469;&#25506;&#32034;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#65292;&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19988;&#24471;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#26377;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL), powered by advanced computing machinery, can explore the entire state space, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.05989</link><description>&lt;p&gt;
&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quartile-Based Seasonality Decomposition for Time Series Forecasting and Anomaly Detection. (arXiv:2306.05989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QBSD&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#20449;&#39046;&#22495;&#65292;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#21644;&#34920;&#24449;&#19981;&#35268;&#21017;&#27169;&#24335;&#12289;&#24322;&#24120;&#34892;&#20026;&#21644;&#32593;&#32476;&#24322;&#24120;&#65292;&#20174;&#32780;&#25552;&#39640;&#26381;&#21153;&#36136;&#37327;&#21644;&#25805;&#20316;&#25928;&#29575;&#12290;&#31934;&#30830;&#22320;&#39044;&#27979;&#21644;&#28040;&#38500;&#21487;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#26159;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#22235;&#20998;&#20301;&#25968;&#30340;&#23395;&#33410;&#24615;&#20998;&#35299;&#65288;QBSD&#65289;&#30340;&#23454;&#26102;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#39044;&#27979;&#20934;&#30830;&#29575;&#20043;&#38388;&#21462;&#24471;&#26368;&#20339;&#24179;&#34913;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;QBSD&#19982;&#29616;&#26377;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#21450;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The timely detection of anomalies is essential in the telecom domain as it facilitates the identification and characterization of irregular patterns, abnormal behaviors, and network anomalies, contributing to enhanced service quality and operational efficiency. Precisely forecasting and eliminating predictable time series patterns constitutes a vital component of time series anomaly detection. While the state-of-the-art methods aim to maximize forecasting accuracy, the computational performance takes a hit. In a system composed of a large number of time series variables, e.g., cell Key Performance Indicators (KPIs), the time and space complexity of the forecasting employed is of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a live forecasting method proposed in this paper to make an optimal trade-off between computational complexity and forecasting accuracy. This paper compares the performance of QBSD to the state-of-the-art forecasting methods and their applic
&lt;/p&gt;</description></item><item><title>FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.05172</link><description>&lt;p&gt;
FLEdge&#65306;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FLEdge: Benchmarking Federated Machine Learning Applications in Edge Computing Systems. (arXiv:2306.05172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05172
&lt;/p&gt;
&lt;p&gt;
FLEdge&#26159;&#19968;&#20010;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#36890;&#36807;&#30740;&#31350;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#33021;&#37327;&#25928;&#29575;&#21644;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#26426;&#22120;&#23398;&#20064;&#65288;FL&#65289;&#22791;&#21463;&#20851;&#27880;&#12290; FL&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#22312;&#27169;&#25311;&#31995;&#32479;&#25110;&#25968;&#25454;&#20013;&#24515;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#24573;&#30053;&#20102;&#19982;&#36793;&#32536;&#35745;&#31639;&#23494;&#20999;&#30456;&#20851;&#30340;&#23454;&#38469;&#31995;&#32479;&#35774;&#32622;&#12290; &#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;FL&#24037;&#20316;&#37327;&#30340;&#22522;&#20934;&#27979;&#35797;FLEdge&#26469;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#25928;&#29575;&#20197;&#21450;&#21508;&#31181;&#19981;&#21516;&#38544;&#31169;&#32423;&#21035;&#23545;FL&#31995;&#32479;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23458;&#25143;&#31471;&#36864;&#20986;&#23545;&#20855;&#26377;&#39640;&#36798;50&#65285;&#22833;&#25928;&#29575;&#30340;&#26368;&#26032;FL&#31574;&#30053;&#30340;&#24433;&#21709;&#12290; FLEdge&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#20363;&#22914;&#65292;&#22312;&#26087;GPU&#21152;&#36895;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;FL&#24037;&#20316;&#36127;&#36733;&#27604;&#22312;&#29616;&#20195;&#26381;&#21153;&#22120;&#32423;GPU&#19978;&#35757;&#32451;&#39640;&#36798;3&#20493;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Machine Learning (FL) has received considerable attention in recent years. FL benchmarks are predominantly explored in either simulated systems or data center environments, neglecting the setups of real-world systems, which are often closely linked to edge computing. We close this research gap by introducing FLEdge, a benchmark targeting FL workloads in edge computing systems. We systematically study hardware heterogeneity, energy efficiency during training, and the effect of various differential privacy levels on training in FL systems. To make this benchmark applicable to real-world scenarios, we evaluate the impact of client dropouts on state-of-the-art FL strategies with failure rates as high as 50%. FLEdge provides new insights, such as that training state-of-the-art FL workloads on older GPU-accelerated embedded devices is up to 3x more energy efficient than on modern server-grade GPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02709</link><description>&lt;p&gt;
&#29992;&#20110;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on Semi-supervised Learning Applied for Anomaly Detection in Hydraulic Condition Monitoring System. (arXiv:2306.02709v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#32500;&#25252;&#22312;&#28082;&#21387;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#24322;&#24120;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#24456;&#23569;&#65292;&#26631;&#35760;&#36825;&#20123;&#25968;&#25454;&#26159;&#36153;&#26102;&#36153;&#21147;&#29978;&#33267;&#21361;&#38505;&#30340;&#12290;&#22240;&#27492;&#65292;&#24314;&#35758;&#20351;&#29992;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21482;&#26377;&#23569;&#37327;&#26631;&#31614;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#26426;&#21046;&#26469;&#36741;&#21161;&#30417;&#30563;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#22312;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#31995;&#32479;&#20013;&#24212;&#29992;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#24320;&#28304;&#30340;&#28082;&#21387;&#29366;&#24577;&#30417;&#27979;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#29420;&#31435;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#40065;&#26834;&#21327;&#26041;&#24046;&#65289;&#12289;&#38598;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#23396;&#31435;&#26862;&#26519;&#65289;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#65292;&#32780;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Condition-based maintenance is becoming increasingly important in hydraulic systems. However, anomaly detection for these systems remains challenging, especially since that anomalous data is scarce and labeling such data is tedious and even dangerous. Therefore, it is advisable to make use of unsupervised or semi-supervised methods, especially for semi-supervised learning which utilizes unsupervised learning as a feature extraction mechanism to aid the supervised part when only a small number of labels are available. This study systematically compares semi-supervised learning methods applied for anomaly detection in hydraulic condition monitoring systems. Firstly, thorough data analysis and feature learning were carried out to understand the open-sourced hydraulic condition monitoring dataset. Then, various methods were implemented and evaluated including traditional stand-alone semi-supervised learning models (e.g., one-class SVM, Robust Covariance), ensemble models (e.g., Isolation F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#38477;&#32500;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#65292;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#22343;&#20248;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#36866;&#21512;&#20110;&#24182;&#34892;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2305.10356</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Spectral Clustering via Orthogonalization-Free Methods. (arXiv:2305.10356v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#38477;&#32500;&#65292;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#65292;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#22343;&#20248;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#36866;&#21512;&#20110;&#24182;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35889;&#32858;&#31867;&#30340;&#38477;&#32500;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#38656;&#35201;&#26114;&#36149;&#30340;&#29305;&#24449;&#20540;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#26368;&#20248;&#21270;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#28388;&#27874;&#22120;&#24182;&#25552;&#20986;&#20351;&#29992;&#22235;&#31181;&#26080;&#27491;&#20132;&#21270;&#26041;&#27861;&#20316;&#20026;&#35889;&#32858;&#31867;&#20013;&#30340;&#38477;&#32500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#21033;&#29992;&#20219;&#20309;&#27491;&#20132;&#21270;&#26041;&#27861;&#65292;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#19981;&#21487;&#20280;&#32553;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#26500;&#36896;&#20102;&#36275;&#22815;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#26368;&#22810;&#26159;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#29305;&#24449;&#31354;&#38388;&#30340;&#21152;&#26435;&#25913;&#21464;&#12290;&#25105;&#20204;&#22312;&#25968;&#20540;&#19978;&#20551;&#35774;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#21033;&#29992;&#31934;&#30830;&#29305;&#24449;&#20540;&#20294;&#38656;&#35201;&#26114;&#36149;&#29305;&#24449;&#20540;&#20272;&#35745;&#30340;&#29702;&#24819;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#22312;&#32858;&#31867;&#36136;&#37327;&#19978;&#31561;&#25928;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#22522;&#20110;&#24130;&#36845;&#20195;&#30340;&#26041;&#27861;&#21644;&#22270;&#20449;&#21495;&#28388;&#27874;&#22120;&#12290;&#19982;&#22522;&#20110;&#24130;&#36845;&#20195;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Signal Filter used as dimensionality reduction in spectral clustering usually requires expensive eigenvalue estimation. We analyze the filter in an optimization setting and propose to use four orthogonalization-free methods by optimizing objective functions as dimensionality reduction in spectral clustering. The proposed methods do not utilize any orthogonalization, which is known as not well scalable in a parallel computing environment. Our methods theoretically construct adequate feature space, which is, at most, a weighted alteration to the eigenspace of a normalized Laplacian matrix. We numerically hypothesize that the proposed methods are equivalent in clustering quality to the ideal Graph Signal Filter, which exploits the exact eigenvalue needed without expensive eigenvalue estimation. Numerical results show that the proposed methods outperform Power Iteration-based methods and Graph Signal Filter in clustering quality and computation cost. Unlike Power Iteration-based meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.13917</link><description>&lt;p&gt;
&#27604;&#20363;&#20195;&#34920;&#24615;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proportionally Representative Clustering. (arXiv:2304.13917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#26377;&#25928;&#30340;&#31639;&#27861;&#28385;&#36275;&#35813;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#23545;&#20844;&#24179;&#27010;&#24565;&#30340;&#24418;&#24335;&#21270;&#34920;&#36848;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#32858;&#31867;&#65292;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#30784;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#20934;&#21017;&#8212;&#8212;&#27604;&#20363;&#20195;&#34920;&#24615;&#20844;&#24179;&#24615;&#65288;PRF&#65289;&#65292;&#25105;&#20204;&#35748;&#20026;&#35813;&#27010;&#24565;&#20197;&#19968;&#31181;&#26356;&#26377;&#35828;&#26381;&#21147;&#30340;&#26041;&#24335;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#20960;&#20010;&#29616;&#23384;&#27010;&#24565;&#30340;&#29702;&#30001;&#12290;&#20294;&#29616;&#26377;&#30340;&#20844;&#24179;&#32858;&#31867;&#31639;&#27861;&#19981;&#33021;&#28385;&#36275;&#25105;&#20204;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20197;&#28385;&#36275;&#26080;&#32422;&#26463;&#32858;&#31867;&#21644;&#31163;&#25955;&#32858;&#31867;&#38382;&#39064;&#30340;PRF&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in effort to formalize notions of fairness in machine learning. We focus on clustering -- one of the fundamental tasks in unsupervised machine learning. We propose a new axiom that captures proportional representation fairness (PRF). We make a case that the concept achieves the raison d'{\^{e}}tre of several existing concepts in the literature in an arguably more convincing manner. Our fairness concept is not satisfied by existing fair clustering algorithms. We design efficient algorithms to achieve PRF both for unconstrained and discrete clustering problems.
&lt;/p&gt;</description></item><item><title>Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12693</link><description>&lt;p&gt;
Phylo2Vec: &#19968;&#31181;&#20108;&#21449;&#26641;&#30340;&#21521;&#37327;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12693
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#25968;&#25454;&#25512;&#26029;&#24471;&#21040;&#30340;&#20108;&#21449;&#36827;&#21270;&#26641;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#20043;&#38388;&#20849;&#20139;&#30340;&#36827;&#21270;&#21382;&#21490;&#33267;&#20851;&#37325;&#35201;&#12290;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#31561;&#26576;&#20010;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#26029;&#20986;&#26641;&#20013;&#28508;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#26159;NP-hard&#38382;&#39064;&#65292;&#36825;&#25512;&#21160;&#20102;&#22823;&#37327;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#22343;&#21248;&#37319;&#26679;&#38543;&#26426;&#26641;&#25110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#26641;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31561;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phylo2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#36827;&#21270;&#26641;&#12290;Phylo2Vec&#23558;&#20219;&#20309;&#20855;&#26377;n&#20010;&#21494;&#23376;&#30340;&#20108;&#21449;&#26641;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;n&#30340;&#25972;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#22312;&#31354;&#38388;&#20013;&#26082;&#26159;&#33391;&#23450;&#30340;&#21448;&#26159;&#21452;&#23556;&#30340;&#12290;Phylo2Vec&#30340;&#20248;&#28857;&#26159;&#65306;i&#65289;&#36731;&#26494;&#22343;&#21248;&#37319;&#26679;&#20108;&#21449;&#26641;&#65307;ii&#65289;&#20197;&#38750;&#24120;&#22823;&#25110;&#23567;&#30340;&#27493;&#38271;&#31995;&#32479;&#22320;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Phylo2Vec&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
&lt;/p&gt;</description></item><item><title>KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.06827</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06827
&lt;/p&gt;
&lt;p&gt;
KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#34892;&#20026;&#26469;&#25512;&#26029;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#28857;&#20272;&#35745;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#21487;&#33021;&#26377;&#22810;&#20010;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#25551;&#36848;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#27169;&#25311;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;Q&#20540;&#20989;&#25968;&#20195;&#26367;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#21518;&#39564;&#35745;&#31639;&#37327;&#22823;&#65292;&#29702;&#35770;&#20445;&#35777;&#23569;&#65292;&#24182;&#19988;Q&#20540;&#20989;&#25968;&#36890;&#24120;&#23545;&#20284;&#28982;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;KD-BIRL&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#26680;&#23494;&#24230;&#20272;&#35745;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#25913;&#36827;&#30340;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#19979;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2301.05294</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#22797;&#26434;&#21644;&#26080;&#20449;&#21495;&#30340;&#20132;&#21449;&#21475;&#20013;&#23398;&#20064;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;
&lt;/p&gt;
&lt;p&gt;
Learning to Control and Coordinate Mixed Traffic Through Robot Vehicles at Complex and Unsignalized Intersections. (arXiv:2301.05294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#21644;&#21327;&#35843;&#28151;&#21512;&#20132;&#36890;&#65292;&#29305;&#21035;&#26159;&#20154;&#39550;&#39542;&#36710;&#36742;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#22312;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;5%&#30340;&#26426;&#22120;&#20154;&#36710;&#36742;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#21475;&#26159;&#29616;&#20195;&#22823;&#37117;&#24066;&#20132;&#36890;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#20107;&#25925;&#25110;&#32570;&#20047;&#20132;&#36890;&#21327;&#35843;&#26426;&#21046;&#65288;&#22914;&#20132;&#36890;&#20449;&#21495;&#28783;&#65289;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#25104;&#20026;&#20132;&#36890;&#27969;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#36229;&#36234;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#20132;&#21449;&#21475;&#20132;&#36890;&#30340;&#25928;&#29575;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#25511;&#21046;&#21487;&#39044;&#35265;&#30340;&#21253;&#21547;&#20154;&#39550;&#39542;&#36710;&#36742;&#65288;HVs&#65289;&#21644;&#26426;&#22120;&#20154;&#36710;&#36742;&#65288;RVs&#65289;&#30340;&#28151;&#21512;&#20132;&#36890;&#24050;&#32463;&#20986;&#29616;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#38469;&#22797;&#26434;&#20132;&#21449;&#21475;&#30340;&#28151;&#21512;&#20132;&#36890;&#30340;&#25511;&#21046;&#21644;&#21327;&#35843;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#36807;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#20132;&#36890;&#26465;&#20214;&#19979;&#65292;&#20351;&#29992;5%&#30340;RVs&#65292;&#25105;&#20204;&#21487;&#20197;&#38450;&#27490;&#22797;&#26434;&#20132;&#21449;&#21475;&#20869;&#30340;&#25317;&#22581;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intersections are essential road infrastructures for traffic in modern metropolises. However, they can also be the bottleneck of traffic flows as a result of traffic incidents or the absence of traffic coordination mechanisms such as traffic lights. Recently, various control and coordination mechanisms that are beyond traditional control methods have been proposed to improve the efficiency of intersection traffic. Amongst these methods, the control of foreseeable mixed traffic that consists of human-driven vehicles (HVs) and robot vehicles (RVs) has emerged. In this project, we propose a decentralized multi-agent reinforcement learning approach for the control and coordination of mixed traffic at real-world, complex intersections--a topic that has not been previously explored. Comprehensive experiments are conducted to show the effectiveness of our approach. In particular, we show that using 5% RVs, we can prevent congestion formation inside a complex intersection under the actual traf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.04883</link><description>&lt;p&gt;
&#31070;&#32463;&#20250;&#21512;&#65306;&#38754;&#21521;&#26143;&#38469;&#29289;&#20307;&#30340;&#21487;&#38752;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20250;&#21512;&#65292;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#22320;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#26143;&#38469;&#29289;&#20307;&#12290;&#23427;&#36890;&#36807;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#21644;&#35889;&#24402;&#19968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24341;&#23548;&#31574;&#30053;&#26469;&#25552;&#20379;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26143;&#38469;&#29289;&#20307;&#65288;ISOs&#65289;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#26367;&#20195;&#30340;&#21407;&#22987;&#26448;&#26009;&#65292;&#22312;&#29702;&#35299;&#31995;&#22806;&#34892;&#26143;&#26143;&#31995;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#36816;&#34892;&#36712;&#36947;&#38590;&#20197;&#32422;&#26463;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#39640;&#30340;&#20542;&#35282;&#21644;&#30456;&#23545;&#36895;&#24230;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20154;&#22312;&#29615;&#36335;&#26041;&#27861;&#25506;&#32034;ISOs&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#20250;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#23548;&#33322;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23454;&#26102;&#20013;&#20197;&#21487;&#38752;&#12289;&#20934;&#30830;&#21644;&#33258;&#20027;&#30340;&#26041;&#24335;&#36973;&#36935;&#24555;&#36895;&#31227;&#21160;&#30340;&#29289;&#20307;&#65292;&#21253;&#25324;ISOs&#12290;&#23427;&#22312;&#22522;&#20110;&#35889;&#24402;&#19968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24341;&#23548;&#31574;&#30053;&#20043;&#19978;&#20351;&#29992;&#28857;&#26368;&#23567;&#33539;&#25968;&#36861;&#36394;&#25511;&#21046;&#65292;&#20854;&#20013;&#21442;&#25968;&#36890;&#36807;&#30452;&#25509;&#24809;&#32602;MPC&#29366;&#24577;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#20250;&#21512;&#22312;&#39044;&#26399;&#30340;&#39134;&#34892;&#22120;&#20132;&#20184;&#35823;&#24046;&#19978;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#25351;&#25968;&#19978;&#30028;&#65292;&#20854;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#36882;&#22686;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous, a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.11660</link><description>&lt;p&gt;
&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Black-box NLP Classifier Attacker. (arXiv:2112.11660v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#40657;&#30418;NLP&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#36873;&#25321;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#65292;&#35299;&#20915;&#20102;&#24433;&#21709;NLP&#39046;&#22495;&#20256;&#32479;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#19981;&#36866;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20026;&#20363;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#34987;&#19968;&#20010;&#19982;&#21407;&#22987;&#25991;&#26412;&#39640;&#24230;&#30456;&#20284;&#30340;&#12289;&#32463;&#36807;&#20180;&#32454;&#20462;&#25913;&#30340;&#25991;&#26412;&#25152;&#36855;&#24785;&#12290;&#26681;&#25454;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22270;&#20687;&#39046;&#22495;&#65307;&#19982;&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#19981;&#21516;&#65292;&#25991;&#26412;&#20197;&#31163;&#25955;&#24207;&#21015;&#34920;&#31034;&#65292;&#20256;&#32479;&#30340;&#22270;&#20687;&#25915;&#20987;&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#19981;&#36866;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#35789;&#32423;NLP&#24773;&#24863;&#20998;&#31867;&#22120;&#25915;&#20987;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#35789;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#36138;&#23146;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#35789;&#26367;&#25442;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;...
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experimen
&lt;/p&gt;</description></item></channel></rss>