<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04783</link><description>&lt;p&gt;
AutoDefense: &#22810;Agent LLM &#38450;&#24481;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;LLMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#36947;&#24503;&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20197;&#38450;&#27490;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21709;&#24212;&#36807;&#28388;&#30340;&#22810;Agent&#38450;&#24481;&#26694;&#26550;AutoDefense&#65292;&#29992;&#20110;&#20174;LLMs&#20013;&#36807;&#28388;&#26377;&#23475;&#22238;&#22797;&#12290; &#27492;&#26694;&#26550;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#19981;&#21516;&#35282;&#33394;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#20849;&#21516;&#23436;&#25104;&#38450;&#24481;&#20219;&#21153;&#12290; &#20219;&#21153;&#30340;&#21010;&#20998;&#22686;&#24378;&#20102;LLMs&#30340;&#25972;&#20307;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#65292;&#24182;&#20351;&#20854;&#20182;&#38450;&#24481;&#32452;&#20214;&#20316;&#20026;&#24037;&#20855;&#38598;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290; AutoDefense &#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#35268;&#27169;&#21644;&#31181;&#31867;&#30340;&#24320;&#28304;LLMs&#20316;&#20026;&#20195;&#29702;&#12290; &#36890;&#36807;&#23545;&#22823;&#37327;&#26377;&#23475;&#21644;&#23433;&#20840;&#25552;&#31034;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;AutoDefense&#22312;&#25552;&#39640;&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20102;&#27491;&#24120;&#29992;&#25143;&#35831;&#27714;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04783v1 Announce Type: cross  Abstract: Despite extensive pre-training and fine-tuning in moral alignment to prevent generating harmful information at user request, large language models (LLMs) remain vulnerable to jailbreak attacks. In this paper, we propose AutoDefense, a response-filtering based multi-agent defense framework that filters harmful responses from LLMs. This framework assigns different roles to LLM agents and employs them to complete the defense task collaboratively. The division in tasks enhances the overall instruction-following of LLMs and enables the integration of other defense components as tools. AutoDefense can adapt to various sizes and kinds of open-source LLMs that serve as agents. Through conducting extensive experiments on a large scale of harmful and safe prompts, we validate the effectiveness of the proposed AutoDefense in improving the robustness against jailbreak attacks, while maintaining the performance at normal user request. Our code and 
&lt;/p&gt;</description></item><item><title>IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03227</link><description>&lt;p&gt;
IGUANe: &#19968;&#31181;&#36866;&#29992;&#20110;&#33041;MR&#22270;&#20687;&#22810;&#20013;&#24515;&#21327;&#35843;&#30340;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of brain MR images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03227
&lt;/p&gt;
&lt;p&gt;
IGUANe&#26159;&#19968;&#31181;&#19977;&#32500;&#36890;&#29992;CycleGAN&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#22495;&#30340;&#35757;&#32451;&#23454;&#29616;&#20102;&#33041;MR&#22270;&#20687;&#30340;&#22810;&#20013;&#24515;&#21327;&#35843;&#65292;&#20351;&#20854;&#25104;&#20026;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;MRI&#30740;&#31350;&#20013;&#65292;&#26469;&#33258;&#22810;&#20010;&#37319;&#38598;&#28857;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#32858;&#21512;&#21487;&#20197;&#22686;&#21152;&#26679;&#26412;&#22823;&#23567;&#65292;&#20294;&#21487;&#33021;&#24341;&#20837;&#38459;&#30861;&#21518;&#32493;&#20998;&#26512;&#19968;&#33268;&#24615;&#30340;&#19982;&#37319;&#38598;&#28857;&#30456;&#20851;&#30340;&#21464;&#24322;&#12290;&#22270;&#20687;&#32763;&#35793;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#21327;&#35843;MR&#22270;&#20687;&#36328;&#31449;&#28857;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;IGUANe&#65288;&#20855;&#26377;&#32479;&#19968;&#23545;&#25239;&#32593;&#32476;&#30340;&#22270;&#20687;&#29983;&#25104;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21407;&#22987;&#30340;&#19977;&#32500;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#22495;&#36716;&#25442;&#30340;&#20248;&#21183;&#21644;&#30452;&#25509;&#24212;&#29992;&#26679;&#24335;&#36716;&#31227;&#26041;&#27861;&#26469;&#23454;&#29616;&#22810;&#20013;&#24515;&#33041;MR&#22270;&#20687;&#21327;&#35843;&#12290;IGUANe&#36890;&#36807;&#22810;&#23545;&#19968;&#31574;&#30053;&#65292;&#38598;&#25104;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;CycleGAN&#26550;&#26500;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22270;&#20687;&#65292;&#29978;&#33267;&#26469;&#33258;&#26410;&#30693;&#37319;&#38598;&#28857;&#65292;&#20351;&#20854;&#25104;&#20026;&#21327;&#35843;&#30340;&#36890;&#29992;&#29983;&#25104;&#22120;&#12290;&#22312;&#30001;11&#21488;&#19981;&#21516;&#25195;&#25551;&#20202;&#30340;T1&#21152;&#26435;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;IGUANe&#22312;&#26410;&#35265;&#31449;&#28857;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In MRI studies, the aggregation of imaging data from multiple acquisition sites enhances sample size but may introduce site-related variabilities that hinder consistency in subsequent analyses. Deep learning methods for image translation have emerged as a solution for harmonizing MR images across sites. In this study, we introduce IGUANe (Image Generation with Unified Adversarial Networks), an original 3D model that leverages the strengths of domain translation and straightforward application of style transfer methods for multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture by integrating an arbitrary number of domains for training through a many-to-one strategy. During inference, the model can be applied to any image, even from an unknown acquisition site, making it a universal generator for harmonization. Trained on a dataset comprising T1-weighted images from 11 different scanners, IGUANe was evaluated on data from unseen sites. The assessments included the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03017</link><description>&lt;p&gt;
&#21521;&#32511;&#33394;&#19988;&#31867;&#20154;&#30340;&#20154;&#24037;&#26234;&#33021;&#36808;&#36827;&#65306;&#24403;&#20195;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#30340;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Toward Green and Human-Like Artificial Intelligence: A Complete Survey on Contemporary Few-Shot Learning Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#38480;&#21046;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#23545;&#25968;&#25454;&#30340;&#38656;&#27714;&#21644;&#35745;&#31639;&#30340;&#26114;&#36149;&#24615;&#20351;&#20854;&#22312;&#35768;&#22810;&#25968;&#25454;&#21463;&#38480;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#19981;&#23454;&#29992;&#12290;&#23569;&#26679;&#26412;&#23398;&#20064;&#65288;FSL&#65289;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#23545;&#26032;&#23398;&#20064;&#20219;&#21153;&#30340;&#24555;&#36895;&#36866;&#24212;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#26368;&#26032;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#27491;&#24335;&#23450;&#20041;&#20102;FSL&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#19982;&#19981;&#21516;&#23398;&#20064;&#39046;&#22495;&#30340;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#25193;&#23637;&#20102;&#20197;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#32463;&#20856;&#21644;&#26032;&#39046;&#22495;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#22609;&#36896;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36235;&#21183;&#12289;&#31361;&#20986;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning's widespread success, its data-hungry and computationally expensive nature makes it impractical for many data-constrained real-world applications. Few-Shot Learning (FSL) aims to address these limitations by enabling rapid adaptation to novel learning tasks, seeing significant growth in recent years. This survey provides a comprehensive overview of the field's latest advancements. Initially, FSL is formally defined, and its relationship with different learning fields is presented. A novel taxonomy is introduced, extending previously proposed ones, and real-world applications in classic and novel fields are described. Finally, recent trends shaping the field, outstanding challenges, and promising future research directions are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02930</link><description>&lt;p&gt;
&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#31163;&#25955;&#22522;&#22240;&#35757;&#32451;&#20013;&#20197;&#29992;&#20110;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Embedding Hardware Approximations in Discrete Genetic-based Training for Printed MLPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#31163;&#25955;&#36951;&#20256;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30828;&#20214;&#36817;&#20284;&#30340;&#25928;&#30410;&#65292;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#38754;&#31215;&#21644;&#21151;&#32791;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#26159;&#19968;&#31181;&#26377;&#30528;&#20302;&#25104;&#26412;&#21644;&#28789;&#27963;&#21046;&#36896;&#31561;&#29420;&#29305;&#29305;&#28857;&#30340;&#26377;&#26395;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#30340;&#30789;&#22522;&#25216;&#26415;&#19981;&#21516;&#65292;&#21360;&#21047;&#30005;&#23376;&#21487;&#20197;&#23454;&#29616;&#21487;&#20280;&#32553;&#12289;&#21487;&#36866;&#24212;&#12289;&#38750;&#27602;&#24615;&#30340;&#30828;&#20214;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21360;&#21047;&#30005;&#23376;&#30340;&#29305;&#24615;&#23610;&#23544;&#36739;&#22823;&#65292;&#35201;&#23454;&#29616;&#22797;&#26434;&#30340;&#30005;&#36335;&#22914;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#20284;&#35745;&#31639;&#34987;&#35777;&#26126;&#21487;&#20197;&#38477;&#20302;&#26426;&#22120;&#23398;&#20064;&#30005;&#36335;&#65288;&#22914;&#22810;&#23618;&#24863;&#30693;&#22120;&#65289;&#30340;&#30828;&#20214;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30828;&#20214;&#36817;&#20284;&#23884;&#20837;&#21040;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26469;&#26368;&#22823;&#21270;&#36817;&#20284;&#35745;&#31639;&#30340;&#30410;&#22788;&#12290;&#30001;&#20110;&#30828;&#20214;&#36817;&#20284;&#30340;&#31163;&#25955;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#30828;&#20214;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#65292;&#19987;&#38376;&#20026;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#22120;&#35774;&#35745;&#12290;&#22312;5%&#30340;&#31934;&#24230;&#25439;&#22833;&#19979;&#65292;&#30456;&#27604;&#22522;&#32447;&#65292;&#25105;&#20204;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#22312;&#38754;&#31215;&#21644;&#21151;&#32791;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;5&#20493;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed Electronics (PE) stands out as a promisingtechnology for widespread computing due to its distinct attributes, such as low costs and flexible manufacturing. Unlike traditional silicon-based technologies, PE enables stretchable, conformal,and non-toxic hardware. However, PE are constrained by larger feature sizes, making it challenging to implement complex circuits such as machine learning (ML) classifiers. Approximate computing has been proven to reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs). In this paper, we maximize the benefits of approximate computing by integrating hardware approximation into the MLP training process. Due to the discrete nature of hardware approximation, we propose and implement a genetic-based, approximate, hardware-aware training approach specifically designed for printed MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction compared to the baseline while outperforming state of-the-art approximate
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02681</link><description>&lt;p&gt;
&#31561;&#21464;&#23545;&#31216;&#30772;&#32570;&#38598;
&lt;/p&gt;
&lt;p&gt;
Equivariant Symmetry Breaking Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02681
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#31216;&#30772;&#32570;&#38598;&#26469;&#30772;&#22351;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#29992;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#28508;&#22312;&#23545;&#31216;&#24615;&#30340;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#35774;&#35745;&#65292;ENN&#22312;&#32473;&#23450;&#26356;&#39640;&#23545;&#31216;&#24615;&#36755;&#20837;&#26102;&#26080;&#27861;&#20135;&#29983;&#36739;&#20302;&#23545;&#31216;&#24615;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29289;&#29702;&#31995;&#32479;&#20013;&#20250;&#21457;&#29983;&#33258;&#21457;&#23545;&#31216;&#30772;&#32570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#19968;&#20010;&#21021;&#22987;&#39640;&#24230;&#23545;&#31216;&#30340;&#29366;&#24577;&#33719;&#24471;&#19968;&#20010;&#36739;&#19981;&#23545;&#31216;&#30340;&#31283;&#23450;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#22914;&#20309;&#31995;&#32479;&#22320;&#22312;ENN&#20013;&#30772;&#22351;&#23545;&#31216;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#31561;&#21464;&#30340;&#26032;&#22411;&#23545;&#31216;&#30772;&#32570;&#26694;&#26550;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#20219;&#20309;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#31216;&#30772;&#32570;&#38598;&#65288;SBS&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#26159;&#37325;&#26032;&#35774;&#35745;&#29616;&#26377;&#30340;&#32593;&#32476;&#65292;&#32780;&#26159;&#35774;&#35745;&#20102;&#19968;&#32452;&#23545;&#31216;&#30772;&#32570;&#23545;&#35937;&#65292;&#26681;&#25454;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23545;&#31216;&#24615;&#23558;&#20854;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#23450;&#20041;&#31561;&#21464;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;... (the abstract is incomplete and cut off)
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks (ENNs) have been shown to be extremely effective in applications involving underlying symmetries. By construction ENNs cannot produce lower symmetry outputs given a higher symmetry input. However, spontaneous symmetry breaking occurs in many physical systems and we may obtain a less symmetric stable state from an initial highly symmetric one. Hence, it is imperative that we understand how to systematically break symmetry in ENNs. In this work, we propose a novel symmetry breaking framework that is fully equivariant. We emphasize that our approach is general and applicable to equivariance under any group. To achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather than redesign existing networks, we design sets of symmetry breaking objects which we feed into our network based on the symmetry of our inputs and outputs. We show there is a natural way to define equivariance on these sets, which gives an additional constraint. Minimizing the si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;Vim&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#20301;&#32622;&#23884;&#20837;&#26469;&#39640;&#25928;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#22914;DeiT&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2401.09417</link><description>&lt;p&gt;
Vision Mamba: &#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#39640;&#25928;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;Vim&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#20301;&#32622;&#23884;&#20837;&#26469;&#39640;&#25928;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#22914;DeiT&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#35774;&#35745;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#21363;Mamba&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;SSMs&#19978;&#26500;&#24314;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;&#20063;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#20301;&#32622;&#25935;&#24863;&#24615;&#21644;&#23545;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#65292;&#23545;&#20110;SSMs&#26469;&#35828;&#65292;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26469;&#35828;&#65292;&#20381;&#36182;&#33258;&#27880;&#24847;&#21147;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;&#65292;&#21363;&#24102;&#26377;&#21452;&#21521;Mamba&#22359;&#65288;Vim&#65289;&#65292;&#23427;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#26631;&#35760;&#22270;&#20687;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21387;&#32553;&#35270;&#35273;&#34920;&#31034;&#12290;&#22312;ImageNet&#20998;&#31867;&#12289;COCO&#30446;&#26631;&#26816;&#27979;&#21644;ADE20k&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;Vim&#30456;&#27604;&#20110;DeiT&#31561;&#32463;&#36807;&#33391;&#22909;&#39564;&#35777;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.17612</link><description>&lt;p&gt;
&#38754;&#21521;&#21360;&#21047;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#23450;&#21046;&#36817;&#20284;&#20056;&#31215;&#32047;&#21152;&#21644;&#28608;&#27963;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Bespoke Approximation of Multiplication-Accumulation and Activation Targeting Printed Multilayer Perceptrons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#23454;&#29616;&#30495;&#27491;&#26080;&#22788;&#19981;&#22312;&#35745;&#31639;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#36229;&#20302;&#21151;&#32791;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#36817;&#20284;&#35745;&#31639;&#21644;&#23450;&#21046;&#21270;&#35774;&#35745;&#30340;&#21407;&#21017;&#26469;&#20811;&#26381;&#21360;&#21047;&#30005;&#23376;&#25216;&#26415;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Printed Electronics (PE) feature distinct and remarkable characteristics that make them a prominent technology for achieving true ubiquitous computing. This is particularly relevant in application domains that require conformal and ultra-low cost solutions, which have experienced limited penetration of computing until now. Unlike silicon-based technologies, PE offer unparalleled features such as non-recurring engineering costs, ultra-low manufacturing cost, and on-demand fabrication of conformal, flexible, non-toxic, and stretchable hardware. However, PE face certain limitations due to their large feature sizes, that impede the realization of complex circuits, such as machine learning classifiers. In this work, we address these limitations by leveraging the principles of Approximate Computing and Bespoke (fully-customized) design. We propose an automated framework for designing ultra-low power Multilayer Perceptron (MLP) classifiers which employs, for the first time, a holistic approac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.17196</link><description>&lt;p&gt;
ResBit: &#22522;&#20110;&#27531;&#24046;&#20301;&#21521;&#37327;&#30340;&#31163;&#25955;&#20540;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResBit&#30340;&#27531;&#24046;&#20301;&#21521;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#32500;&#24230;&#22686;&#21152;&#21644;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#31867;&#21035;&#20540;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#29420;&#28909;&#32534;&#30721;&#21521;&#37327;&#19968;&#30452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#32500;&#24230;&#38543;&#30528;&#35201;&#34920;&#31034;&#30340;&#31163;&#25955;&#25968;&#25454;&#32447;&#24615;&#22686;&#21152;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#35270;&#20026;&#31354;&#38388;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38382;&#39064;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20301;&#24207;&#21015;&#34920;&#31034;&#31163;&#25955;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21363;Analog Bits&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#35201;&#34920;&#31034;&#30340;&#31867;&#21035;&#31867;&#22411;&#25968;&#37327;&#19981;&#19968;&#23450;&#26159;2&#30340;&#24130;&#27425;&#65292;&#23548;&#33268;Analog Bits&#33021;&#22815;&#34920;&#31034;&#30340;&#33539;&#22260;&#19982;&#31867;&#21035;&#25968;&#25454;&#30340;&#33539;&#22260;&#23384;&#22312;&#24046;&#24322;&#12290;&#22914;&#26524;&#29983;&#25104;&#20102;&#36825;&#26679;&#30340;&#20540;&#65292;&#38382;&#39064;&#23601;&#26159;&#26080;&#27861;&#24674;&#22797;&#21407;&#22987;&#30340;&#31867;&#21035;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#20301;&#21521;&#37327;&#65288;ResBit&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618;&#30340;&#20301;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;</title><link>http://arxiv.org/abs/2301.13306</link><description>&lt;p&gt;
&#24102;&#26377;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#30340;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#65306;&#25928;&#29575;&#12289;&#21518;&#24724;&#21644;&#33410;&#22863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Autobidders with Budget and ROI Constraints: Efficiency, Regret, and Pacing Dynamics. (arXiv:2301.13306v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#31181;&#25293;&#21334;&#26041;&#24335;&#19979;&#28385;&#36275;&#39044;&#31639;&#21644;ROI&#32422;&#26463;&#65292;&#24182;&#36798;&#21040;&#20010;&#20307;&#21518;&#24724;&#36880;&#28176;&#20943;&#23567;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21508;&#33258;&#31454;&#20105;&#26102;&#65292;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160;&#33267;&#23569;&#36798;&#21040;&#26368;&#20248;&#20998;&#37197;&#30340;&#26399;&#26395;&#27969;&#21160;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#22312;&#22312;&#32447;&#24191;&#21578;&#24179;&#21488;&#19978;&#36827;&#34892;&#21338;&#24328;&#30340;&#24773;&#20917;&#12290;&#27599;&#20010;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#34987;&#36171;&#20104;&#20219;&#21153;&#65292;&#22312;&#22810;&#36718;&#37325;&#22797;&#25293;&#21334;&#20013;&#65292;&#26368;&#22823;&#21270;&#20854;&#24191;&#21578;&#20027;&#30340;&#24635;&#20215;&#20540;&#65292;&#21516;&#26102;&#21463;&#21040;&#39044;&#31639;&#21644;/&#25110;&#25237;&#36164;&#22238;&#25253;&#29575;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#20445;&#35777;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#36798;&#21040;&#36880;&#28176;&#20943;&#23567;&#30340;&#20010;&#20307;&#21518;&#24724;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20165;&#20351;&#29992;&#33258;&#21161;&#21453;&#39304;&#65292;&#24182;&#21487;&#19982;&#31532;&#19968;&#25110;&#31532;&#20108;&#20215;&#26684;&#25293;&#21334;&#20197;&#21450;&#20219;&#20309;&#8220;&#20013;&#38388;&#8221;&#25293;&#21334;&#26684;&#24335;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24403;&#36825;&#20123;&#33258;&#21160;&#20986;&#20215;&#31639;&#27861;&#30456;&#20114;&#31454;&#20105;&#26102;&#65292;&#25152;&#26377;&#36718;&#27425;&#30340;&#26399;&#26395;&#36164;&#37329;&#27969;&#21160; welfare &#37117;&#33267;&#23569;&#36798;&#21040;&#20102;&#20219;&#20309;&#20998;&#37197;&#25152;&#23454;&#29616;&#30340;&#26399;&#26395;&#26368;&#20248;&#27969;&#21160; welfare &#30340;&#19968;&#21322;&#12290;&#36825;&#22312;&#20986;&#20215;&#21160;&#24577;&#26159;&#21542;&#25910;&#25947;&#21040;&#22343;&#34913;&#20197;&#21450;&#24191;&#21578;&#20027;&#20272;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#22914;&#20309;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#22343;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a game between autobidding algorithms that compete in an online advertising platform. Each autobidder is tasked with maximizing its advertiser's total value over multiple rounds of a repeated auction, subject to budget and/or return-on-investment constraints. We propose a gradient-based learning algorithm that is guaranteed to satisfy all constraints and achieves vanishing individual regret. Our algorithm uses only bandit feedback and can be used with the first- or second-price auction, as well as with any "intermediate" auction format. Our main result is that when these autobidders play against each other, the resulting expected liquid welfare over all rounds is at least half of the expected optimal liquid welfare achieved by any allocation. This holds whether or not the bidding dynamics converges to an equilibrium and regardless of the correlation structure between advertiser valuations.
&lt;/p&gt;</description></item></channel></rss>